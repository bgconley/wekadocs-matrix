# WEKA Documentation - Complete Collection

This is the complete combined documentation for WEKA.
Total files: 259

---



<!-- ============================================ -->
<!-- File 1/259: planning-and-installation.md -->
<!-- ============================================ -->

# Planning and Installation

## Topics in this section

### Prerequisites and compatibility

This page describes the prerequisites and compatibility for the installation of the WEKA system.

### WEKA cluster installation on bare metal servers

This topic provides an overview of the automated tools and workflow paths for installing and configuring the WEKA software on a group of bare metal servers (on-premises environment).

### WEKA Cloud Deployment Manager Web (CDM Web) User Guide

### WEKA Cloud Deployment Manager Local (CDM Local) User Guide

### WEKA installation on AWS

This section provides detailed instructions on installing a WEKA system on AWS.

### WEKA installation on Azure

This section aims at a system engineer familiar with the Azure fundamentals and experienced in using Terraform to deploy a system on Azure.

### WEKA installation on GCP

This section aims at a system engineer familiar with the GCP concepts and experienced in using Terraform to deploy a system on GCP.

### WEKA installation on OCI

WEKA Data Platform deployment on Oracle Cloud Infrastructure (OCI) follows bare-metal installation with cloud-specific considerations.

<!-- ============================================ -->
<!-- File 2/259: planning-and-installation_prerequisites-and-compatibility.md -->
<!-- ============================================ -->

---
description:
---

# Prerequisites and compatibility

Note: **Important:** The versions mentioned on the prerequisites and compatibility page apply to the WEKA system's **latest minor version** (4.4.**X**). For information on new features and supported prerequisites released with each minor version, refer to the relevant release notes available at get.weka.io.
Check the release notes for details about any updates or changes accompanying the latest releases.

Note: In certain instances, WEKA collaborates with Strategic Server Partners to conduct platform qualifications alongside complementary components. If you have any inquiries, contact your designated WEKA representative.

## Minimal server configuration for a WEKA cluster

The minimal configuration for a new WEKA cluster installation is **8 servers**. This ensures optimal performance, resilience, and scalability for most deployments.

Note: For cloud-based installations, WEKA supports a minimal configuration of **6 servers** to accommodate the unique requirements of cloud environments.

## CPU

 | CPU family/architecture | Supported on backends | Supported on clients |
 | --- | --- | --- |
 | 2013 Intel¬Æ Core‚Ñ¢ processor family and later | üëçDual-socket | üëçDual-socket |
 | AMD EPYC‚Ñ¢ processor families 2nd (Rome), 3rd (Milan-X), and 4th (Genoa) Generations | üëçSingle-socket | üëç Single-socket and dual-socket |
 | Aarch64 |  | üëçNvidia Grace |

Note: The following requirements must be met:
* AES is enabled.
* Secure Boot is disabled.
* AVX2 is enabled.

## Memory

* Sufficient memory to support the WEKA system needs as described in [memory requirements](../bare-metal/planning-a-weka-system-installation#memory-resource-planning).
* More memory support for the OS kernel or any other application.

## Operating system

Note: WEKA will support upcoming releases of the operating systems in the lists within one quarter (three months) of their respective General Availability (GA) dates.

* **Rocky Linux:**
  * 9.4, 9.3, 9.2, 9.1, 9.0
  * 8.10, 8.9, 8.8, 8.7, 8.6
* **RHEL:**
  * 9.4, 9.3, 9.2, 9.1, 9.0
  * 8.10, 8.9, 8.8, 8.7, 8.6, 8.5, 8.4, 8.3, 8.2, 8.1, 8.0
* **CentOS:**
  * 8.5, 8.4, 8.3, 8.2, 8.1, 8.0
* **Ubuntu:**
  * 24.04
  * 22.04
  * 20.04
  * 18.04
* **Amazon Linux:**
  * AMI 2018.03
  * AMI 2017.09
* **Amazon Linux 2 LTS** (formerly Amazon Linux 2 LTS 17.12)
  * Latest update package that was tested: 5.10.176-157.645.amzn2.x86_64

* **Rocky Linux:**
  * 9.5, 9.4, 9.3, 9.2, 9.1, 9.0
  * 8.10, 8.9, 8.8, 8.7, 8.6
* **RHEL:**
  * 9.4, 9.3, 9.2, 9.1, 9.0
  * 8.10, 8.9, 8.8, 8.7, 8.6, 8.5, 8.4, 8.3, 8.2, 8.1, 8.0
* **CentOS:**
  * 8.5, 8.4, 8.3, 8.2, 8.1, 8.0
* **Ubuntu:**
  * 24.04
  * 22.04
  * 20.04
  * 18.04
* **Amazon Linux:**
  * AMI 2018.03
  * AMI 2017.09
* **Amazon Linux 2 LTS** (formerly Amazon Linux 2 LTS 17.12)
  * Latest update package that was tested: 5.10.176-157.645.amzn2.x86_64
* **SLES:**
  * 15 DP6
  * 15 SP5
  * 15 SP4
  * 15 SP2
  * 12 SP5
* **Oracle Linux:**
  * 9
  * 8.9
* **Debian:**
  * 12 (with Linux kernel 6.6)
  * 10
* **AlmaLinux OS:**
  * 9.4
  * 8.10
* **Proxmox Virtual Environment**:
  * 8.2
  * 8.14

The following kernel versions are supported:

* 6.8
* 6.0 to 6.5
* 5.3 to 5.19
* 4.4.0-1106 to 4.19
* 3.10

Note: - Kernels 5.15 and higher are not supported with Amazon Linux operating systems.
- It is recommended to turn off auto kernel updates, so it will not get upgraded to an unsupported version.
- Confirm that both the kernel version and the operating system version are listed as supported, as these are distinct components with their own compatibility considerations.
- For clarity, the range of supported versions is inclusive.

#### General

* All WEKA servers must be synchronized in date/time (NTP recommended)
* A watchdog driver should be installed in /dev/watchdog (hardware watchdog recommended); search the WEKA knowledge base in the WEKA support portal for more information and how-to articles.
* If using `mlocate` or alike, it's advisable to exclude `wekafs` from `updatedb` filesystems lists; search the WEKA knowledge base in the WEKA support portal for more information and how-to articles.

#### SELinux

* SELinux is supported in both `permissive` and `enforcing` modes.
  * `The targeted` policy is supported.
  * The `mls` policy is not supported yet.

Note: - To set the SELinux security context for files,  use the `-o acl` in the mount command, and define the `wekafs` to use extended attributes in the SELinux policy configuration (`fs_use_xattr`).
- The maximum size for the Extended Attributes (xattr) is limited to 1024. This attribute is crucial in supporting Access Control Lists (ACL) and Alternate Data Streams (ADS) in SMB. Given its finite capacity, exercise caution when using ACLs and ADS on a filesystem using SELinux.

#### cgroups

* WEKA backends and clients that serve protocols must be deployed on a supported OS with **cgroupsV1**.
* **cgroupsV2** is supported on backends and clients, but not in deployments with protocol clusters.

Note: As of version 4.3.2, RHEL 7.X and CentOS 7.X are no longer supported due to their end-of-life status. If you need assistance upgrading your operating system, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team) for guidance.

## WEKA installation directory

* **WEKA installation directory**:
  * The WEKA installation directory must be set to `/opt/weka`.
  * Use a direct path; symbolic links (symlinks) are not supported.
  * The `/opt/weka` directory is critical for proper WEKA operation. If it resides on shared storage, the storage must be highly available.
* **Boot drive minimum requirements**:
  * Capacity: NVMe SSD with 960 GB capacity
  * Durability: 1 DWPD (Drive Writes Per Day)
  * Write throughput: 1 GB/s
* **Boot drive considerations**:
  * Do not share the boot drive.
  * Do not mount using NFS.
  * Do not use a RAM drive remotely.
  * If two boot drives are available:
    * It is recommended to dedicate one boot drive for the OS and the other for the `/opt/weka` directory.
    * Do not use software RAID to have two boot drives.
* **Software required space**:
  * Ensure that at least 26 GB is available for the WEKA system installation.
  * Allocate an additional 10 GB per core used by WEKA.
* **Filesystem requirement**:
  * Set a separate filesystem on a separate partition for `/opt/weka`.

## Networking

Adhere to the following considerations when choosing the adapters:

* **LACP****:**  LACP is supported when bonding ports from dual-port Mellanox NICs into a single Mellanox device but is not compatible when using Virtual Functions (VFs).
* **MTU**\
  It is recommended to set the MTU to at least 4k on the NICs of WEKA cluster servers and the connected switches.
* **Jumbo Frames**\
  If any network connection, irrespective of whether it‚Äôs InfiniBand or Ethernet, on a given backend possess the capability to transmit frames exceeding 4 KB in size, it is mandatory for all network connections used directly by WEKA on that same backend to have the ability to transmit frames of at least 4 KB.
* **IOMMU** **support**\
  WEKA automatically detects and enables IOMMU for the server and PCI devices. Manual enablement is not required.

Note: When the Linux operating system is configured with `iommu=1`, IOMMU is enabled system-wide, and all PCI devices operate under IOMMU control. It is not possible to selectively exclude specific PCI devices from IOMMU when this mode is active.

* **Shared networking**\
  Shared networking (also known as single IP) allows a single IP address to be assigned to the Physical Function (PF) and shared across multiple Virtual Functions (VFs). This means that a single IP can be shared by every WEKA process on that server, while still being available to the host operating system.
*   **SR-IOV VF**

    Single Root I/O Virtualization Virtual Functions enable direct hardware access for virtual machines, improving network performance by reducing CPU overhead.

Note: Shared networking configuration for NIC models:
* NVIDIA NICs: When implementing Shared Networking (Single IP), Virtual Functions (VFs) are not required.
* Broadcom NICs: VFs must be configured when deploying Shared Networking architecture.

*   **Mixed networks**

    A mixed network configuration refers to a setup where a WEKA cluster connects to both InfiniBand and Ethernet networks.

    Certain features and configurations are not supported in mixed network setups. Review the following limitations and supported settings:

    * **Non-supported features in mixed networks:**
      * RDMA
      * VLAN
      * IPv6
    * **Supported MTU settings in mixed networks:**
      * Ethernet (9000) + InfiniBand (4K)
    * **Non-supported MTU settings in mixed networks:**
      * Ethernet (1500) + InfiniBand (4K)
      * Ethernet (9000) + InfiniBand (2K)
*   **Routed network**

    Enables communication between subnets using Layer 3 routing, allowing WEKA clusters to span multiple network segments.
*   **HA (High Availability)**

    Ensures system uptime through redundant components and automatic failover.
*   **RX Interrupts**

    Receive interrupts that notify the CPU when network packets arrive, critical for optimizing network processing performance.
* **IP addressing for dataplane NICs**\
  Exclusively use static IP addressing. DHCP is not supported for dataplane NICs.
*   **WEKA peer connectivity requires NAT-free networking**

    WEKA requires visibility and connectivity to all peers, without interference from networking technologies like network address translation, or NAT.

**Related topics**

### Supported network adapters <a href="#networking-ethernet" id="networking-ethernet"></a>

The following table provides the supported network adapters along with their supported features for backends and clients, and clients-only.

#### Supported network adapters for backends and clients

 | Adapter | Protocol | Supported features |
 | --- | --- | --- |
 | Amazon ENA | Ethernet | SR-IOV VF |
 | Broadcom BCM957508-P2100GDual-port (2x100Gb/s)Single-port (1x200Gb/s | Ethernet | Shared networkingSR-IOV VFHARouted network |
 | Broadcom BCM957608-P2200GDual-port (2x200Gb/s)Single-port (1x400Gb/s | Ethernet | Shared networkingSR-IOV VFHARouted network |
 | NVIDIA Mellanox CX-7 single-port | InfiniBand | Shared networkingRX interruptsRDMAHAPKEYIOMMU |
 | NVIDIA Mellanox CX-7 dual-port | InfiniBand | Shared networkingRX interruptsRDMAHAPKEYIOMMU |
 | NVIDIA Mellanox CX-7-ETH single-port | Ethernet | Shared networkingRDMAHARouted network (ETH only)IOMMU |
 | NVIDIA Mellanox CX-7-ETH dual-port | Ethernet | LACPShared networkingRDMAHARouted network (ETH only)IOMMU |
 | NVIDIA Mellanox CX-6 LX | Ethernet | Shared networkingRDMARX interruptsHARouted network (ETH only)IOMMU |
 | NVIDIA Mellanox CX-6 DX | Ethernet | LACPShared networkingRX interruptsRDMAHARouted network (ETH only)IOMMU |
 | NVIDIA Mellanox CX-6 | Ethernet InfiniBand | Mixed networksShared networkingRX interruptsRDMAHAIOMMU |
 | NVIDIA Mellanox CX-5 EX | Ethernet InfiniBand | Mixed networksRDMAHAPKEY (IB only)IOMMU |
 | NVIDIA Mellanox CX-5 BF | Ethernet | Mixed networksRDMAHAIOMMU |
 | NVIDIA Mellanox CX-5 | Ethernet InfiniBand | Mixed networksRX interruptsRDMAHAPKEY (IB only)Routed network (ETH only)IOMMU |
 | NVIDIA Mellanox CX-4 LX | Ethernet InfiniBand | Mixed networksRX interruptsHARouted network (ETH only)IOMMU |
 | NVIDIA Mellanox CX-4 | Ethernet InfiniBand | Mixed networksRX interruptsHARouted network (ETH only)IOMMU |
 | VirtIO | Ethernet | HARouted network |

#### Supported network adapters for clients-only

The following network adapters support Ethernet and SRIOV VF for clients only:

* Intel X540
* Intel X550-T1 (avoid using this adapter in a single client connected to multiple clusters)
* Intel X710
* Intel X710-DA2
* Intel XL710
* Intel XL710-Q2
* Intel XXV710
* Intel 82599ES
* Intel 82599

### Ethernet drivers and configurations

*   **Supported Mellanox OFED versions for the Ethernet NICs:**

    * 24.04-0.7.0.0
    * 23.10-0.5.5.0
    * 23.04-1.1.3.0
    * 5.9-0.5.6.0
    * 5.8-1.1.2.1 LTS
    * 5.8-3.0.7.0
    * 5.7-1.0.2.0
    * 5.6-2.0.9.0
    * 5.6-1.0.3.3
    * 5.4-3.5.8.0 LTS
    * 5.4-3.4.0.0 LTS
    * 5.1-2.6.2.0
    * 5.1-2.5.8.0

    **Note:** Subsequent OFED minor versions are expected to be compatible with Nvidia hardware due to Nvidia's commitment to backwards compatibility.
* **Supported ENA drivers:**
  * 1.0.2 - 2.0.2
  * A current driver from an official OS repository is recommended
* **Supported ixgbevf drivers:**
  * 3.2.2 - 4.1.2
  * A current driver from an official OS repository is recommended
* **Supported Broadcom drivers**:
  * 228: Minimum required for 100/200 Gbps 57508 NIC
  * 231: Minimum required for 200/400 Gbps 57608 NIC

* **Ethernet speeds:**
  * 400 GbE / 200 GbE / 100 GbE / 50GbE / 40 GbE / 25 GbE / 10 GbE.
* **NICs bonding:**
  * Supports bonding dual ports on the same NVIDIA Mellanox NIC using mode 4 (LACP) to enhance redundancy and performance.
* **IEEE 802.1Q VLAN encapsulation:**
  * Supports VLAN tagging with a single VLAN tag on NVIDIA Mellanox NICs.
* **VXLAN:**
  * Virtual Extensible LANs are not supported.
* **DPDK backends and clients using NICs supporting shared networking (single IP):**
  * Require one IP address per client for both management and data plane.
  * SR-IOV enabled is not required.
* **DPDK backends clients using NICs supporting non-shared networking:**
  * IP address for management: One per NIC (configured before WEKA installation).
  * IP address for data plane: One per [WEKA core](../bare-metal/planning-a-weka-system-installation#cpu-resource-planning) in each server (applied during cluster initialization).
  * Virtual Functions (VFs):
    * Ensure the device supports a maximum number of VFs greater than the number of physical cores on the server.
    * Set the number of VFs to match the cores you intend to dedicate to WEKA.
    * Note that some BIOS configurations may be necessary.
  * SR-IOV: Enabled in BIOS.
* **UDP clients:**
  * Use a single IP address for all purposes.

Note: When assigning a network device to the WEKA system, no other application can create VFs on that device.

### InfiniBand drivers and configurations <a href="#networking-infiniband" id="networking-infiniband"></a>

WEKA supports the following Mellanox OFED versions for the InfiniBand adapters:

* 24.04-0.7.0.0
* 23.10-0.5.5.0
* 23.04-1.1.3.0
* 5.9-0.5.6.0
* 5.8-1.1.2.1 LTS
* 5.8-3.0.7.0
* 5.7-1.0.2.0
* 5.6-2.0.9.0
* 5.6-1.0.3.3
* 5.4-3.5.8.0 LTS
* 5.4-3.4.0.0 LTS
* 5.1-2.6.2.0
* 5.1-2.5.8.0

**Note:** Subsequent OFED minor versions are expected to be compatible with Nvidia hardware due to Nvidia's commitment to backwards compatibility.

WEKA supports the following InfiniBand configurations:

* InfiniBand speeds: Determined by the InfiniBand adapter supported speeds (FDR / EDR / HDR / NDR).
* Subnet manager: Configured to 4092.
* One WEKA system IP address for management and data plane.
* PKEYs: One partition key is supported by WEKA.
* Redundant InfiniBand ports can be used for both HA and higher bandwidth.

Note: If it is necessary to change PKEYs, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contacting-weka-technical-support-team).

### Required ports

When configuring firewall ingress and egress rules the following access must be allowed.

Note: Right-scroll the table to view all columns.

 | Purpose | Source | Target | Target Ports | Protocol | Comments |
 | --- | --- | --- | --- | --- | --- |
 | WEKA server traffic for bare-metal deployments | All WEKA backend IPs | All WEKA backend IPs | 14000-14100 (drives)14200-14300 (frontend)14300-14400 (compute) | TCP and UDPTCP and UDPTCP and UDP | These ports are the default for the Resources Generator for the first three containers. You can customize the ports. |
 | WEKA client traffic | Client host IPs | All WEKA backend IPs | 14000-14100 (drives)14300-14400 (compute) | TCP and UDPTCP and UDP | These ports are the default. You can customize the ports. |
 | WEKA backend to client traffic | All WEKA backend IPs | Client host IPs | 14000-14100 (frontend) | TCP and UDP | These ports are the default. You can customize the ports. |
 | WEKA SSH management traffic | All WEKA backend IPs | All WEKA backend IPs | 22 | TCP |  |
 | WEKA server traffic for cloud deployments | All WEKA backend IPs | All WEKA backend IPs | 14000-14100 (drives)15000-15100 (compute)16000-16100 (frontend) | TCP and UDPTCP and UDPTCP and UDP | These ports are the default. You can customize the ports. |
 | WEKA client traffic (on cloud) | Client host IPs | All WEKA backend IPs | 14000-14100 (drives)15000-15100 (compute) | TCP and UDPTCP and UDP | These ports are the default. You can customize the ports. |
 | WEKA backend to client traffic (on cloud) | All WEKA backend IPs | Client host IPs | 14000-14100 (frontend) | TCP and UDP | These ports are the default. You can customize the ports. |
 | WEKA GUI access | Admin workstation IPs | All WEKA management IPs | 14000 | TCP | User web browser IP |
 | NFS | NFS client IPs | WEKA NFS backend IPs | 2049<mountd port> | TCP and UDPTCP and UDP | You can set the mountd port using the command: weka nfs global-config set --mountd-port |
 | NFSv3 (used for locking) | NFS client IPs | WEKA NFS backend IPs | 46999 (status monitor)47000 (lock manager) | TCP and UDP |  |
 | SMB/SMB-W | SMB client IPs | WEKA SMB backend IPs | 139445 | TCPTCP |  |
 | SMB-W | All WEKA SMB-W backend IPs | All WEKA SMB-W backend IPs | 2224 | TCP | This port is required for internal clustering processes. |
 | SMB/SMB-W | WEKA SMB backend IPs | All Domain Controllers for the selected Active Directory Domain | 8838946463632683269 | TCP and UDPTCP and UDPTCP and UDPTCP and UDPTCP and UDPTCP and UDP | These ports are required for SMB/SMB-W to use Active Directory as the identity source. Furthermore, every Domain Controller within the selected AD domain must be accessible from the WEKA SMB servers. |
 | SMB/SMB-W | WEKA SMB backend IPs | DNS servers | 53 | TCP and UDP |  |
 | S3 | S3 client IPs | WEKA S3 backend IPs | 9000 | TCP | This port is the default. You can customize the port. |
 | wekatester | All WEKA backend IPs | All WEKA backend IPs | 85019090 | TCPTCP | Port 8501 is used by wekanetperf. |
 | WEKA Management Station | User web browser IP | WEKA Management Station IP | 80 <LWH>443 <LWH>3000 <mon>7860 <admin UI>8760 <deploy>8090 <snap>8501 <mgmt>9090 <mgmt>9091 <mon>9093 <alerts> | HTTPHTTPSTCPTCPTCPTCPTCPTCPTCP |  |
 | Cloud WEKA Home, Local WEKA Home | All WEKA backend IPs | Cloud WEKA Home or Local WEKA Home | 80443 | HTTPHTTPS | Open according to the directions in the deployment scenario:- WEKA server IPs to CWH or LWH.- LWH to CWH (if forwarding data from LWH to CWH) |
 | Troubleshooting by the Customer Success Team (CST) | All WEKA backend IPs | CST remote access | 40004001 | TCPTCP |  |
 | Traces remote viewer | All WEKA backend IPs | CST remote access | 443 | TCP |  |
 | KMS: Hashicorp Vault | All WEKA backend IPs | Hashicorp Vault server | 82008201 | TCPTCP | Default vault ports: 8200 is configurable for client requests, while 8201 (base_port+1) handles internal cluster communication. |
 | KMS: KMIP | All WEKA backend IPs | KMIP server | 5696 | TCP | The default KMIP port, 5696, is configurable. Per the KMIP specification, servers must use this port when operating with the TTLV encoding format. |

## HA

See #high-availability

## SSDs

* The SSDs must support PLP (Power Loss Protection).
* WEKA system storage must be dedicated, and partitioning is not supported.
* The supported drive capacity is up to 30 TB.
* The ratio between the cluster's smallest and the largest SSD capacity must not exceed 8:1.

Note: To get the best performance, ensure TRIM) is supported by the device and enabled in the operating system.

## Object store

* API must be S3 compatible:
  * GET
    * Including byte-range support with expected performance gain when fetching partial objects
  * PUT
    * Supports any byte size of up to 65 MiB
  * DELETE
* Data Consistency: Amazon S3 consistency model:
  * GET after a single PUT is strongly consistent
  * Multiple PUTs are eventually consistent

### Certified object stores

* Amazon S3
  * S3 Standard
  * S3 Intelligent-Tiering
  *   These storage classes are ideal for remote buckets where data is written once and accessed in critical situations, such as during disaster recovery:

      * S3 Standard-IA
      * S3 One Zone-IA
      * S3 Glacier Instant Retrieval

      Remember, retrieval times, minimum storage periods, and potential charges due to object compaction may apply. If unsure, use S3 Intelligent-Tiering.
* Azure Blob Storage
* Google Cloud Storage (GCS)
* Cloudian HyperStore (version 7.3)
* Dell EMC ECS (version 3.5)
* Dell PowerScale S3 (version 9.8.0.0)
* HCP Classic V9.2 and up (with versioned buckets only)
* HCP for Cloud-Scale V2.x
* IBM Cloud Object Storage System (version 3.14.7)
* Lenovo MagnaScale (version 3.0)
* Quantum ActiveScale (version 5.5.1)
* Red Hat Ceph Storage (version 5.0)
* Scality RING with S3 connector (version 8.5)
* Scality RING with WEKA connector (version 9.5)
* Scality Artesca (version 1.5.2)
* SwiftStack (version 6.30)
* WEKA S3

## Virtual Machines

This section outlines the use of virtual machines (VMs) with WEKA, covering backends, clients, VMware platforms, and cloud environments. While VMs can be used in certain configurations, there are specific limitations and best practices to follow.

### Backends

Virtual machines may be used as backends for internal training purposes only and are not recommended for production environments.

WEKA provides best-effort support for backends deployed on virtual machines, but full support is not guaranteed. Additionally, WEKA does not guarantee support for components or configurations outside of our documented and supported cloud environments, and performance may vary.

### Clients

Virtual Machines (VMs) can be used as clients. Ensure the following prerequisites are met for each client type:

* **UDP clients**:
  * Reserve CPU resources and dedicate a core to the client to prevent CPU starvation of the WEKA process.
  * Ensure the root filesystem supports a 3K IOPS load for the WEKA client.
* **DPDK clients**:
  * Meet all the requirements for UDP clients.
  * Additionally, verify that the virtual platform (hypervisor, NICs, CPUs, and their respective versions) fully supports DPDK and the required virtual network drivers.

### **VMware platform (**&#x63;lient only)

When using **vmxnet3** devices, do not enable the SR-IOV feature, because it disables the vMotion functionality. Each frontend process requires a dedicated **vmxnet3** device and IP address, with an additional device and IP for each client VM to support the management process.

Core dedication is required when using **vmxnet3** devices.

### VMs and instances on cloud environments

Refer to the cloud deployment sections for the most up-to-date list of supported virtual machines and instances in various cloud environments.

**Related topics**

AWS:

Azure:

GCP:

\
**Related information**

For additional information and how-to articles, search the WEKA Knowledge Base in the WEKA support portal or contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contacting-weka-technical-support-team).

## KMS

**Supported KMS types:**

* **KMIP-compliant KMS****:** Supports protocol versions 1.2+ and 2.x. Only TTLV is supported as the messaging protocol. Supports commercial solutions such as Thales CipherTrust Manager.
* **HashiCorp Vault****:** Supports versions 1.1.5 to 1.14.x.

<!-- ============================================ -->
<!-- File 3/259: planning-and-installation_bare-metal.md -->
<!-- ============================================ -->

---
description:
---

# WEKA cluster installation on bare metal servers

WEKA provides a variety of tools for automating the WEKA software installation process. These include:

* WEKA Management Station (WMS)
* WEKA Software Appliance (WSA)
* WEKA Configurator

### WEKA Management Station (WMS)

WMS can be used to speed the WEKA Software Appliance (WSA) deployment on the supported bare metal servers: HPe, Dell, and Supermicro.

This is the preferred installation method, the simplest and fastest method to get from bare metal to a working WEKA cluster. If you cannot meet the prerequisites for deploying WMS, use the WSA package.

### WEKA Software Appliance (WSA)

WSA is a WEKA server image deployed with a preconfigured operating system. This method significantly speeds up the OS and WEKA cluster installation and provides a WEKA-supported operating environment.

After installation, the server is in **STEM** mode, which is the initial state before the configuration.

If you cannot use the WSA for WEKA cluster installation, review the requirements and follow the instructions for deploying WEKA using the WEKA Configurator.

### WEKA Configurator

The WEKA Configurator automatically generates the WEKA Cluster configurations (`config.sh`) to apply on the cluster servers.

## High-level deployment workflow

The following illustrates a high-level deployment workflow on a group of bare metal servers.

### Deployment workflow paths summary

The following summarizes the three workflow paths to install and configure the WEKA cluster.

* **Path A:** Automated with WMS and WSA
* **Path B:** Automated with WSA only
* **Path C:** Manual installation and configuration

Select the path applicable to your needs.

This method is the most preferable option to install the WEKA cluster assuming the prerequisites are met. For example, the bare metal servers are HPe, Dell, or Supermicro, the OS (Rocky 8.6) meets your needs, and a physical server is available for installing the WMS.

If the OS (Rocky 8.6) meets your needs but the bare-metal servers are not HPe, Dell, or Supermicro, this is the second preferred option to install and configure the WEKA cluster.\

Manually install and configure the WEKA cluster if:

* The bare metal servers are not HPe, Dell, or Supermicro, or
* You want to use a different OS than Rocky 8.6, or
* You need special customization, where you cannot use the WEKA Configurator.

Note: The manual installation workflow requires deep level of knowledge with WEKA architecture. Visit WEKA U for training materials (requires sign-in).

<details>

<summary>Frequently asked questions</summary>

1. What is the root password? Is this configurable, and can it be encrypted?
   * `WekaService`. It is encrypted in the kickstart file.
2. Can we choose the number of cores and containers to use?
   * Yes. During post-install configuration. See Configure a WEKA cluster with the WEKA Configurator.
3. Will the ISO setup mirror RAID on the dual-boot SSDs?
   * Yes, automatically.
4. Can I set up WEKA with 8 SSDs per node even though I have 12 installed?
   * Not automatically. Pull the drives or manually adjust the configuration before running it (edit the `config.sh` output from `wekaconfig`).
5. What must be done to direct the ISO to set up for High Availability (HA)? How about no HA?
   * That‚Äôs determined in `wekaconfig`.
6. If there are multiple NIC cards (for WEKA and Ceph), how to choose the NICs to use for the WEKA backend server?
   * The WSA is not intended for that configuration directly. However, if you make them different subnets or networks, you can select the subnet to use. one, the other, or both.
7. With the ISO, are there different licensing processes? Or is it the standard to get cluster GUID and storage size and input it into the Weka webpage to get a license key and then input that key on the command prompt?
   * Licensing has not changed.
8. Does the ISO set up the IP address for Admin or the high-speed WEKA backend network?
   * The WMS will do that when it deploys the WSA.
9. What needs to be passed in to configure Ethernet or Infiniband?
   * Select the network type from the list in WMS.
10. Can all the parameters the ISO needs be in the script?
    * No. We use Ansible after installation to make the settings.
11. How do you use the kickstart file in the ISO?
    * Use the WMS. The `kickstart` file was written to work with WMS.
12. What additional settings must be configured on WEKA after the ISO installation?
    * There are no required settings that need to be manually set if you use the WMS.

</details>

## What do do next?

 (all paths)

<!-- ============================================ -->
<!-- File 4/259: planning-and-installation_bare-metal_planning-a-weka-system-installation.md -->
<!-- ============================================ -->

# Plan the WEKA system hardware requirements

The planning of a WEKA system is essential before the actual installation process. It involves the planning of the following:

1. Total SSD net capacity and performance requirements
2. SSD resources
3. Memory resources
4. CPU resources
5. Network

Note: When implementing an AWS configuration, it is possible to go to the [Self-Service Portal in start.weka.io](../aws/weka-installation-on-aws-using-the-cloud-formation/self-service-portal) to map capacity and performance requirements into various configurations automatically.

## Total SSD net capacity and performance planning

A WEKA system cluster runs on a group of servers with local SSDs. To plan these servers, the following information must be clarified and defined:

1. **Capacity:** Plan your net SSD capacity. The data management to object stores can be added after the installation. In the context of the planning stage, only the SSD capacity is required.
2. **Redundancy scheme:** Define the optimal redundancy scheme required for the WEKA system, as explained in [Selecting a Redundancy Scheme](../../../weka-system-overview/about#selecting-a-redundancy-scheme).
3. **Failure domains:** Determine whether to use failure domains (optional), and if yes, determine the number of failure domains and the potential number of servers in each failure domain, as described in [Failure Domains](broken-reference), and plan accordingly.
4. **Hot spare**: Define the required hot spare count (see #hot-spare-capacity).

Once all this data is clarified, you can plan the SSD net storage capacity accordingly (see #ssd-net-storage-capacity-calculation). Adhere to the following information, which is required during the installation process:

1. Cluster size (number of servers).
2. SSD capacity for each server, for example, 12 servers with a capacity of 6 TB each.
3. Planned protection scheme, for example, 6+2.
4. Planned failure domains (optional).
5. Planned hot spare.

Note: This is an iterative process. Depending on the scenario, some options can be fixed constraints while others are flexible.

## SSD resource planning

SSD resource planning involves how the defined capacity is implemented for the SSDs. For each server, the following has to be determined:

* The number of SSDs and capacity for each SSD (where the multiplication of the two should satisfy the required capacity per server).
* The selected technology, NVME, SAS, or SATA, and the specific SSD models have implications on SSD endurance and performance.

Note: For on-premises planning, it is possible to consult with the Customer Success Team to map between performance requirements and the recommended WEKA system configuration.

## Memory resource planning <a href="#memory-resource-planning" id="memory-resource-planning"></a>

### Backend servers memory requirements

The total per server memory requirements is the sum of the following requirements:

 | Purpose | Per-server memory |
 | --- | --- |
 | Fixed | 2.8 GB |
 | Frontend processes | 2.2 GB x # of Frontend processes |
 | Compute processes | 3.9 GB x # of Compute processes |
 | Drive processes | 2 GB x # of Drive processes |
 | SSD capacity management | (Total SSD Raw Capacity / Number of Servers / 2,000) + (Number of Cores x 3 GB) |
 | Operating System | The maximum between 8 GB and 2% from the total RAM |
 | Additional protocols (NFS/SMB/S3) | 16 GB |
 | RDMA | 2 GB |
 | Metadata (pointers) | 20 Bytes x # Metadata units per serverSee Metadata units calculation. |
 | Dedicated Data Services container | If you intend to add a Data Services container for background tasks, it requires additional memory:3.5 GB (without dedicated core)5.5 GB (with dedicated core) |

Note: Contact the Customer Success Team to explore options for configurations requiring more than 384 GB of memory per server.

#### Example 1: A system with large files

A system with 16 servers with the following details:

* Fixed: 2.8 GB\
  Number of Frontend processes: 1
* Number of Compute processes: 13
* Number of Drive processes: 6
* Total raw capacity: 983 TB (983,000 GB)
* Total net capacity: 725 TB
* NFS/SMB services
* RDMA
* Average file size: 1 MB (potentially up to 755 million files for all servers; \~47 million files per server)

Calculations:

* Frontend processes: 1 x 2.2 = 2.2 GB
* Compute processes: 13 x 3.9 = 50.7 GB
* Drive processes: 6 x 2 = 12 GB
* SSD capacity management: 983,000 GB / 16 / 2000 + 20 x 3 GB = \~90.7 GB
* Additional protocols = 16 GB
* RDMA = 2 GB
* Metadata: 20 Bytes x 47 million files x 2 units = \~1.9 GB

Total memory requirement per server = 2.8 + 2.2 + 50.7 + 12 + 90.7 + 16 + 2 + 1.9 = \~178.3 GB

#### Example 2: A system with small files

For the same system as in example 1, but with smaller files, the required memory for metadata would be larger.

For an average file size of 64 KB, the number of files is potentially up to:

* \~12 billion files for all servers.
* \~980 million files per server.

Required memory for metadata: 20 Bytes x 980 million files x 1 unit = \~19.6 GB

Total memory requirement per server = 2.8 + 2.2 + 50.7 + 12 + 90.7 + 16 + 2 + 19.6 = \~196 GB

Note: The memory requirements are conservative and can be reduced in some situations, such as in systems with mostly large files or a system with files 4 KB in size. Contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team) to receive an estimate for your specific configuration.

### Client's memory requirements

The WEKA software on a client requires 5 GB minimum additional memory.

## CPU resource planning

### CPU allocation strategy

The WEKA system implements a Non-Uniform Memory Access (NUMA) aware CPU allocation strategy to maximize the overall performance of the system. The cores allocation uses all NUMAs equally to balance memory usage from all NUMAs.

Consider the following regarding the CPU allocation strategy:

* The code allocates CPU resources by assigning individual cores to tasks in a cgroup.
* Cores in a cgroup are not available to run any other user processes.
* On systems with Intel hyper-threading enabled, the corresponding sibling cores are placed into a cgroup along with the physical ones.

### Backend CPU usage

Plan the number of physical cores dedicated to the WEKA software according to the following guidelines and limitations:

* Dedicate at least one physical core to the operating system; the rest can be allocated to the WEKA software.
  * Generally, it is recommended to allocate as many cores as possible to the WEKA system.
  * A backend server can have as many cores as possible. However, a container within a backend server can have a maximum of 19 physical cores.
  * Leave enough cores for the container serving the protocol if it runs on the same server.
* Allocate enough cores to support performance targets.
  * Generally, use 1 drive process per SSD for up to 6 SSDs and 1 drive process per 2 SSDs for more, with a ratio of 2 compute processes per drive process.
  * For finer tuning, please contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team).
* Allocate enough memory to match core allocation, as discussed above.
* Running other applications on the same server (converged WEKA system deployment) is supported. For details, contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team).

### Client CPU usage

The WEKA client software requires one physical CPU core by default. When running on systems with hyper-threading enabled, WEKA consumes two logical cores.

In UDP networking, the operating system pins WEKA processes to specific CPU cores. These processes maintain guaranteed access to their assigned cores, but the operating system can still schedule other processes to run on the same cores. This contrasts with exclusive CPU allocation, where WEKA reserves cores solely for its processes.

## Network planning

### Backend servers

WEKA backend servers support connections to both InfiniBand and Ethernet networks, using [compatible network interface cards](../../prerequisites-and-compatibility#networking-ethernet) (NICs). When deploying backend servers, ensure that all servers in the WEKA system are connected using the same network technology for each type of network.

InfiniBand connections are prioritized over Ethernet links for data traffic. Both network types must be operational to ensure system availability, so consider adding redundant ports for each network type.

Clients can connect to the WEKA system over either InfiniBand or Ethernet.

A network port can be dedicated exclusively to the WEKA system or shared between the WEKA system and other applications.

### Clients

Clients can be configured with networking as described above to achieve the highest performance and lowest latency; however, this setup requires compatible hardware and dedicated CPU core resources. If compatible hardware is not available or a dedicated CPU core cannot be allocated to the WEKA system, client networking can instead be configured to use the kernel‚Äôs UDP service. This configuration results in reduced performance and increased latency.

## What to do next?

 (all paths)

<!-- ============================================ -->
<!-- File 5/259: planning-and-installation_bare-metal_obtaining-the-weka-install-file.md -->
<!-- ============================================ -->

---
description:
---

# Obtain the WEKA installation packages

## **Register to get.weka.io**

To sign in to get.weka.io, you first need to create an account and fill in your details. If you already have a registered account for get.weka.io, skip this procedure.

**Procedure**

1. Go to the get.weka.io download site, and select **Create an account.**

The Send Registration Email page opens.

2\. Fill in your organization's email address (private mail is prohibited).\
    Select **I‚Äôm not a robot**, and then select **Send Registration Email.**

3\. Check your inbox for a registration email from Weka.io. \
    To confirm your registration, select the link.\
    The Create Your Account page opens.

4\. Fill in your email address, full name, and password. Then, select **Create Account**.

Your request for access to get.weka.io is sent to WEKA for review. Wait for a validation email. Once your registration is approved, you can sign in to get.weka.io.

## **Download the** WEKA **installation packages**

Download the required WEKA installation packages according to the workflow path.

* Path A (automated with WMS and WSA): Download the WMS and WSA ISOs from get.weka.io. The WMS is downloaded from a dedicated dropdown.  The WSA is found in the relevant release page.
* Path B (automated with WSA): Download the WSA package from get.weka.io The WSA is found in the relevant release page.
* Path C (manual installation and configuration): Download the WEKA software tarball from get.weka.io. The tarball is found in the relevant release page.

You can only sign in and download the packages if you are a registered user.

**Procedure: Download from get.weka.io**

1. Go to the get.weka.io download site, and sign in with your registered account.

get.weka.io page opens.

2. Do one of the following:
   * Select the required package from the dashboard.
   * Select the **Releases** tab, select the required release, and follow the download instructions.\
     (The token in the download link is purposely blurred.)

## What to do next?

Depending on the workflow path you follow, go to one of the following:

 (path A)

 (path B)

 (path C)

<!-- ============================================ -->
<!-- File 6/259: planning-and-installation_bare-metal_setting-up-the-hosts.md -->
<!-- ============================================ -->

---
description:
---

# Manually prepare the system for WEKA configuration

Once the hardware and software prerequisites are met, prepare the backend servers and clients for the WEKA system configuration.

This preparation consists of the following steps:

1. Install NIC drivers
2. Enable SR-IOV (when required)
3. Set up ConnectX cards
4. Configure the networking
5. Configure the HA networking
6. Verify the network configuration
7. Configure the clock synchronization
8. Disable the NUMA balancing
9. Enable kdump and set kernel panic reboot timer
10. Disable swap (if any)
11. Validate the system preparation

Note: Some of the examples contain version-specific information. The software is updated frequently, so the package versions available to you may differ from those presented here.

**Related topics**

## 1. Install NIC drivers <a href="#install-nic-drivers" id="install-nic-drivers"></a>

* To install Mellanox OFED, see NVIDIA Documentation - Installing Mellanox OFED.
* To install Broadcom driver, see .
* To install Intel driver, see Latest Drivers & Software downloads.

## 2. Enable SR-IOV <a href="#enable-sr-iov" id="enable-sr-iov"></a>

Single Root I/O Virtualization (SR-IOV) enablement is mandatory in the following cases:

* The servers are equipped with Intel NICs.
* When working with client VMs, a physical NIC's virtual functions (VFs) must be exposed to the virtual NICs.

**Related topic**

## 3. Set up ConnectX cards

1.  **Configure firmware parameters:** All ConnectX ports used directly with WEKA  servers and clients require specific firmware settings for optimal performance. Set the following non-default parameters:

    * `ADVANCED_PCI_SETTINGS=1`
    * `PCI_WR_ORDERING=1`

    Use the following command to apply these settings to all MLX devices:

    ```

    ```
    mst start && for MLXDEV in /dev/mst/* ; do mlxconfig -d ${MLXDEV} -y set ADVANCED_PCI_SETTINGS=1 PCI_WR_ORDERING=1; done
    ```

```
2. **Set link type:** Certain ConnectX VPI cards require modification of the link type, to specifically set the port to use InfiniBand or Ethernet networking.\
   \
   If applicable, set the port mode with the following command, where 1=InfiniBand and 2=Ethernet:\
   `mlxconfig -y -d /dev/mst/<dev> set LINK_TYPE_P<1,2>=<1,2>`\
   \
   For example, the following command sets port 2 to InfiniBand:\
   `mlxconfig -y -d /dev/mst/<dev> set LINK_TYPE_P2=1`\

3. **Reboot the system:** A reboot is required after applying the firmware settings to ensure the changes take effect.

**Related information**

For additional details, refer to the NVIDIA ConnectX documentation.

## 4. Configure the networking <a href="#configure-the-networking" id="configure-the-networking"></a>

### Ethernet configuration

The following example of the `ifcfg` script is a reference for configuring the Ethernet interface.

```

```
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="none"
DEFROUTE="no"
IPV4_FAILURE_FATAL="no"
IPV6INIT="no"
IPV6_AUTOCONF="no"
IPV6_DEFROUTE="no"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="enp24s0"
DEVICE="enp24s0"
ONBOOT="yes"
NM_CONTROLLED=no
IPADDR=192.168.1.1
NETMASK=255.255.0.0
MTU=9000
```

```

MTU 9000 (jumbo frame) is recommended for the best performance. Refer to your switch vendor documentation for jumbo frame configuration.

Bring the interface up using the following command:

```
# ifup enp24s0
```

### InfiniBand configuration

InfiniBand network configuration normally includes Subnet Manager (SM), but the procedure involved is beyond the scope of this document. However, it is important to be aware of the specifics of your SM configuration, such as partitioning and MTU, because they can affect the configuration of the endpoint ports in Linux. For best performance, MTU of 4092 is recommended.

Refer to the following `ifcfg` script when the IB network only has the default partition, i.e., "no `pkey`":

```

```
TYPE=Infiniband
ONBOOT=yes
BOOTPROTO=static
STARTMODE=auto
USERCTL=no
NM_CONTROLLED=no
DEVICE=ib1
IPADDR=192.168.1.1
NETMASK=255.255.0.0
MTU=4092
```

```

Bring the interface up using the following command:

```
# ifup ib1
```

Verify that the ‚Äúdefault partition‚Äù connection is up, with all the attributes set:

```
# ip a s ib1
4: ib1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 4092 qdisc mq state UP group default qlen 256
  link/infiniband 00:00:03:72:fe:80:00:00:00:00:00:00:24:8a:07:03:00:a8:09:48
brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    inet 10.0.20.84/24 brd 10.0.20.255 scope global noprefixroute ib0
       valid_lft forever preferred_lft forever
```

On an InfiniBand network with a non-default partition number, `p-key` must be configured on the interface if the InfiniBand ports on your network are members of an InfiniBand partition other than the default (`0x7FFF`). The p-key should associate the port as a full member of the partition (full members are those where the p-key number with the most-significant bit (MSB) of the 16-bits is set to 1).

Note: **Example:** If the partition number is `0x2`, the limited member p-key will equal the p-key itself, i.e.,`0x2`. The full member p-key will be calculated as the logical OR of `0x8000` and the p-key (`0x2`) and therefore will be equal to `0x8002`.

Note: **Note:** All InfiniBand ports communicating with the Weka cluster must be full members.

For each `pkey-ed IPoIB` interface, it's necessary to create two `ifcfg` scripts. To configure your own `pkey-ed IPoIB` interface, refer to the following examples, where a `pkey` of `0x8002` is used. You may need to manually create the child device.

```

```
TYPE=Infiniband
ONBOOT=yes
MTU=4092
BOOTPROTO=static
STARTMODE=auto
USERCTL=no
NM_CONTROLLED=no
DEVICE=ib1
```

```

```

```
TYPE=Infiniband
BOOTPROTO=none
CONNECTED_MODE=yes
DEVICE=ib1.8002
IPV4_FAILURE_FATAL=yes
IPV6INIT=no
MTU=4092
NAME=ib1.8002
NM_CONTROLLED=no
ONBOOT=yes
PHYSDEV=ib1
PKEY_ID=2
PKEY=yes
BROADCAST=192.168.255.255
NETMASK=255.255.0.0
IPADDR=192.168.1.1
```

```

Bring the interface up using the following command:

```
# ifup ib1.8002
```

Verify the connection is up with all the non-default partition attributes set:

```
# ip a s ib1.8002
5: ib1.8002@ib0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 4092 qdisc mq state UP qlen 256
    link/infiniband 00:00:11:03:fe:80:00:00:00:00:00:00:24:8a:07:03:00:a8:09:48 brd 00:ff:ff:ff:ff:12:40:1b:80:02:00:00:00:00:00:00:ff:ff:ff:ff
    inet 192.168.1.1/16 brd 192.168.255.255 scope global noprefixroute ib1.8002
       valid_lft forever preferred_lft forever
```

### Define the NICs with `ignore-carrier`

`ignore-carrier` is a NetworkManager configuration option. When set, it keeps the network interface up even if the physical link is down. It‚Äôs useful when services need to bind to the interface address at boot.

Note: The following is an example of configuring `ignore-carrier` on systems that use NetworkManager on Rocky Linux 8. The exact steps may vary depending on your operating system and its specific network configuration tools. Always refer to your system‚Äôs official documentation for accurate information.

1. Open the  `/etc/NetworkManager/NetworkManager.conf` file to edit it.
2. Under the `[main]` section, add one of the following lines depending on the operating system:
   * For some versions of Rocky Linux, RHEL, and CentOS: `ignore-carrier=*`
   * For some other versions:  `ignore-carrier=<device-name1>,<device-name2>`. \
     Replace `<device-name1>,<device-name2>` with the actual device names you want to apply this setting to.

Example for RockyLinux and RHEL 8.7:

```

```
[main]
ignore-carrier=*
```

```

Example for some other versions:

```
[main]
ignore-carrier=ib0,ib1
```

3. Restart the NetworkManager service for the changes to take effect.

## 5. Configure dual-network links with policy-based routing <a href="#configure-the-ha-networking" id="configure-the-ha-networking"></a>

The following steps provide guidance for configuring dual-network links with policy-based routing on Linux systems. Adjust IP addresses and interface names according to your environment.

### **General Settings in `/etc/sysctl.conf`**

1. Open the `/etc/sysctl.conf` file using a text editor.
2.  Add the following lines at the end of the file to set minimal configurations per InfiniBand (IB) or Ethernet (Eth) interface:

    ```bash
    # Minimal configuration, set per IB/Eth interface
    net.ipv4.conf.ib0.arp_announce = 2
    net.ipv4.conf.ib1.arp_announce = 2
    net.ipv4.conf.ib0.arp_filter = 1
    net.ipv4.conf.ib1.arp_filter = 1
    net.ipv4.conf.ib0.arp_ignore = 0
    net.ipv4.conf.ib1.arp_ignore = 0

    # As an alternative set for all interfaces by default
    net.ipv4.conf.all.arp_filter = 1
    net.ipv4.conf.default.arp_filter = 1
    net.ipv4.conf.all.arp_announce = 2
    net.ipv4.conf.default.arp_announce = 2
    net.ipv4.conf.all.arp_ignore = 0
    net.ipv4.conf.default.arp_ignore = 0
    ```
3. Save the file.
4.  Apply the new settings by running:

    ```bash
    sysctl -p /etc/sysctl.conf
    ```

### **RHEL/Rocky/CentOS routing configuration using the network scripts**

Note: Network scripts are deprecated in RHEL/Rocky 8. For RHEL/Rocky 8 and onwards, use the Network Manager.

1. Navigate to `/etc/sysconfig/network-scripts/`.
2.  Create the file `/etc/sysconfig/network-scripts/route-mlnx0` with the following content:

    ```bash
    10.90.0.0/16 dev mlnx0 src 10.90.0.1 table weka1
    default via 10.90.2.1 dev mlnx0 table weka1
    ```
3.  Create the file `/etc/sysconfig/network-scripts/route-mlnx1` with the following content:

    ```bash
    10.90.0.0/16 dev mlnx1 src 10.90.1.1 table weka2
    default via 10.90.2.1 dev mlnx1 table weka2
    ```
4.  Create the files `/etc/sysconfig/network-scripts/rule-mlnx0` and `/etc/sysconfig/network-scripts/rule-mlnx1` with the following content:

    ```bash
    table weka1 from 10.90.0.1
    table weka2 from 10.90.1.1
    ```
5.  Open `/etc/iproute2/rt_tables` and add the following lines:

    ```bash
    100 weka1
    101 weka2
    ```
6. Save the changes.

### RHEL/Rocky 8+ routing configuration using the Network Manager

You can configure routing for your Ethernet or InfiniBand connections using Network Manager command-line interface (`nmcli`) commands.

**Configure ethernet routing**

To set up routing for Ethernet connections, use the following `nmcli` commands. In these commands, the first IP address of the route (`10.10.10.0/24`) represents the subnet of the network to which the NIC connects. The last address in the routing rule (`10.10.10.1` for `eth1`) is the IP address of the NIC you are configuring.

```

```bash
nmcli connection modify eth1 ipv4.routes "10.10.10.0/24 src=10.10.10.1 table=100" ipv4.routing-rules "priority 101 from 10.10.10.1 table 100"
nmcli connection modify eth2 ipv4.routes "10.10.10.0/24 src=10.10.10.101 table=200" ipv4.routing-rules "priority 102 from 10.10.10.101 table 200"
```

```

**Configure InfiniBand routing**

To set up routing for InfiniBand connections, use the following `nmcli` commands. The route's first IP address (`10.10.10.0/24`) signifies the network's subnet for the NIC. The last address in the routing rules (`10.10.10.1` for `ib0`) is the IP address of the NIC you are configuring.

```

```bash
nmcli connection modify ib0 ipv4.route-metric 100
nmcli connection modify ib1 ipv4.route-metric 101

nmcli connection modify ib0 ipv4.routes "10.10.10.0/24 src=10.10.10.1 table=100"
nmcli connection modify ib0 ipv4.routing-rules "priority 101 from 10.10.10.1 table 100"
nmcli connection modify ib1 ipv4.routes "10.10.10.0/24 src=10.10.10.101 table=200"
nmcli connection modify ib1 ipv4.routing-rules "priority 102 from 10.10.10.101 table 200"
```

```

**View network configuration**

Run the following command. to view the current network configuration, including interfaces, IP addresses, routes, and DNS settings.

```
nmcli -
```

The command returns a detailed list of all network interfaces and their status.

**Example**

```bash
eno12409: connected to eno12409
        "Mellanox MT2894"
        ethernet (mlx5_core), 50:00:E6:42:FC:27, hw, mtu 9000
        ip4 default
        inet4 10.10.35.140/25
        route4 10.10.35.128/25 metric 101
        route4 default via 10.10.35.129 metric 101
        inet6 fe80::207c:c202:d22f:d26b/64
        route6 fe80::/64 metric 1024

ens1: connected to ens1
        "Mellanox MT2910"
        ethernet (mlx5_core), 9C:63:C0:EB:7C:02, hw, mtu 9000
        inet4 10.10.50.1/24
        route4 10.10.50.0/24 metric 0
        route4 10.10.30.0/24 metric 0
        route4 10.10.37.0/25 via 10.10.50.1 metric 0
        inet6 fe80::f0d6:8ea:5be4:f4ed/64
        route6 fe80::/64 metric 1024

DNS configuration:
        servers: 10.219.59.120 10.211.188.61 10.211.188.73
        domains: coupang.net
        interface: eno12409
```

### **Ubuntu Netplan configuration**

1.  Open the Netplan configuration file `/etc/netplan/01-netcfg.yaml` and adjust it:

    ```yaml
    network:
        version: 2
        renderer: networkd
        ethernets:
            enp2s0:
                dhcp4: true
                nameservers:
                        addresses: [8.8.8.8]
            ib1:
                addresses:
                        [10.222.0.10/24]
                routes:
                        - to: 10.222.0.0/24
                          via: 10.222.0.10
                          table: 100
                routing-policy:
                        - from: 10.222.0.10
                          table: 100
                          priority: 32764
                ignore-carrier: true

            ib2:
                addresses:
                        [10.222.0.20/24]
                routes:
                        - to: 10.222.0.0/24
                          via: 10.222.0.20
                          table: 101
                routing-policy:
                        - from: 10.222.0.20
                          table: 101
                          priority: 32765
                ignore-carrier: true
    ```
2.  After adjusting the Netplan configuration file, run the following commands:

    ```bash
    ip route add 10.222.0.0/24 via 10.222.0.10 dev ib1 table 100
    ip route add 10.222.0.0/24 via 10.222.0.20 dev ib2 table 101
    ```

### **SLES/SUSE configuration**

1.  Create `/etc/sysconfig/network/ifrule-eth2` with:

    ```bash
    ipv4 from 192.168.11.21 table 100
    ```
2.  Create `/etc/sysconfig/network/ifrule-eth4` with:

    ```bash
    ipv4 from 192.168.11.31 table 101
    ```
3.  Create `/etc/sysconfig/network/scripts/ifup-route.eth2` with:

    ```bash
    ip route add 192.168.11.0/24 dev eth2 src 192.168.11.21 table weka1
    ```
4.  Create `/etc/sysconfig/network/scripts/ifup-route.eth4` with:

    ```bash
    ip route add 192.168.11.0/24 dev eth4 src 192.168.11.31 table weka2
    ```
5.  Add the weka lines to `/etc/iproute2/rt_tables`:

    ```bash
    100 weka1
    101 weka2
    ```
6.  Restart the interfaces or reboot the machine:

    ```bash
    ifdown eth2; ifdown eth4; ifup eth2; ifup eth4
    ```

**Related topic**

#high-availability-ha

## 6. Verify the network configuration <a href="#verify-the-network-configuration" id="verify-the-network-configuration"></a>

Use a large-size ICMP ping to check the basic TCP/IP connectivity between the interfaces of the servers:

```
# ping -M do -s 8972 -c 3 192.168.1.2
PING 192.168.1.2 (192.168.1.2) 8972(9000) bytes of data.
8980 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=0.063 ms
8980 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=0.087 ms
8980 bytes from 192.168.1.2: icmp_seq=3 ttl=64 time=0.075 ms

--- 192.168.2.0 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.063/0.075/0.087/0.009 ms
```

The`-M do` flag prohibits packet fragmentation, which allows verification of correct MTU configuration between the two endpoints.

`-s 8972` is the maximum ICMP packet size that can be transferred with MTU 9000, due to the overhead of ICMP and IP protocols.

Note: All WEKA server interfaces within the same subnet must have connectivity and be able to ping each other.

## 7. Configure the clock synchronization <a href="#configure-sync" id="configure-sync"></a>

The synchronization of time on computers and networks is considered good practice and is vitally important for the stability of the WEKA system. Proper timestamp alignment in packets and logs is very helpful for the efficient and quick resolution of issues.

Configure the clock synchronization software on the backends and clients according to the specific vendor instructions (see your OS documentation), before installing the WEKA software.

## **8. Disable** persistent **NUMA balancing** <a href="#disable-the-numa-balancing" id="disable-the-numa-balancing"></a>

The WEKA system autonomously manages NUMA balancing to make optimal decisions. Disabling the Linux kernel‚Äôs NUMA balancing feature is a mandatory requirement to prevent adding latencies to operations. It is crucial that NUMA balancing remains disabled and is not altered by a server reboot.

This procedure modifies the `sysctl.conf` file to ensure the setting persists across server reboots.

**Procedure**

1.  Open the `/etc/sysctl.conf` file using a text editor, such as `vi` or `nano`, with root privileges.

    ```
    sudo vi /etc/sysctl.conf
    ```
2.  Add the following line to the file:

    ```
    kernel.numa_balancing = 0
    ```
3. Save your changes and exit the editor.
4.  Apply the setting immediately without rebooting the server.

    ```
    sudo sysctl -p
    ```

    The command's output confirms the change:

    ```
    kernel.numa_balancing = 0
    ```

## **9. Enable kdump and set kernel panic reboot timer**

Enabling kdump and configuring the kernel panic reboot timer ensures system crashes leave log files for analysis and automate system reboot after a kernel panic to minimize downtime.

<details>

<summary><strong>Enable kdump</strong></summary>

Enabling kdump ensures crash diagnostic data is captured (`/var/crash`).

1. Install kdump tools (if not exist): `sudo yum install kexec-tools crash`.
2. Enable the kdump service: `sudo systemctl enable kdump.service`.
3. Open the file located at: `/etc/kdump.conf`.
4. Set the crash dump path and size. Example:

```plaintext
path /var/crash
core_collector makedumpfile -c --message-level 1 -d 31
```

</details>

<details>

<summary><strong>Set kernel panic reboot timer</strong></summary>

Setting `kernel.panic` to reboot after 300 seconds automates recovery from kernel panics, reducing server downtime and aiding in faster issue resolution.

1. Open the file located at: `/etc/sysctl.conf`
2. Append the following line: `kernel.panic = 300`
3. Apply changes: `sudo sysctl -p`

</details>

## 10. Disable swap (if any)

WEKA highly recommends that any servers used as backends have no swap configured. This is distribution-dependent but is often a case of commenting out any `swap` entries in `/etc/fstab` and rebooting.

## 11. Validate the system preparation

The `wekachecker` is a tool that validates the readiness of the servers in the cluster before installing the WEKA software.

The `wekachecker` performs the following validations:

* Dataplane IP, jumbo frames, and routing
* ssh connection to all servers
* Timesync
* OS release
* Sufficient capacity in /opt/weka
* Available RAM
* Internet connection availability
* NTP
* DNS configuration
* Firewall rules
* WEKA required packages
* OFED required packages
* Recommended packages
* HT/AMT is disabled
* The kernel is supported
* CPU has a supported AES, and it is enabled
* Numa balancing is enabled
* RAM state
* XFS FS type installed
* Mellanox OFED is installed
* IOMMU setting in all servers is consistent, either all enabled or all disabled.
* rpcbind utility is enabled
* SquashFS is enabled
* noexec mount option on /tmp

Note: The `wekachecker`tool applies to all WEKA versions. From V4.0, the following validations are not relevant, although the tool displays them:
* OS has SELinux disabled or in permissive mode.
* Network Manager is disabled.

**Procedure**

1. Download the **wekachecker** tarball from https://github.com/weka/tools/blob/master/install/wekachecker and extract it.
2. From the install directory, run `./wekachecker <hostnames/IPs>`\
   Where:\
   The `hostnames/IPs` is a space-separated list of all the cluster hostnames or IP addresses connected to the **high-speed networking**.\
   Example:\
   `./wekachecker 10.1.1.11 10.1.1.12 10.1.1.4 10.1.1.5 10.1.1.6 10.1.1.7 10.1.1.8`
3. Review the output. If failures or warnings are reported, investigate them and correct them as necessary. Repeat the validation until no important issues are reported.\
   The `wekachecker` writes any failures or warnings to the file: **`test_results.txt`**.

Once the report has no failures or warnings that must be fixed, you can install the WEKA software.

<details>

<summary><strong>wekachecker report example</strong></summary>

```
Dataplane IP Jumbo Frames/Routing test                       [PASS]
Check ssh to all hosts                                       [PASS]
Verify timesync                                              [PASS]
Check if OS has SELinux disabled or in permissive mode       [PASS]
Check OS Release...                                          [PASS]
Check /opt/weka for sufficient capacity...                   [WARN]
Check available RAM...                                       [PASS]
Check if internet connection available...                    [PASS]
Check for NTP...                                             [PASS]
Check DNS configuration...                                   [PASS]
Check Firewall rules...                                      [PASS]
Check for WEKA Required Packages...                          [PASS]
Check for OFED Required Packages...                          [PASS]
Check for Recommended Packages...                            [WARN]
Check if HT/AMT is disabled                                  [WARN]
Check if kernel is supported...                              [PASS]
Check if CPU has AES enabled and supported                   [PASS]
Check if Network Manager is disabled                         [WARN]
Checking if Numa balancing is enabled                        [WARN]
Checking RAM state for errors                                [PASS]
Check for XFS FS type installed                              [PASS]
Check if Mellanox OFED is installed                          [PASS]
Check for consistent IOMMU                                   [PASS]
Check for rpcbind enabled                                    [PASS]
Check for squashfs enabled                                   [PASS]
Check for /tmp noexec mount                                  [PASS]

RESULTS: 21 Tests Passed, 0 Failed, 5 Warnings
```

</details>

## What to do next?

If you can use the WEKA Configurator, go to:

Otherwise, go to:

<!-- ============================================ -->
<!-- File 7/259: planning-and-installation_bare-metal_setting-up-the-hosts_sr-iov-enablement.md -->
<!-- ============================================ -->

# Enable the SR-IOV

Many hardware vendors ship their products with the SR-IOV feature disabled. The feature must be enabled on such platforms before installing the Weka system. Enabling the SR-IOV applies to the server BIOS.

If the SR-IOV is already enabled, it is recommended to verify the current state before proceeding with the installation of the WEKA system.

## Before you begin

Verify that the NIC drivers are installed and loaded successfully. If it still needs to be done, perform the [Install NIC drivers](..#install-nic-drivers) procedure.

## Enable SR-IOV in the server BIOS

The following procedure is a vendor-specific example and is provided as a courtesy. Depending on the vendor, the same settings may appear differently or be located elsewhere. Therefore, refer to your hardware platform and NIC vendor documentation for the latest information and updates.

Reboot the server and enter the BIOS Setup.

From the Advanced menu, select the PCIe Configuration to display its properties.

Select the SR-IOV support and enable it.

Save the configuration changes and exit.

<!-- ============================================ -->
<!-- File 8/259: planning-and-installation_bare-metal_setting-up-the-hosts_broadcom-adapter-setup-for-weka-system.md -->
<!-- ============================================ -->

# Broadcom adapter setup for WEKA system

Before a WEKA system can use a Broadcom adapters, the server must have the necessary drivers and firmware from Broadcom's download center.

## **Set up the drivers and software**

**Procedure:**

1. **Prerequisites:** Enable SR-IOV in both the BIOS and the NIC settings. See .
2. **Download software bundle**: Access Broadcom's download center and download the software bundle onto the target server. Carefully review the instructions included in the bundle.
3. **Compile and install**: Follow the provided instructions to compile and install the following components:
   * `bnxt_en` driver.
   * `sliff` driver.
   * `niccli` command line utility.
4. **Post-installation steps**: After installation, run one of the following commands based on the Linux distribution:
   * `dracut -f`
   * `update-initramfs -u`
5. **Reboot the server**: Reboot the server to apply the changes.

## **Install the firmware**

After installing Broadcom drivers and software, install the firmware included in the download bundle. Firmware files are typically named after the adapter they are intended for, such as `BCM957508-P2100G.pkg`.

**Procedure:**

1. **Identify the target adapter**: Use the command `niccli --list` to list Broadcom adapters and identify the target adapter by its decimal device number:

```shell
# niccli --list
----------------------------------------------------------------------------
Scrutiny NIC CLI v227.0.130.0 - Broadcom Inc. (c) 2023 (Bld-61.52.25.90.16.0)
----------------------------------------------------------------------------
     BoardId     MAC Address        FwVersion    PCIAddr      Type   Mode
  1) BCM57508    84:16:0A:3E:0E:20  224.1.102.0  00:0d:00:00  NIC    PCI
  2) BCM57508    84:16:0A:3E:0E:21  224.1.102.0  00:0d:00:01  NIC    PCI
```

2. **Identify the device**: From the `niccli --list` output, choose the device identifier (for example, `1` for `BCM57508`).

```
# niccli -dev 1 install BCM957508-P2100G.pkg
```

3. **Confirm and complete the installation**: Follow the prompts to confirm and complete the firmware update.

```

```shell
Broadcom NetXtreme-C/E/S firmware update and configuration utility version v227.0.120.0
NetXtreme-E Controller #1 at PCI Domain:0000 Bus:3b Dev:00 Firmware on NVM - v224.1.102.0
NetXtreme-E Controller #1 will be updated to firmware version v227.1.111.0

Do you want to continue (Y/N)?y

NetXtreme-C/E/S Controller #1 is being updated....................................................
Firmware update is completed.
A system reboot is needed for the firmware update to take effect.
```

```

4. **Reboot the server**: Reboot the server to apply the firmware update.

## **Update NVM settings**

To enable WEKA system compatibility, configure certain NVM options to increase the number of Virtual Functions (VFs) and enable TruFlow.

**Procedure:**

1.  **Increase the number of VFs to 64**: Run the following commands:

    ```shell
    niccli -dev 1 nvm -setoption enable_sriov -value 1
    niccli -dev 1 nvm -setoption number_of_vfs_per_pf -scope 0 -value 0x40
    niccli -dev 1 nvm -setoption number_of_vfs_per_pf -scope 1 -value 0x40
    ```
2.  **Enable TruFlow**: Run the following commands:

    ```shell
    niccli -dev 1 nvm -setoption enable_truflow -scope 0 -value 1
    niccli -dev 1 nvm -setoption enable_truflow -scope 1 -value 1
    ```
3.  **Additional configuration for BCM57508-P2100G**: Run the following command:

    ```shell
    niccli -dev 1 nvm -setoption afm_rm_resc_strategy -value 1
    ```
4. **Reboot the server**: Reboot the server to apply the changes.

The adapter is ready for use by the WEKA system.

## Configure Broadcom P2100G adapters for 200 Gbps operation

Convert a dual-port 100 Gbps Broadcom P2100G adapter into a single-port 200 Gbps configuration by consolidating both ports. This mode is optimal for high-throughput applications requiring maximum bandwidth on a single interface.

#### Procedure

1.  **Determine NIC index:** Run the following command to list all installed NICs and determine the correct index (`<index>`) of the target P2100G adapter:

    ```
    niccli list
    ```
2.  **Set link speed and port:** Use the Linux `niccli` tools to set the default link speed to 200G for both the driver (when the OS is running) and the firmware (PXE boot), and hide the second network port with the following commands:

    ```
    niccli -i <index> nvm --setoption firmware_link_speed_d0 --scope 0 --value 0x07
    niccli -i <index> nvm --setoption port_operation_mode --value
    ```

<!-- ============================================ -->
<!-- File 9/259: planning-and-installation_bare-metal_manually-install-os-and-weka-on-servers.md -->
<!-- ============================================ -->

---
description:
---

# Manually install OS and WEKA on servers

If you are not using the WMS or WSA automated tools for installing a WEKA cluster, manually install a supported OS and the WEKA software on the bare metal server.

Note: For optimal server performance and configuration, it is recommended to use the `bios_tool` to set BIOS settings on your servers.
Refer to the Appendix:  for detailed instructions on using `bios_tool` to ensure that your servers meet the recommended BIOS configurations. Using this tool can significantly streamline the setup process and save time.

**Procedure**

1. Follow the relevant Linux documentation to install the operating system, including the required packages.

**Required packages**

 | RHEL and derivatives | Ubuntu |
 | --- | --- |
 | elfutils-libelf-devel gcc glibc-headers glibc-devel make perl rpcbind xfsprogs kernel-devel sssd | libelf-dev linux-headers-$(uname -r) gcc make perl rpcbind xfsprogs sssd |

<details>

<summary>Recommended packages for remote support and maintenance</summary>

#### RHEL and derivatives

```
@network-tools
@large-systems
@hardware-monitoring
bind-utils
elfutils
ipmitool
kexec-tools
nvme-cli
python3
yum-utils
sysstat
telnet
nmap
git
sshpass
lldpd
fio
numactl
numactl-devel
libaio-devel
hwloc
tmux
pdsh
pdsh-rcmd-ssh
pdsh-mod-dshgroup
tmate
iperf
htop
nload
screen
ice
```

#### Ubuntu

```
elfutils
fio
git
hwloc
iperf
ipmitool
kexec-tools
jk
ldap-client
libaio-dev
lldpd
nfs-client
nload
nmap
numactl
nvme-cli
pdsh
python3
sshpass
sysstat
tmate
```

</details>

2. Install the WEKA software.
   * Once the WEKA software tarball is downloaded from get.weka.io, run the untar command.
   * Run the `install.sh` command on each server, following the instructions in the **Install** tab of get.weka.io.

Once completed, the WEKA software is installed on all the allocated servers and runs in stem mode (no cluster is attached).

Note: If a failure occurs during the WEKA software installation process, an error message prompts detailing the source of the failure. Review the details and try to resolve the failure. If required, contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team).

**Related topic**

#operating-system (on the Prerequisites and compatibility topic)

## What to do next?

<!-- ============================================ -->
<!-- File 10/259: planning-and-installation_bare-metal_configure-the-weka-cluster-using-the-weka-configurator.md -->
<!-- ============================================ -->

---
description:
---

# Configure the WEKA cluster using the WEKA Configurator

The WEKA Configurator tool facilitates cluster configuration. It performs the following:

* Scans your environment to detect the network, verifies various attributes such as hostnames, and discovers components such as gateway routers.
* Selects the servers that can be included in the cluster and verifies that all servers run the same WEKA version.
* Guides you through the configuration options.
* Generates a valid configuration file that you can apply to form a WEKA cluster from a group of servers.

## Before you begin

Adhere to the following concepts:

* **STEM mode:** STEM mode is the initial state before configuration. The term STEM comes from the concept of stem cells in biology, which are undifferentiated. In WEKA clusters, STEM mode carries the same connotation of being an undifferentiated state.
* **Reference host:** The `wekaconfig` normally runs on one of the servers designated as part of the final cluster. The server that `wekaconfig` runs on is called the reference host. When `wekaconfig` runs, it expects to find a group of servers in STEM mode. If the reference host is not in STEM mode, an error message is issued, and the program terminates.
* **Same networks:** It is assumed that all other servers forming the cluster are connected to the same networks as the `reference host` and have the same configuration (all servers have a homogeneous hardware configuration).
* **Homogeneous configuration:** Two or more servers with the same core count, RAM size, number and size of drives, and network configurations are considered homogeneous.
  * It is best practice to create the WEKA cluster from a group of homogeneous servers (it is typically the case because the hardware is typically purchased all at the same time). `wekaconfig` checks if the servers are homogeneous; if they are not, it points out the discrepancies (such as varying numbers of drives, RAM, or cores).
  * `wekaconfig` allows the configuration of heterogeneous clusters. However, because most times, the servers are supposed to be homogeneous, it can be an error that they are not. For example, if one of the drives is defective (DOA) from the factory or a memory stick is defective. These hardware issues are uncommon and can be difficult to discover in large clusters.
* **Passwordless ssh connection:** Enabling passwordless ssh between all the servers is very convenient and makes most tools work more smoothly. At a minimum, a regular user with passwordless `sudo` privileges and passwordless ssh is required for configuration. However, it is most convenient to have the `root` user has passwordless ssh, even if only temporarily during configuration.\
  Ensure you can ssh without a password by doing an ssh to each server.
* **Stripe width:** A RAID-like concept refers to the total width of the data stripe for data protection mechanisms. Typically, the DATA and PARITY combined are the stripe width. In WEKA terms, the stripe width must be less than the total number of servers in the cluster. For example, in a 10-server cluster, the stripe width can be 9 (7 data + 2 parity) plus 1 spare.

## Prerequisites

* **System preparation validation:**\
  Ensure the system preparation is validated using the `wekachecker` tool (on WSA installations this is already installed under `/opt/weka/tools`). For additional details, refer to the #id-11.-validate-the-system-preparation section.
* **WEKA software installation on cluster servers:**\
  Verify that the WEKA software is installed on all cluster servers. If it is not installed through the WSA, follow the installation instructions provided in the **Install** tab of get.weka.io. Once the installation is complete, the WEKA software will be deployed on all allocated servers and running in STEM mode.

## Workflow

1. Access the WEKA tools.
2. Configure a WEKA cluster with the WEKA Configurator.
3. Apply the configuration (`config.sh`).

### 1. Access the WEKA tools

1.  **If you do not use WSA or WEKApod**:\
    Download the WEKA tools repository to one of the servers by running the following command:

    ```bash
    git clone https://github.com/weka/tools
    ```
2.  **If you use WSA or WEKApod**:\
    The tools are pre-installed and can be found at the following location:

    ```bash
    /opt/tools/install
    ```

### 2. Configure a WEKA cluster with the WEKA Configurator

1. **Run the `wekaconfig` Tool:**
   1. Connect to one of the backend servers or the WMS server (if it exists) using SSH.
   2.  Navigate to the tools directory directory:

       ```bash
       cd tools/install
       - or -
       cd opt/tools/install
       ```
   3.  Run the `wekaconfig` tool:

       ```bash
       ./wekaconfig
       ```

The `wekaconfig` tool scans the environment, detects the servers, and evaluates whether the group of servers is homogeneous. The following example demonstrates a scenario where the servers do not have a homogeneous number of CPU cores.

2. **Review the detection results:**
   1. If the detected configuration meets your requirements, press **Enter** to proceed.
   2. Navigate through the available tabs to configure the WEKA settings as needed.

The `wekaconfig` displays the data plane networks (DP Networks) detected previously.  The list under **Select DP Networks** reflects the high-speed (100Gb+) networks used for the WEKA storage traffic.

Verify that the list of networks, speed, and number of detected hosts are correct.

If the values are not as expected, such as an incorrect number of servers, incorrect or missing networks, investigate it and check the messages. Typically, network configuration issues are the source of the problem.

Select the required networks to configure WEKA POSIX protocol to run on.

Use the arrow and Tab keys to move between the fields and sections, and the space-bar to select the value.

**Note:** The green labels have entry fields. The yellow labels have read-only fields.

Press Tab to move to the **Hosts** section.

`wekaconfig` pre-populated the hostnames of the servers that are on this network and running the same version of WEKA and are in STEM mode.

Use the arrow keys to move between the servers, and space bar to select or deselect specific servers. Press Tab to accept values and move to the next field: High Availability.

High Availability (HA) is used for networks with more than one network interface.

In this example, only one network is selected, so the HA default is No. When there are two or more networks selected, you can change the the HA option to suit your needs. Consult the WEKA Customer Success Team before changing this default value.

Press Tab to accept value and move to the next field: Multicontainer. The default is Yes and it is mandatory from WEKA version 4.1.

Press Tab to move to the lower-right. Use the arrow to move to **Next**. Then, press the space-bar.

This page shows the following sections:

* Host Configuration Reference
* Bias
* Cores details

#### Host Configuration Reference

This section shows the `reference host` cores and drives configuration, and the total number of hosts (servers).

#### Bias

The Bias options determine the optimal CPU core and memory allocation scheme.

* **Enable Protocols:** If you intend to use the cluster for NFS, SMB, or S3 protocols, select this option. This option reserves some CPU and memory for the protocols.
* **Protocols are Primary:** If you intend to use the cluster primarily or heavily with NFS, SMB, or S3 protocols, select this option. It reserves more CPU and memory (then in the first option) for the protocols .
* **DRIVES over COMPUTE:** In high-core-count configurations (48+ cores), the standard algorithm for determining optimal core allocations may reduce the drive:core ratio in favor of additional COMPUTE cores. This bias setting favors a DRIVE core allocation of 1:1 (if possible) over additional COMPUTE cores. For advice on core allocations, consult with the Customer Success Team if you are configuring high-core-count systems.

#### **Core details**

`wekaconfig` suggests a reasonable set of core allocations (FE/COMPUTE/DRIVES) depending on your selections. You may override these values as needed.

* **Cores for OS:** The number of cores reserved for the OS (fixed at 2).
* **Cores for Protocols:** The number of cores reserved for protocols. It depends on the selected Bias option.
* **Usable Weka Cores:** The number of cores can be used for FE, COMPTE, and DRIVES processes.
* **Used Weka Cores:** The number of cores selected for use as FE, COMPUTE, or DRIVES cores.

The **Usable Weka Cores** and **Available Weka Cores** read-only fields are updated as you make changes so you can ensure you are not exceeding the number of available cores as you change any values. This is an advanced feature, and core allocation must not be changed without consulting the Customer Success Team.

Move to the Cluster Name field and set a unique name for your WEKA cluster.

The stripe and other settings include:

* **Data Drives:** The number of data members in the Stripe Width.
* **Parity Drives:** The number of parity members.
* **Hot Spares:** The number of Hot Spare members.
* **Reserved RAM per Host:** Extra RAM in GB reserved on each host for various purposes, like supporting Protocols or Applications.

These settings are in terms of servers, not SSDs. WEKA stripes over the entire servers, not over individual drives. For more details, see .

The following example shows a stripe width of 6 (4+2) on 7 servers, and one hot spare.

3.  **Finalize the configuration:**

    After completing the WEKA configuration, use the arrow keys to select **Done** and press **Enter**.

    The `wekaconfig` tool generates the `config.sh` file based on your settings.

<details>

<summary><code>config.sh</code> output example</summary>

```bash
#!/bin/bash
usage() {
\techo "Usage: $0 [--no-parallel]"
\techo "  Use --no-parallel to prevent parallel execution"
\texit 1
}

para() {
\tTF=$1; shift
\techo $*
\t$* &
\t#[ !$TF ] && { echo para waiting; wait; }
\t[ $TF == "FALSE" ] && { echo para waiting; wait; }
}

PARA="TRUE"

# parse args
if [ $# != 0 ]; then
\tif [ $# != 1 ]; then
\t\tusage
\telif [ $1 == "--no-parallel" ]; then
\t\tPARA="FALSE"
\telse
\t\techo "Error: unknown command line switch - $1"
\t\tusage
\tfi
fi

echo starting - PARA is $PARA

# ------------------ custom script below --------------

echo Stopping weka on weka63
para ${PARA} scp -p ./resources_generator.py weka63:/tmp/
para ${PARA} ssh weka63 "sudo weka local stop; sudo weka local rm -f default"
echo Stopping weka on weka64
para ${PARA} scp -p ./resources_generator.py weka64:/tmp/
para ${PARA} ssh weka64 "sudo weka local stop; sudo weka local rm -f default"
echo Stopping weka on weka65
para ${PARA} scp -p ./resources_generator.py weka65:/tmp/
para ${PARA} ssh weka65 "sudo weka local stop; sudo weka local rm -f default"
echo Stopping weka on weka66
para ${PARA} scp -p ./resources_generator.py weka66:/tmp/
para ${PARA} ssh weka66 "sudo weka local stop; sudo weka local rm -f default"
echo Stopping weka on weka67
para ${PARA} scp -p ./resources_generator.py weka67:/tmp/
para ${PARA} ssh weka67 "sudo weka local stop; sudo weka local rm -f default"
echo Stopping weka on weka68
para ${PARA} scp -p ./resources_generator.py weka68:/tmp/
para ${PARA} ssh weka68 "sudo weka local stop; sudo weka local rm -f default"
echo Stopping weka on weka69
para ${PARA} scp -p ./resources_generator.py weka69:/tmp/
para ${PARA} ssh weka69 "sudo weka local stop; sudo weka local rm -f default"

wait
echo Running Resources generator on host weka63
para ${PARA} ssh weka63 sudo /tmp/resources_generator.py -f --path /tmp --net ib0/10.1.1.63/16 --compute-dedicated-cores 15 --drive-dedicated-cores 6 --frontend-dedicated-cores 1
echo Running Resources generator on host weka64
para ${PARA} ssh weka64 sudo /tmp/resources_generator.py -f --path /tmp --net ib0/10.1.1.64/16 --compute-dedicated-cores 15 --drive-dedicated-cores 6 --frontend-dedicated-cores 1
echo Running Resources generator on host weka65
para ${PARA} ssh weka65 sudo /tmp/resources_generator.py -f --path /tmp --net ib0/10.1.1.65/16 --compute-dedicated-cores 15 --drive-dedicated-cores 6 --frontend-dedicated-cores 1
echo Running Resources generator on host weka66
para ${PARA} ssh weka66 sudo /tmp/resources_generator.py -f --path /tmp --net ib0/10.1.1.66/16 --compute-dedicated-cores 15 --drive-dedicated-cores 6 --frontend-dedicated-cores 1
echo Running Resources generator on host weka67
para ${PARA} ssh weka67 sudo /tmp/resources_generator.py -f --path /tmp --net ib0/10.1.1.67/16 --compute-dedicated-cores 15 --drive-dedicated-cores 6 --frontend-dedicated-cores 1
echo Running Resources generator on host weka68
para ${PARA} ssh weka68 sudo /tmp/resources_generator.py -f --path /tmp --net ib0/10.1.1.68/16 --compute-dedicated-cores 15 --drive-dedicated-cores 6 --frontend-dedicated-cores 1
echo Running Resources generator on host weka69
para ${PARA} ssh weka69 sudo /tmp/resources_generator.py -f --path /tmp --net ib0/10.1.1.69/16 --compute-dedicated-cores 15 --drive-dedicated-cores 6 --frontend-dedicated-cores 1
wait
echo Starting Drives container on server weka63
para ${PARA} ssh weka63 "sudo weka local setup container --name drives0 --resources-path /tmp/drives0.json"
echo Starting Drives container on server weka64
para ${PARA} ssh weka64 "sudo weka local setup container --name drives0 --resources-path /tmp/drives0.json"
echo Starting Drives container on server weka65
para ${PARA} ssh weka65 "sudo weka local setup container --name drives0 --resources-path /tmp/drives0.json"
echo Starting Drives container on server weka66
para ${PARA} ssh weka66 "sudo weka local setup container --name drives0 --resources-path /tmp/drives0.json"
echo Starting Drives container on server weka67
para ${PARA} ssh weka67 "sudo weka local setup container --name drives0 --resources-path /tmp/drives0.json"
echo Starting Drives container on server weka68
para ${PARA} ssh weka68 "sudo weka local setup container --name drives0 --resources-path /tmp/drives0.json"
echo Starting Drives container on server weka69
para ${PARA} ssh weka69 "sudo weka local setup container --name drives0 --resources-path /tmp/drives0.json"

wait

sudo weka cluster create weka63 weka64 weka65 weka66 weka67 weka68 weka69 --host-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 -T infinite
echo Starting Compute container 0 on host weka63
para ${PARA} ssh weka63 sudo weka local setup container --name compute0 --resources-path /tmp/compute0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.63
echo Starting Compute container 0 on host weka64
para ${PARA} ssh weka64 sudo weka local setup container --name compute0 --resources-path /tmp/compute0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.64
echo Starting Compute container 0 on host weka65
para ${PARA} ssh weka65 sudo weka local setup container --name compute0 --resources-path /tmp/compute0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.65
echo Starting Compute container 0 on host weka66
para ${PARA} ssh weka66 sudo weka local setup container --name compute0 --resources-path /tmp/compute0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.66
echo Starting Compute container 0 on host weka67
para ${PARA} ssh weka67 sudo weka local setup container --name compute0 --resources-path /tmp/compute0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.67
echo Starting Compute container 0 on host weka68
para ${PARA} ssh weka68 sudo weka local setup container --name compute0 --resources-path /tmp/compute0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.68
echo Starting Compute container 0 on host weka69
para ${PARA} ssh weka69 sudo weka local setup container --name compute0 --resources-path /tmp/compute0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.69
wait

para ${PARA} sudo weka cluster drive add 0 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1
para ${PARA} sudo weka cluster drive add 1 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1
para ${PARA} sudo weka cluster drive add 2 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1
para ${PARA} sudo weka cluster drive add 3 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1
para ${PARA} sudo weka cluster drive add 4 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1
para ${PARA} sudo weka cluster drive add 5 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1
para ${PARA} sudo weka cluster drive add 6 /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1

wait
sudo weka cluster update --data-drives=4 --parity-drives=2
sudo weka cluster hot-spare 1
sudo weka cluster update --cluster-name=fred

echo Starting Front container on host weka63
para ${PARA} ssh weka63 sudo weka local setup container --name frontend0 --resources-path /tmp/frontend0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.63
echo Starting Front container on host weka64
para ${PARA} ssh weka64 sudo weka local setup container --name frontend0 --resources-path /tmp/frontend0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.64
echo Starting Front container on host weka65
para ${PARA} ssh weka65 sudo weka local setup container --name frontend0 --resources-path /tmp/frontend0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.65
echo Starting Front container on host weka66
para ${PARA} ssh weka66 sudo weka local setup container --name frontend0 --resources-path /tmp/frontend0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.66
echo Starting Front container on host weka67
para ${PARA} ssh weka67 sudo weka local setup container --name frontend0 --resources-path /tmp/frontend0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.67
echo Starting Front container on host weka68
para ${PARA} ssh weka68 sudo weka local setup container --name frontend0 --resources-path /tmp/frontend0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.68
echo Starting Front container on host weka69
para ${PARA} ssh weka69 sudo weka local setup container --name frontend0 --resources-path /tmp/frontend0.json --join-ips=10.1.1.63,10.1.1.64,10.1.1.65,10.1.1.66,10.1.1.67,10.1.1.68,10.1.1.69 --management-ips=10.1.1.69

wait
echo Configuration process complete
```

</details>

Note: Advanced users can customize the configuration by editing the `config.sh` file using a text editor such as `vim` or `nano`. If modifications are required, consult the Customer Success Team for guidance.
Regarding drive selection: it is not possible to manually select the data drives (NVMe SSDs) for inclusion in the configuration. WEKA clusters are typically dedicated to running WEKA services and are designed to be homogeneous. As a result, the `wekaconfig` tool automatically includes all NVMe drives that are larger than approximately 1.5 GB in size.
To modify the drives used in the cluster, manually edit the `config.sh` file. Refer to the example output for `config.sh` provided above for guidance.

### 2. Apply the configuration

* From the install directory, run `./config.sh`.

The configuration takes a few minutes and possibly longer for large clusters. See some examples of the configuration process and WEKA status.

## What to do next?

<!-- ============================================ -->
<!-- File 11/259: planning-and-installation_bare-metal_manually-configure-the-weka-cluster-using-the-resource-generator.md -->
<!-- ============================================ -->

---
description:
---

# Manually configure the WEKA cluster using the resources generator

Perform this workflow using the resources generator only if you are not using the automated WMS, WSA, or WEKA Configurator.

The resources generator generates three resource files on each server in the `/tmp` directory: `drives0.json`, `compute0.json`, and `frontend0.json`. Then, you create the containers using these generated files of the cluster servers.

## Before you begin

1. Download the resources generator from the GitHub repository to your local server: https://github.com/weka/tools/blob/master/install/resources_generator.py.

Example:

```
wget https://raw.githubusercontent.com/weka/tools/master/install/resources_generator.py

```

2. Copy the resources generator from your local server to all servers in the cluster.

Example for a cluster with 8 servers:

```
for i in {0..7}; do scp resources_generator.py weka0-$i:/tmp/resources_generator.py; done

```

2. To enable execution, change the mode of the resources generator on all servers in the cluster.

Example for a cluster with 8 servers:

```
pdsh -R ssh -w "weka0-[0-7]" 'chmod +x /tmp/resources_generator.py'

```

## Workflow

1. Remove the default container
2. Generate the resource files
3. Create drive containers
4. Create a cluster
5. Configure the SSD drives
6. Create compute containers
7. Create frontend containers
8. Configure number of data and parity drives
9. Configure number of hot spares
10. Name the cluster

### 1. Remove the default container

**Command:** `weka local stop default && weka local rm -f default`

Stop and remove the auto-created default container created on each server.

### 2. Generate the resource files

**Command:** `resources_generator.py`

To generate the resource files for the drive, compute, and frontend processes, run the following command on each backend server:

`./resources_generator.py --net <net-devices> [options]`

The resources generator allocates the number of cores, memory, and other resources according to the values specified in the parameters.

The best practice for resources allocation is as follows:

* 1 drive core per NVMe device (SSD).
* 2-3 compute cores per drive core.
* 1-2 frontend cores if deploying a protocol container. If there is a spare core, it is used for a frontend container.
* Minimum of 1 core for the OS.

#### Example 1: according to the best practice

For a server with **24** cores and 6 SSDs, allocate 6 drive cores and 12 compute cores, and optionally you can use 2 cores of the remaining cores for the frontend container. The OS uses the remaining 4 cores.

Run the following command line:\
`./resources_generator.py --net eth1 eth2 --drive-dedicated-cores 6 --compute-dedicated-cores 12 --frontend-dedicated-cores 2`

#### Example 2: a server with a limited number of cores

For a server with **14** cores and 6 SSDs, allocate 6 drive cores and 6 compute cores, and optionally you can use 1 core of the remaining cores for the frontend container. The OS uses the remaining 1 core.

Run the following command line:\
`./resources_generator.py --net eth1 eth2 --drive-dedicated-cores 6 --compute-dedicated-cores 6 --frontend-dedicated-cores 1`

Note: Contact Professional Services for the recommended resource allocation settings for your system.

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | compute-core-ids | Specify the CPUs to allocate for the compute processes.Format: space-separated numbers |  |
 | compute-dedicated-cores | Specify the number of cores to dedicate for the compute processes. | The maximum available cores |
 | compute-memory | Specify the total memory to allocate for the compute processes.Format: value and unit without a space.Examples: 1024B, 10GiB, 5TiB. | The maximum available memory |
 | core-ids | Specify the CPUs to allocate for the WEKA processes.Format: space-separated numbers. |  |
 | drive-core-ids | Specify the CPUs to allocate for the drive processes.Format: space-separated numbers. |  |
 | drive-dedicated-cores | Specify the number of cores to dedicate for the drive processes. | 1 core per each detected drive |
 | drives | Specify the drives to use. This option overrides automatic detection.Format: space-separated strings. | All unmounted NVME devices |
 | frontend-core-ids | Specify the CPUs to allocate for the frontend processes.Format: space-separated numbers. | - |
 | frontend-dedicated-cores | Specify the number of cores to dedicate for the frontend processes. | 1 |
 | max-cores-per-container | Override the default maximum number of cores per container for IO processes (19). If provided, the new value must be lower. | 19 |
 | minimal-memory | Set each container's hugepages memory to 1.4 GiB * number of IO processes on the container. |  |
 | net* | Specify the network devices to use.Format: space-separated strings. |  |
 | no-rdma | Don't take RDMA support into account when computing memory requirements. | False |
 | num-cores | Override the auto-deduction of the number of cores. | All available cores |
 | path | Specify the path to write the resource files. | '.' |
 | spare-cores | Specify the number of cores to leave for OS and non-WEKA processes. | 1 |
 | spare-memory | Specify the memory to reserve for non-WEKA requirements.Argument format: a value and unit without a space.Examples: 10GiB, 1024B, 5TiB. | The maximum between 8 GiB and 2% of the total RAM |
 | weka-hugepages-memory | Specify the memory to allocate for compute, frontend, and drive processes.Argument format: a value and unit without a space.Examples: 10GiB, 1024B, 5TiB. | The maximum available memory |

### 3. Create drive containers

**Command:** `weka local setup container`

For each server in the cluster, create the drive containers using the resources generator output file `drives0.json`.

The drives JSON file includes all the required values for creating the drive containers. Only the path to the JSON resource file is required (before cluster creation, the optional parameter `join-ips`  is not relevant).

```
weka local setup container --resources-path <resources-path>/drives0.json
```

**Parameters**

 | Name | Value |
 | ------------------ | ---------------------------------- |
 | `resources-path`* | A valid path to the resource file. |

### 4. Create a cluster

**Command:** `weka cluster add`

To create a cluster of the allocated containers, use the following command:

```
| weka cluster add <hostnames> [--host-ips <ips | ip+ip+ip+ip>] |
```

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | hostnames* | Hostnames or IP addresses.If port 14000 is not the default for the drives, you can specify hostnames:port or ips:port.Minimum cluster size: 6Format: Space-separated strings |  |
 | host-ips | IP addresses of the management interfaces. Use a list of ip+ip address pairs for HA configuration.If the cluster is connected to both IB and Ethernet, you can specify up to four management IPs (ip+ip+ip+ip) for redundancy across both networks.Format: Comma-separated IP addresses. | IP of the first network device of the container |

Note: **Notes:**
* The command preserves existing valid IP configurations in the host's `resource.json` file, only modifying entries that are empty or contain the default local address `127.0.0.1`. This allows preconfigured addresses in the resource file to be used without requiring manual input. To force overwriting the `ips` field, use the\
`--overwrite_resource_ips` flag.
* It is possible to use a hostname or an IP address. This string serves as the container's identifier in subsequent commands.
* If a hostname is used, ensure the hostname to IP resolution mechanism is reliable.
* Once the cluster creation is successfully completed, the cluster is in the initialization phase, and some commands can only run in this phase.
* To configure high availability (HA), at least two cards must be defined for each container.
* On successful completion of the formation of the cluster, every container receives a container-ID. To display the list of the containers and IDs, run `weka cluster container`.
* In IB installations the `--containers-ips` parameter must specify the IP addresses of the IPoIB interfaces.

### 5. Configure the SSD drives

**Command:** `weka cluster drive add`

To configure the SSD drives on each server in the cluster, or add multiple drive paths, use the following command:

```
weka cluster drive add <container-id> <device-paths>
```

**Parameters**

 | Name | Value |
 | --- | --- |
 | container-id* | The Identifier of the drive container to add the local SSD drives. |
 | device-paths* | List of block devices that identify local SSDs. It must be a valid Unix network device name.Format: Space-separated strings.Example, /dev/nvme0n1 /dev/nvme1n1 |

### 6. Create compute containers

**Command:** `weka local setup container`

For each server in the cluster, create the compute containers using the resources generator output file `compute0.json`.

```
weka local setup container --join-ips <IP addresses> --resources-path <resources-path>/compute0.json
```

**Parameters**

 | Name | Value |
 | --- | --- |
 | resources-path* | A valid path to the resource file. |
 | join-ips | IP:port pairs for the management processes to join the cluster. In the absence of a specified port, the command defaults to using the standard WEKA port 14000. Set the values, only if you want to customize the port.To restrict the client‚Äôs operations to only the essential APIs for mounting and unmounting operations, connect to WEKA clusters through TCP base port + 3 (for example, 14003).The IP:port value must match the value used to create the container.Format: comma-separated IP addresses.Example: --join-ips 10.10.10.1,10.10.10.2,10.10.10.3:15000 |

### 7.  Create frontend containers

**Command:** `weka local setup container`

For each server in the cluster, create the frontend containers using the resources generator output file `frontend0.json`.

```

```bash
weka local setup container --join-ips <IP addresses> --resources-path <resources-path>/frontend0.json
```

```

**Command example for installing a stateful client with restricted privileges**

```

```bash
weka local setup container --client --auto-remove-timeout <auto-remove-timeout> --restricted --join-ips <IP addresses> --resources-path <resources-path>/frontend0.json
```

```

**Parameters**

 | Name | Value |
 | --- | --- |
 | resources-path* | A valid path to the resource file. |
 | join-ips | IP:port pairs for the management processes to join the cluster. In the absence of a specified port, the command defaults to using the standard WEKA port 14000. Set the values, only if you want to customize the port.Format: comma-separated IP addresses.Example: --join-ips 10.10.10.1,10.10.10.2,10.10.10.3:15000 |
 | client | Set the container as a client. |
 | auto-remove-timeout | Specify timeout (in seconds) for automatically removing inactive client containers. Only applicable when used with the --client flag. |
 | restricted | Set a client container with restricted privileges as a regular user regardless of the logged-in role. |

### 8.  Configure the number of data and parity drives

**Command:** `weka cluster update --data-drives=<count> --parity-drives=<count>`

**Example:** `weka cluster update --data-drives=4 --parity-drives=2`

### 9.  Configure the number of hot spares

**Command:** `weka cluster hot-spare <count>`

**Example:** `weka cluster hot-spare 1`

### 10. Name the cluster

**Command:** `weka cluster update --cluster-name=<cluster name>`

## What to do next?

<!-- ============================================ -->
<!-- File 12/259: planning-and-installation_bare-metal_manually-configure-the-weka-cluster-using-the-resource-generator_vlan-tagging-in-the-weka-system.md -->
<!-- ============================================ -->

---
description:
---

# VLAN tagging in the WEKA system

## Overview

WEKA support for IEEE 802.1Q VLAN encapsulation ("tagged VLAN IDs" or "tagged VLANs") enables isolation and segregation of network traffic while still granting connectivity between WEKA clients and backend servers.

A WEKA tenant cluster operates on a single VLAN ID, where all containers within that cluster must use the same VLAN ID. All clients connecting to a tenant cluster must use that cluster's assigned VLAN ID. When using the WEKA Kubernetes Operator, this VLAN ID consistency between the tenant cluster and its clients is automatically maintained.

In multi-cluster environments, each tenant cluster can operate on a different VLAN ID. For example, you can assign VLAN ID 100 to Tenant Cluster A and VLAN ID 200 to Tenant Cluster B, providing network isolation between clusters.

## **Enable WEKA tagged VLAN support**

To enable WEKA tagged VLAN support, add the desired VLAN IDs to the switch ports connected to WEKA backends. It is common to include one untagged VLAN and multiple tagged VLAN IDs.

After configuring the switch, update the Linux system interfaces to recognize the VLAN IDs and verify connectivity.

Use the following procedure on each WEKA backend and each WEKA stateful client. You do not need to perform this procedure on WEKA stateless clients because it is automatically applied during the mount command.

**Procedure**

1.  **Assign a VLAN tag to a network interface**\
    Associate a VLAN tag with a NIC using the following command:

    ```sql
    weka local resources net add <nic> --vlan <tag>
    ```

    Example:

    ```sql
    weka local resources net add mlnx0 --vlan 501
    ```
2.  **Add a VLAN interface**\
    Infer the VLAN tag by adding a VLAN interface:

    ```sql
    weka local resources net add vlan<tag>
    ```

    Example:

    ```sql
    weka local resources net add vlan501
    ```
3. **Apply configuration changes**\
   Restart all containers to apply the VLAN configuration updates.

## Confirm which tagged VLAN is attached to the interfaces

To determine which tagged VLANs are configured, query the local resources of a container using a command like this:

```
weka local resources -C <containername>
```

Example:

```bash
weka local resources -C drives0
```

You can also display the cluster container network data with this command by requesting verbose output with `-v` to list the VLAN ID or by using syntax like  `-o id,hostname,name,ips,vlan`:

```
weka cluster container net -v
```

## Mount filesystems with tagged VLANs

When running a mount command on a WEKA stateless client where the backends have a tagged VLAN, specify the same VLAN in the mount command as follows:

### **Basic VLAN tagging**

Mount a filesystem with a specified NIC and VLAN tag:

```bash
mount -o net=<nic>/vlan@<tag> <backendip/filesystem> <mountpoint>
```

Example:

```bash
mount -o net=mlnx0/vlan@501 10.10.0.10/default /mnt/weka
```

### **Extended network configuration**

Include gateway, IP, and netmask for advanced configurations:

```

```bash
mount -o net=<nic>/vlan@<tag>/gw@<gateway>/ip@<ip>/netmask@<netmask> <backendip/filesystem> <mountpoint>
```

```

Example:

```

```bash
mount -o net=mlnx0/vlan@501/gw@192.168.1.1/ip@192.168.1.10/netmask@255.255.255.0 10.10.0.10/default /mnt/weka
```

```

**Syntax guidelines:**

* Include additional named parameters (for example, `gw@`, `ip@`, `netmask@`) directly in the command syntax.
* Alternatively, use the legacy style by specifying name-value pairs after the positional parameters.
* Separate all parameters using `/`.

This syntax is also supported for the `weka local setup container --net ...` command.

<!-- ============================================ -->
<!-- File 13/259: planning-and-installation_bare-metal_install-the-weka-cluster-using-the-wsa.md -->
<!-- ============================================ -->

---
description:
---

# Install the WEKA cluster using the WSA

WSA is a package consisting of a base version of Linux (based on Rocky 8.6), network drivers and other required packages, WEKA software, and various diagnostic and configuration tools. Using the WSA facilitates the post-installation administration, security, and other KB updates controlled and distributed by WEKA, following a Long Term Support (LTS) plan.

The WSA generally works like any OS install disk (Linux/Windows).

Note: Do not attempt to install the WSA using PXE boot. The WSA has a specific kickstart methodology only compatible with WMS or manual boot from ISO.

Note: WEKA releases WSA updates addressing critical security issues found in the underlying Linux distribution within five days of discovery and availability. Customers can update their WSA instance from the repository where these updates are provided. WEKA notifies customers when updates are available, enabling timely updates to minimize potential risks. For any questions, contact the Customer Success Team.
For the update procedure, see .

## WSA deployment prerequisites

A physical server that meets the following requirements:

* **Boot drives:** One or two identical boot drives as an installation target.
  * A system with two identical boot drives has the OS installed on mirrored partitions (LVM).
  * A system with one drive has a simple partition.
* **Minimum boot drive capacity:** 125 GB (to support the pre-defined disk partition map).
* **Boot type:** UEFI.

## Before you begin

Before deploying the WSA, adhere to the following:

* Download the latest release of the WSA package from get.weka.io dashboard.
* The root password is `WekaService`
* The WEKA user password is `weka.io123`
* If errors occur during installation and the installation halts (no error messages appear), use the system console to review the logs in `/tmp`. The primary log is `/tmp/ks-pre.log`.
* To get a command prompt from the Installation GUI, do one of the following:
  * On macOS, type **ctrl+option+f2**
  * On Windows, type **ctrl+alt+f2**.

## WSA deployment workflow

1. Install the WSA
2. Configure the WSA
3. Test the environment
4. Validate the WEKA software installation

### 1. Install the WSA

1. Boot the server from the WSA image. The following are some options to do that:

Copy the WSA image to an appropriate location so that the server‚Äôs BMC can mount it to a virtual CDROM/DVD.

Depending on the server manufacturer, consult the documentation for the server‚Äôs BMC (for example, iLO, iDRAC, and IPMI) for detailed instructions on mounting and booting from a bootable WSA image, such as:

* A workstation or laptop sent to the BMC through the web browser.
* An SMB share in a Windows server or a Samba server.
* An NFS share.

Burn the WSA image to a DVD or USB stick and boot the server from this physical media.

Once you boot the server, the WSA installs the WEKA OS, drivers, WEKA software. and other packages automatically and unattended (no human interaction required).

Depending on network speed, this can take about 10-60 mins (or more) per server.

### 2. Configure the WSA

Once the WSA installation is complete and the server is rebooted, configure the WSA.

Note: Normally, the WEKA Software Appliance is deployed with the help of the WEKA Management Station (WMS), which can be used to complete the configuration of the servers.
However, if not deployed with the WMS, configure the WEKA cluster manually according to the following steps.

1. Log-in to the server using one of the following methods:

* BMC's Console
* Cockpit web interface on port 9090

Username/password: `root`/`WekaService`.

Run the OS through the BMC‚Äôs Console. See the specific manufacturer‚Äôs BMC documentation.

Run the OS through the Cockpit Web Interface on port 9090 of the OS management network.

If you don‚Äôt know the WSA hostname or IP address, go to the console and press the **Return** key a couple of times until it prompts the URL of the WSA OS Web Console (Cockpit) on port 9090.

When the server boots for the first time, the WSA automatically installs the WEKA software on the bare metal servers unattended.

Then the server reboots, it runs with WEKA in STEM mode.

2. Set the following networking details:
   * Hostname
   * IP addresses for network interfaces, including:
     * Server management interface (typically a 1Gb interface on a management network) if not automatically set via DHCP.
     * Dataplane network interfaces (typically 1 or 2. Can be several up to 8).
   * DNS settings and/or an `/etc/hosts` file.
   * Network gateways and routing table adjustments as necessary.
   * Timeserver configuration.

Note: For detailed instructions on setting the configuration options, see general Linux documentation for RedHat-based Linux Distributions.

### 3. Test the environment

Each server has the WEKA Tools pre-installed in `/opt/tools`, including:

* `wekanetperf`: This tool runs `iperf` between the servers to ensure line rate can be achieved.
* `wekachecker`: This tool checks a variety of network settings and more. For details, see #id-11.-validate-the-system-preparation.
* `bios_tool`: This tool helps you to set the required BIOS settings on the servers.

### 4. Validate the WEKA software installation

Verify that the WEKA software is installed and running on the server.

Log-in to the server and run the command `weka status`.

The server provides a status report indicating the system is in STEM mode, and is ready for the cluster configuration.

## What to do next?

<!-- ============================================ -->
<!-- File 14/259: planning-and-installation_bare-metal_install-the-weka-cluster-using-the-wms-with-wsa.md -->
<!-- ============================================ -->

# Install the WEKA cluster using the WMS with WSA

The WEKA Management Station (WMS) is an install kit similar to an OS install disk that simplifies the installation and configuration of the WEKA cluster in an on-premises environment by deploying the WEKA Software Appliance (WSA) package on bare metal servers. The WMS installs the WEKA OS, drivers, and WEKA software automatically and unattended.

The WMS is also used for installing the monitoring tools: Local WEKA Home (LWH), WEKAmon, and SnapTool (for details, see .

Note: WEKA releases WMS and WSA updates addressing critical security issues found in the underlying Linux distribution within five days of discovery and availability. Customers can update their WMS/WSA instance from the repository where these updates are provided. WEKA notifies customers when updates are available, enabling timely updates to minimize potential risks. For any questions, contact the Customer Success Team.
For the update procedure, see .

## WMS deployment prerequisites

Using the WMS with WSA to install a WEKA cluster requires a physical server (or VM) that meets the following requirements:

* **Boot drives:** One or two identical boot drives as an installation target.
  * A system with two identical boot drives has the OS installed on mirrored partitions (LVM).
  * A system with one drive has a simple partition.
* **Minimum boot drive capacity:**
  * If not configuring LWH: SSD 141 GB (131 GiB).
  * If configuring LWH: See the SSD-backed storage requirements section in #1.-verify-prerequisites.
* **Boot type:** UEFI boot.
* **Cores and RAM:**
  * If not configuring LWH: minimum 4 cores and 16 GiB.
  * If configuring LWH, see the Server minimum CPU and RAM requirements section in #1.-verify-prerequisites.
* **Network interface:** 1 Gbps.

### Prerequisites for the target bare metal servers

* Target servers must be **Dell, HPE,** **Supermicro,** or **Lenovo**. Other servers are not supported.
* The RedFish interface must be installed, enabled, and licensed for all target servers.
* The WMS must be able to connect over Ethernet to the following servers‚Äô interfaces:
  * OS management interface, typically 1 Gbps. It must be connected to a switch.
  * Base Management Controller (BMC), such as IPMI, iDRAC, or iLO interfaces. The BMC interface must be configured with an IP address.
* All the servers' dataplane interfaces must be connected to the switches.
* The bare metal servers must conform to the .
* The bare metal servers must have an OS management network interface for administering the servers.
* The boot type must be set to UEFI boot.

Note: For cluster configurations exceeding 25 servers, it‚Äôs advisable to equip the WMS with an ETH interface of superior speed, such as 10/25/50 Gbps, during the installation phase. As an alternative, you could bond two or more 1 Gbps interfaces to increase the bandwidth. Once the installation phase is completed, a bandwidth of 1 Gbps is sufficient.

## Before you begin

Before deploying the WMS, adhere to the following:

* Obtain the WMS package. For details, see .
* The root password is `WekaService`
* The WEKA user password is `weka.io123`
* If errors occur during installation and the installation halts (no error messages appear), use the system console to review the logs in `/tmp`. The primary log is `/tmp/ks-pre.log`.
* To get a command prompt from the Installation GUI, do one of the following:
  * On macOS, type **ctrl+option+f2**
  * On Windows, type **ctrl+alt+f2**.

## WMS deployment workflow

1. Install the WMS
2. Configure the WMS
3. Add the WSA package to the WMS
4. Install a WEKA Cluster

### 1. Install the WMS

1. Boot the server from the WMS image. The following are some options to do that:

Copy the WEKA Management Station ISO image to an appropriate location so the server‚Äôs BMC (Baseboard Management Controller) can mount it or be served through a PXE (Preboot Execution Environment).

Depending on the server manufacturer, consult the documentation for the server‚Äôs BMC (for example, iLO, iDRAC, and IPMI) for detailed instructions on mounting and booting from a bootable ISO image, such as:

* A workstation or laptop sent to the BMC through the web browser.
* An SMB share in a Windows server or a Samba server.
* An NFS share.

To use PXE boot, use the WEKA Management Station as any other Operating System ISO image and configure accordingly.

Burn the WMS image to a DVD and boot it from the physical DVD. However, most modern servers do not have DVD readers anymore.

A bootable USB drive should work (follow online directions for creating a bootable USB drive) but has not been tested yet.

Once you boot the server, the WEKA Management Station installs the WEKA OS (Rocky Linux), drivers, and WEKA software automatically and unattended (no human interaction required).

Depending on network speed, this can take about 10-60 mins (or more) per server.

### 2. Configure the WMS

Once the WMS installation is complete and rebooted, configure the WMS.

1. Run the OS using one of the following options:

Run the OS through the BMC‚Äôs Console. See the specific manufacturer‚Äôs BMC documentation.

Run the OS through the Cockpit Web Interface on port 9090 of the OS management network.

If you don‚Äôt know the WMS hostname or IP address, go to the console and press the **Return** key a couple of times until it prompts the URL of the WMS OS Web Console (Cockpit) on port 9090.

Change the port from 9090 to 8051, which is the WMS Admin port.

2. Configure a local time server: If the WMS does not have access to the Internet, ensure that your local network includes a time server.
   1. Update the configuration file `/etc/chrony.conf` to point to the local time server.
   2. Refer to the _Chrony_ documentation for detailed instructions on configuring this file.
3. Browse to the WMS Admin UI using the following URL:\
   `http://<WMS-hostname-or-ip>:8501`

3. Enter username and password (default: _admin_/_admin_), and select **Login**.\
   The Landing Page appears.

### 3. Add the WSA package to the WMS

1. Download the latest release of the WSA package from get.weka.io dashboard.
2. Copy the WSA package to **/home/weka** .\
   For example:  `scp <wsa.iso> weka@<wms-server>:`

### 4. Install a WEKA Cluster

1. Go to the WMS Admin UI (landing page) and select **Deploy a WEKA Custer**.

The WSA setup page opens.

2. Select **Standard Install/Reset**.
3. Open **Step 1 - Choose source ISO**, select the WSA package (ISO) you intend to deploy, and click **Next**.

Note: The WSA packages that appear in the list are taken from `/home/weka`. You can have more than one in the directory. If none are displayed, click **Refresh ISO List**. If none are displayed after that, copy a WSA package to `/home/weka` and click **Refresh ISO List** again. Once you select a WSA ISO, click **Next.**

3. In **Step 2 - Value entry method**, select one of the following options:
   * **Option 1: Manually enter values:**\
     Click this option and go to **Step 4** to enter the number of servers to deploy.
   * **Option 2: Upload CSV file to pre-populate data:**\
     If you have the environment data in a CSV file, click this option. **Step 3 - CSV File Upload** section opens. Drag or click to upload the CSV file, and click **Next**.

**CSV template example**

You can prepare a CSV file with the columns as specified in the following example:

```
IPMI_IP,Username,Password,OS_Mgmt_IP,Hostname,OS_Netmask,OS_Gateway,MTU,DNS,Hostname_Pattern,Hostname_Startnum,Server_Count,Data1_IP,Data1_Type,Data1_Netmask,Data1_MTU,Data1_Gateway,Data2_IP,Data2_Type,Data2_Netmask,Data2_MTU,Data2_Gateway
172.29.1.63,ADMIN,ADMIN,10.10.20.11,weka01,24,10.10.20.1,1500,8.8.8.8,weka%02d,1,7,10.100.10.11,Ethernet,16,9000,,10.100.20.11,Ethernet,16,9000,
172.29.1.64,ADMIN,ADMIN,10.10.20.12,weka02,24,10.10.20.1,1500,8.8.8.8,weka%02d,1,7,10.100.10.12,Ethernet,16,9000,,10.100.20.12,Ethernet,16,9000,
172.29.1.65,ADMIN,ADMIN,10.10.20.13,weka03,24,10.10.20.1,1500,8.8.8.8,weka%02d,1,7,10.100.10.13,Ethernet,16,9000,,10.100.20.13,Ethernet,16,9000,
172.29.1.66,ADMIN,ADMIN,10.10.20.14,weka04,24,10.10.20.1,1500,8.8.8.8,weka%02d,1,7,10.100.10.14,Ethernet,16,9000,,10.100.20.14,Ethernet,16,9000,
172.29.1.67,ADMIN,ADMIN,10.10.20.15,weka05,24,10.10.20.1,1500,8.8.8.8,weka%02d,1,7,10.100.10.15,Ethernet,16,9000,,10.100.20.15,Ethernet,16,9000,
172.29.1.68,ADMIN,ADMIN,10.10.20.16,weka06,24,10.10.20.1,1500,8.8.8.8,weka%02d,1,7,10.100.10.16,Ethernet,16,9000,,10.100.20.16,Ethernet,16,9000,
172.29.1.69,ADMIN,ADMIN,10.10.20.17,weka07,24,10.10.20.1,1500,8.8.8.8,weka%02d,1,7,10.100.10.17,Ethernet,16,9000,,10.100.20.17,Ethernet,16,9000,

```

4. In **Step 4 - Number of servers to deploy**, enter a Server Count (default is 8), and click **Next**.

In the following steps, if you uploaded a CSV file, the data is pre-populated. You can review the data and if no editing is necessary, select **Next**.

5. In **Step 5 - IPMI information**, do the following:
   * In the **IPMI First IP**, enter the IPMI IP address of the first server. It requires a consecutive set of IP addresses for the servers (typical).
   * In the **IPMI user** and **IPMI password**, modify the login credentials for the IPMI, iLO, or iDRAC according to your choice.
   * Click **Fill IPMI IPs** to calculate the IP addresses for the number of servers specified in Step 4.
   * You can edit the IP addresses, Usernames, and Passwords as needed if the servers aren‚Äôt consecutive or require different credentials.
   * If you edited the table, click **Verify IPMI IPs** to verify that the WMS can log into the BMCs and detect the manufacturer (Brand column).
   * Verify that all is correct, and then click **Next**.

6. In **Step 6 - Operating System network information**, do the following:
   * In the **OS First IP**, enter the IP address of the OS 1 Gbit management interface. It requires a consecutive set of IP addresses for the servers (typical).
   * In the remaining networking fields, fill in the networking details.
   * Click **Fill OS table** to populate the table. The WMS automatically generates names and IPs.
   * Verify that the OS IP settings are correct. You can repeatedly click **Fill OS table** to make adjustments.
   * Verify that all is correct, and then click **Next**.

7.  In **Step 7 - Dataplane settings**, do the following:

    * Set the number of interfaces in the **Dataplane Interface Count** slider.
    * In the remaining dataplane fields, fill in the details.
    * Click **Update Dataplanes**. The WMS automatically populates the data.
    * You can repeatedly click **Update Dataplanes** to make adjustments.
    * Verify that all is correct, and then click **Next**.

8.  In **Step 8 - Save configuration files and inventory**, click **Save Files** to save the configuration files, and then click **Next**.

9.  In **Step 9 - Prepare WSA ISO for installation**, click **Prepare WSA ISO for install**. \
    The WMS updates the kickstart on the ISO to match the WMS deployment data (it takes about 30 seconds).

When the WSA ISO preparation is completed, the output is displayed. Verify that no errors appear. Then, click **Next**.

10. In **Step 10 - Start Installation**, do the following:
    1. Select **Confirm overwrite of all boot drive OS and data on hosts**.
    2.  Click **Install OS on Servers**.\
        The WMS loads the WSA on the servers previously defined and starts the installation.\
        The installation can take several minutes and displays output when complete. Verify that no errors appear.

        The installation process takes about 30 minutes, depending on several factors, such as network speed. Verify that the server‚Äôs BMC completed the restart.
11. In **Step 11 -  Run OS and Dataplane Configuration Scripts**, click **Run system configuration scripts**. This action runs scripts to configure the servers with the specified dataplane IPs and perform additional tasks, such as populating `/etc/hosts`.

## What to do next?

<!-- ============================================ -->
<!-- File 15/259: planning-and-installation_bare-metal_perform-post-configuration-procedures.md -->
<!-- ============================================ -->

# Perform post-configuration procedures

Once the WEKA cluster is installed and configured, perform the following:

1. Enable event notifications to the cloud (optional).
2. Set the license.
3. Start the cluster IO service.
4. Check the cluster configuration.
5. Bypass the proxy server (optional).
6. Configure default data networking (optional).

## 1. Enable event notifications to the cloud (optional)

Enable event notifications to the cloud for support purposes using one of the following options:

* Enable support through WEKA Home
* Enable support through a private instance of WEKA Home

### **Enable support through Weka Home**

**Command:** `weka cloud enable`

This command enables cloud event notification (via Weka Home), which allows the WEKA Customer Success Team to resolve any issues that may occur.

To learn more about this and how to enable cloud event notification, see .

### **Enable support through** Local WEKA Home

In closed environments, such as dark sites and private VPCs, it is possible to install Local WEKA Home, which is a private instance of WEKA Home.

**Command:** `weka cloud enable --cloud-url=http://<weka-home-ip>:<weka-home-port>`

This command enables the WEKA cluster to send event notifications to the Local WEKA Home.

Note: For details, see .

## 2. Set the license

**Command:** `weka cluster license set`

To run IOs against the cluster, a valid license must be applied. Obtain a valid license and apply it to the WEKA cluster. For details, see .

## 3. Start the cluster IO service

**Command:** `weka cluster start-io`

To start the system IO and exit from the initialization state, use the following command line:

`weka cluster start-io`

## 4. Check the cluster configuration

### Check the cluster container

**Command:** `weka cluster container`

Use this command to display the list of containers and their details.

<details>

<summary>Example of a list of containers and their details</summary>

```bash
$ weka cluster container
HOST ID  HOSTNAME  CONTAINER  IPS             STATUS  RELEASE   FAILURE DOMAIN  CORES  MEMORY    LAST FAILURE  UPTIME
0        av299-0   drives0    10.108.79.121   UP      4.3.0     DOM-000         7      10.45 GB                1:08:30h
1        av299-1   drives0    10.108.115.194  UP      4.3.0     DOM-001         7      10.45 GB                1:08:30h
2        av299-2   drives0    10.108.2.136    UP      4.3.0     DOM-002         7      10.45 GB                1:08:29h
3        av299-3   drives0    10.108.165.185  UP      4.3.0     DOM-003         7      10.45 GB                1:08:30h
4        av299-4   drives0    10.108.116.49   UP      4.3.0     DOM-004         7      10.45 GB                1:08:29h
5        av299-5   drives0    10.108.7.63     UP      4.3.0     DOM-005         7      10.45 GB                1:08:30h
6        av299-6   drives0    10.108.80.75    UP      4.3.0     DOM-006         7      10.45 GB                1:08:29h
7        av299-7   drives0    10.108.173.56   UP      4.3.0     DOM-007         7      10.45 GB                1:08:30h
8        av299-8   drives0    10.108.253.194  UP      4.3.0     DOM-008         7      10.45 GB                1:08:29h
9        av299-9   drives0    10.108.220.115  UP      4.3.0     DOM-009         7      10.45 GB                1:08:29h
10       av299-0   compute0   10.108.79.121   UP      4.3.0     DOM-000         6      20.22 GB                1:08:08h
11       av299-1   compute0   10.108.115.194  UP      4.3.0     DOM-001         6      20.22 GB                1:08:08h
12       av299-2   compute0   10.108.2.136    UP      4.3.0     DOM-002         6      20.22 GB                1:08:09h
13       av299-3   compute0   10.108.165.185  UP      4.3.0     DOM-003         6      20.22 GB                1:08:09h
14       av299-4   compute0   10.108.116.49   UP      4.3.0     DOM-004         6      20.22 GB                1:08:09h
15       av299-5   compute0   10.108.7.63     UP      4.3.0     DOM-005         6      20.22 GB                1:08:08h
16       av299-6   compute0   10.108.80.75    UP      4.3.0     DOM-006         6      20.22 GB                1:08:09h
17       av299-7   compute0   10.108.173.56   UP      4.3.0     DOM-007         6      20.22 GB                1:08:08h
18       av299-8   compute0   10.108.253.194  UP      4.3.0     DOM-008         6      20.22 GB                1:08:09h
19       av299-9   compute0   10.108.220.115  UP      4.3.0     DOM-009         6      20.22 GB                1:08:08h
20       av299-0   frontend0  10.108.79.121   UP      4.3.0     DOM-000         1      1.47 GB                 1:06:57h
21       av299-1   frontend0  10.108.115.194  UP      4.3.0     DOM-001         1      1.47 GB                 1:06:57h
22       av299-2   frontend0  10.108.2.136    UP      4.3.0     DOM-002         1      1.47 GB                 1:06:57h
23       av299-3   frontend0  10.108.165.185  UP      4.3.0     DOM-003         1      1.47 GB                 1:06:56h
24       av299-4   frontend0  10.108.116.49   UP      4.3.0     DOM-004         1      1.47 GB                 1:06:57h
25       av299-5   frontend0  10.108.7.63     UP      4.3.0     DOM-005         1      1.47 GB                 1:06:56h
26       av299-6   frontend0  10.108.80.75    UP      4.3.0     DOM-006         1      1.47 GB                 1:06:57h
27       av299-7   frontend0  10.108.173.56   UP      4.3.0     DOM-007         1      1.47 GB                 1:06:56h
28       av299-8   frontend0  10.108.253.194  UP      4.3.0     DOM-008         1      1.47 GB                 1:06:57h
29       av299-9   frontend0  10.108.220.115  UP      4.3.0     DOM-009         1      1.47 GB                 1:06:56h
```

</details>

### Check cluster container resources

**Command:** `weka cluster container resources`

Use this command to check the resources of each container in the cluster.

`weka cluster container resources <container-id>`

<details>

<summary>Example for a drive container resources output</summary>

```bash
$ weka cluster container resources 1
ROLES       NODE ID  CORE ID
MANAGEMENT  0        <auto>
DRIVES      1        4

NET DEVICE    IDENTIFIER    DEFAULT GATEWAY  IPS             NETMASK  NETWORK LABEL
0000:00:06.0  0000:00:06.0  10.108.0.1       10.108.115.194  UP  16

Allow Protocols           false
Bandwidth                 <auto>
Base Port                 14000
Dedicate Memory           true
Disable NUMA Balancing    true
Failure Domain            DOM-001
Hardware Watchdog         false
Management IPs            10.108.238.217
Mask Interrupts           true
Memory                    <dedicated>
Mode                      BACKEND
Non-Weka Reserved Memory  20
Set CPU Governors         PERFORMANCE
```

</details>

<details>

<summary>Example of a compute container resources output</summary>

```bash
$ weka cluster container resources 10
ROLES       NODE ID  CORE ID
MANAGEMENT  0        <auto>
COMPUTE     1        16
COMPUTE     2        4
COMPUTE     3        18
COMPUTE     4        26
COMPUTE     5        28
COMPUTE     6        10

NET DEVICE    IDENTIFIER    DEFAULT GATEWAY  IPS             NETMASK  NETWORK LABEL
0000:00:04.0  0000:00:04.0  10.108.0.1       10.108.145.137  16
0000:00:05.0  0000:00:05.0  10.108.0.1       10.108.212.87   16
0000:00:06.0  0000:00:06.0  10.108.0.1       10.108.199.231  16
0000:00:07.0  0000:00:07.0  10.108.0.1       10.108.86.172   16
0000:00:08.0  0000:00:08.0  10.108.0.1       10.108.190.88   16
0000:00:09.0  0000:00:09.0  10.108.0.1       10.108.77.31    16

Allow Protocols         false
Bandwidth               <auto>
Base Port               14300
Dedicate Memory         true
Disable NUMA Balancing  true
Failure Domain          DOM-000
Hardware Watchdog       false
Management IPs          10.108.79.121
Mask Interrupts         true
Memory                  20224982280
Mode                    BACKEND
Set CPU Governors       PERFORMANCE
```

</details>

<details>

<summary>Example of a frontend container resources output</summary>

```bash
$ weka cluster container resources 20
ROLES       NODE ID  CORE ID
MANAGEMENT  0        <auto>
FRONTEND    1        24

NET DEVICE    IDENTIFIER    DEFAULT GATEWAY  IPS             NETMASK  NETWORK LABEL
0000:00:13.0  0000:00:13.0  10.108.0.1       10.108.217.249  16

Allow Protocols         true
Bandwidth               <auto>
Base Port               14200
Dedicate Memory         true
Disable NUMA Balancing  true
Failure Domain          DOM-000
Hardware Watchdog       false
Management IPs          10.108.79.121
Mask Interrupts         true
Memory                  <dedicated>
Mode                    BACKEND
Set CPU Governors       PERFORMANCE
```

</details>

#### Check cluster drives

**Command:** `weka cluster drive`

Use this command to check all drives in the cluster.

<details>

<summary>Example</summary>

```bash
$ weka cluster drive
DISK ID  UUID                                  HOSTNAME  NODE ID  SIZE        STATUS  LIFETIME % USED  ATTACHMENT  DRIVE STATUS
0        d3d000d4-a76b-405d-a226-c40dcd8d622c  av299-4   87       399.99 GiB  ACTIVE  0                OK          OK
1        c68cf47a-f91d-499f-83c8-69aa06ed37d4  av299-7   143      399.99 GiB  ACTIVE  0                OK          OK
2        c97f83b5-b9e3-4ccd-bfb8-d78537fa8a6f  av299-1   23       399.99 GiB  ACTIVE  0                OK          OK
3        908dadc5-740c-4e08-9cc2-290b4b311f81  av299-0   7        399.99 GiB  ACTIVE  0                OK          OK
.
.
.
68       1c4c4d54-6553-44b2-bc61-0f0e946919fb  av299-4   84       399.99 GiB  ACTIVE  0                OK          OK
69       969d3521-9057-4db9-8304-157f50719683  av299-3   62       399.99 GiB  ACTIVE  0                OK          OK
```

</details>

### Check the Weka cluster status

**Command:** `weka status`

The `weka status` command displays the overall status of the WEKA cluster.

For details, see #cluster-status.

## 5. Bypass the proxy server (optional)

If the WEKA cluster is deployed in an environment with a proxy server, a WEKA client trying to mount or download the client installation from the WEKA cluster may be blocked by the proxy server. You can disable the proxy for specific URLs using the shell `no_proxy` environment variable.

#### Procedure

1. Connect to one of the WEKA backend servers (configuration changes made on this server are synchronized with all other servers in the cluster).
2. Open the `/etc/wekaio/service.conf` file.
3.  In the `[downloads_proxy]` section, add to the `no_proxy` parameter a comma-separated list of IP addresses or qualified domain names of your WEKA clients and cluster backend servers. Do not use wildcards (*).

    ```makefile
    [downloads_proxy]
    force_no_proxy=true
    proxy=
    no_proxy=<comma-separated list of IPs or domains>
    ```
4.  Restart the agent service using the command:

    ```bash
    service weka-agent restart
    ```

## 6. Configure default data networking (optional)

**Command:** `weka cluster default-net set`

Instead of individually configuring IP addresses for each network device, WEKA supports dynamic IP address allocation. Users can define a range of IP addresses to create a dynamic pool, and these addresses can be automatically allocated on demand.

Note: **Mixed approach for Ethernet networking:** For Ethernet networking, a mixed approach is supported. Administrators can explicitly assign IP addresses for specific network devices, while others in the cluster can receive automatic allocations from the specified IP range. This feature is particularly useful in environments with automated client spawning.

Use the following command to configure default data networking:

`weka cluster default-net set --range <range> [--gateway=<gateway>] [--netmask-bits=<netmask-bits>]`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | range* | A range of IP addresses reserved for dynamic allocation across the entire cluster..Format: A.B.C.D-E Example: 10.10.0.1-100 |
 | netmask-bits* | Number of bits in the netmask that define a network ID in CIDR notation. |
 | gateway | The IP address assigned to the default routing gateway. It is imperative that the gateway resides within the same IP network as defined by the specified range and netmask-bits.This parameter is not applicable to InfiniBand (IB) or Layer 2 (L2) non-routable networks. |

**View current settings:** To view the current default data networking settings, use the command: \
`weka cluster default-net`

**Remove default data networking:** If a default data networking configuration was previously set up on a cluster and is no longer needed, you can remove it using the command:\
`weka cluster default-net reset`

**End of the installation and configuration for all workflow paths**

## **What do next?**

<!-- ============================================ -->
<!-- File 16/259: planning-and-installation_bare-metal_adding-clients-bare-metal.md -->
<!-- ============================================ -->

---
description: This page describes how to add clients to a bare-metal cluster.
---

# Add clients to an on-premises WEKA cluster

## cgroups configuration

Clients run applications that access the WEKA filesystem but do not contribute CPUs or drives to the cluster. They connect solely to use the filesystems.

By default, WEKA uses cgroups to limit or isolate resources for its exclusive use, such as assigning specific CPUs.

cgroups (Control Groups) is a Linux kernel feature that allows you to limit, prioritize, and isolate a collection of processes' resource usage (CPU, memory, disk I/O, network). It helps allocate resources among user-defined groups of tasks and manage their performance effectively.

**Versions of cgroups:**

* **cgroupsV1**: Uses multiple hierarchies for different resource controllers, offering fine-grained control but with increased complexity.
* **cgroupsV2**: Combines all resource controllers into a single unified hierarchy, simplifying management and providing better resource isolation and a more consistent interface.

Note: **Hybrid mode**: If the OS is configured with hybrid mode (cgroupsV1 and cgroupsV2), WEKA defaults to using cgroupsV1.

**WEKA requirements:**

* **Backends and clients serving protocols:** Must run on an OS with cgroupsV1 support. cgroupsV2 is supported on backends and clients but is incompatible with protocol cluster deployments.
* **cgroups mode compatibility:** When setting up cgroups on clients or backends, ensure that the cgroups configuration (whether using cgroupsV1 or cgroupsV2) aligns with the operating system's capabilities and configuration.

### cgroups configuration and compatibility

The configuration of cgroups depends on the installed operating system, and it is important that the cluster server settings match the OS configuration to ensure proper resource management and compatibility.

Customers using a supported OS with cgroupsV2 or wanting to modify the cgroups usage can set the cgroups usage during the agent installation or by editing the service configuration file. The specified mode must match the existing cgroups configuration in the OS.

The cgroups setting includes the following modes:

* `auto`: WEKA tries using cgroupsV1 (default). If it fails, the cgroups is set to none automatically.
* `force`: WEKA uses cgroupsV1. If the OS does not support it, WEKA fails.
* `force_v2`: WEKA uses cgroupsV2. If the OS does not support it, WEKA fails. This mode is not supported in protocol cluster deployments.
* `none`: WEKA never uses cgroups, even if it runs on an OS with cgroupsV1.

### Set the cgroups mode during the client or backend installation

In the installation command line, specify the required cgroups mode (`WEKA_CGROUPS_MODE`).

Example:

```bash
| curl http://Backend-1:14000/dist/v1/install | WEKA_CGROUPS_MODE=none sh |
```

### Set the cgroups mode in the service configuration file

You can set the cgroups mode in the service configuration file for clients and backends.

1. Open the service configuration file `/etc/wekaio/service.conf` and add one of the following:
   * `cgroups_mode=auto`
   * `cgroups_mode=force`
   * `cgroups_mode=force_v2`
   * `cgroups_mode=none`
2. Restart the WEKA agent service: Run `service weka-agent restart`.
3. Restart the containers to apply the cgroups settings:
   * Run `weka local restart` to restart all containers, or specify a container, for example, `weka local restart client` for the client container. If WEKA is mounted, unmount it before restarting.
4. Verify the cgroups settings by running the `weka local status` command.

Example:

```bash
[root@weka-cluster] #weka local status
Weka v4.2.0 (CLI build 4.2.0)
cgroups: mode=auto, enabled=true

Containers: 1/1 running (1 weka)
Nodes: 2/2 running (2 READY)
Mounts: 1
```

## Add a stateless client to the cluster

A **stateless client** is a client that does not persistently store any software or configuration state locally. Instead, it dynamically installs the required software and configuration each time it interacts with the WEKA system. This approach simplifies client management by eliminating the need for persistent local installations, though the client still temporarily joins the cluster during operations. Stateless clients are particularly useful for deployment on lightweight, diskless servers.

To enable a stateless client to use the WEKA filesystem, the `mount` command is used. This command installs the WEKA software automatically and configures the client without requiring manual intervention.

#### Before you begin

Ensure each client has a unique IP address and fully qualified domain name (FQDN) for proper cluster identification.

**Procedure**

1.  **Install the WEKA agent (One-time setup):**\
    Install the WEKA agent from one of the backend instances. This step prepares the client to interact with the WEKA system. Run the following command (where `backend-1` resolves to the IP address of one of the WEKA backend servers):

    ```bash
| curl http://backend-1:14000/dist/v1/install | sh |
    ```
2.  **Create a mount point (one-time setup):**\
    Create a directory on the client system where the WEKA filesystem will be mounted. For example:

    ```bash
    mkdir -p /mnt/weka
    ```
3.  **Mount the WEKA filesystem:**\
    Use the `mount` command to attach the WEKA filesystem to the client (where `my_fs` is the name of the WEKA filesystem). For example:

    ```bash
    mount -t wekafs -o net=eth0 backend-1/my_fs /mnt/weka
    ```

    * During the first mount, the required WEKA software is installed, and the client is configured automatically.

**Additional configuration**

* **Automatic mounting at boot:**\
  To configure the client OS to automatically mount the filesystem at boot time, you can use traditional methods or configure `autofs`. For more details, refer to the relevant documentation on  #mounting-filesystems-using-stateless-clients **Mount a filesystem using the traditional method** or **Mount filesystems using autofs**.
* **Diskless deployment:**\
  Stateless clients can be deployed on diskless nodes by storing the WEKA client software in RAM and using an NFS mount for traces. For assistance with this setup, contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team).

**Related topic**

#mounting-filesystems-using-stateless-clients (for detailed mount and configuration options)

## Add a persistent client (stateful client) to the cluster

A **persistent client** (or stateful client) is a client that remains an integral part of the cluster. It does not contribute resources to the cluster but is used for mounting filesystems or serving purposes, such as NFS/SMB servers, that require continuous availability. Adding persistent clients ensures that these servers are always up and accessible for file system operations.

There are two methods for adding a persistent client to the cluster: a **shorter**, streamlined method and a **longer**, more detailed method. Both methods achieve the same outcome but offer different levels of flexibility and control.

* **Shorter method**: Quick and efficient for most users. It sets up and joins the container with minimal configuration, ideal for persistent clients that do not require specific resource allocations or custom networking.
* **Longer method**: Provides more control and flexibility, allowing for detailed configuration of the client, making it suitable for environments with specific performance or network requirements.

Choose the method that best fits your needs based on the level of customization required.

### Option 1: Shorter method (recommended for most use cases)

This method sets up the client with all required resources in a single step and self-joins the container to the cluster using its management and join IPs.

**Procedure**

1.  **Setup container locally with resources**\
    This step sets up the client with all necessary resources, such as cores, memory, networking, and ports.

    ```

    ```bash
    weka local setup container --join-ips <join-ips> --base-port <base-port> --cores <cores> --core-ids <core-ids> --only-frontend-cores
    ```

```

    Example:

    ```

    ```bash
    weka local setup container --join-ips 10.108.81.144 --base-port 14000 --cores 1 --core-ids 2 --only-frontend-cores
    ```

```

    * `join-ips`: The IP address for joining the cluster.
    * `base-port`: The base port for container communication.
    * `cores`: The number of cores to allocate.
    * `core-ids`: The cores' identifiers.
    * `only-frontend-cores`: Indicates that only frontend cores are used.

### Option 2: Longer method (more control and flexibility)

This method involves more detailed steps, allowing you to manually set up the client with specific configurations, including core allocation, networking settings, and container setup.

**Procedure**

1.  **Install the WEKA software**

    * Install the WEKA software on the client by running the `install.sh` script after downloading the tarball from get.weka.io. Follow the instructions in the Install tab to complete the installation.

    All clients in a WEKA system cluster must use the same software version as the backends or a maximum of one version back. The backend containers must run the same WEKA software version except during upgrades (as managed by the upgrade process).
2.  **Join the cluster**

    * Once the client is in stem mode, use the following command to add it to the cluster.

    ```bash
    weka -H <backend-hostname> cluster container add <client-hostname>
    ```

    Example:

    ```bash
    weka -H backend1.cluster.local cluster container add client1.cluster.local
    ```

    * `backend-hostname`: The hostname (FQDN) or IP address of an existing backend instance.
    * `client-hostname`: The unique hostname (FQDN) of the client to add.

    Once this step is complete, the `container-id` of the newly added container will be displayed. Record it for use in the following steps.
3.  **Configure the container as a client**

    * After adding the client to the cluster, configure it by setting the number of cores and frontend-dedicated cores.

    ```

    ```bash
    weka cluster container cores <container-id> <cores> --frontend-dedicated-cores=<frontend-dedicated-cores>
    ```

```

    Example:

    ```bash
    weka cluster container cores container1 4 --frontend-dedicated-cores=4
    ```

    * `container-id`: The unique identifier of the container.
    * `cores`: The number of physical cores to allocate to the client.
    * `frontend-dedicated-cores`: The number of physical cores dedicated to frontend processes (must match `cores` for clients). You can set up to 19 cores.
4.  **Configure client networking**

    * If needed, configure the client‚Äôs network interface for high-performance communication with the WEKA cluster. (For UDP, this step is not needed.)
    * When configuring an InfiniBand client, do not pass the `--ips`, `--netmask`, or `--gateway` parameters.
    * InfiniBand and Ethernet clients can only join a cluster with the same network technology connectivity. However, it is possible to mix InfiniBand and Ethernet clients in the same cluster as long as the cluster backends are connected to both network technologies.

    ```

    ```bash
    weka cluster container net add <container-id> <device> --ips=<ips> --netmask=<netmask> --gateway=<gateway>
    ```

```

    Example:

    ```

    ```bash
    weka cluster container net add container1 eth1 --ips=10.108.81.100 --netmask=255.255.255.0 --gateway=10.108.81.1
    ```

```

    * `container-id`: A valid identifier for the container to add to the cluster.
    * `device`: A valid network interface device name (for example, `eth1`).
    * `ips`: A valid IP address for the new interface.
    * `gateway`: The IP address of the default routing gateway.\
      The gateway must be within the same IP network as the provided `ips`, as defined by the `netmask`. Not applicable for IB / L2 non-routable networks.
    * `netmask`: The number of bits that define the network ID (CIDR notation). For example, a netmask of `255.255.0.0` corresponds to `16` netmask bits.
5.  **Apply the container configuration**

    * After configuring the container and networking, apply the changes to activate the client container.

    ```

    ```bash
    weka cluster container apply <container-id> [--force]
    ```

```

    Example:

    ```bash
    weka cluster container apply container1 --force
    ```

<!-- ============================================ -->
<!-- File 17/259: planning-and-installation_weka-cdm-web-user-guide.md -->
<!-- ============================================ -->

# WEKA Cloud Deployment Manager Web (CDM Web) User Guide

## Overview

The WEKA Cloud Deployment Manager Web (CDM Web) simplifies the deployment of WEKA clusters in the AWS, Azure, and GCP public cloud environments. Leveraging WEKA‚Äôs validated Terraform deployment modules, the CDM provides a user-friendly interface to guide users through the initial configuration process.

Key features of the CDM Web:

* **Streamlined deployment:** The CDM streamlines the deployment of WEKA clusters, making it easier for users to set up their infrastructure.
* **Web-hosted solution:** The CDM is fully web-hosted, eliminating the need for downloads or installations. Users can quickly begin configuring their WEKA clusters.
* **Terraform configuration file:** The CDM process results in the main Terraform configuration file `(main.tf)`, which can be directly applied when deploying WEKA.

## Access the CDM Web

To access the CDM Web, follow these steps:

1. Navigate to cloud.weka.io.
2. On the welcome page, select the cloud environment (AWS, Azure, or GCP) for your WEKA cluster deployment. (This guide uses Azure as an example, but the deployment workflow is similar across all supported cloud platforms.)

3. After selecting a public cloud, you are redirected to a login screen. Log in using your get.weka.io credentials. Internal WEKA users can use their Google SSO login to access CDM. Adhere to the following guidelines:
   * Ensure you have a get.weka.io token provisioned and available for a successful deployment.
   * If you are an internal WEKA user deploying a WEKA cluster for a customer, log in using the customer‚Äôs get.weka.io credentials. The signed-in user‚Äôs get.weka.io token automatically populates into the CDM configuration workflow.

Once logged in, you are presented with the main configuration dashboard of the Cloud Deployment Manager.

## CDM Web dashboard overview

The CDM Web features a simple and clean configuration interface, offering the power and flexibility of our Terraform deployment modules. Below, each part of the interface is detailed for better understanding and usage.

The CDM dashboard consists of three main components:

* The workflow navigation panel (outlined in green)
* The configuration input panel (outlined in orange)
* The dynamic content sidebar (outlined in teal)

### Workflow navigation panel

The workflow navigation panel provides convenient access to various WEKA cluster configuration variables. You can switch between different aspects of cluster configuration and adjust settings according to their deployment needs.

The tabs within the panel correspond to primary configurable aspects for a WEKA cluster:

* Basic WEKA cluster configuration
* Cloud networking configuration
* Cloud security configuration
* Optional object storage (OBS) configuration
* Optional deployment of NFS protocol gateways
* Optional deployment of SMB protocol gateways
* Optional deployment of WEKA clients
* Optional advanced configuration (granular cluster-level adjustments)

To ensure completeness from a basic requirements perspective, specific fields within the configuration input panel are marked as mandatory based on the selected configuration options.

The workflow navigation panel visually indicates the completeness of the configuration. A green check or a red **x** appears next to each tab, helping users identify areas that require additional attention. For example, if both Basic Configuration and Security Configuration have fields that need attention, the panel reflects this.

You can navigate between different workflow pages and view associated configuration input panels by clicking the **Next** button or selecting the desired tab from the workflow navigation panel.

### Configuration input panel

The configuration input panel provides a user-friendly interface for customizing input fields related to the WEKA cluster deployment. These fields correspond to variables in WEKA Terraform modules, which traditionally require manual formatting and entry into a `main.tf` file. With CDM, these variables are presented visually, streamlining the configuration process.

* You can tailor the input fields to match their needs and deployment objectives.
* Required fields are marked with a red asterisk.
* The following example illustrates the Basic Configuration workflow tab, where some required fields are populated, while others remain empty. Fields lacking input are highlighted in bright red, and the red outline disappears once the user provides the necessary information.

Certain fields within the configuration input panel require manual user input. Other fields, such as Instance Type, WEKA Version, and Region, are provided as selectable dropdown menus.

The WEKA software release dropdown menu is designed to auto-populate with the most recent Long-Term Support (LTS) version by default. You can select the previous software release by opening the dropdown menu and choosing from the list. The top two entries in the dropdown are always LTS releases, while the bottom two are innovation releases.

To enter a WEKA software release that is not listed in the dropdown, click directly in the WEKA Version input field and type the desired release. This feature is particularly useful when deploying a WEKA cluster with a customer-specific software release.

### Dynamic content sidebar

The dynamic content sidebar enhances user experience by displaying contextually relevant information during various activities within CDM. Its primary functions include:

#### Real-time configuration guidance

* **Purpose:** Assists users in understanding the role of specific variables or input fields in the configuration input panel.
* **Functionality:** Automatically displays pertinent information when an input field, such as the Terraform Module Release Version, is selected. This feature covers every input field for AWS, Azure, and GCP configurations.

#### Real-time file representation

* **Purpose:** Provides a preview of the file that will be generated for download once all configuration inputs are completed.
* **Functionality:** Next to the configuration guidance tab, a new tab labeled ‚Äútf file preview‚Äù showcases the file in real-time.

#### JSON and HCL format options for main.tf

* **Purpose:** Allows flexibility in file format based on deployment requirements.
* **Functionality:** Includes a toggle switch to change the main.tf file format between JSON and HCL.

#### Download finalized terraform configuration file

* **Purpose:** Enables users to download the completed configuration file for local use.
* **Functionality:** A **Download** button allows you to save the file locally, manually execute the relevant Terraform `plan`, and `apply` commands for WEKA cluster deployment.

Note: All tabs in the workflow navigation panel display green status bubbles with check marks, indicating the configuration is complete and ready for a minimally viable WEKA deployment based on the user's selected parameters. Once all status bubbles are green, the dynamic content sidebar only shows the **TF File Preview** tab, **File Format** toggle, and **Download** button.

### Finalize the WEKA deployment

Once you download the CDM-generated Terraform file, manually execute the relevant Terraform commands to deploy their generated WEKA cluster configuration into the cloud of choice.

This means that Terraform, all its dependencies, relevant public cloud CLIs, and SDKs must exist, and the login uses an adequately privileged account before applying the Terraform file.

**Related topics**

<!-- ============================================ -->
<!-- File 18/259: planning-and-installation_weka-cdm-local-user-manager.md -->
<!-- ============================================ -->

# WEKA Cloud Deployment Manager Local (CDM Local) User Guide

## Overview

The WEKA Cloud Deployment Manager Local (CDM Local) offers a locally installed solution for deploying WEKA clusters in AWS, Azure, and GCP public cloud environments. Like CDM Web, it leverages WEKA‚Äôs validated Terraform deployment modules and provides a user-friendly interface to guide users through the configuration process. However, CDM Local adds new features tailored for users who prefer a local installation, including public cloud environment configuration polling, validation, and deployment execution, which is not available in CDM Web.

Key features of CDM Local:

* **Locally installed solution**: CDM Local is packaged as a Go binary for download and can be run locally or on a cloud instance in the customer‚Äôs public cloud environment.
* **Cross-platform compatibility**: CDM Local provides individually downloadable binaries for different platforms, including Apple MacOS (Intel and Apple Silicon) and Linux (Intel and ARM).
* **Configuration polling and validation**: CDM Local includes a unique feature that automatically polls the public cloud environment to populate key variables like VPC and Subnet details. Additionally, it validates the cluster configuration to detect conflicts before deployment.
* **Automated Terraform deployment**: CDM Local generates a Terraform configuration file and state, which are stored in a user-defined cloud object bucket. Additionally, if Terraform is in the user's execution path, CDM Local uses it to automate the execution of the configuration file. If Terraform is not present, CDM Local installs it to execute the configuration file automatically, eliminating the need for manual Terraform commands.

## CDM Local prerequisites

Before deploying CDM Local, ensure the following components are installed on the system:

* Go
* The appropriate Cloud CLI or SDK for the target cloud environment:
  * AWS CLI
  * Azure CLI
  * Google Gcloud CLI

**Required permissions:**

* **AWS:** To run cluster validation in CDM Local, the user logged into the AWS CLI must have the permission `iam:SimulatePrincipalPolicy`. Ensure this permission is granted by attaching an AWS IAM policy that includes the action to the user's account.

Note: Ensure the Cloud CLI is configured and logged in with the same user account used to deploy the WEKA cluster. This guarantees the logged-in user has the necessary permissions.
For Google Cloud CLI, use the `gcloud auth application-default login` command to authenticate.

## Download CDM Local

1. Go to **get.weka.io** and select the **CDM** tab.

2. CDM Local is available as a platform-specific binary. Choose the binary that matches your target host platform for installation:
   * MacOS (Darwin):
     * Intel-based: cdm-darwin-amd64
     * Apple Silicon-based: cdm-darwin-arm64
   * **linux**:
     * Intel-based: cdm-linux-amd64
     * ARM-based: cdm-linux-arm64

## Launch CDM Local

CDM Local uses Terraform to finalize the deployment of WEKA cluster resources and execute post-installation scripts, similar to CDM Web. However, CDM Local is run locally through a binary and launched through a browser-based UI.

**Before you begin**

* Ensure you are authenticated to the relevant Cloud CLI (AWS, Azure, GCP) to grant the necessary permissions for the deployment.
* A valid get.weka.io token is required to complete the deployment. Ensure you have this token available before proceeding.
* Choose the cloud where you will store the CDM Local configuration state. For all cloud providers except GCP, CDM Local can create the bucket for you. For GCP, you must use an existing bucket.

**Procedure**

1. **Navigate to the CDM Local binary:** Open a terminal and navigate to the directory where the downloaded CDM Local binary is located.
2.  **Make the binary executable:** Change the file permissions to allow execution. Example on an Apple Silicon system:

    ```bash
    chmod +x cdm-darwin-arm64-v1.2.0-tech-preview
    ```
3.  Run the CDM Local binary to launch CDM Local. Define the `state-backend` parameter based on your selected cloud provider:

    * **AWS**: `--state-backend=aws`
    * **Azure**: `--state-backend=azure --azure-subscription-id=[$YOUR_AZURE_SUBSCRIPTION_ID] --azure-resource-group=[$YOUR_AZURE_RESOURCE_GROUP]`
    * **GCP**: `--state-backend=gcp --gcp-project-id=[$YOUR_GCP_PROJECT_ID] --gcp-region=[$YOUR_GCP_REGION] --gcp-bucket=[$YOUR_GCP_BUCKET]`

    **Example** (on an Apple Silicon system with AWS for state backup):\
    `./cdm-darwin-arm64-v1.2.0-tech-preview --state-backend=aws`
4. **Access the CDM Local UI:** After running the binary, a browser window with the CDM Local interface opens automatically.

5. **Accept Statistic Gathering**: Select **Accept** to allow WEKA to collect statistics to enhance the system.
6. **Deploy a Cluster**: Click **DEPLOY CLUSTER** to start deploying your first WEKA cluster.
7. **Select Cloud Provider**: Choose your deployment cloud (AWS, Azure, or GCP) and select **Deploy**. The CDM Local dashboard appears.

## CDM Local dashboard overview

The CDM Local dashboard provides a streamlined way to configure WEKA clusters, leveraging the power of Terraform modules with a graphical UI.

The CDM dashboard consists of three main sections:

* The workflow navigation panel (outlined in green)
* The configuration input panel (outlined in orange)
* The dynamic content sidebar (outlined in teal)

### **Workflow navigation panel**

The workflow navigation panel provides convenient access to various WEKA cluster configuration variables. You can switch between different aspects of cluster configuration and adjust settings according to their deployment needs.

The tabs within the panel correspond to primary configurable aspects for a WEKA cluster:

* Basic Configuration
* Networking Configuration
* Security Configuration
* OBS: Optional object storage configuration
* NFS Protocol Gateways: Optional deployment of NFS protocol servers.
* SMB Protocol Gateways: Optional deployment of SMB protocol servers.
* Clients: Optional deployment of WEKA clients
* Advanced Configuration: Optional, granular cluster-level adjustments

To ensure completeness from a basic requirements perspective, specific fields within the configuration input panel are marked as mandatory based on the selected configuration options.

The workflow navigation panel visually indicates the completeness of the configuration. A green check or a red **x** appears next to each tab, helping users identify areas that require additional attention. For example, if both Basic Configuration and Security Configuration have fields that need attention, the panel reflects this.

You can navigate between different workflow pages and view associated configuration input panels by clicking the **Next** button or selecting the desired tab from the workflow navigation panel.

### **Configuration input panel**

The configuration input panel enables customizing input fields related to the WEKA cluster deployment. These fields correspond to variables in WEKA Terraform modules, which traditionally require manual formatting and entry into a `main.tf` file. With CDM, these variables are presented visually, streamlining the configuration process.

* You can tailor the input fields to match their needs and deployment objectives.
* Required fields are marked with a red asterisk.

The following example illustrates the Basic Configuration tab, where some required fields are populated, while others remain empty. Fields lacking input are highlighted in bright red, and the red outline disappears once the user provides the necessary information.

Certain fields within the configuration input panel require manual user input. Other fields, such as Instance Type, WEKA Version, and Region, are provided as selectable dropdown menus.

The WEKA software release dropdown menu is designed to auto-populate with the most recent Long-Term Support (LTS) version by default. You can select the previous software release by opening the dropdown menu and choosing from the list. The top two entries in the dropdown are always LTS releases, while the bottom two are innovation releases.

To enter a WEKA software release that is not listed in the dropdown, click directly in the WEKA Version input field and type the desired release. This feature is particularly useful when deploying a WEKA cluster with a customer-specific software release.

### **Dynamic content sidebar**

The dynamic content sidebar enhances user experience by displaying contextually relevant information during various activities within CDM. Its primary functions include:

#### Real-time configuration guidance

* **Purpose:** Assists users in understanding the role of specific variables or input fields in the configuration input panel.
* **Functionality:** Automatically displays pertinent information when an input field, such as the Terraform Module Release Version, is selected. This feature covers every input field for AWS, Azure, and GCP configurations.

#### Real-time file representation

* **Purpose:** Provides a preview of the file that will be generated for download once all configuration inputs are completed.
* **Functionality:** Next to the configuration guidance tab, a new tab labeled ‚Äútf file preview‚Äù showcases the file in real-time.

#### JSON and HCL format options for main.tf

* **Purpose:** Allows flexibility in file format based on deployment requirements.
* **Functionality:** Includes a toggle switch to change the main.tf file format between JSON and HCL.

#### Validation of the finalized terraform configuration file

* **Purpose:** The validation process ensures that the completed Terraform configuration is accurate and ready for local deployment. This step helps identify and resolve any issues before proceeding with the deployment.
* **Functionality:** Before copying or downloading the generated `main.tf` file, it is highly recommended to validate the configuration using the **VALIDATE WEKA CLUSTER** button. The CDM Local performs the following checks during validation:
  * **Permissions:** Ensures that the user has the necessary permissions to deploy and run the WEKA cluster.
  * **Subnet IP addresses:** Confirms that the specified subnet has enough available IP addresses to accommodate the WEKA resources to be deployed.
  * **Compute resource quota:** Verifies that the chosen machine type for WEKA components meets the required compute resource quotas for the deployment.

If any errors occur during the validation, a popup window appears with details about the encountered issues. Users can then correct these errors and revalidate the configuration before continuing. Once validation is successful, the file can be copied or downloaded for use in the deployment process.

#### Download and copy the finalized terraform configuration file

* **Purpose:** Enables users to download or copy the completed configuration file for local use.
* **Functionality:** The **Download** and **Copy** button allow you to save the file locally or copy it, to manually execute the relevant Terraform `plan`, and `apply` commands for WEKA cluster deployment.

#### Deploy WEKA Cluster

* **Purpose:** Executes the deployment of the configured WEKA cluster.
* **Functionality:** The **CREATE** button initiates the execution of a Terraform `apply` action. If Terraform is not present on the client system, it installs in the user's home directory.

### Modify the configuration of a deployed WEKA cluster

After selecting **CREATE** or when connecting to CDM Local with at least one WEKA cluster deployed, the screen displays a list of deployed clusters.

Clicking on a cluster opens its configuration, allowing you to make changes. Use the **UPDATE** button, which replaces **CREATE**, to rerun the deployment configuration.

#### Update WEKA cluster

* **Purpose**: Applies updates to the configuration of the deployed WEKA cluster.
* **Functionality**: The **UPDATE** button initiates a Terraform `apply` action, enabling post-deployment changes to the cluster configuration, such as adding Protocol Servers.

### Retrieve Information on deployed WEKA clusters

When clusters are listed in the CDM Local inventory, you can gather the following details about each deployment:

* #clusterization-progress
* #weka-cluster-status
* #terraform-output
* #password
* #backend-ips

#### **Clusterization progress**

During the initial build of the WEKA cluster, various cloud services collect progress information. You can retrieve the clusterization progress from the **Get Clusterization Progress** menu option.

#### WEKA cluster status

You can retrieve the status of the WEKA cluster at any time using the **Get WEKA Status** menu option.

#### Terraform output

You can retrieve the output information from running Terraform using the **Get Terraform Output** menu option.

#### Password

When the WEKA clusterization process finishes, a random password is generated for the 'admin' account. You can retrieve this password using the **Get Password** menu option.

#### Backend IPs

You can retrieve the IP addresses for backend access using the **Get Backend IPs** menu option in CDM Local.

**Related topics**

<!-- ============================================ -->
<!-- File 19/259: planning-and-installation_aws.md -->
<!-- ============================================ -->

---
description:
---

# WEKA installation on AWS

The WEKA¬Æ Data Platform on AWS provides a fast and scalable platform for running performance-intensive applications and hybrid cloud workflows.

WEKA provides a ready-to-deploy Terraform package that you can customize for installing the WEKA cluster on AWS. Optionally, you can install the WEKA cluster using the AWS CloudFormation.

Ensure you are familiar with the following concepts and services that are used for the WEKA installation on AWS:

<details>

<summary>AWS IAM - Identity and access management</summary>

AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. With IAM, you can centrally manage permissions that control which AWS resources users can access. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.

**Related information**

What is IAM?

</details>

<details>

<summary>Amazon VPCs, subnets, and security groups</summary>

A _virtual private cloud_ (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. You can specify an IP address range for the VPC, add subnets and gateways, and associate security groups.

A _subnet_ is a range of IP addresses in your VPC. You launch AWS resources, such as Amazon EC2 instances, into your subnets. Using route tables, you can connect a subnet to the internet, other VPCs, and your data centers and route traffic to and from your subnets.

A _security group_ controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance. You can associate a security group only with resources in the VPC for which it is created.

**Related information**

What is Amazon VPC?

How Amazon VPC works

Control traffic to your AWS resources using security groups

</details>

<details>

<summary>Amazon EC2 instances</summary>

Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable computing capacity‚Äîliterally, servers in Amazon's data centers‚Äîthat you use to build and host your software systems.

Amazon EC2 provides different instance types to choose the CPU, memory, storage, and networking capacity you need to run your applications.

**Related information**

What is Amazon EC2?

</details>

<details>

<summary>Amazon EC2 key pairs for SSH</summary>

A key pair, consisting of a public key and a private key, is a set of security credentials you use to prove your identity when connecting to an Amazon EC2 instance. Amazon EC2 stores the public key on your instance, and you store the private key. The private key allows you to SSH into your instance securely for Linux instances.

**Related information**

Amazon EC2 key pairs and Linux instances

</details>

<details>

<summary>Amazon S3 protocol and object store</summary>

Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. It is used for tiering data from the WEKA cluster to Amazon object store buckets.

**Related information**

What is Amazon S3?

</details>

<details>

<summary>Terraform</summary>

Terraform is an open-source project from Hashicorp. It creates and manages resources on cloud platforms and on-premises clouds. Unlike AWS CloudFormation, it works with many APIs from multiple platforms and services.

### How does Terraform work?

A deployment with Terraform involves three phases:

* **Write:** Define the infrastructure in configuration files and customize the project variables provided in the Terraform package.
* **Plan:** Review the changes Terraform will make to your infrastructure.
* **Apply:** Terraform provisions the infrastructure, including the EC2 instances, installs the WEKA software, and creates the cluster. Once completed, the WEKA cluster runs on AWS.

**Related information**

Get Started with Terraform on AWS

</details>

<details>

<summary>AWS Cloud Formation</summary>

AWS CloudFormation enables you to create and provision AWS infrastructure deployments predictably and repeatedly.

**Related information**

AWS CloudFormation Documentation

</details>

To install WEKA on AWS, an AWS account is required. Visit the AWS site to create an AWS account.

**Related topics**

<!-- ============================================ -->
<!-- File 20/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation.md -->
<!-- ============================================ -->

# WEKA installation on AWS using the Cloud Formation

During the deployment of the WEKA system, the EC2 instances require access to the internet to download the WEKA software. For this reason, you need to deploy the WEKA system in one of the following deployment types in AWS:

* **Public subnet:** Use a public subnet within your VPC with an internet gateway, and allow public IP addresses for your instances.
* **Private subnet with NAT Gateway:** Create a private subnet with a route to a NAT gateway with an elastic IP in the public subnet.
* **Private subnet using WEKA VPC endpoint:** Requires the creation of a [Cluster CloudFormation stack](self-service-portal#cluster-cloudformation-stack) (once per VPC) that creates the necessary resources.
* **Private subnet using custom proxy:** Requires the creation of a [Cluster CloudFormation stack](self-service-portal#cluster-cloudformation-stack) (once per VPC) that creates the necessary resources.

The following diagrams illustrate the components of the _public subnet_ and _private subnet with NAT gateway deployment_ types in AWS.

## Update the number of vCPU limits in EC2

By default, AWS does not provide enough vCPUs to install a WEKA system. Use the Limits Calculator for your region from the AWS EC2 dashboard.

**Procedure**

1. On the AWS EC2 dashboard, select the **Limits** option from the left menu.

2\. In the Limits Calculator, do the following:

* In the **Current Limit**, set the number of vCPUs you currently have for a region.
* In the **vCPUs needed**, set the required number of vCPUs for your specific deployment.

Select the **Request on-demand limit increase** link to get more vCPUs.

Note: vCPU increase is not an instant action and can take minutes to days for AWS to evaluate and approve your request.

The following example shows the required vCPUs for a six servers cluster with two clients of type i3en.2xlarge. This example is the smallest type of instance for a WEKA system deployment.

## After the installation on AWS best practices

### Backup and recovery

#### Resiliency

The Weka system is a distributed cluster protected from 2 or 4 failure domain failures, providing fast rebuild times. For details, see the [About the WEKA system](../../weka-system-overview/about) section.

#### Instance failure

If an instance failure occurs, the Weka system rebuilds the data. Add a new instance to the cluster to regain the reduced compute and storage due to the instance failure.

#### Upload snapshots to S3

It is advisable to use periodic (incremental) snapshots to back up the data and protect it from multiple EC2 instances failures.

The recovery point objective (RPO) is determined by the cadence in which the snapshots are taken and uploaded to S3. The RPO changes between the type of data, regulations, and company policies, but it is advisable to upload at least daily snapshots of the critical filesystems. For details, see the [Snap-To-Object](../../weka-filesystems-and-object-stores/snap-to-obj) section.

If a failure occurs and it is required to recover from a backup, spin up a cluster using the [Self-Service Portal](weka-installation-on-aws-using-the-cloud-formation/self-service-portal) or [CloudFormation](weka-installation-on-aws-using-the-cloud-formation/cloudformation), and create filesystems from those snapshots. You do not need to wait for the data to reach the EC2 volumes. It is instantly accessible through S3. The recovery time objective (RTO) for this operation mainly depends on the time it takes to deploy the [Cluster CloudFormation stack](self-service-portal#cluster-cloudformation-stack) and is typically below 30 min.

#### Cross AZ failure

See the [Data protection against cloud availability zone failures](../../../weka-filesystems-and-object-stores/snap-to-obj#data-protection-against-cloud-availability-zone-failures) section.

#### Region failure

Using Weka snapshots uploaded to S3 combined with S3 cross-region replication enables protection from an AWS region failure.

### SSH keys rotation

For security reasons, it is advisable to rotate the SSH keys used for the EC2 instances.

To rotate the SSH keys, follow these steps:

* Adding or replacing a key pair for your instance, and
* How to use AWS Secrets Manager to securely store and rotate SSH key pairs.

**Related topic**

<!-- ============================================ -->
<!-- File 21/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_deployment-types.md -->
<!-- ============================================ -->

---
description:
---

# Deployment types

## Deployment prerequisites

* Check that your AWS account limits allow for the deployment of your selected configuration. You can check your limits under the Limits tab in the EC2 console.
* Deploying a WEKA cluster in AWS requires at least six EC2 instances with SSD or NVMe drives, also known as an instance store, and potentially additional instances that connect as clients.
* Ensure that WEKA has access to instance metadata. The system uses Instance Metadata Service Version 2 (IMDSv2) by default for enhanced security.
* If you deploy in AWS without using the CloudFormation template, or if you add capabilities such as tiering after deployment, provide permissions to several AWS APIs. For details, refer to the #iam-role-created-in-the-template.
* Ensure the selected subnet has enough available IP addresses. Each core allocated to the WEKA system requires an Elastic Network Interface (ENI).

## AWS deployment types

The selection and configuration of instance types determine the two deployment types:

* Client backend deployment
* Converged deployment

## Client backend deployment

A client backend deployment uses two different instance types:

* **Backend instances:** Instances that contribute their drives and all possible CPU and network resources to the cluster.
* **Client instances:** Instances that connect to the cluster created by the backend instances and run an application using one or more shared filesystems.

In client backend deployments, you can add or remove clients according to the application's resource requirements.

You can also add backend instances to increase cluster capacity or performance. To remove backend instances, you must first deactivate them to allow for safe data migration.

Note: Stopping or terminating backend instances causes a loss of all data of the instance store. For more information, refer to Amazon EC2 Instance Store.

## Converged deployment

In a converged deployment, every instance contributes its resources, such as drives, CPUs, and network interfaces, to the cluster.

A converged deployment is suitable for the following scenarios:

* **Small applications:** For applications with low resource requirements that need a high-performance filesystem. The application can run on the same instances that store the data.
* **Cloud-bursting:** For cloud-bursting workloads where you need to maximize resource allocation to both the application and the WEKA cluster to achieve peak performance.

<!-- ============================================ -->
<!-- File 22/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_cloudformation.md -->
<!-- ============================================ -->

---
description:
---

# CloudFormation template generator

## Before you begin

The APIs described here require an API token which can be obtained at https://get.weka.io/ui/account/api-tokens. Obtaining this token requires registration if you do not have an account.

## API overview

To generate a CloudFormation template, it is first necessary to decide which WEKA system version is to be installed. This is performed using the `https://<token>@get.weka.io/dist/v1/release` API which provides a list of all available versions:

```bash
$ curl https://<token>@get.weka.io/dist/v1/release
{
   "num_results" : 8,
   "page" : 1,
   "page_size" : 50,
   "num_pages" : 1,
   "objects" : [
      {
         "id" : "3.6.1",
         "public" : true,
         "final" : true,
         "trunk_id" : "",
         "s3_path" : "releases/3.6.1"
         .
         .
         .
      },
      ...
   ]
}
```

This list of releases available for installation is sorted backward from the most recent release. By default, 50 results are provided per page. To receive more results, use the `page=N` query parameter to receive the `Nth` page.

Note: **Note:** Usually, a request for more results is not necessary, since the first page contains the most recent releases.

Each release contains an ID field that identifies the release. In the examples below, version 3.6.1 has been used.

To generate a CloudFormation template, make a `POST` request to the `https://<token>@get.weka.io/dist/v1/aws/cfn/<version>`API:

```bash
$ spec='
{
  "cluster": [
    {
      "role": "backend",
      "instance_type": "i3en.2xlarge",
      "count": 10
    },
    {
      "role": "client",
      "instance_type": "r3.xlarge",
      "count": 2
    }
  ]
}
'
$ curl -X POST -H 'Content-Type: application/json' -d "$spec" https://<token>@get.weka.io/dist/v1/aws/cfn/3.6.1
{
   "url" : "https://wekaio-cfn-templates-prod.s3.amazonaws.com/cjibjp7ps000001o9pncqywv6.json",
   "quick_create_stack" : {
      "ap-southeast-2" : "...",
      ...
   }
}
```

In the example above, a template is generated for a cluster with 10 `i3en.2xlarge` backend instances and 2 `r3.xlarge` client instances. For details, see the [Deployment Types](deployment-types) and  [Supported EC2 instance types](../weka-installation-on-aws-using-terraform/supported-ec2-instance-types) sections.

## Request body

The `https://<token>@get.weka.io/dist/v1/aws/cfn/<version>` API provides a JSON object with a `cluster` property. `cluster` is a list of instance types, roles, and counts:

 | Property | Description |
 | --- | --- |
 | Property | Description |
 | role | Either backend or client.See the Deployment Types section. |
 | instance_type | One of the supported instance types, according to the role and supported instances.See the Supported EC2 Instance Types section. |
 | count | The number of instances of this type to include in the template. |
 | ami_id | When role is client, it is possible to specify a custom AMI-ID. For details, see the Custom Client AMI section. |
 | net | Either dedicated or shared, in client role only. For details, see the Dedicated vs. shared client networking section. |

It is possible to specify multiple groups of instances by adding more `role`/`instance_type`/`count` objects to the `cluster`array, as long as there are at least 6 backend instances (the minimum number of backend instances required to deploy a cluster).

### Custom client AMI

When specifying an `ami_id` in `client` groups, the specified AMI will be used when launching the client instances. The Weka system will be installed on top of this AMI in a separate EBS volume.

When `ami_id` is not specified, the client instances are launched with the latest Amazon Linux supported by the Weka system version selected to be installed.

Note the following when using a custom AMI-ID:

* AMIs are stored per region. Make sure to specify an AMI-ID that matches the region in which the CloudFormation template is deployed.
* The AMI operating system must be one of the supported operating systems listed in the [Prerequisites and compatibility](../../../prerequisites-and-compatibility#operating-system) section of the version installed. If the AMI defined is not supported or has an unsupported operating system, the installation may fail, and the CloudFormation stack will not be created successfully.

### Dedicated vs. shared client networking

By default, both client and backend instances are launched in the dedicated networking mode. Although this cannot be changed for backends, it can be controlled for client instances.

Dedicated networking means an ENI is created for internal cluster traffic in the client instances. This allows the WEKA system to bypass the kernel and provide throughput only limited by the instance network.

In shared networking, the client shares the instance‚Äôs network interface with all traffic passing through the kernel. Although slower, this mode is sometimes desirable when an ENI cannot be allocated or if the operating system does not allow more than one NIC.

## Returned result

The returned result is a JSON object with two properties: `url` and `quick_create_stack`.

The `url` property is a URL to an S3 object containing the generated template.

To deploy the CloudFormation template through the AWS console, a `quick_create_stack` property contains links to the console for each public AWS region. These links are pre-filled with your API token as a parameter to the template.

Note: **Note:** CloudFormation template URLs are valid for up to 1 week.

It is also possible to receive the template directly from the API call without saving it in a bucket. To do this, use a `?type=template`query parameter:

```bash
$ spec='...'  # same as above
$ curl -X POST -H 'Content-Type: application/json' -d "$spec" https://<token>@get.weka.io/dist/v1/aws/cfn/3.6.1?type=template
{"AWSTemplateFormatVersion": "2010-09-09", ...
```

## CloudFormation template parameters

The  CloudFormation stack parameters are described in the [Cluster CloudFormation Stack](../self-service-portal#cluster-cloudformation-stack) section.

## IAM role created in the template

The CloudFormation template contains an instance role that allows the WEKA cluster instances to call the following AWS APIs:

* `ec2:DescribeInstances`
* `ec2:DescribeNetworkInterfaces`
* `ec2:AttachNetworkInterface`
* `ec2:CreateNetworkInterface`
* `ec2:ModifyNetworkInterfaceAttribute`
* `ec2:DeleteNetworkInterface`

In case tiering is configured, additional AWS APIs permissions are given:

* `s3:DeleteObject`
* `s3:GetObject`
* `s3:PutObject`
* `s3:ListBucket`

## Additional operations

Once a CloudFormation template has been generated, it is possible to create a stack using the AWS console or the AWS CLI.

When the deployment is complete, the stack status updates to `CREATE_COMPLETE,` and it is possible to access the WEKA cluster GUI by going to the Outputs tab of the CloudFormation stack and clicking the GUI link.

Note: If there is a valid license in get.weka.io, the stack attempts to create a license, deploy it to the cluster, and start IO automatically.
With that, a filesystem is created and mounted on all instances. This shared filesystem is mounted on `/mnt/weka` in each cluster instance.

If the deployment is unsuccessful, see [Troubleshooting](troubleshooting) for the resolution of common deployment issues.

<!-- ============================================ -->
<!-- File 23/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_supported-ec2-instance-types-using-cloud-formation.md -->
<!-- ============================================ -->

---
description:
---

# Supported EC2 instance types using Cloud Formation

## Storage EC2 instances

The following EC2 instance types can operate as **backend**, **client,** or **converged** instances:

 | EC2 instance type | Supported models |
 | --- | --- |
 | I3en | i3en.2xlarge, i3en.3xlarge, i3en.6xlarge, i3en.12xlarge, i3en.24xlarge |

## Client EC2 instances

The following EC2 instance types can operate as **client** instances.

Note: Any backend instance can also be a client instance.

 | EC2 instance type | Supported models |
 | --- | --- |
 | C5 | c5.2xlarge, c5.4xlarge, c5.9xlarge, c5.12xlarge, c5.18xlarge, c5.24xlarge |
 | C5n | c5n.2xlarge, c5n.4xlarge, c5n.9xlarge, c5n.18xlarge |
 | C6a | c6a.2xlarge, c6a.4xlarge, c6a.8xlarge, c6a.12xlarge, c6a.16xlarge, c6a.32xlarge, c6a.48xlarge |
 | C6in | c6in.2xlarge, c6in.4xlarge, c6in.8xlarge, c6in.12xlarge, c6in.16xlarge, c6in.24xlarge, c6in.32xlarge |
 | C7i | c7i.2xlarge, c7i.4xlarge, c7i.8xlarge, c7i.12xlarge, c7i.16xlarge, c7i.24xlarge, cC7i.48xlarge |
 | G3 | g3.4xlarge, g3.8xlarge, g3.16xlarge |
 | G4 | g4dn.2xlarge, g4dn.4xlarge, g4dn.8xlarge, g4dn.12xlarge, g4dn.16xlarge |
 | G5 | g5.xlarge, g5.2xlarge, g5.4xlarge, g5.8xlarge, g5.12xlarge, g5.16xlarge |
 | HPc7a | hpc7a.2xlarge, hpc7a.48xlarge, hpc7a.96xlarge |
 | I3 | i3.xlarge, i3.2xlarge, i3.4xlarge, i3.8xlarge, i3.16xlarge |
 | I3en | i3en.xlarge, i3en.2xlarge, i3en.3xlarge, i3en.6xlarge, i3en.12xlarge, i3en.24xlarge |
 | Inf1 | inf1.2xlarge, inf1.6xlarge, inf1.24xlarge |
 | Inf2 | inf2.xlarge, inf2.8xlarge, inf2.24xlarge, inf2.48xlarge |
 | M5 | m5.xlarge, m5.2xlarge, m5.4xlarge, m5.8xlarge, m5.12xlarge, m5.16xlarge, m5.24xlarge |
 | M5n | m5n.xlarge, m5n.2xlarge, m5n.4xlarge, m5n.8xlarge, m5n.12xlarge, m5n.16xlarge, m5n.24xlarge, m5dn.xlarge, m5dn.2xlarge, m5dn.4xlarge, m5dn.8xlarge, m5dn.12xlarge, m5dn.16xlarge, m5dn.24xlarge |
 | M6a | m6a.xlarge, m6a.2xlarge, m6a.4xlarge, m6a.8xlarge, m6a.12xlarge, m6a.16xlarge, m6a24xlarge, m6a.32xlarge, m6a.48xlarge |
 | M6i | m6i.xlarge, m6i.2xlarge, m6i.4xlarge, m6i.8xlarge, m6i.12xlarge, m6i.16xlarge, m6i.24xlarge, m6i.32xlarge |
 | M6id | m6id.xlarge, m6id.2xlarge, m6id.4xlarge, m6id.8xlarge, m6id.12xlarge, m6id.16xlarge, m6id.24xlarge, m6id.32xlarge |
 | M6idn | m6idn.xlarge, m6idn.2xlarge, m6idn.4xlarge, m6idn.8xlarge, m6idn.12xlarge, m6idn.16xlarge, m6idn.24xlarge, m6idn.32xlarge |
 | P2 | p2.xlarge, p2.8xlarge, p2.16xlarge |
 | P3 | p3.2xlarge, p3.8xlarge, p3.16xlarge |
 | P4 | p4d.24xlarge, p4de.24xlarge |
 | R5 | r5.xlarge, r5.2xlarge, r5.4xlarge, r5.8xlarge, r5.12xlarge, r5.16xlarge, r5.24xlarge |
 | R5n | r5n.xlarge, r5n.2xlarge, r5n.4xlarge, r5n.8xlarge, r5n.12xlarge, r5n.16xlarge, r5n.24xlarge |
 | R6a | r6a.xlarge, r6a.2xlarge, r6a.4xlarge, r6a.8xlarge, r6a.12xlarge, r6a.16xlarge, r6a.32xlarge, r6a.48xlarge |
 | R6i | r6i.xlarge, r6i.2xlarge, r6i.4xlarge, r6i.8xlarge, r6i.12xlarge, r6i.16xlarge, r6i.24xlarge, r6i.32xlarge |
 | R6id | r6id.xlarge, r6id.2xlarge, r6id.4xlarge, r6id.8xlarge, r6id.12xlarge, r6id.16xlarge, r6id.24xlarge, r6id.32xlarge |
 | R6idn | r6idn.xlarge, r6idn.2xlarge, r6idn.4xlarge, r6idn.8xlarge, r6idn.12xlarge, r6idn.16xlarge, r6idn.24xlarge, r6idn.32xlarge |
 | R6in | r6in.xlarge, r6in.2xlarge, r6in.4xlarge, r6in.8xlarge, r6in.12xlarge, r6in.16xlarge, r6in.24xlarge, r6in.32xlarge |
 | X1 | x1.16xlarge, x1.32xlarge |
 | X1e | x1e.16xlarge, x1e.32xlarge |

**Related topics**

**Related information**

AWS instance types

<!-- ============================================ -->
<!-- File 24/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_self-service-portal.md -->
<!-- ============================================ -->

---
description:
---

# Self-service portal

## Overview

The WEKA Self-Service Portal is a planning tool for WEKA clusters to meet storage requirements when installing in AWS.

It is possible to start by just entering the capacity required, configuring advanced parameters such as required performance and even provision of a multi-AZ cluster for added reliability.

Each configuration can be immediately deployed as a CloudFormation stack by redirecting to the AWS console.

Note: **Note:** CloudFormation should only be used for initial deployment. To expand cluster resources, refer to [Expanding & Shrinking Cluster Resources](../../../operation-guide/expanding-and-shrinking-cluster-resources).

Once the cluster is deployed:

1. Refer to Getting Started with WEKA section. See [Manage the system using the WEKA GUI](../../../getting-started-with-weka/manage-the-system-using-weka-gui) or [Manage the system using the WEKA CLI](../../../getting-started-with-weka/manage-the-system-using-weka-cli).
2. Refer to [Run first IOs with WEKA filesystem](../../../getting-started-with-weka/performing-the-first-io) to quickly get familiar with creating, mounting, and writing to a WEKA filesystem.

## Plan a cluster

The Self-Service Portal is available at https://start.weka.io. Its main screen is divided into two panes: the left pane, which is used for input requirements, and the right pane which displays possible configurations for the defined requirements.

As shown in the screen above, configuration options include the total capacity, the desired deployment model, and additional performance requirements. For more information about deployment types, refer to [Deployment Types](deployment-types).

## Deploy a cluster

Once the configuration to be deployed has been found, click the Deploy to AWS button next to the desired configuration. At this point, it is possible to specify additional options for the deployment, such as adding client instances or selecting the WEKA system version to be deployed.

Once everything is ready to deploy the cluster, click the Deploy to AWS button. This will display the AWS CloudFormation screen with a template containing the configured cluster.

Note: **Note:** Before deploying the configuratio&#x6E;**,** please refer to the [Prerequisites for Deployment](../deployment-types#prerequisites-for-deployment) section.

## CloudFormation screen

After clicking the Deploy to AWS button, the AWS CloudFormation screen is displayed, requiring the creation of stacks.

In the Create Stack screen, define the parameters which are specific to your AWS account.

### Cluster CloudFormation stack

 | **Parameter** | **Description** |
 | ------------- | ------------------------------------------------------------------------------------------------------ |
 | `Stack name` | The name that will be given to the stack in CloudFormation. This name has to be unique in the account. |
 | `SSH Key` | The SSH-key for the `ec2-user` that will be used to connect to the instances. |
 | `VPC` | The VPC in which the WEKA cluster will be deployed. |
 | `Subnet` | The subnet in which the WEKA cluster will be deployed. |

Define the parameters for WEKA cluster configuration:

 | **Parameter** | **Description** |
 | ------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `Network Topology` | Network topology of the environment: <ul><li> Public Subnet </li><li> Private subnet with NAT internet routing </li><li> Private subnet using Weka VPC endpoint - requires to create a CloudFormation stack (once per VPC) that creates the required resources.</li><li> Private subnet using custom proxy - requires to create a CloudFormation stack (once per VPC) that creates the required resources.</li></ul> Related topic: <a href="#cluster-cloudformation-stack">Cluster CloudFormation stack</a> |
 | `Custom Proxy` | A custom proxy for private network internet access. Only relevant when `Private network using custom proxy` is selected as the `Network Topology` parameter. |
 | `WekaVolumeType` | Volume type for the WEKA partition. `GP3` is not yet available in all zones/regions (e.g., not available in local zones). In such a case, you must select the `GP2` volume type. When available, using `GP3` is preferred. |
 | `API Token` | The API token for WEKA's distribution site. This can be obtained at https://get.weka.io/ui/account/api-tokens. |
 | `Admin Password` | Sets the admin password after the cluster has been created. If no value is provided, the password is set to `admin.` |

Define the following optional parameters if tiering to S3 is desired:

 | **Parameter** | **Description** |
 | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `New S3 Bucket` | The new S3 bucket name to create and attach to the filesystem created by the template. The bucket will not be deleted when the stack is destroyed. |
 | `Existing S3 Bucket` | The existing S3 bucket name to attach to the filesystem created by the template. The bucket has to be in the same region where the cluster is deployed. If this parameter is provided, the `New S3 Bucket` parameter is ignored. |
 | `Tiering SSD Percent` | Sets how much of the filesystem capacity (in percent) should reside on SSD. This parameter is applicable only if `New S3 Bucket` or `Existing S3 Bucket` parameters have been defined. |

Note: For public subnets, make sure to select a subnet that has the Enable Auto-Assign Public IPv4 Address setting turned on, or select a subnet that has Internet connectivity.

Once all required parameters have been filled, make sure to check the "I acknowledge that AWS CloudFormation might create IAM resources‚Äù checkbox at the bottom and click the Create Stack button:

## Deploying in a Private Network

When deploying in a private network, without a NAT (using a WEKA proxy or a custom proxy), some resources should be created (once) per VPC (such as the WEKA VPC endpoint, S3 gateway, or EC2 endpoint).

Copy the link under the Network Topology parameter, and run it in a new browser tab. The AWS CloudFormation screen is displayed, requiring the creation of the prerequisites stack.

In the Create Stack screen, define the parameters specific to your AWS account.

### Prerequisites CloudFormation stack

Note: To run this stack, `enableDnsHostnames`  and `enableDnsSupport` DNS attributes should be enabled for the VPC.

 | **Parameter** | **Description** |
 | ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `Stack name` | The name that will be given to the stack in CloudFormation. This name has to be unique in the account. |
 | `VPC` | The VPC in which the prerequisites resources (and WEKA cluster) will be deployed. |
 | `Subnet` | The subnet in which the prerequisites resources (and WEKA cluster) will be deployed. |
 | `RouteTable` | Route table ID of the chosen subnet for S3 gateway creation. |
 | `Network Topology` | Network topology of the environment: <ul><li> Private subnet using Weka VPC endpoint </li><li> Private subnet using custom proxy </li></ul> |
 | `S3 Gateway` | Only choose to create an S3 Gateway if non already exist for the VPC |
 | `Ec2 Endpoint` | Only choose to create an EC2 Endpoint if non already exist for the VPC |

## Cluster deployment process

The cluster deployment process takes about 10 minutes. During this time, the following occurs:

1. The AWS resources required for the cluster are provisioned.
2. The WEKA system is installed on each instance provisioned for the cluster.
3. A cluster is created using all backend instances.
4. All client instances are created.
5. A filesystem is created using all the available capacity and is mounted on all client instances. This shared filesystem is mounted on `/mnt/weka` in each cluster instance.

Note: If the deployment is unsuccessful, see [Troubleshooting ](troubleshooting)for how to resolve common deployment issues.

Once the deployment is complete, the CloudFormation stack status will be updated to `CREATE_COMPLETE`. At this point, it is possible to access the WEKA system cluster GUI by going to the Outputs tab of the CloudFormation stack and clicking the GUI link (or by http://\<backend server name or IP address>:14000).

**Related topics**

<!-- ============================================ -->
<!-- File 25/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_adding-clients.md -->
<!-- ============================================ -->

# Add clients to a WEKA cluster on AWS

## Introduction

When launching a WEKA cluster, either through the [Self-service portal](self-service-portal) or via a [CloudFormation template](cloudformation), it is also possible to launch client instances. However, sometimes it may be required to add more clients after the cluster has been installed. To add more clients as separate instances, follow the instructions below.

Note: It is advisable to turn off auto kernel updates so it will not get upgraded to a yet unsupported version.

## Add clients as separate instances

### Step 1: Launch the new instances <a href="#step-1-launch-new-instances" id="step-1-launch-new-instances"></a>

Note: New client instances must be one of the types specified in the [Supported EC2 instance types](../weka-installation-on-aws-using-terraform/supported-ec2-instance-types) section.

When launching new clients, ensure the following concerning networking and root volume:

#### **Networking**

* For best performance, it is recommended that the new clients will be in the **same subnet** as the backend instances. Alternatively, they can be in a routable subnet to the backend instances in the same AZ (note that cross-AZ traffic also incurs expensive network charges).
* They must use the same **security group** as the backends they will connect to, or alternatively, use a **security group** that allows them to connect to the backend instances.
* **Enhanced networking** is enabled as specified in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html.

#### IAM instance profile

When adding a client, it is required to provide permissions to several AWS APIs, as described in [IAM role created in the template](../cloudformation#iam-role-created-in-the-template).

These permissions are automatically created in an instance profile as part of the CloudFormation stack. It is possible to use the same instance profile as one of the backend instances to ensure the same credentials are given to the new client.

The network interface permissions are required to create and attach a network interface to the new client. A separate NIC is required to allow the WEKA client to preallocate the network resource for the fastest performance.

If the client is not provided with these permissions, it can only provide `ec2:*` and create an additional NIC in the same security group and subnet described above when mounting a second cluster from a single client (see [Mount filesystems from multiple clusters on a single client](../../../weka-filesystems-and-object-stores/mounting-filesystems/mount-fs-from-scmc)).

#### Root volume

The client's **root volume** must be at least 48 GiB in size and either `GP2` or `IO1` type.

The WEKA software is installed under `/opt/weka`. If it is not possible to change the size of the root volume, an additional EBS volume can be created, formatted, and mounted under `/opt/weka`. Make sure that the new volume is either `GP2` or `IO1` type.

### Step 2: Mount the filesystems

Note: The clients created using the Self-Service Portal are stateless. The mount command automatically installs the software version, and there is no need to join the client to the cluster.

To mount a filesystem in this manner, first install the WEKA agent from one of the backend instances and then mount the filesystem. For example:

```
# Agent Installation (one time)
| curl http://Backend-1:14000/dist/v1/install | sh |

# Creating a mount point (one time)
mkdir -p /mnt/weka

# Mounting a filesystem
mount -t wekafs Backend-1/my_fs /mnt/weka
```

For the first mount, this will install the WEKA software and automatically configure the client. For more information on mount and configuration options, see the [Mount filesystems using the stateless clients feature](../../../../weka-filesystems-and-object-stores/mounting-filesystems#mounting-filesystems-using-stateless-clients) section.

It is possible to configure the client OS to mount the filesystem at boot time automatically. For more information, see the [Mount filesystems using fstab](../../../../weka-filesystems-and-object-stores/mounting-filesystems#mounting-filesystems-using-fstab) or [Mount filesystems using autofs](../../../../weka-filesystems-and-object-stores/mounting-filesystems#mounting-filesystems-using-autofs) sections.

## Add clients that are always part of the cluster

Note: It is possible to add instances that do not contribute resources to the cluster but are used for mounting filesystems. It is recommended to use the previously described method for adding client instances for mounting purposes. However, in some cases, it could be useful to permanently add them to the cluster, e.g., to use these instances as NFS/SMB servers which are always expected to be up.

### Step 1: Launch the new instances

This is the same step as in the previous method of adding a client.

### Step 2: Install the WEKA software <a href="#step-2-install-wekaio-software" id="step-2-install-wekaio-software"></a>

To download the WEKA software, go to https://get.weka.io  and select the software version. After selecting the version, select the operating system to install and run the download command line as `root` on all the new client instances.

When the download is complete, untar the downloaded package and run the `install.sh` command in the package directory.

Note: **Example:**
If you downloaded version 3.6.1, run `cd weka-3.6.1` and then run `./install.sh`.

Note: **ENA Driver Notice**
When installing on an AWS instance with Elastic Network Adapter (ENA) and a non-up-to-date kernel, it may be necessary to install the ENA drivers or upgrade to a more recent operating system version. The ENA driver is automatically available on operating systems starting with Red Hat/CentOS 7.4, Ubuntu 16, and Amazon Linux 2017.09.

### Step 3: Add clients to the cluster <a href="#step-3-add-clients-to-cluster" id="step-3-add-clients-to-cluster"></a>

Once the WEKA software is installed, the clients are ready to join the cluster. To add the clients, run the following command line on each of the client instances:

```
weka local run -e WEKA_HOST=<backend-ip> aws-add-client <client-instance-id>

```

where `<backend-ip>` is the IP address or hostname of one of the backend instances.

On most shells the following would get the client instance ID and add it to the cluster:

```
weka local run -e WEKA_HOST=<backend-ip> aws-add-client $(curl -s http://169.254.169.254/latest/meta-data/instance-id)

```

If successful, running the`aws-add-client` command will display the following line:

```
Client has joined the cluster

```

Note: **Dedicated client resources**
Once the `aws-add-client` command is complete, one core and 6.3 GB of RAM are allocated for the WEKA system on the client instance. This is performed as part of the WEKA system preallocating resources, ensuring that variance in client activity does not result in allocating resources that may affect the programs running on the client. For more information, see [Memory resource planning](../../../bare-metal/planning-a-weka-system-installation#memory-resource-planning).

### Step 4: Mount filesystems on the clients <a href="#step-4-mount-filesystem-on-clients" id="step-4-mount-filesystem-on-clients"></a>

It is now possible to mount the filesystems on the client instances.

Note: **Example:**
Using the `mkdir -p /mnt/weka && mount -t wekafs default /mnt/weka` command will mount the `default` filesystem under `/mnt/weka.`

Note: For more information about available mount options, see [Mount filesystems](../../../weka-filesystems-and-object-stores/mounting-filesystems).

<!-- ============================================ -->
<!-- File 26/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_auto-scaling-group.md -->
<!-- ============================================ -->

# Auto scaling group

Auto-scaling is useful to easily scale the number of EC2 instances up or down at need.

After deploying the WEKA cluster via CloudFormation, it is possible to create an auto-scaling group to ease the WEKA cluster size management.

You can create an auto-scaling group for your cluster by running the wekactl utility.

You can control the number of instances by either changing the desired capacity of instances from the AWS auto-scaling group console or defining your custom metrics and scaling policy in AWS. Once the desired capacity has changed, WEKA takes care of safely scaling the instances.

Note: When scaling the number of instances increase/decrease and, along with that, the cluster resources, to automatically take advantage of the extra SSD capacity, it is possible to use [thin-provisioned](../../../../weka-system-overview/filesystems#thin-provisioning) filesystems.
When downscaling, ensure the minimum SSD capacity of the filesystems can fit into the lower capacity cluster or tiered to S3.

For more information and documentation on the utility, refer to the wekactl GitHub repository.

<!-- ============================================ -->
<!-- File 27/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_aws-outposts-deployment.md -->
<!-- ============================================ -->

---
description: This page describes how to install WEKA on AWS Outposts
---

# AWS Outposts deployment

## Overview

AWS Outposts is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to virtually any data center, co-location space, or on-premises facility for a consistent hybrid experience. AWS Outposts is ideal for workloads that require low latency access to on-premises systems, local data processing, or local data storage.

## Deployment of a WEKA cluster in AWS Outposts

A WEKA cluster deployment in AWS Outposts follows the guidelines specified in the [Deployment types](deployment-types) section.

To deploy a WEKA cluster in AWS Outposts, use a CloudFormation template, which can be obtained as specified in the [CloudFormation template Generator](cloudformation) section.

Note: AWS Outposts do not currently support placement groups, so the placement group from the template should be removed.

This template can be customized. For further assistance, contact the [Customer Success Team](../../../../support/getting-support-for-your-weka-system#contact-customer-success-team).

<!-- ============================================ -->
<!-- File 28/259: planning-and-installation_aws_weka-installation-on-aws-using-the-cloud-formation_troubleshooting.md -->
<!-- ============================================ -->

---
description:
---

# Troubleshooting

Using CloudFormation deployment saves a lot of potential errors that may occur during installation, especially in the configuration of security groups and other connectivity-related issues. However, the following errors related to the following subjects may occur during installation:

* Installation logs
* AWS account limits
* AWS instance launch error
* Launch in placement group error
* Instance type not supported in AZ
* ClusterBootCondition timeout
* Clients failed to join cluster

## Installation logs

As explained in [Self-Service Installation](self-service-portal), each instance launched in a WEKA CloudFormation template starts by installing WEKA on itself. This is performed using a script named `wekaio-instance-boot.sh` and launched by cloud-init. All logs generated by this script are written to the instance‚Äôs Syslog.

Additionally, the CloudWatch Logs Agent is installed on each instance, dumping Syslog to CloudWatch under a log-group named`/wekaio/<stack-name>`. For example, if the stack is named`cluster1,` a log-group named `/wekaio/cluster1` should appear in CloudWatch a few moments after the template shows the instances have reached CREATE_COMPLETE state.

Under the log-group, there should be a log-stream for each instance Syslog matching the instance name in the CloudFormation template. For example, in a cluster with 6 backend instances, log-streams named `Backend0-syslog` through `Backend5-syslog` should be observed.

## AWS account limits

When deploying the stack, this error may be received in the description of a CREATE_FAILED event for one or more instances, indicating that more instances (N) have been requested than that permitted by the current instance limit of L for the specified instance type. To request an adjustment to this limit, go to aws.amazon.com to open a support case with AWS.

## AWS instance launch error

If the error _Instance i-0a41ba7327062338e failed to stabilize. Current state: shutting-down. Reason: Server.InternalError: Internal error on launch_ is received, one of the instances was unable to start. This is an internal AWS error and it is necessary to try to deploy the stack again.

## Launch in placement group error

If the error _We currently do not have sufficient capacity to launch all of the additional requested instances into Placement Group 'PG'_ is received, it was not possible to place all the requested instances in one placement-group.

The CloudFormation template creates all instances in one placement-group to guarantee best performance. Consequently, if the deployment fails with this error, try to deploy in another AZ.

## Instance type not supported in AZ

If the error _The requested configuration is currently not supported. Please check the documentation for supported configurations_ or _Your requested instance type (T) is not supported in your requested Availability Zone (AZ). Please retry your request by not specifying an Availability Zone or choosing az1, az2, az3_ is received, the instance type that you tried to provision is not supported in the specified AZ. Try selecting another subnet to deploy the cluster in, which will implicitly select another AZ.

## ClusterBootCondition timeout

When a _ClusterBootCondition timeout_ occurs, there was a problem creating the initial WEKA system cluster. To debug this error, look in the `Backend0-syslog` log-stream (as described above). The first backend instance is responsible for creating the cluster and therefore, its log should provide the information necessary to debug this error.

## Clients failed to join

When the message _Clients failed to join for uniqueId: ClientN_ is received while in the WaitCondition, one of the clients was unable to join the cluster. Look at the Syslog of the client specified in uniqueId as described above.

Note: **Example:** If the error message specifies that client 3 failed to join, a message ending with `uniqueId: Client3` should be displayed. Look at the log-stream named `Client3-syslog`.

## Health monitoring

You can monitor the cluster instances by checking the cluster EC2 instances in the AWS EC2 service. You can set up Cloud Watch as external monitoring to the cluster.

Connecting to the WEKA cluster GUI provides a [system dashboard](../../../../getting-started-with-weka/manage-the-system-using-weka-gui#system-dashboard) where you can see if any component is not properly functioning and view system [alerts](../../../operation-guide/alerts), [events](../../../operation-guide/events), and [statistics](../../../operation-guide/statistics).

<!-- ============================================ -->
<!-- File 29/259: planning-and-installation_aws_weka-installation-on-aws-using-terraform.md -->
<!-- ============================================ -->

# WEKA installation on AWS using Terraform

WEKA provides a ready-to-deploy Terraform package for installing the WEKA cluster on AWS Virtual Private Cloud (VPC).

The following diagram provides an overview of the various steps automated with the Terraform-driven provisioning of the WEKA cluster backend servers on AWS EC2 instances.

### Workflow description

* **Create AWS Placement Groups:** Create Cluster Placement Groups to reduce network latency between WEKA nodes. This configuration prioritizes performance over resilience and may reduce fault tolerance in the event of hardware failures.
*   **Create AWS Launch Template and Auto Scaling Group for WEKA cluster expansion:** Create an AWS Launch Template and Auto Scaling Group to provision EC2 instances for the WEKA cluster.

    The launch template automates the deployment script to install and configure WEKA software during initial cluster creation and expand the cluster with additional instances.
* **Configure AWS Secrets Manager for secure WEKA cluster operations:** Create secrets in AWS Secrets Manager to facilitate secure communication between AWS Lambda functions and the WEKA cluster. This ensures smooth scale-out, scale-in, and auto-healing operations.
* **Configure DynamoDB for Terraform state:** Create state items in an Amazon DynamoDB table to effectively manage Terraform's declarative state.
* **Create CloudWatch log groups for WEKA cluster logs:** Create Amazon CloudWatch log groups to store logs generated by the WEKA cluster.
* **Deploy AWS Lambda functions for WEKA software configuration:** Create AWS Lambda functions to run after CloudWatch log groups are created. These functions assist in installing and configuring WEKA software on EC2 instances.
* **Create AWS Step Function for WEKA cluster scaling:** Create an AWS Step Function state machine to facilitate user-driven automated scale-out and scale-in operations for the WEKA cluster.
* **Create CloudWatch event rule for WEKA cluster monitoring:** Create a CloudWatch event rule to periodically check the state of the WEKA cluster and trigger healing or scaling actions as necessary.

<!-- ============================================ -->
<!-- File 30/259: planning-and-installation_aws_weka-installation-on-aws-using-terraform_required-services-and-supported-regions.md -->
<!-- ============================================ -->

# Required services and supported regions

## Required services

The AWS region must support the following services used in WEKA on AWS.

* AWS EC2 Instances
* AWS Lambda
* AWS Step Functions
* AWS CloudWatch event rule
* AWS S3 (required for snap/tiering to object)

## Supported regions

 | Code | Name |
 | -------------- | ------------------------- |
 | us-east-1 | US East (N. Virginia) |
 | us-east-2 | US East (Ohio) |
 | us-west-1 | US West (N. California) |
 | us-west-2 | US West (Oregon) |
 | ap-south-1 | Asia Pacific (Mumbai) |
 | ap-northeast-1 | Asia Pacific (Tokyo) |
 | ap-northeast-2 | Asia Pacific (Seoul) |
 | ap-southeast-1 | Asia Pacific (Singapore) |
 | ap-southeast-2 | Asia Pacific (Sydney) |
 | ca-central-1 | Canada (Central) |
 | eu-central-1 | Europe (Frankfurt) |
 | eu-north-1 | Europe (Stockholm) |
 | eu-west-1 | Europe (Ireland) |
 | eu-west-2 | Europe (London) |
 | sa-east-1 | South America (S√£o Paulo) |

**Related topic**

**Related information**

Regions and Zones

<!-- ============================================ -->
<!-- File 31/259: planning-and-installation_aws_weka-installation-on-aws-using-terraform_supported-ec2-instance-types.md -->
<!-- ============================================ -->

# Supported EC2 instance types using Terraform

## Backend EC2 instances

The following EC2 instance models can operate as **backend**, **client,** or **converged** instances. The default EC2 instance model for backends is **i3en.2xlarge**.

 | EC2 instance type | Supported instances |
 | --- | --- |
 | I3en | i3en.2xlarge, i3en.3xlarge, i3en.6xlarge, i3en.12xlarge, i3en.24xlarge |

## Client EC2 instances

The following EC2 instance models can operate as **client** instances. The default EC2 instance model for clients is **c5.2xlarge**.

Note: * Support for WEKA client over UDP mode is extended to any Intel or AMD CPU-based instance type, provided that the VM type meets the resource requirements specified in the  topic.
* Any backend instance can also be a client instance.

### General purpose <a href="#general_purpose" id="general_purpose"></a>

 | EC2 instance type | Supported instances |
 | --- | --- |
 | M5 | m5.xlarge, m5.2xlarge, m5.4xlarge, m5.8xlarge, m5.12xlarge, m5.16xlarge, m5.24xlarge |
 | M5n | m5n.xlarge, m5n.2xlarge, m5n.4xlarge, m5n.8xlarge, m5n.12xlarge, m5n.16xlarge, m5n.24xlarge, m5dn.xlarge, m5dn.2xlarge, m5dn.4xlarge, m5dn.8xlarge, m5dn.12xlarge, m5dn.16xlarge, m5dn.24xlarge |
 | M6a | m6a.xlarge, m6a.2xlarge, m6a.4xlarge, m6a.8xlarge, m6a.12xlarge, m6a.16xlarge, m6a24xlarge, m6a.32xlarge, m6a.48xlarge |
 | M6g | m6g.xlarge, m6g.2xlarge, m6g.4xlarge, m6g.8xlarge, m6g.12xlarge, m6g.16xlarge |
 | M6gd | m6gd.xlarge, m6gd.2xlarge, m6gd.4xlarge, m6gd.8xlarge, 1m6gd.2xlarge, m6gd.16xlarge |
 | M6i | m6i.xlarge, m6i.2xlarge, m6i.4xlarge, m6i.8xlarge, m6i.12xlarge, m6i.16xlarge, m6i.24xlarge, m6i.32xlarge |
 | M6id | m6id.xlarge, m6id.2xlarge, m6id.4xlarge, m6id.8xlarge, m6id.12xlarge, m6id.16xlarge, m6id.24xlarge, m6id.32xlarge |
 | M6idn | m6idn.xlarge, m6idn.2xlarge, m6idn.4xlarge, m6idn.8xlarge, m6idn.12xlarge, m6idn.16xlarge, m6idn.24xlarge, m6idn.32xlarge |
 | M6in | m6in.xlarge , m6in.2xlarge , m6in.4xlarge , m6in.8xlarge, m6in.12xlarge , m6in.16xlarge , m6in.24xlarge |
 | M7a | m7a.xlarge , m7a.2xlarge, m7a.4xlarge, m7a.8xlarge, m7a.12xlarge, m7a.16xlarge, m7a.24xlarge, m7a.32xlarge, m7a.48xlarge |
 | M7i | m7i.xlarge, m7i.2xlarge, m7i.4xlarge, m7i.8xlarge, m7i.12xlarge, m7i.16xlarge, m7i.24xlarge, m7i.48xlarge |
 | M7g | m7g.xlarge, m7g.2xlarge, m7g.4xlarge, m7g.8xlarge, m7g.12xlarge, m7g.16xlarge |
 | M7gd | m7gd.xlarge, m7gd.2xlarge, m7gd.4xlarge, m7gd.8xlarge, m7gd.12xlarge, m7gd.16xlarge |

### Compute optimized <a href="#compute_optimized" id="compute_optimized"></a>

 | EC2 instance type | Supported instances |
 | --- | --- |
 | C5 | c5.2xlarge, c5.4xlarge, c5.9xlarge, c5.12xlarge, c5.18xlarge, c5.24xlarge |
 | C5a | c5a.2xlarge , c5a.4xlarge, c5a.8xlarge, c5a.12xlarge, c5a.16xlarge, c5a.24xlarge |
 | C5ad | c5ad.2xlarge , c5ad.4xlarge, c5ad.8xlarge, c5ad.12xlarge, c5ad.16xlarge, c5ad.24xlarge |
 | C5n | c5n.2xlarge, c5n.4xlarge, c5n.9xlarge, c5n.18xlarge |
 | C6a | c6a.2xlarge, c6a.4xlarge, c6a.8xlarge, c6a.12xlarge, c6a.16xlarge, c6a.32xlarge, c6a.48xlarge |
 | C6g | c6g.2xlarge, c6g.4xlarge, c6g.8xlarge, c6g.12xlarge, c6g.16xlarge |
 | C6gd | c6gd.2xlarge, c6gd.4xlarge, c6gd.8xlarge, c6gd.12xlarge, c6gd.16xlarge |
 | C6gn | c6gn.2xlarge, c6gn.4xlarge, c6gn.8xlarge, c6gn.12xlarge, c6gn.16xlarge |
 | C6in | c6in.2xlarge, c6in.4xlarge, c6in.8xlarge, c6in.12xlarge, c6in.16xlarge, c6in.24xlarge, c6in.32xlarge |
 | C7a | c7a.2xlarge, c7a.4xlarge, c7a.8xlarge, c7a.12xlarge, c7a.16xlarge, c7a.24xlarge, c7a.32xlarge, c7a.48xlarge |
 | C7g | c7g.2xlarge, c7g.4xlarge, c7g.8xlarge, c7g.12xlarge, c7g.16xlarge |
 | C7gd | c7gd.2xlarge, c7gd.4xlarge, c7gd.8xlarge, c7gd.12xlarge, c7gd.16xlarge |
 | C7i | c7i.2xlarge, c7i.4xlarge, c7i.8xlarge, c7i.12xlarge, c7i.16xlarge, c7i.24xlarge, cC7i.48xlarge |

### Memory optimized <a href="#memory_optimized" id="memory_optimized"></a>

 | EC2 instance type | Supported instances |
 | --- | --- |
 | R5 | r5.xlarge, r5.2xlarge, r5.4xlarge, r5.8xlarge, r5.12xlarge, r5.16xlarge, r5.24xlarge |
 | R5n | r5n.xlarge, r5n.2xlarge, r5n.4xlarge, r5n.8xlarge, r5n.12xlarge, r5n.16xlarge, r5n.24xlarge |
 | R6a | r6a.xlarge, r6a.2xlarge, r6a.4xlarge, r6a.8xlarge, r6a.12xlarge, r6a.16xlarge, r6a.32xlarge, r6a.48xlarge |
 | R6i | r6i.xlarge, r6i.2xlarge, r6i.4xlarge, r6i.8xlarge, r6i.12xlarge, r6i.16xlarge, r6i.24xlarge, r6i.32xlarge |
 | R6id | r6id.xlarge, r6id.2xlarge, r6id.4xlarge, r6id.8xlarge, r6id.12xlarge, r6id.16xlarge, r6id.24xlarge, r6id.32xlarge |
 | R6idn | r6idn.xlarge, r6idn.2xlarge, r6idn.4xlarge, r6idn.8xlarge, r6idn.12xlarge, r6idn.16xlarge, r6idn.24xlarge, r6idn.32xlarge |
 | R6in | r6in.xlarge, r6in.2xlarge, r6in.4xlarge, r6in.8xlarge, r6in.12xlarge, r6in.16xlarge, r6in.24xlarge, r6in.32xlarge |
 | R6g | r6g.xlarge, r6g.2xlarge, r6g.4xlarge, r6g.8xlarge, r6g.12xlarge, r6g.16xlarge |
 | R6gd | r6gd.xlarge, r6gd.2xlarge, r6gd.4xlarge, r6gd.8xlarge, r6gd.12xlarge, r6gd.16xlarge |
 | R7a | r7a.xlarge, r7a.2xlarge, r7a.4xlarge, r7a.8xlarge, r7a.12xlarge, r7a.16xlarge, r7a.24xlarge, r7a.32xlarge, r7a.48xlarge |
 | R7iz | r7iz.xlarge, r7iz.2xlarge, r7iz.4xlarge, r7iz.8xlarge, r7iz.12xlarge, r7iz.16xlarge, r7iz.32xlarge |
 | R7g | r7g.xlarge, r7g.2xlarge, r7g.4xlarge, r7g.8xlarge, r7g.12xlarge, r7g.16xlarge |
 | R7gd | r7gd.xlarge, r7gd.2xlarge, r7gd.4xlarge, r7gd.8xlarge, r7gd.12xlarge, r7gd.16xlarge |
 | X1 | x1.16xlarge, x1.32xlarge |
 | X1e | x1e.16xlarge, x1e.32xlarge |
 | X2idn | x2idn.16xlarge, x2idn.24xlarge, x2idn.32xlarge |
 | X2iedn | x2iedn.xlarge, x2iedn.2xlarge, x2iedn.4xlarge, x2iedn.8xlarge, x2iedn.16xlarge, x2iedn.24xlarge |
 | Z1d | z1d.xlarge, z1d.2xlarge, z1d.3xlarge, z1d.6xlarge, z1d.12xlarge |

### Accelerated computing <a href="#accelerated_computing" id="accelerated_computing"></a>

 | EC2 instance type | Supported instances |
 | --- | --- |
 | F1 | f1.2xlarge, f1.4xlarge, f1.16xlarge |
 | G3 | g3.4xlarge, g3.8xlarge, g3.16xlarge |
 | G4dn | g4dn.2xlarge, g4dn.4xlarge, g4dn.8xlarge, g4dn.12xlarge, g4dn.16xlarge |
 | G5 | g5.xlarge, g5.2xlarge, g5.4xlarge, g5.8xlarge, g5.12xlarge, g5.16xlarge |
 | G5g | g5g.2xlarge, g5g.4xlarge, g5g.8xlarge, g5g.16xlarge |
 | G6 | g6.xlarge, g6.2xlarge, g6.4xlarge, g6.8xlarge, g6.12xlarge, g6.16xlarge, g6.24xlarge, g6.48xlarge |
 | GR6 | gr6.4xlarge, gr6.8xlarge |
 | Inf1 | inf1.2xlarge, inf1.6xlarge, inf1.24xlarge |
 | Inf2 | inf2.xlarge, inf2.8xlarge, inf2.24xlarge, inf2.48xlarge |
 | P2 | p2.xlarge, p2.8xlarge, p2.16xlarge |
 | P3 | p3.2xlarge, p3.8xlarge, p3.16xlarge |
 | P4 | p4d.24xlarge, p4de.24xlarge |
 | P5 | p5.48xlarge |
 | Trn1 | trn1.2xlarge, trn1.32xlarge , trn1n.32xlarge |

### Storage optimized

 | EC2 instance type | Supported instances |
 | --- | --- |
 | I3en | i3en.xlarge, i3en.2xlarge, i3en.3xlarge, i3en.6xlarge, i3en.12xlarge, i3en.24xlarge |

### HPC optimized <a href="#hpc_optimized" id="hpc_optimized"></a>

 | EC2 instance type | Supported instances |
 | --- | --- |
 | HPc6 | hpc6.48xlarge |
 | HPc6a | hpc6a.48xlarge |
 | HPc7a | hpc7a.12xlarge, hpc7a.48xlarge, hpc7a.96xlarge |

**Related information**

AWS instance types

<!-- ============================================ -->
<!-- File 32/259: planning-and-installation_aws_weka-installation-on-aws-using-terraform_aws-weka-terraform-deployment-module-description.md -->
<!-- ============================================ -->

# Terraform-AWS-WEKA module description

The Terraform-AWS-WEKA module is an open-source repository. It contains modules to customize the WEKA cluster installation on AWS. The default protocol deployed using the module is POSIX.

The Terraform-AWS-WEKA module supports public and private cloud deployments. All deployment types require passing the `get.weka.io` token to Terraform to download the WEKA release from the public get.weka.io service.

Note: The WEKA release can only be downloaded over the internet if the private cloud network has NAT Gateway associated.

## Terraform-AWS-WEKA module components

The Terraform-AWS-WEKA module consists of the following components:

* **Required module:**
  * WEKA Root Module is located in the main Terraform module.
* **Optional sub-modules:**
  * **protocol_gateways:** Enables creating dedicated WEKA Frontend servers for protocol access (NFS, SMB, or SMB-W).
  * **clients:** Enables creating stateless WEKA clients that automatically join the WEKA cluster during cluster creation. The WEKA clients host applications or workloads.
  * **endpoints:** Creates private network VPC endpoints, including EC2 VPC endpoints, S3 gateway, Lambda VPC endpoint, WEKA proxy VPC endpoint, and a security group to open port 1080 for the WEKA proxy VPC endpoint.
  * **IAM:** Creates IAM roles for EC2 instances, CloudWatch events, WEKA Lambda functions, and Step Function. IAM roles can be created in advance, or if module variables are unspecified, WEKA automatically creates them.
  * **network:** Creates VPC, Internet Gateway/NAT, public/private subnets, and so on if pre-existing network variables are not supplied in advance.
  * **security_group:** Automatically creates the required security group if not provided in advance.

### Terraform-AWS-WEKA example

The following is a basic example in which you provide the minimum detail of your cluster, and the Terraform module completes the remaining required resources, such as VPC, subnets, security group, placement group, DNS zone, and IAM roles.

You can use this example as a reference to create the `main.tf` file.

```hcl
terraform {
  required_version = ">= 1.4.6"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 5.5.0"
    }
  }
}

provider "aws" {
}

module "weka_deployment" {
  source             = "weka/weka/aws"
  version            = "1.0.1"
  prefix             = "weka-tf"
  cluster_name       = "poc"
  availability_zones = ["eu-west-1c"]
  allow_ssh_cidrs    = ["0.0.0.0/0"]
  get_weka_io_token  = "Your get.weka.io token"
}

output "weka_deployment_output" {
  value = module.weka_deployment
}
```

Note: For the parameters' descriptions, refer to the terraform-aws-weka module.

<!-- ============================================ -->
<!-- File 33/259: planning-and-installation_aws_weka-installation-on-aws-using-terraform_deployment-on-aws-using-terraform.md -->
<!-- ============================================ -->

# Deployment on AWS using Terraform

## Create a main.tf file

The main Terraform configuration settings are included in the `main.tf` file. You can create it by following this procedure or using the WEKA Cloud Deployment Manager. See .

#### Before you begin

The Terraform must be installed on the workstation used for the deployment. Check the minimum required Terraform version specified in the **Requirements** section of the Terraform-AWS-WEKA module.

#### Procedure

1. Review the Terraform-AWS-WEKA example and use it as a reference for creating the `main.tf` according to your deployment specifics on AWS.
2. Tailor the `main.tf` file to create SMB-W or NFS protocol clusters by adding the relevant code snippet. Adjust parameters like the number of gateways, instance types, domain name, and share naming:

* **SMB-W**

```makefile
smb_protocol_gateways_number = 3
smb_protocol_gateway_instance_type = "c5.2xlarge"
smbw_enabled = true
smb_domain_name = "CUSTOMER_DOMAIN"
smb_share_name = "SPECIFY_SMB_SHARE_NAMING"
smb_setup_protocol = true
```

* **NFS**

```makefile
nfs_protocol_gateways_number = 1
nfs_protocol_gateway_instance_type = "c5.2xlarge"
nfs_setup_protocol = true
```

4. Add WEKA POSIX clients (optional)**:** If needed, add [WEKA POSIX clients](../../../weka-system-overview/weka-client-and-mount-modes) to support your workload by incorporating the specified variables into the `main.tf` file:

```makefile
clients_number = 2
client_instance_type = "c5.2xlarge"
```

## Apply the main.tf file

Once you complete the main.tf settings, apply it: Run `terraform apply`

Note: **Note:** Terraform creates a second password specifically for lambda functions, eliminating the need to create your own password.

## Set the license

To run IOs against the cluster, a valid license must be applied. Obtain a valid license and apply it to the WEKA cluster. For details, see .

**Related topic**

<!-- ============================================ -->
<!-- File 34/259: planning-and-installation_aws_weka-installation-on-aws-using-terraform_detailed-deployment-tutorial-weka-on-aws-using-terraform.md -->
<!-- ============================================ -->

# Detailed deployment tutorial: WEKA on AWS using Terraform

## Introduction

Deploying WEKA in AWS requires knowledge of several technologies, including AWS, Terraform, basic Linux operations, and the WEKA software. Recognizing that not all individuals responsible for this deployment are experts in each of these areas, this document aims to provide comprehensive, end-to-end instructions. This ensures that readers with minimal prior knowledge can successfully deploy a functional WEKA cluster on AWS.

#### Document scope

This document provides a guide for deploying WEKA in an AWS environment using Terraform. It is applicable for both POC and production setups. While no pre-existing AWS elements are required beyond an appropriate user account, the guide includes examples with some pre-created resources.

This document guides you through:

* General AWS requirements.
* Networking requirements to support WEKA.
* Deployment of WEKA using Terraform.
* Verification of a successful WEKA deployment.

Note: Images embedded in this document can be enlarged with a single click for ease of viewing and a clearer and more detailed examination.

## Terraform preparation and installation

HashiCorp Terraform is a tool that enables you to define, provision, and manage infrastructure as code. It simplifies infrastructure setup by using a configuration file instead of manual processes.

You describe your desired infrastructure in a configuration file using HashiCorp Configuration Language (HCL) or optionally JSON. Terraform then automates the creation, modification, or deletion of resources.

This automation ensures that your infrastructure is consistently and predictably deployed, aligning with the specifications in your configuration file. Terraform helps maintain a reliable and repeatable infrastructure environment.

Organizations worldwide use Terraform to deploy stateful infrastructure both on-premises and across public clouds like AWS, Azure, and Google Cloud Platform.

Note: You can deploy WEKA in AWS using AWS CloudFormation, allowing them to choose their preferred deployment method.

To install Terraform, we recommend following the official installation guides provided by HashiCorp.

### Locate the AWS Account

1. Access the AWS Management Consol&#x65;**.**
2. In the top-right corner, search for **Account ID**.

Note: * If deploying into a WEKA customer environment, ensure the customer understands their subscription structure.
* If deploying internally at WEKA and you don't see the Account ID or haven't been added to the correct account, contact the appropriate cloud team for assistance.

### Confirm user account permissions

To ensure a successful WEKA deployment in AWS using Terraform, verify that the AWS IAM user has the required permissions listed in #appendix-b-terraforms-required-permissions. The user must have permissions to create, modify, and delete AWS resources as specified by the Terraform configuration files.

If the IAM user lacks these permissions, update their permissions or create a new IAM user with the necessary permissions.

**Procedure**

1. **Access the AWS Management Console:** Log in using the account intended for the WEKA deployment.
2. **Navigate to the IAM dashboard:** From the Services menu, select **IAM** to open the Identity and Access Management dashboard.

3. **Locate the IAM user:** Search for the IAM user or go to the **Users** section.

4. **Verify permissions.** Click on the user‚Äôs name to review their permissions. Ensure they have policies that grant the necessary permissions for managing AWS resources through Terraform.

Note: The user shown in the screenshot above has full administrative access to allow Terraform to deploy WEKA. However, it is recommended to follow the principle of least privilege by granting only the necessary permissions listed in #appendix-b-terraforms-required-permissions.

### Set AWS Service Quotas

Before deploying WEKA on AWS using Terraform, ensure your AWS account has sufficient quotas for the necessary resources. Specifically, when deploying EC2 instances like the i3en for the WEKA backend cluster, manage quotas based on the vCPU count for each instance type or family.

**Requirements:**

* **EC2 Instance vCPU quotas:** Verify that your vCPU requirements are within your current quota limits. If not, adjust the quotas before running the Terraform commands (details are provided later in this document).
* **Cumulative vCPU count:** Ensure your quotas cover the total vCPU count needed for all instances. For example, deploying 10 i3en.6xlarge instances (each with 24 vCPUs) requires 240 vCPUs in total. Meeting these quotas is essential to avoid execution failures during the Terraform process, as detailed in the following sections.

**Procedure**

1. **Access Service Quotas:** Open the AWS Management Console at AWS Service Quotas. Use the search bar to locate the **Service Quotas** service.

2. **Select Amazon EC2:** On the Service Quotas page, select **Amazon EC2**.

3. **Identify instance type:** WEKA supports only i3en instance types for backend cluster nodes. Ensure you adjust the quota for the appropriate instance type (Spot, On-Demand, or Dedicated).

4. **Request quota increase:** Choose the relevant instance type from the Standard categories (A, C, D, H, I, M, R, T, Z), then click **Request increase at account-level**.

5. **Specify  number of vCPUs:** In the Request quota increase form, specify the number of vCPUs you need. For example, if 150 vCPUs are required for the i3en instance family, enter this number and submit your request.

Note: Quota increase requests are typically processed immediately. However, requests for a large number of vCPUs or specialized instance types may require manual review by AWS support. \
Confirm that you have requested and obtained the necessary quotas for all instance types used for WEKA backend servers and any associated application servers running WEKA client software. WEKA supports i3en series instances for backend servers.

**Related topic**

## AWS resource prerequisites

The WEKA deployment requires several AWS components, including VPCs, Subnets, Security Groups, and Endpoints. These components can either be created during the Terraform process or be pre-existing if manually configured.

Minimum requirements:

* A Virtual Private Cloud (VPC)
* Two Subnets in different Availability Zones (AZs)
* A Security Group

### Networking requirements

If you choose not to have Terraform auto-create networking components, ensure your VPC configuration includes:

* Two subnets (either private or public) in separate AZs.
* A subnet that allows WEKA to access the internet, either through an Internet Gateway (IGW) with an Elastic IP (EIP), NAT gateway, proxy, or egress VPC.

Although the WEKA deployment is not multi-AZ, a minimum of two subnets in different AZs is required for the Application Load Balancer (ALB).

### **View AWS Network Access Control Lists (ACLs)**

AWS Network Access Control Lists (ACLs) enable basic firewalls that control inbound and outbound network traffic based on security rules. They apply to network interfaces (NICs), EC2 instances, and subnets.

By default, ACLs include rules that ensure basic connectivity, such as allowing outbound communication from all AWS resources and denying all inbound traffic from the internet. These default rules have high priority numbers, so custom rules can override them. The security groups created by `main.tf` handle most traffic restrictions and allowances.

**Procedure**

1. Go to the VPC details page and select **Main network AC**L.

2. From the Network ACLs page, select the **Inbound rules** and **Outbound rules**.

**Related topic**

#appendix-a-security-groups-network-acl-ports (ensure you have defined all the relevant ports before manually creating ACLs ).

## Deploy WEKA on AWS using Terraform

If using existing resources, collect their AWS IDs as shown in the following examples:

### Modules overview

This section covers modules for creating IAM roles, networks, and security groups necessary for WEKA deployment. If you do not provide specific IDs for security groups or subnets, the modules automatically create them.

#### **Network configuration**

* **Availability zones:** The `availability_zones` variable is required when creating a network and is currently limited to a single subnet. If no specific subnet is provided, it is automatically created.
* **Private network deployment:** To deploy a private network with NAT, set the `subnet_autocreate_as_private` variable to `true` and provide a private CIDR range. To prevent instances from receiving public IPs, set `assign_public_ip` to `false`.

#### **SSH access**

For SSH access, use the username `ec2-user`. You can either:

* Provide an existing key pair name, or
* Provide a public SSH key.

If neither is provided, the system creates a key pair and store the private key locally.

#### **Application Load Balancer (ALB)**

To create an ALB for backend UI and WEKA client connections:

* Set `create_alb` to `true`.
* Provide additional required variables.
* To integrate ALB DNS with your DNS zone, provide variables for the Route 53 zone ID and alias name.

#### **Object Storage (OBS)**

For S3 integration for tiered storage:

* Set `tiering_enable_obs_integration` to `true`.
* Provide the name of the S3 bucket.
* Optionally, specify the SSD cache percentage.

#### **Optional configurations**

* **Clients:** For automatically mounting clients to the WEKA cluster, specify the number of clients to create. Optional variables include instance type, number of network interfaces (NICs), and AMI ID.
* **NFS protocol gateways:** Specify the number of NFS protocol gateways required. Additional configurations include instance type and disk size.
* **SMB protocol gateways:** Create at least three SMB protocol gateways. Configuration options are similar to NFS gateways.

#### **Secret Manager**

Use the Secret Manager to store sensitive information, such as usernames and passwords. If you do not provide a secret manager endpoint, disable it by setting `secretmanager_use_vpc_endpoint` to `false`.

#### **VPC Endpoints**

Enable VPC endpoints for services like EC2, S3, or a proxy by setting the respective variables to `true`.

#### **Terraform output**

The Terraform module output includes:

* SSH username.
* WEKA password secret ID.
* Helper commands for learning about the clusterization process.

### Locate the user‚Äôs token on get.weka.io

The WEKA user token grants access to WEKA binaries and is required for accessing https://get.weka.io during installation.

**Procedure**

1. Open a web browser and navigate to get.weka.io.
2. In the upper right-hand corner, click the user‚Äôs name.

3. From the left-hand menu, select **API Tokens**.
4. The user‚Äôs API token displays on the screen. Use this token later in the installation process.

### Deploy WEKA in AWS with Terraform

The Terraform module facilitates the deployment of various AWS resources, including EC2 instances, DynamoDB tables, Lambda functions, and State Machines, to support WEKA deployment.

#### Procedure

1. Create a directory for the Terraform configuration files.

```bash
mkdir deploy
```

Note: All Terraform deployments must be separated into their own directories to manage state information effectively. By creating a specific directory for this deployment, you can duplicate these instructions for future deployments by naming the directory uniquely, such as `deploy1`.

2. Navigate to the directory.

```bash
cd deploy
```

3. Create the `main.tf` file.\
   The `main.tf` file defines the Terraform options. Create this file using the WEKA Cloud Deployment Manager (CDM). See  for assistance.
4. Authenticate the AWS CLI.

```bash
aws configure
```

* Fill in the required information and press **Enter**.

5. After creating and saving the `main.tf` file, initialize the Terraform directory to ensure the proper AWS resource files are available.

```json
terraform init
```

7. As a best practice, run the terraform plan to preview the changes.

```json
terraform plan
```

8. Execute the creation of AWS resources necessary to run WEKA.

```json
terraform apply
```

9. When prompt, type `yes` to confirm the deployment.

**Deployment output**

Upon successful completion, Terraform displays output similar to the following. If the deployment fails, an error message appears.

```json
Outputs:

weka_deployment = {
  "alb_alias_record" = null
  "alb_dns_name" = "internal-WEKA-Prod-lb-697001983.us-east-1.elb.amazonaws.com"
  "asg_name" = "WEKA-Prod-autoscaling-group"
  "client_ips" = null
  "cluster_helper_commands" = <<-EOT
  aws ec2 describe-instances --instance-ids $(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-name WEKA-Prod-autoscaling-group --query "AutoScalingGroups[].Instances[].InstanceId" --output text) --query 'Reservations[].Instances[].PublicIpAddress' --output json
  aws lambda invoke --function-name WEKA-Prod-status-lambda --payload '{"type": "progress"}' --cli-binary-format raw-in-base64-out /dev/stdout
  aws secretsmanager get-secret-value --secret-id arn:aws:secretsmanager:us-east-1:459693375476:secret:weka/WEKA-Prod/weka-password-g9bH-T2og7D --query SecretString --output text

  EOT
  "cluster_name" = "Prod"
  "ips_type" = "PublicIpAddress"
  "lambda_status_name" = "WEKA-Prod-status-lambda"
  "local_ssh_private_key" = "/tmp/WEKA-Prod-private-key.pem"
  "nfs_protocol_gateways_ips" = tostring(null)
  "smb_protocol_gateways_ips" = tostring(null)
  "ssh_user" = "ec2-user"
  "weka_cluster_password_secret_id" = "arn:aws:secretsmanager:us-east-1:459693375476:secret:weka/WEKA-Prod/weka-password-g9bH-T2og7D"
}
weka_deployment_output = {
  "alb_alias_record" = null
  **"alb_dns_name" = "internal-WEKA-Prod-lb-697001983.us-east-1.elb.amazonaws.com"**
  "asg_name" = "WEKA-Prod-autoscaling-group"
  "client_ips" = null
  "cluster_helper_commands" = <<-EOT
  **aws ec2 describe-instances --instance-ids $(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-name WEKA-Prod-autoscaling-group --query "AutoScalingGroups[].Instances[].InstanceId" --output text) --query 'Reservations[].Instances[].PublicIpAddress' --output json
  aws lambda invoke --function-name WEKA-Prod-status-lambda --payload '{"type": "progress"}' --cli-binary-format raw-in-base64-out /dev/stdout
  aws secretsmanager get-secret-value --secret-id arn:aws:secretsmanager:us-east-1:459693375476:secret:weka/WEKA-Prod/weka-password-g9bH-T2og7D --query SecretString --output text**

  EOT
  "cluster_name" = "Prod"
  "ips_type" = "PublicIpAddress"
  "lambda_status_name" = "WEKA-Prod-status-lambda"
  **"local_ssh_private_key" = "/tmp/WEKA-Prod-private-key.pem"**
  "nfs_protocol_gateways_ips" = tostring(null)
  "smb_protocol_gateways_ips" = tostring(null)
  **"ssh_user" = "ec2-user"**
  "weka_cluster_password_secret_id" = "arn:aws:secretsmanager:us-east-1:459693375476:secret:weka/WEKA-Prod/weka-password-g9bH-T2og7D"
}
```

9. Take note of the `alb_dns_name`, `local_ssh_private_key`, and `ssh_user` values. You need these details for SSH access to the deployed instances.\
   The output includes a `cluster_helper_commands` section, offering three AWS CLI commands to retrieve essential information.

**Core resources created:**

* **Database (DynamoDB):** Stores the state of the WEKA cluster.
* **EC2:** Launch templates for auto-scaling groups and individual instances.
* **Networking:** Includes a Placement Group, Auto Scaling Group, and an optional ALB for the UI and backends.
* **CloudWatch:** Triggers the state machine every minute.
* **IAM:** Roles and policies for various WEKA components.
* **Secret Manager:** Securely stores WEKA credentials and tokens.

**Lambda functions created:**

* **deploy:** Provides installation scripts for new machines.
* **clusterize:** Executes the script for cluster formation.
* **clusterize-finalization:** Updates the cluster state after cluster formation is completed.
* **report:** Reports the progress of cluster formation and machine installation.
* **status:** Displays the current status of cluster formation.

**State machine functions:**

* **fetch:** Retrieves cluster or autoscaling group details and passes them to the next stage.
* **scale-down:** Uses the retrieved information to manage the WEKA cluster, including deactivating drives or hosts. An error is triggered if an unsupported target, like scaling down to two backend instances, is provided.
* **terminate:** Shuts down deactivated hosts.
* **transient:** Handles and reports transient errors, such as if some hosts couldn't be deactivated, while others were, allowing the operation to continue.

### Deploy protocol servers

The Terraform deployment process allows for the easy addition of instances to serve as protocol servers for NFS or SMB. These protocol servers are separate from the instances specified for the WEKA backend cluster.

**Procedure**

1. Open the `main.tf` file for editing.
2.  Add the required configurations to define the number of protocol servers for each type (NFS or SMB). Use the default settings for all other parameters.\
    Insert the configuration lines before the last closing brace (`}`) in the `main.tf` file.

    Example configurations:

    ```bash
    ## Deploying NFS Protocol Servers ##
    nfs_protocol_gateways_number = 2  # Minimum of two required

    ## Deploying SMB Protocol Servers ##
    smb_protocol_gateways_number = 3  # Minimum of three required
    ```
3. Save and close the file.

## Obtain access information about WEKA cluster

### **Determine the WEKA cluster IP address(es)**

1. Navigate to the EC2 Dashboard page in AWS and select **Instances (running)**.

2. Locate the instances for the WEKA backend servers, named `<prefix>-<cluster_name>-instance-backend`.

Note: The `prefix` and `cluster_name` correspond to the values specified in the `main.tf` file.

3. To access and manage the WEKA cluster, select any of the WEKA backend instances and note the IP address.

4. If your subnet provides a public IP address (configured in EC2), it is listed. WEKA primarily uses private IPv4 addresses for communication. To find the primary private address, check the **Hostname type** and note the **IP address** listed.

### **Obtain WEKA cluster access password**

The password for your WEKA cluster is securely stored in AWS Secrets Manager. This password is crucial for managing your WEKA cluster.

You can retrieve the password using one of the following options:

* Run the `aws secretsmanager get-secret-value` command and include the arguments provided in the Terraform output. See the deployment output above.
* Use the AWS Management Console. See the following procedure.

**Procedure**

1. Navigate to the AWS Management Console.
2. In the search bar, type **Secrets Manager** and select it from the search results.
3. In the Secrets Manager, select **Secrets** from the left-hand menu.
4. Find the secret corresponding to your deployment by looking for a name that includes your deployment‚Äôs `prefix` and `cluster_name`, along with the word **password**.
5. Retrieve the password: Click the identified secret to open its details, and select the **Retrieve secret value** button. \
   The console displays the randomly generated password assigned to the WEKA user `admin`.\
   Store it securely and use it according to your organization's security policies.

### Access the WEKA cluster backend servers

To manage your WEKA cluster, you need to access the backend servers. This is typically done using SSH from a system that can communicate with the AWS subnet where the instances are located. If the backend instances lack public IP addresses, ensure that you connect from a system within the same network or use a Jump Host or Bastion Host.

**Procedure**

1. **Prepare for SSH access**:
   * Identify the IP address of the WEKA backend server (obtained during the Terraform deployment).
   * Locate the SSH private key file used during the deployment. The key path is provided in the Terraform output.
2. **Connect to the backend server**:
   * If your system is within the AWS network or has direct access to the relevant subnet, proceed with the SSH connection.
   * If your system is outside the AWS network, set up a Jump Host or Bastion Host that can access the subnet.
3.  **Execute the SSH command**:

    * Use the following SSH command to access the backend server:

    ```bash
    ssh -l ec2-user -i /path/to/private-key.pem <server-ip-address>
    ```

    * Replace `/path/to/private-key.pem` with the actual path to your SSH private key file.
    * Replace `<server-ip-address>` with the IP address of the WEKA backend server.

## **WEKA GUI Login and Review**

To manage your WEKA cluster through the GUI) you'll need access to a jump box (a system with a GUI) that is deployed in the same VPC and subnet as your WEKA cluster. This allows you to securely access the WEKA GUI through a web browser.

The following procedure provides an example of using a Windows 10 instance.

**Procedure**

1. **Set up the jump box**:
   * Deploy a Windows 10 instance in the same VPC, subnet, and security group as your WEKA cluster.
   * Assign a public IP address to the Windows 10 instance.
   * Modify the network security group rules to allow Remote Desktop Protocol (RDP) access to the Windows 10 system.
2. **Access the WEKA GUI**:
   * Open a web browser on the Windows 10 jump box.
   *   Navigate to the WEKA GUI by entering the following URL:

       ```arduino
       https://<IP>:14000
       ```

       * Replace `<IP>` with the IP address of your WEKA cluster. For example: `https://10.5.0.11`.
   * The WEKA GUI login screen appears.
3. **Log In to the WEKA GUI**:
   * Log in using the username `admin` and the password obtained from AWS Secrets Manager (as described in the earlier steps).

4. **Review the WEKA Cluster**:

* **Cluster home screen**: View the cluster home screen for an overview of the system status.

* **Cluster Backends**: Review the status and details of the backend servers within the cluster (the server names may differ from those shown in examples).

* **Clients**: If there are any clients attached to the cluster, review their details and status.

* **Filesystems**: Review the filesystems associated with the cluster for their status and configuration.

## Scaling WEKA clusters with automated workflows

Scaling your WEKA cluster, whether scale-out (expanding) or scale-in (contracting), is streamlined using the AWS AutoScaling Group Policy set up by Terraform. This process leverages Terraform-created Lambda functions to manage automation tasks, ensuring that additional computing resources (new backend instances) are efficiently integrated or removed from the cluster as needed.

**Advantages of auto-scaling**

* **Integration with ALB:**
  * **Traffic distribution:** Auto Scaling Groups work seamlessly with an Application Load Balancer (ALB) to distribute traffic efficiently among instances.
  * **Health checks:** The ALB directs traffic only to healthy instances, based on results from the associated Auto Scaling Group's health checks.
* **Auto-healing:**
  * **Instance replacement:** If an instance fails a health check, auto-scaling automatically replaces it by launching a new instance.
  * **Health verification:** The new instance is only added to the ALB‚Äôs target group after passing health checks, ensuring continuous availability and responsiveness.
* **Graceful scaling:**
  * **Controlled adjustments:** Auto-scaling configurations can be customized to execute scaling actions gradually.
  * **Demand adaptation:** This approach prevents sudden traffic spikes and maintains stability while adapting to changing demand.

**Procedure**

1. Navigate to the AutoScaling Group page in the AWS Management Console.
2. Select **Edit** to adjust the desired capacity.

3. Set the capacity to your preferred cluster size (for example, increase from 6 to 10 servers).

4. Select **Update** to save the updated settings to initiate scaling operations.

## Test WEKA cluster self-healing functionality

Testing the self-healing functionality of a WEKA cluster involves decommissioning an existing instance and observing whether the Auto Scaling Group (ASG) automatically replaces it with a new instance. This process verifies the cluster‚Äôs ability to maintain capacity and performance despite instance failures or terminations.

**Procedure**

1. **Identify the old instance:** Determine the EC2 instance you want to decommission. Selection criteria may include age, outdated configurations, or specific maintenance requirements.
2. **Verify auto-scaling configuration:** Ensure your Auto Scaling Group is configured with a minimum of 7 instances (or more) and that the desired capacity is set to maintain the appropriate number of instances in the cluster.
3. **Terminate the old instance:** Use the AWS Management Console, AWS CLI, or SDKs to manually terminate the selected EC2 instance. This action prompts the ASG to initiate the replacement process.
4. **Monitor auto-scaling activities:** Track the ASG‚Äôs activities through the AWS Console or AWS CloudWatch. Confirm that the ASG recognizes the terminated instance and begins launching a new instance.
5. **Verify the new instance:** After it is launched, ensure it passes all health checks and integrates successfully into the cluster, maintaining overall cluster capacity.
6. **Check load balancer:**
   * If a load balancer is part of your setup, verify that it detects and registers the new instance to ensure proper load distribution across the cluster.
7. **Review auto-scaling logs:** Examine CloudWatch logs or auto-scaling events for any issues or errors related to terminating the old instance and introducing the new one.
8. **Document and monitor:** Record the decommissioning process and continuously monitor the cluster to confirm that it operates smoothly with the new instance.

## APPENDICES

### Appendix A: Security Groups / network ACL ports

See #required-ports

### Appendix B: **Terraform‚Äôs r**equired permissions examples <a href="#appendix-b-terraforms-required-permissions" id="appendix-b-terraforms-required-permissions"></a>

The minimum IAM Policies needed are based on the assumption that the network, including VPC, subnets, VPC Endpoints, and Security Groups, is created by the end user. If IAM roles or policies are pre-established, some permissions may be omitted.

Note: The policy exceeds the 6144 character limit for IAM Policies, necessitating its division into two separate policies.

In each policy, replace the placeholders, such as `account-number`, `prefix`, and `cluster-name`, with the corresponding actual values.

<details>

<summary>IAM policy 1</summary>

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::weka-tf-aws-releases*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DeletePlacementGroup"
            ],
            "Resource": "arn:aws:ec2:us-east-1:account-number:placement-group/prefix-cluster-name*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribePlacementGroups"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateLaunchTemplate",
                "ec2:CreateLaunchTemplateVersion",
                "ec2:DeleteLaunchTemplate",
                "ec2:DeleteLaunchTemplateVersions",
                "ec2:ModifyLaunchTemplate",
                "ec2:GetLaunchTemplateData"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "autoscaling:DescribeAutoScalingGroups",
                "autoscaling:DescribeScalingActivities"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "autoscaling:CreateAutoScalingGroup",
                "autoscaling:DeleteAutoScalingGroup",
                "autoscaling:UpdateAutoScalingGroup",
                "autoscaling:SetInstanceProtection",
                "autoscaling:SuspendProcesses",
                "autoscaling:AttachLoadBalancerTargetGroups",
                "autoscaling:DetachLoadBalancerTargetGroups"
            ],
            "Resource": [
                "arn:aws:autoscaling:*:account-number:autoScalingGroup:*:autoScalingGroupName/prefix-cluster-name-autoscaling-group"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "lambda:CreateFunction",
                "lambda:DeleteFunction",
                "lambda:GetFunction",
                "lambda:ListFunctions",
                "lambda:UpdateFunctionCode",
                "lambda:UpdateFunctionConfiguration",
                "lambda:ListVersionsByFunction",
                "lambda:GetFunctionCodeSigningConfig",
                "lambda:GetFunctionUrlConfig",
                "lambda:CreateFunctionUrlConfig",
                "lambda:DeleteFunctionUrlConfig",
                "lambda:AddPermission",
                "lambda:GetPolicy",
                "lambda:RemovePermission"
            ],
            "Resource": "arn:aws:lambda:*:account-number:function:prefix-cluster-name-*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "lambda:CreateEventSourceMapping",
                "lambda:DeleteEventSourceMapping",
                "lambda:GetEventSourceMapping",
                "lambda:ListEventSourceMappings"
            ],
            "Resource": "arn:aws:lambda:*:account-number:event-source-mapping:prefix-cluster-name-*"
        },
        {
            "Sid": "ReadAMIData",
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeImages",
                "ec2:DescribeImageAttribute",
                "ec2:CopyImage"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:ImportKeyPair",
                "ec2:CreateKeyPair",
                "ec2:DeleteKeyPair",
                "ec2:DescribeKeyPairs"
            ],
            "Resource": "*"
        },
        {
            "Action": [
                "ec2:MonitorInstances",
                "ec2:UnmonitorInstances",
                "ec2:ModifyInstanceAttribute",
                "ec2:RunInstances",
                "ec2:CreateTags"
            ],
            "Effect": "Allow",
            "Resource": "*"
        },
        {
            "Sid": "DescribeSubnets",
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeSubnets"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "DescribeALB",
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:DescribeLoadBalancers",
                "elasticloadbalancing:DescribeTargetGroups",
                "elasticloadbalancing:DescribeListeners"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreatePlacementGroup"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:CreateLoadBalancer",
                "elasticloadbalancing:AddTags",
                "elasticloadbalancing:CreateTargetGroup",
                "elasticloadbalancing:ModifyLoadBalancerAttributes",
                "elasticloadbalancing:ModifyTargetGroupAttributes",
                "elasticloadbalancing:DeleteLoadBalancer",
                "elasticloadbalancing:DeleteTargetGroup",
                "elasticloadbalancing:CreateListener",
                "elasticloadbalancing:DeleteListener"
            ],
            "Resource": [
                "arn:aws:elasticloadbalancing:us-east-1:account-number:loadbalancer/app/prefix-cluster-name*",
                "arn:aws:elasticloadbalancing:us-east-1:account-number:targetgroup/prefix-cluster-name*",
                "arn:aws:elasticloadbalancing:us-east-1:account-number:listener/app/prefix-cluster-name*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:DescribeLoadBalancerAttributes",
                "elasticloadbalancing:DescribeTargetGroupAttributes",
                "elasticloadbalancing:DescribeTags"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeVpcs",
                "ec2:DescribeLaunchTemplates",
                "ec2:DescribeLaunchTemplateVersions",
                "ec2:DescribeInstances",
                "ec2:DescribeTags",
                "ec2:DescribeInstanceAttribute",
                "ec2:DescribeVolumes"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Sid": "Statement1",
            "Effect": "Allow",
            "Action": [
                "states:Createcluster-nameMachine",
                "states:Deletecluster-nameMachine",
                "states:TagResource",
                "states:DescribeStateMachine",
                "states:ListStateMachineVersions",
                "states:ListStateMachines",
                "states:ListTagsForResource"
            ],
            "Resource": [
                "arn:aws:states:us-east-1:account-number:stateMachine:prefix-cluster-name*"
            ]
        },
        {
            "Sid": "Statement2",
            "Effect": "Allow",
            "Action": [
                "ec2:TerminateInstances"
            ],
            "Resource": [
                "*"
            ]
        }
    ]
}
```

</details>

<details>

<summary>IAM policy 2</summary>

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "secretsmanager:CreateSecret",
                "secretsmanager:DeleteSecret",
                "secretsmanager:DescribeSecret",
                "secretsmanager:GetSecretValue",
                "secretsmanager:ListSecrets",
                "secretsmanager:UpdateSecret",
                "secretsmanager:GetResourcePolicy",
                "secretsmanager:ListSecretVersionIds",
                "secretsmanager:PutSecretValue"
            ],
            "Resource": [
                "arn:aws:secretsmanager:*:account-number:secret:weka/prefix-cluster-name/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "dynamodb:PutItem",
                "dynamodb:DeleteItem",
                "dynamodb:GetItem",
                "dynamodb:UpdateItem"
            ],
            "Resource": "arn:aws:dynamodb:*:account-number:table/prefix-cluster-name*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreatePolicy",
                "iam:CreateRole",
                "iam:DeleteRole",
                "iam:DeletePolicy",
                "iam:GetPolicy",
                "iam:GetRole",
                "iam:GetPolicyVersion",
                "iam:ListRolePolicies",
                "iam:ListInstanceProfilesForRole",
                "iam:PassRole",
                "iam:ListPolicyVersions",
                "iam:ListAttachedRolePolicies",
                "iam:ListAttachedGroupPolicies",
                "iam:ListAttachedUserPolicies"
            ],
            "Resource": [
                "arn:aws:iam::account-number:policy/prefix-cluster-name-*",
                "arn:aws:iam::account-number:role/prefix-cluster-name-*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:AttachRolePolicy",
                "iam:AttachGroupPolicy",
                "iam:AttachUserPolicy",
                "iam:DetachRolePolicy",
                "iam:DetachGroupPolicy",
                "iam:DetachUserPolicy"
            ],
            "Resource": [
                "arn:aws:iam::account-number:policy/prefix-cluster-name-*",
                "arn:aws:iam::account-number:role/prefix-cluster-name-*",
                "arn:aws:iam::account-number:role/ck-cluster-name-weka-iam-role"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:GetPolicy",
                "iam:ListEntitiesForPolicy"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:GetInstanceProfile",
                "iam:CreateInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:AddRoleToInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile"
            ],
            "Resource": "arn:aws:iam::*:instance-profile/prefix-cluster-name-*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:PutRetentionPolicy",
                "logs:DeleteLogGroup"
            ],
            "Resource": [
                "arn:aws:logs:us-east-1:account-number:log-group:/aws/lambda/prefix-cluster-name*",
                "arn:aws:logs:us-east-1:account-number:log-group:/aws/vendedlogs/states/prefix-cluster-name*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "events:TagResource",
                "events:PutRule",
                "events:DescribeRule",
                "events:ListTagsForResource",
                "events:DeleteRule",
                "events:PutTargets",
                "events:ListTargetsByRule",
                "events:RemoveTargets"
            ],
            "Resource": [
                "arn:aws:events:us-east-1:account-number:rule/prefix-cluster-name*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "dynamodb:CreateTable",
                "dynamodb:DescribeTable",
                "dynamodb:DescribeContinuousBackups",
                "dynamodb:DescribeTimeToLive",
                "dynamodb:ListTagsOfResource",
                "dynamodb:DeleteTable"
            ],
            "Resource": [
                "arn:aws:dynamodb:us-east-1:account-number:table/prefix-cluster-name*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "logs:DescribeLogGroups",
                "logs:ListTagsLogGroup"
            ],
            "Resource": [
                "*"
            ]
        }
        {
            "Effect": "Allow",
            "Action": "iam:CreateServiceLinkedRole",
            "Resource": "arn:aws:iam::*:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoscaling*",
            "Condition": {
                "StringLike": {
                    "iam:AWSServiceName": "autoscaling.amazonaws.com"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:AttachRolePolicy",
                "iam:PutRolePolicy"
            ],
            "Resource": "arn:aws:iam::*:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoscaling*"
        }
    ]
}
```

</details>

**Parameters:**

* **DynamoDB**: Full access is granted as your setup requires creating and managing DynamoDB tables.
* **Lambda**: Full access is needed for managing various Lambda functions mentioned.
* **State Machine (AWS Step Functions)**: Full access is given for managing state machines.
* **Auto Scaling Group & EC2 Instances**: Permissions for managing Auto Scaling groups and EC2 instances.
* **Application Load Balancer (ALB)**: Required for operations related to load balancing.
* **CloudWatch**: Necessary for monitoring and managing CloudWatch rules and metrics.
* **Secrets Manager**: Access for managing secrets in AWS Secrets Manager.
* **IAM**: **`PassRole`** and **`GetRole`** are essential for allowing resources to assume specific roles.
* **KMS**: Permissions for Key Management Service, assuming you use KMS for encryption.

**Customization:**

1. **Resource Names and ARNs**: Replace **`"Resource": "*"`** with specific ARNs for your resources to tighten security. Use specific ARNs for KMS keys as well.
2. **Region and Account ID**: Replace **`region`** and **`account-id`** with your AWS region and account ID.
3. **Key ID**: Replace **`key-id`** with the ID of the KMS key used in your setup.

Note: **Important notes:**
* This is a broad policy for demonstration. It's recommended to refine it based on your actual resource usage and access patterns.
* You may need to add or remove permissions based on specific requirements of your Terraform module and AWS environment.
* Testing the policy in a controlled environment before applying it to production is advisable to ensure it meets your needs without overly restricting or exposing your resources.

### Appendix C: IAM Policies required by WEKA

The following policies are essential for all components to function on AWS. Terraform automatically creates these policies as part of the automation process. However, you also have the option to create and define them manually within your Terraform modules.

<details>

<summary><strong>EC2 policies (Required for the backends that are part of the WEKA cluster)</strong></summary>

```json
{
"Statement": [
{
"Action": [
"ec2:DescribeNetworkInterfaces",
"ec2:AttachNetworkInterface",
"ec2:CreateNetworkInterface",
"ec2:ModifyNetworkInterfaceAttribute",
"ec2:DeleteNetworkInterface"
],
"Effect": "Allow",
"Resource": "*"
},
{
"Action": [
"lambda:InvokeFunction"
],
"Effect": "Allow",
"Resource": [
"arn:aws:lambda:*:*:function:prefix-cluster_name*"
]
},
{
"Action": [
"s3:DeleteObject",
"s3:GetObject",
"s3:ListBucket",
"s3:PutObject"
],
"Effect": "Allow",
"Resource": [
"arn:aws:s3:::prefix-cluster_name-obs/*"
]
},
{
"Action": [
"logs:CreateLogGroup",
"logs:CreateLogStream",
"logs:PutLogEvents",
"logs:DescribeLogStreams",
"logs:PutRetentionPolicy"
],
"Effect": "Allow",
"Resource": [
"arn:aws:logs:*:*:log-group:/wekaio/prefix-cluster_name*"
]
}
],
"Version": "2012-10-17"
}
```

</details>

<details>

<summary><strong>Lambda IAM policy</strong></summary>

```json
{
"Statement": [
{
"Action": [
"s3:CreateBucket"
],
"Effect": "Allow",
"Resource": [
"arn:aws:s3:::prefix-cluster_name-obs"
]
},
{
"Action": [
"logs:CreateLogGroup",
"logs:CreateLogStream",
"logs:PutLogEvents"
],
"Effect": "Allow",
"Resource": [
"arn:aws:logs:*:*:log-group:/aws/lambda/prefix-cluster_name*:*"
]
},
{
"Action": [
"ec2:CreateNetworkInterface",
"ec2:DescribeNetworkInterfaces",
"ec2:DeleteNetworkInterface",
"ec2:ModifyInstanceAttribute",
"ec2:TerminateInstances",
"ec2:DescribeInstances"
],
"Effect": "Allow",
"Resource": [
"*"
]
},
{
"Action": [
"dynamodb:GetItem",
"dynamodb:UpdateItem"
],
"Effect": "Allow",
"Resource": [
"arn:aws:dynamodb:*:*:table/prefix-cluster_name-weka-deployment"
]
},
{
"Action": [
"secretsmanager:GetSecretValue",
"secretsmanager:PutSecretValue"
],
"Effect": "Allow",
"Resource": [
"arn:aws:secretsmanager:*:*:secret:weka/prefix-cluster_name/*"
]
},
{
"Action": [
"autoscaling:DetachInstances",
"autoscaling:DescribeAutoScalingGroups",
"autoscaling:SetInstanceProtection"
],
"Effect": "Allow",
"Resource": [
"*"
]
}
],
"Version": "2012-10-17"
}
```

</details>

<details>

<summary><strong>State machine IAM policy</strong></summary>

```json
{
"Statement": [
{
"Action": [
"lambda:InvokeFunction"
],
"Effect": "Allow",
"Resource": [
"arn:aws:lambda:*:*:function:prefix-cluster_name-*-lambda"
]
},
{
"Action": [
"logs:CreateLogDelivery",
"logs:GetLogDelivery",
"logs:UpdateLogDelivery",
"logs:DeleteLogDelivery",
"logs:ListLogDeliveries",
"logs:PutLogEvents",
"logs:PutResourcePolicy",
"logs:DescribeResourcePolicies",
"logs:DescribeLogGroups"
],
"Effect": "Allow",
"Resource": [
"*"
]
}
],
"Version": "2012-10-17"
}
```

</details>

<details>

<summary><strong>CloudWatch IAM policy</strong></summary>

```
{
"Statement": [
{
"Action": [
"states:StartExecution"
],
"Effect": "Allow",
"Resource": [
"arn:aws:states:*:*:stateMachine:prefix-cluster_name-scale-down-state-machine"
]
}
],
"Version": "2012-10-17"
}
```

</details>

<details>

<summary><strong>Client instances IAM policy</strong></summary>

```
{
    "Statement": [
        {
            "Action": [
                "autoscaling:DescribeAutoScalingGroups"
            ],
            "Effect": "Allow",
            "Resource": [
                "*"
            ]
        },
      {
        "Action": [
          "ec2:DescribeNetworkInterfaces",
          "ec2:AttachNetworkInterface",
          "ec2:CreateNetworkInterface",
          "ec2:ModifyNetworkInterfaceAttribute",
          "ec2:DeleteNetworkInterface",
          "ec2:DescribeInstances",
          "ec2:CreateTags"
        ],
        "Effect": "Allow",
        "Resource": "*"
      },
      {
        "Action": [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents",
          "logs:DescribeLogStreams",
          "logs:PutRetentionPolicy"
        ],
        "Effect": "Allow",
        "Resource": [
          "arn:aws:logs:*:*:log-group:/wekaio/clients/prefix-cluster_name-client*"
        ]
      }
    ],
    "Version": "2012-10-17"
}
```

</details>

<details>

<summary><strong>Protocol gateway IAM policy</strong></summary>

```
{
  "Statement": [
    {
      "Effect": "Allow",
      "Action":
    [
      "ec2:DescribeNetworkInterfaces",
      "ec2:AttachNetworkInterface",
      "ec2:CreateNetworkInterface",
      "ec2:ModifyNetworkInterfaceAttribute",
      "ec2:DeleteNetworkInterface",
      "ec2:DescribeInstances",
      "ec2:DescribeTags",
      "ec2:AssignPrivateIpAddresses"
    ],
    "Resource":  "*",
    },
    {
      "Effect": "Allow",
      "Action":
    [
      "secretsmanager:GetSecretValue"
    ]
    "Resource":
    [
      "arn:aws:secretsmanager:*:*„äôweka/prefix-cluster_name/*"
    ]
    },
    {
      "Effect": "Allow",
      "Action":
    [
      "logs:CreateLogGroup",
      "logs:CreateLogStream",
      "logs:PutLogEvents",
      "logs:DescribeLogStreams",
      "logs:PutRetentionPolicy"
    ],
    "Resource":
    [
      "arn:aws:logs:*:*:log-group:/wekaio/clients/gateways_name*"
    ]
    },
    {
      "Effect": "Allow",
      "Action":
    [
      "autoscaling:DescribeAutoScalingGroups"
    ],
    "Resource":
    [
      "*"
    ]
    },
    {
      "Action": [
        "lambda:InvokeFunction"
      ],
      "Effect": "Allow",
      "Resource": [
        "arn:aws:lambda:*:*:function:prefix-cluster_name*"
      ]
    },
  ]
}
```

</details>

<!-- ============================================ -->
<!-- File 35/259: planning-and-installation_aws_weka-installation-on-aws-using-terraform_weka-cluster-auto-scaling-in-aws.md -->
<!-- ============================================ -->

# WEKA cluster auto-scaling in AWS

## Scale-out the WEKA cluster backend servers

Scale-out is the process of increasing the number of EC2 instances in the system to handle higher workloads or enhance redundancy.

Scale-out is essential to ensure a system can meet growing demands, maintain performance, and distribute workloads effectively. This proactive approach helps prevent overloads, reduce response times, and maintain high availability.

**Action**

* Increase the desired size of the Auto-Scaling Group (ASG) associated with your WEKA cluster. You can do this through the AWS Console or AWS CLI.

**Result**

* AWS automatically launches the new EC2 instance.
* AWS triggers the Lambda Function to create a `join` script that runs once as part of the instance user data and, subsequently, integrates the new EC2 instance into the existing WEKA cluster.

You can monitor the process in the AWS Step Function GUI.

## Scale-in the WEKA cluster backend servers

Scale-in is the process of reducing the number of EC2 instances of a system to align with decreased workloads or to optimize resource utilization.

Scale-in is essential for efficient resource management, cost reduction, and ensuring the appropriate allocation of resources. It helps prevent over-provisioning, lowers operational expenses, and safeguards against unintentional removal of EC2 instances from the existing WEKA cluster in AWS.

The cluster is configured with scale-in protection and instance termination protection to enhance the safety of this process.

**Action**

* Decrease the desired size of the Auto-Scaling Group (ASG) associated with your WEKA cluster. You can do this through the AWS Console, AWS CLI, or other compatible methods.

**Result**

* After modifying the desired size, it doesn't immediately impact the Auto Scaling Group (ASG). Instead, a Step Function continuously monitors the configuration.
* This Step Function runs every minute and identifies that the desired size is less than the current WEKA system's size.
* When this condition is met, it initiates a scale-in process, but only if certain conditions are met, such as having enough capacity on the filesystem.
* If the scale-down is successful, the Step Function subsequently removes the protection from the scaled-in instance, thereby allowing the Auto Scaling Group to proceed with removing it.

<!-- ============================================ -->
<!-- File 36/259: planning-and-installation_weka-installation-on-azure.md -->
<!-- ============================================ -->

---
description:
---

# WEKA installation on Azure

The WEKA¬Æ Data Platform on Microsoft Azure provides a fast and scalable platform for running performance-intensive applications and hybrid cloud workflows. It can also be used for object stores, tiering, and snapshots using the Azure Blob service, for example, to create backups and DR copies.

WEKA provides a ready-to-deploy Terraform package that you can customize for installing the WEKA cluster on Azure. The WEKA cluster is deployed with a multiple containers architecture, in which each container serves a single process type: Compute, Drives, or Frontend.

The WEKA cluster is deployed in a single virtual network (VNet, similar to VPC in other clouds), where peering is not required. Each VNet has a subnet, routers, firewalls, and an internal DNS. The networking supports MTU 3900. The VM includes 4 or 8 NICs, each NIC is mapped to a dedicated core to support DPDK.

Depending on the required security level, you can deploy the WEKA cluster using the Terraform package on one of the following subnet types:

* **Public subnets:** Use a single public subnet within your VNet with an internet gateway, and allow public IP addresses for your virtual machines.
* **Private subnets:** Use a single private subnet within your VNet with access to an APT repository containing the required deployment packages.

<details>

<summary>Introduction to Azure fundamentals</summary>

Azure is a cloud computing platform with an ever-expanding set of services to help you build solutions to meet your business goals. Azure services range from simple web services for hosting your business presence in the cloud to running fully virtualized computers for you to run your custom software solutions.

Azure provides a wealth of cloud-based services like remote storage, database hosting, and centralized account management. Azure also offers new capabilities like AI and the Internet of Things (IoT).

To learn about Azure fundamentals, Microsoft provides learning modules at https://learn.microsoft.com/en-us/training/. You can start with the Introduction to Azure fundamentals.

</details>

<details>

<summary>Terraform overview</summary>

Terraform is an open-source project from Hashicorp. It creates and manages resources on cloud platforms and on-premises clouds. Unlike AWS CloudFormation, it works with many APIs from multiple platforms and services.

Terraform is the primary tool for deploying WEKA on Azure.

### How does Terraform work?

A deployment with Terraform involves three phases:

* **Write:** Define the infrastructure in configuration files and customize the project variables provided in the Terraform package.
* **Plan**: Review the changes Terraform will make to your infrastructure.
* **Apply:** Terraform provisions the infrastructure, including the VMs and instances, installs the WEKA software, and creates the cluster. Once completed, the WEKA cluster runs on Azure.

**Related information**

Terraform Tutorials

Terraform Installation

</details>

**Related topics**

**Related information**

Weka¬Æ Data Platform on Microsoft Azure Marketplace

<!-- ============================================ -->
<!-- File 37/259: planning-and-installation_weka-installation-on-azure_required-services-and-supported-regions.md -->
<!-- ============================================ -->

# Required services and supported regions

The region must support the services used in WEKA on Azure. The following sections list these services and the current regions that support them. See the related information below for updates.

## Required services used in WEKA on Azure

* Lsv3-series (VM type)
* Functions
* Virtual Machine Scale Sets (VMSS)
* Key Vault
* Zone-redundant storage (ZRS) Blobs
* Virtual Network
* Load Balancer (LB)
* Logic App
* Elastic Premium EP2 (service plan for the Logic App)

## Supported regions

* **Americas**
  * Brazil South
  * Canada Central
  * Central US
  * East US
  * East US 2
  * North Central US
  * South Central US
  * West US 2
  * West US 3
* **Europe**
  * France Central
  * Germany West Central
  * North Europe
  * West Europe
  * UK South
  * Sweden Central
* **Middle East**
  * Qatar Central
* **Asia**
  * Australia East
  * Australia Southeast
  * Central India
  * Japan East
  * East Asia
  * Southeast Asia

Note: Australia Southeast, Canada East, and North Central US do not include the Zone-redundant storage (ZRS) blob. If the snap-to-object feature is required, use a bucket from a different region or a lower durability blob such as the Locally-redundant storage (LRS) blob.

**Related information**

Azure Products available by region

Azure regions with availability zone support

<!-- ============================================ -->
<!-- File 38/259: planning-and-installation_weka-installation-on-azure_supported-virtual-machine-types.md -->
<!-- ============================================ -->

# Supported virtual machine types

## Supported VM sizes for backends

On Azure, WEKA is deployed in a multi-container architecture using the storage-optimized Lsv3-series and Lasv3-series Azure Virtual Machines (Azure VMs). These VMs feature high throughput, low latency, and directly mapped local NVMe storage.

Each VM size has a specific number of NICs, but only one is used for all traffic in UDP mode through the management interface.

The following table lists the VM sizes applied by the Terraform package on the backends:

 | VM size | vCPU | Memory (GiB) | NVMe disks | Max NICs | BW (Mbps) |
 | --- | --- | --- | --- | --- | --- |
 | Standard_L8s_v3 | 8 | 64 | 1x1.92 TB | 4 | 12500 |
 | Standard_L16s_v3 | 16 | 128 | 2x1.92 TB | 8 | 12500 |
 | Standard_L32s_v3 | 32 | 256 | 4x1.92 TB | 8 | 16000 |
 | Standard_L48s_v3 | 48 | 384 | 6x1.92 TB | 8 | 24000 |
 | Standard_L64s_v3 | 64 | 512 | 8x1.92 TB | 8 | 30000 |
 | Standard_L80s_v3 | 80 | 640 | 10x1.92 TB | 8 | 32000 |
 | Standard_L8as_v3 | 8 | 64 | 1x1.92 TB | 4 | 12500 |
 | Standard_L16as_v3 | 16 | 128 | 2x1.92 TB | 8 | 12500 |
 | Standard_L32as_v3 | 32 | 256 | 4x1.92 TB | 8 | 16000 |
 | Standard_L48as_v3 | 48 | 384 | 6x1.92 TB | 8 | 24000 |
 | Standard_L64as_v3 | 64 | 512 | 8x1.92 TB | 8 | 32000 |
 | Standard_L80as_v3 | 80 | 640 | 10x1.92 TB | 8 | 32000 |

Note: Using the Azure Console, the client instances can have different virtual machine types provisioned separately from the WEKA cluster.

**Related information**

Lsv3-series and Lasv3-series (Azure learning site)

### Mapped cores to processes

In each virtual machine size, the cores are mapped to a specific number of the compute, drive, and frontend processes. For example, in the Standard_L16s_v3 size, the cores are mapped to the following processes:

* Compute: 4
* Drive: 2
* Frontend: 1

 | VM size | # of compute cores | # of drive cores | # of frontend cores |
 | --- | --- | --- | --- |
 | Standard_L8s_v3 | 1 | 1 | 1 |
 | Standard_L16s_v3 | 4 | 2 | 1 |
 | Standard_L32s_v3 | 4 | 2 | 1 |
 | Standard_L48s_v3 | 3 | 3 | 1 |
 | Standard_L64s_v3 | 4 | 2 | 1 |
 | Standard_L80s_v3 | 4 | 2 | 1 |
 | Standard_L8as_v3 | 1 | 1 | 1 |
 | Standard_L16as_v3 | 4 | 2 | 1 |
 | Standard_L32as_v3 | 4 | 2 | 1 |
 | Standard_L48as_v3 | 3 | 3 | 1 |
 | Standard_L64as_v3 | 4 | 2 | 1 |
 | Standard_L80as_v3 | 4 | 2 | 1 |

## Supported VM sizes for clients

### General purpose virtual machine sizes <a href="#general-purpose-virtual-machine-sizes" id="general-purpose-virtual-machine-sizes"></a>

 | VM series | VM size |
 | --- | --- |
 | Dsv3 | Standard_D4s_v3, Standard_D8s_v3, Standard_D16s_v3 |
 | Dasv4 | Standard_D2as_v4, Standard_D4as_v4, Standard_D8as_v4 |
 | Ddsv4 | Standard_D16ds_v4 |
 | Dasv4 | Standard_D4as_v4, Standard_D16as_v4, Standard_D32as_v4, Standard_D96as_v4 |
 | Dv5 | Standard_D8_v5 |
 | Dsv5 | Standard_D4s_v5 ,Standard_D16s_v5, Standard_D48s_v5 , Standard_D64s_v5 |
 | Dadsv5 | Standard_D4ads_v5, Standard_D16ads_v5, Standard_D48ads_v5, Standard_D96ads_v5 |
 | Dcsv2 | Standard_DC4s_v2 (UDP only) |
 | Dasv5 | Standard_D2as_v5, Standard_D8as_v5 |
 | Dpldsv5 | Standard_D8plds_v5, Standard_D32plds_v5, Standard_D64plds_v5 |
 | Dpsv5 | Standard_D4ps_v5, Standard_D8ps_v5, Standard_D16ps_v5, Standard_D32ps_v5, Standard_D48ps_v5, Standard_D64ps_v5 |

### Memory optimized virtual machine sizes

 | VM series | VM size |
 | --- | --- |
 | Edsv4 | Standard_E16ds_v4, Standard_E16-8ds_v4, Standard_E32ds_v4, |
 | Easv4 | Standard_E32-16as_v4 |
 | Easv5 | Standard_E32-16as_v5 |
 | Edsv4 | Standard_E32-16ds_v4, Standard_E48ds_v4 |
 | Eadsv5 | Standard_E96ads_v5 |
 | Mdmsv2 | Standard_M64dms_v2 |
 | Msv2 | Standard_M208s_v2 |
 | Mmsv2 | Standard_M208ms_v2, Standard_M416ms_v2 |
 | Epsv5 | Standard_E4ps_v5, Standard_E8ps_v5, Standard_E16ps_v5, Standard_E20ps_v5, Standard_E32ps_v5 |

### Compute optimized virtual machine sizes

 | VM series | VM size |
 | --- | --- |
 | FXmds | Standard_FX48mds |
 | Fsv2 | Standard_F8s_v2, Standard_F32s_v2, Standard_F64s_v2, Standard_F72s_v2 |

### Storage optimized virtual machine sizes

 | VM series | VM size |
 | --- | --- |
 | Lsv3 | Standard_L8s_v3, Standard_L16s_v3, Standard_L32s_v3, Standard_L48s_v3, Standard_L64s_v3, Standard_L80s_v3 |

### High performance optimized

 | VM series | VM size |
 | --- | --- |
 | HBv4 | Standard_HB176rs_v4, Standard_HB176-24rs_v4, Standard_HB176-96rs_v4 |
 | HBv3 | Standard_HB120rs_v3 |

### GPU - accelerated compute

 | VM series | VM size |
 | --- | --- |
 | NGads V620 | Standard_NG8ads_V620_v1, Standard_NG16ads_V620_v1, Standard_NG32ads_V620_v1 |
 | NVadsA10 | Standard_NVadsA10_v5 |

**Related information**

Sizes for virtual machines in Azure (Azure site)

<!-- ============================================ -->
<!-- File 39/259: planning-and-installation_weka-installation-on-azure_azure-weka-terraform-package-description.md -->
<!-- ============================================ -->

# Azure-WEKA deployment Terraform package description

The Azure-WEKA deployment Terraform package contains customizable modules for deploying the WEKA cluster on Azure. The default protocol deployed using the module is POSIX. The module supports the following deployment types:

* **Public cloud deployments:** Require passing the `get.weka.io` token to Terraform for downloading the WEKA release from the public get.weka.io service. The following examples are provided:
  * Public network.
  * Public network with existing object store.
* **Private cloud deployments:** Require uploading the WEKA release tar file into an Azure blob container from which the virtual machines can download the WEKA release. The following examples are provided:
  * Existing private network.
  * Existing private network with peering.

Note: WEKA deployment on Azure only supports Ethernet networking.

## Terraform-Azure-WEKA example

The following is a basic example in which you provide the minimum detail of your cluster, and the Terraform module completes the remaining required resources, such as VPC, subnets, security group, placement group, DNS zone, and IAM roles.

You can use this example as a reference to create the `main.tf` file.

```hcl
provider "azurerm" {
  subscription_id = var.subscription_id
  partner_id      = "f13589d1-f10d-4c3b-ae42-3b1a8337eaf1"
  features {
  }
}

terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.75.0"
    }
  }
  required_version = ">= 1.3.7"
}

variable "get_weka_io_token" {}

variable "subscription_id" {}

module "weka_deployment" {
  source                         = "weka/weka/azure"
  version                         = "4.2.7.64"
  prefix                         = "my_prefix"
  rg_name                        = "example"

  subnet_prefix="10.3.2.0/24"
  address_space="10.3.0.0/16"
  logic_app_subnet_delegation_cidr="10.3.3.0/25"
  function_app_subnet_delegation_cidr="10.3.4.0/25"

  get_weka_io_token              = var.get_weka_io_token
  subscription_id                = var.subscription_id
  cluster_name                   = "my_cluster_name"
  tiering_enable_obs_integration = true
  cluster_size                   = 6
  allow_ssh_cidrs                = ["0.0.0.0/0"]
  allow_weka_api_cidrs           = ["0.0.0.0/0"]

}

output "weka_deployment_output" {
  value = module.weka_deployment
}
```

Note: For the descriptions of the parameters, refer to the  Azure-WEKA deployment Terraform package.

<!-- ============================================ -->
<!-- File 40/259: planning-and-installation_weka-installation-on-azure_deployment-on-azure-using-terraform.md -->
<!-- ============================================ -->

# Deployment on Azure using Terraform

This guide outlines the customization process for Terraform configurations to deploy the WEKA cluster on Azure. It is designed for system engineers with expertise in Azure and Terraform.

Note: If you are new to Azure and Terraform, refer to the

The Terraform package contains modules that can be tailored to suit your specific deployment requirements. The installation is based on applying the customized Terraform variables file to a predefined Azure subscription.

Applying the Terraform module performs the following:

* Creates resources in a predefined resource group, such as virtual machines, network interfaces, function apps, load balancer, and more.
* Deploys Azure virtual machines.
* Installs the WEKA software.
* Configures the WEKA cluste&#x72;**.**

The total deployment time is about 30 minutes. Half of that time is for resource deployment. The remainder is for the WEKA cluster installation and configuration.

## Prerequisites

Before installing the WEKA software on Azure, the following prerequisites must be met:

* The following must be installed on the workstation used for the deployment:
  * Azure CLI
  * Terraform (check the minimum required Terraform version specified in the **Requirements** section of Azure-WEKA deployment Terraform package.
* For an ARM-based MAC workstation (for example, M1 or M2), see specific instructions below.
* Initialize the Terraform-Azure-WEKA module using `terraform init` from the local directory. This command initializes a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, and more.
* Required permissions on Azure:
  * Privileged Role Administrator
  * Storage Blob Data Owner
  * Storage Account Contributor
  * Key Vault Administrator
* To login to the Azure account using Azure CLI, use the **az login** command.
* You need to create an Azure resource group within your subscription, which includes the Azure region.

<details>

<summary>Arm-based Mac workstation additional requirements</summary>

Follow these additional requirements to get Terraform working on an Arm-based Mac:

1. Run `brew install tfenv`
2. Run `TFENV_ARCH=amd64 tfenv install 1.3.7`
3. Run `tfenv use 1.3.7`
4. Run `brew install kreuzwerker/taps/m1-terraform-provider-helper`

</details>

## **Create a main.tf file**

The main Terraform configuration settings are included in the `main.tf` file. You can create it by following this procedure or using the WEKA Cloud Deployment Manager. See .

**Procedure**

1. Review the [Terraform-Azure-WEKA example](../azure-weka-terraform-package-description#terraform-azure-weka-example) and use it as a reference for creating the `main.tf` according to your Azure deployment specifics.
2. Tailor the `main.tf` file to create SMB-W or NFS protocol clusters by adding the relevant code snippet. Adjust parameters like the number of gateways, instance types, domain name, and share naming:

* **SMB-W**

<pre><code><strong>smb_protocol_gateways_number = 3
</strong>smb_protocol_gateway_instance_type = "Standard_L48s_v3"
smbw_enabled = true
smb_domain_name = "CUSTOMER_DOMAIN"
smb_share_name = "SPECIFY_SMB_SHARE_NAMING"
smb_setup_protocol = true

```

* **NFS**

```
nfs_protocol_gateways_number = 1
nfs_protocol_gateway_instance_type = "Standard_L48s_v3"
nfs_setup_protocol = true
```

4. Add WEKA POSIX clients (optional)**:** If needed, add [WEKA POSIX clients](../../weka-system-overview/weka-client-and-mount-modes) to support your workload by incorporating the specified variables into the `main.tf` file:

```makefile
clients_number = 2
client_instance_type = "Standard_L48s_v3"
```

## Apply the main.tf file

Once you complete the main.tf settings, apply it: Run `terraform apply`

## Cluster help commands

The system displays the cluster help commands enabling you to perform the following:

* Get the clusterization status
* Get the cluster status
* Fetch the WEKA cluster password
* View the path to SSH keys
* View the virtual machine IP addresses
* Resize the cluster

<details>

<summary>Cluster help commands output example</summary>

In the following example, the prefix is `v41`, and the cluster name is `jack`.

```
get-cluster-helpers-commands = <<EOT
########################################## Get clusterization status #####################################################################
function_key=$(az functionapp keys list --name v41-jack-function-app --resource-group jackm-rg --subscription d2f248b9-d054-477f-b7e8-413921532c2a --query functionKeys -o tsv)
curl --fail  -H "Content-Type:application/json" -d '{"type": "progress"}'

########################################## Get cluster status ############################################################################
function_key=$(az functionapp keys list --name v41-jack-function-app --resource-group jackm-rg --subscription d2f248b9-d054-477f-b7e8-413921532c2a --query functionKeys -o tsv)
curl --fail

######################################### Fetch weka cluster password ####################################################################
| az keyvault secret show --vault-name v41-jack-key-vault --name weka-password | jq .value |

########################################## Download ssh keys command from blob ###########################################################
 az keyvault secret download --file private.pem --encoding utf-8 --vault-name  v41-jack-key-vault --name private-key --query "value"
 az keyvault secret download --file public.pub --encoding utf-8 --vault-name  v41-jack-key-vault --name public-key --query "value"

############################################## Path to ssh keys  ##########################################################################
/tmp/v41-jack-public-key.pub
 /tmp/v41-jack-private-key.pem

################################################ Vms ips ##################################################################################
az vmss list-instance-public-ips -g jackm-rg --name v41-jack-vmss --subscription <azure subscription id> --query "[].ipAddress"

########################################## Resize cluster #################################################################################
function_key=$(az functionapp keys list --name v41-jack-function-app --resource-group jackm-rg --subscription <azure subscription id> --query functionKeys -o tsv)
curl --fail  -H "Content-Type:application/json" -d '{"value":ENTER_NEW_VALUE_HERE}'

EOT
```

</details>

**Related topic**

## Check the deployment progress

Once Terraform applies the configuration and deploys all the required resources, you can use the cluster help commands to check the progress of the cluster deployment.

The following is the command syntax for checking the cluster status during the deployment progress:

```

```bash
curl --fail https://<prefix>-<cluster name>-function-app.azurewebsites.net/api/status?code=$function_key
```

```

### Example

Explore the following phases to check the deployment progress:

<details>

<summary>Preparation</summary>

Once the VM starts, it prepares all the required objects, such as setting the partition to `/opt/weka`, downloading the Weka release, and deploying the container drives.

You can track the progress of the preparation, which can take about 10 minutes.

1. Get your function key by running the command:

```
function_key=$(az functionapp keys list --name v41-jack-function-app --resource-group jackm-rg --subscription <your Azure subscription id> --query functionKeys -o tsv)

```

2. Track the preparation progress by running the command:

```
curl --fail https://v41-jack-function-app.azurewebsites.net/api/status?code=$function_key -H "Content-Type:application/json" -d '{"type": "progress"}'

```

Response example:

```
{
  "ready_for_clusterization": [],
  "progress": {
    "v41-jack-backend000001": [
      "10:02:55 UTC: Running init script",
      "10:03:17 UTC: Installing weka"
    ],
    "v41-jack-backend000002": [
      "10:02:56 UTC: Running init script",
      "10:03:18 UTC: Installing weka"
    ],
    "v41-jack-backend000003": [
      "10:02:54 UTC: Running init script",
      "10:03:16 UTC: Installing weka",
      "10:08:32 UTC: Weka installation completed",
      "10:08:34 UTC: Setting deletion protection authorization error, going to sleep for 2M"
    ],
    "v41-jack-backend000004": [
      "10:02:57 UTC: Running init script",
      "10:03:27 UTC: Installing weka",
      "10:09:07 UTC: Weka installation completed",
      "10:09:09 UTC: Setting deletion protection authorization error, going to sleep for 2M"
    ],
    "v41-jack-backend000005": [
      "10:02:55 UTC: Running init script",
      "10:03:17 UTC: Installing weka"
    ],
    "v41-jack-backend000006": [
      "10:02:54 UTC: Running init script",
      "10:03:24 UTC: Installing weka"
    ]
  },
  "errors": null
}
```

</details>

<details>

<summary>Cluster formation status update</summary>

Once the preparation phase completes, the list of requested virtual machines appears. The number of servers ready for clusterization depends on the required cluster size.

Run the following command to track the clusterization status:

```
curl --fail https://v41-jack-function-app.azurewebsites.net/api/status?code=$function_key -H "Content-Type:application/json" -d '{"type": "progress"}'

```

The `"ready for clusterization"` section provides the list of virtual machines to be clusterized. In the following response example, the last backend `v41-jack-vmss_3` runs the cluster formation:

```
{
  "ready_for_clusterization": [
    "v41-jack-vmss_4:v41-jack-backend000004:20.228.235.225",
    "v41-jack-vmss_6:v41-jack-backend000006:20.228.234.98",
    "v41-jack-vmss_1:v41-jack-backend000001:20.228.234.225",
    "v41-jack-vmss_5:v41-jack-backend000005:20.228.236.6",
    "v41-jack-vmss_2:v41-jack-backend000002:20.228.235.126",
    "v41-jack-vmss_3:v41-jack-backend000003:20.228.235.38"
  ],
  "progress": {
    "v41-jack-backend000001": [
      "10:02:55 UTC: Running init script",
      "10:03:17 UTC: Installing weka",
      "10:09:43 UTC: Weka installation completed",
      "10:09:46 UTC: Setting deletion protection authorization error, going to sleep for 2M",
      "10:11:47 UTC: Deletion protection was set successfully"
    ],

    .
    .
    .

    "v41-jack-backend000006": [
      "10:02:54 UTC: Running init script",
      "10:03:24 UTC: Installing weka",
      "10:09:20 UTC: Weka installation completed",
      "10:09:23 UTC: Setting deletion protection authorization error, going to sleep for 2M",
      "10:11:23 UTC: Deletion protection was set successfully"
    ]
  },
  "errors": null

```

</details>

<details>

<summary>Cluster formation</summary>

Run the following command to check the cluster status:

```
$ curl https://v41-jack-function-app.azurewebsites.net/api/status?code=$function_key

```

In the following response example, the cluster formation is completed as shown in the third line `"clusterized": true`:

```
{
  "initial_size": 6,
  "desired_size": 6,
  "clusterized": true,
  "weka_status": {
    "hot_spare": 1,
    "io_status": "STARTED",
    "drives": {
      "active": 6,
      "total": 6
    },
    "name": "jack",
    "io_status_changed_time": "2023-04-16T10:15:53.35355Z",
    "io_nodes": {
      "active": 18,
      "total": 18
    },
    "cloud": {
      "enabled": true,
      "healthy": true,
      "proxy": "",
      "url": "https://api.home.weka.io"
    },
    "release_hash": "9756a1524e629d6c02c91bfb63d8239a2b4cce5f",
    "hosts": {
      "active_count": 18,
      "backends": {
        "active": 18,
        "total": 18
      },
      "clients": {
        "active": 0,
        "total": 0
      },
      "total_count": 18
    },
    "stripe_data_drives": 3,
    "release": "4.1.0.71",
    "active_alerts_count": 2,
    "capacity": {
      "total_bytes": 5182871000000,
      "hot_spare_bytes": 1036429300000,
      "unprovisioned_bytes": 0
    },
    "is_cluster": true,
    "status": "OK",
    "stripe_protection_drives": 2,
    "guid": "d4363615-bbae-416b-92f8-2d7304904996",
    "nodes": {
      "black_listed": 0,
      "total": 36
    },
    "licensing": {
      "io_start_eligibility": true,
      "usage": {
        "drive_capacity_gb": 11522,
        "usable_capacity_gb": 5182,
        "obs_capacity_gb": 0
      },
      "mode": "Unlicensed"
    }
  }

```

</details>

Note: You can also track the cluster formation progress on the last backend by opening the `/tmp/cluster_creation.log` file.

## **Validate the deployment**

Once the deployment is completed, access the WEKA cluster GUI using the URL: `http://<backend server DNS name or IP address>:14000` and get started with the WEKA cluster.

**Related topics**

## **Update the admin user** password

When deploying a WEKA cluster on the cloud using Terraform, a default username (admin) is automatically generated, and Terraform creates the password. Only the password is stored in the Key Vault of the Azure console. This user facilitates communication between the cloud and the WEKA cluster, particularly during scale-up and scale-down operations.

As a best practice, it‚Äôs recommended to update the admin password in the WEKA cluster and the Azure Key Vault.

**Procedure**

1. In the WEKA cluster, update the admin user's password.
2. In the **Azure console**, navigate to **Key Vault**.
3. Update the `weka_password` service with the newly updated password.
4. Validate the changes by checking the results in the Azure Logic Apps platform and ensuring they pass successfully.

**Related topic**

#change-a-local-user-password

## Set the license

To run IOs against the cluster, a valid license must be applied. Obtain a valid license and apply it to the WEKA cluster. For details, see .

## Set up the WEKA cluster to work with your Azure Blob storage

If you create an Azure Blob storage without using Terraform, you can set up the WEKA cluster to work with it.

**Procedure**

1. Gather the following details from your Azure account (refer to the Azure documentation for guidance):
   * Storage account name
   * Storage account container name
   * Storage account access key
2. Connect to one of the instances in your WEKA cluster and run the following command line, replacing the placeholders with your storage account details:

```

```bash
weka fs tier s3 add azure-obs --site local --obs-name default-local --obs-type AZURE --hostname <Storage account name>.blob.core.windows.net --port 443 --bucket <Storage account container name> --access-key-id <Storage account name> --secret-key <Storage account access key> --protocol https --auth-method AWSSignature4
```

```

**Related information**

Official Azure documentation

## **Clean up the** deployment

If the WEKA cluster is no longer required on Azure or you need to clean up the deployment, use the `terraform destroy` action (a token from get.weka.io is required). The object storage and storage account are not deleted.

Note: The destroy command does not work properly if the Terraform deployment fails for any reason, such as dependencies not being present and Azure resource starvation. Manually remove any resources created at the beginning of the Terraform script using the Azure console or Azure CLI before re-running the Terraform script.

Note: If you need to preserve your data, create a snapshot using [snap-to-object](../../weka-filesystems-and-object-stores/snap-to-obj).

    For details, see https://azure.microsoft.com/en-us/products/key-vault

    For details, see https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview.

<!-- ============================================ -->
<!-- File 41/259: planning-and-installation_weka-installation-on-azure_detailed-deployment-tutorial-weka-on-azure-using-terraform.md -->
<!-- ============================================ -->

---
description:
---

# Detailed deployment tutorial: WEKA on Azure using Terraform

## Introduction

Deploying WEKA in Azure involves familiarity with Microsoft Azure Cloud, Terraform (for infrastructure-as-code provisioning), basic Linux operations, and WEKA software. Recognizing that not all individuals responsible for this deployment may have experience in every area, this document offers a comprehensive, step-by-step guide to successfully deploying a WEKA cluster in Azure, even with minimal prior knowledge.

**Document scope**

This document provides guidance on deploying WEKA in an Azure environment with an existing networking configuration. For Proof of Concept (POC) or production deployments, the process involves using the customer's existing Azure Virtual Network (VNet), subnet, and Network Security Group.

This document guides you through:

* General Azure requirements.
* Azure networking requirements necessary for WEKA.
* Deployment of WEKA using Terraform.
* Verification of a successful of WEKA deployment.

Note: The images embedded in this document may appear small when viewed in-line. Double-clicking on an image enlarges it to its original size for easier viewing.

## Administrative prerequisites

Before deploying WEKA in Microsoft Azure, ensure that the target environment is properly configured. Several key components must be set up before deploying WEKA using Terraform to ensure a successful outcome. The following subsections provide a step-by-step guide for configuring each component according to WEKA requirements.

### Identify your Azure Subscription

Azure environments are organized within a Subscription, which serves as the primary construct containing resource groups, VNets, subnets, security groups, virtual machine instances, and other resources. The initial step in deploying WEKA in Azure is to identify the subscription where the WEKA resources will be deployed.

#### Procedure

1. Navigate to the Microsoft Azure Portal. Search for **Subscriptions** and select it.

2.  On the **Subscriptions** page, locate the subscription you plan to use for deploying WEKA.

    Ensure you understand the Azure Subscription structure for your environment before proceeding with the deployment.

### Verify user privileges assignment

To successfully deploy WEKA in Microsoft Azure, ensure the account used is a Subscription Contributor. If the user lacks this role, the deployment steps will fail. If an existing user cannot be used, create a new user with the necessary rights within the Subscription.

#### Procedure

1. Log in to the Azure Portal using the account intended for the WEKA deployment. Search for **Users** and select it.

2. Locate the user by typing part of their username and select their name from the list.

3. On the user page, select **Azure Role Assignments**.

4. Verify the user's roles to ensure they are assigned as an **Owner** or **Contributor** for the Subscription used for WEKA deployment.

After confirming the user's permissions, verify the resource quotas.

### Verify resource quotas

When deploying resources in Microsoft Azure, ensure sufficient quotas are set for the specific resources needed. For instance, when deploying Lsv3 virtual instances for the WEKA backend cluster, configure an adequate vCPU quota for the Lsv3 instance type. Azure specifies quotas on a per-instance or per-instance-family basis.

If you or your customer have not used a particular instance type before, you must set a sufficient quota to avoid failures during deployment with Terraform. The minimum quota required is equal to the total number of vCPUs needed for the deployment.

#### Procedure

1. In the Azure Portal, search for **Quotas** and select it.

2. On the quotas page, select **Compute**.

3. Search for the instance family or specific instance for which you need to set or check the quota. For example, Dsv5 instances.

4. Select the checkbox next to the desired instance type, open the **Request quota increase** dropdown, and **Enter a new limit**.

5. In the Request quota increase section, enter the desired number of vCPUs for the instance type or family. For instance, request a new vCPU quota of 150 for the Standard Dsv5 family. Select **Submit**.

6. Most quota increase requests are approved in real-time, without needing Azure support. However, if requesting a large number of vCPUs or a specialized instance, contacting Azure support may be necessary. The example demonstrates a successful vCPU increase request.

7. Ensure quotas are set for all instances required for the deployment. For WEKA backends, set the quota for the Lsv3 instance family, and any other instance families used for WEKA clients. For a complete listing of available instance sizes, see .

## Azure resource prerequisites

Running WEKA in Azure requires Azure cloud resources for compute, storage, networking, and security. For internal testing, customer POC, or production deployment, a minimum resource configuration is necessary for successful operation.

Many customers may have pre-existing Azure environments that include the required resources, though confirmation is necessary. The following steps assume WEKA is being deployed into a ‚Äúblank slate‚Äù Azure environment. These instructions also help navigate a customer‚Äôs existing environment to ensure WEKA prerequisites are met.

### Create a Resource Group

A Microsoft Azure Resource Group is a fundamental organizational unit that acts as a logical container for resources within an Azure Subscription. It holds resources such as virtual machines, VNets, security groups, and storage accounts. A Resource Group must be available for deploying WEKA and its dependencies.

Note: If corporate policies require separating WEKA compute or client instances from network resources, Terraform deployment scripts can accommodate this, as detailed in a later section.

#### Procedure

1. In the Azure Portal, search for **Resource groups** and select it.

2. On the Resource groups page, select **Create**.

3. On the Create resource group page, enter the required details, ensuring you select the correct subscription and region. Select **Review + create**.\
   (Once named, a Resource Group cannot be renamed.)

4. Select **Create** to confirm.

5. Review the newly created Resource Group.

### Create a VNet

A Virtual Network (VNet) in Microsoft Azure is essential for secure communication between Azure resources, such as virtual machines (VMs), and for connecting to the internet and on-premises networks.

A VNet provides logical isolation within the Azure cloud, dedicated to a subscription, and includes subnets that allocate IP address space to VMs.

For WEKA deployment, both management and DPDK traffic must use VNets, with all WEKA cluster backends and POSIX clients placed within the same VNet and subnet. Contact the the [Customer Successes Team](../../support/getting-support-for-your-weka-system) for additional guidance.

**Procedure**

1. In the Azure Portal, search for **Virtual networks** and select it.

2. On the Virtual networks page, Select **Create**.

3. On the Create virtual network page, enter the VNet configuration details, including the subscription and resource group from the previous step. Provide a VNet name and region, then Select **Next: IP Addresses**.

4. In the IP Addresses section, specify the IP address space and adjust the default subnet configuration as needed. Select **Review + create** when done.

5. Select **Creat**e to confirm.

6. After creation, review the confirmation page and verify the new VNet.

### Create a Network Security Group (NSG)

A Network Security Group (NSG) manages network traffic to Azure resources by applying security rules to control ingress (incoming) and egress (outgoing) traffic. It functions as a firewall for network interfaces (NICs), virtual machines (VMs), and subnets.

Note: NSGs start with default rules for basic connectivity, such as allowing outbound communication and denying all inbound traffic from the internet. Custom rules can override these defaults.

#### Procedure

1. In the Azure Portal, search for **Network security groups** and select it.

2. On the Network security groups page, Select **Create**.

3. On the Create network security group page, enter the required details, including the subscription and resource group from earlier steps. Ensure the region matches other resources. Select **Review + create**.

4. Select **Create** to confirm.

5. After creation, review the confirmation page and verify the new Network Security Group.

### Associate a Network Security Group with a Subnet

Azure Network Security Groups (NSGs) must be associated with either a subnet or a network interface card (NIC) to be effective. In this deployment example, associate the NSG with the subnet created earlier.

In a customer environment, it might be necessary to adapt these associations based on the existing network architecture and security requirements.

**Procedure**

1. Search for **Virtual networks** in the Azure Portal and select it.

2. Select the relevant virtual network from the list.

3. In the virtual network configuration screen, Select **Subnets**.

4. Select the relevant subnet (in this example, the default subnet).

5. On the subnet configuration screen, locate the **Network security group** dropdown and select the previously created NSG.

6. Confirm the selection and select **Save**.

### Create and associate a NAT Gateway

Azure NAT (Network Address Translation) Gateway simplifies outbound-only Internet connectivity for virtual networks. It translates private IP addresses of VMs or other resources to a public IP address, allowing outbound internet access without exposing resources to inbound traffic.

For WEKA deployments, the NAT Gateway provides outbound internet access needed to reach repositories and obtain installation binaries, enhancing security by avoiding public IP assignments on individual instances.

In environments with restricted internet access, alternative solutions are covered later in this document.

NAT Gateways must be created and associated with the subnet needing outbound internet access. The creation wizard facilitates both steps in one process.

**Procedure**

1. In the Azure Portal, search for **NAT gateways** and select it.

2. On the NAT gateways page, select **Create**.

3. On the Create network address translation (NAT) gateway page, enter the required details. Select the correct subscription and resource group, specify a name and region, and select **Next: Outbound IP**.

4. In the Outbound IP section, select **Create a new public IP address**, enter a name for the public IP, and select **OK**.

5. Ensure the Public IP address dropdown displays the newly created IP name. Select **Next: Subnet**.

6. In the Subnet section, select the previously created VNet and subnet. Select **Review + create**.

7. Select **Create** to confirm.

8. Upon completion, review the newly created NAT Gateway on the confirmation page.

#### Install AzureCLI

Terraform uses Azure CLI to pass commands to Azure. It is recommended to install the latest version. The following steps use version 2.50, which is current at the time of writing.

**Procedure**:

1. Open a terminal and run the following command to install Azure CLI through Homebrew:

```bash
brew update && brew install azure-cli
```

2. Wait for the installation to complete.

3. To confirm the installation, run:

```bash
az version
```

The installed version of Azure CLI is displayed.

### Log in to Azure CLI

Terraform uses Azure CLI to perform operations within an Azure subscription.

**Before you begin**

Before running Terraform, the Azure user must authenticate through the Azure CLI. See #identify-your-azure-subscription

**Procedure:**

1. Open a terminal session and enter the command:

```bash
az login
```

2. A web browser opens, prompting the user to select an account for authentication. Select the user or enter the credentials.

After successful authentication, a confirmation message appears.

3. Return to the terminal, where the authentication status of Azure CLI is displayed.

## Deploy WEKA in Azure using Terraform Registry

The **Terraform Registry** is a repository of modules and resources that simplifies the deployment process by providing reusable components.

Before deploying WEKA in Azure with Terraform, several prerequisites must be met. These requirements depend on the type of deployment‚Äîwhether you're integrating with an existing Azure network and resources or allowing the WEKA Terraform package to automatically create the necessary infrastructure.

The following section outlines the required Azure dependencies for Terraform, explaining each in the context of a successful WEKA deployment.

### Terraform dependencies and constructs

When deploying WEKA in Azure using Terraform, several Azure resources must be created either automatically by Terraform or manually in advance, depending on your deployment method. These resources include:

* **Virtual Machine Scale Set (VMSS):** Azure's VMSS service enables the deployment and management of identical virtual machine instances that scale automatically based on demand. In a WEKA deployment, the VMSS hosts all backend instances and uses Placement Groups to optimize performance.
* **Placement Groups:** These groups control the distribution of VM instances within a scale set, optimizing network traffic and providing fault tolerance. For WEKA, only single-placement groups are supported, allowing up to 100 VM instances in a backend cluster.
* **Resource Groups:** Azure Resource Groups act as logical containers for cloud resources. A Resource Group must be available to organize and deploy all WEKA and Azure dependencies, including virtual machines, VNets, and security components.
* **Virtual Network (VNet):** A VNet is a core networking component that allows secure communication between Azure resources and external networks. WEKA uses VNets for management and DPDK traffic to ensure optimal performance. VNets also contain subnets, which assign IP addresses to virtual machines.
* **Subnet:** Subnets are IP address ranges within a VNet, providing network segmentation to organize and secure resources in a structured manner.
* **Delegated Subnets:** Delegated subnets allow specific Azure services to create resources within a designated subnet. In WEKA deployments, these are used for function and logic apps that enable cluster scaling and auto-healing.
* **Network Security Group (NSG):** An NSG acts as a firewall, filtering network traffic to and from Azure resources based on security rules. For WEKA, a self-referencing rule is recommended to facilitate secure communication within the network.
* **Private DNS Zone:** This zone provides DNS resolution within Azure VNets for private network environments. In a WEKA deployment, it enables name resolution for VMs, application services, and WEKA components connected to the VNets.

### Deployment prerequisites

Before deploying WEKA using Terraform, ensure that any required resources are pre-created if Terraform will not be provisioning them automatically. For example, if you plan to use an existing VNet, it must be created in advance. In most customer environments, many of these resources are likely already available.

The steps in this guide are educational and serve as general guidelines for creating Azure prerequisites, as previously outlined.

#### Navigate the Terraform Registry and obtain the files

Using the Terraform Registry simplifies managing the latest Terraform releases, ensures clean version control, and requires only one `main.tf` file for configuration.

1. Access the WEKA namespace on the Terraform Registry: Terraform Registry WEKA Namespace.
2. Select **weka/weka** module to create weka cluster on Azure using TF.
3. From the **Examples** options, select the deployment type (`public_network` in this example).
4. Select the **GitHub source code** link.
5. On the GitHub page, select the `main.tf` file for the **public_network** example.
6. Click **Download raw file** to save the `main.tf` file.

Note: Examples serve as a starting point. Customize the variables to match your specific deployment needs. Do not use the example "as is" expecting it to deliver the exact outcome for your environment.

#### Resources and guidance in the Terraform Registry page

This module provides extensive resources and guidance, divided into several sections:

* **README:** Serves as a detailed guide on how the module works, replacing the traditional GitHub README file.
* **Inputs:** Lists all configurable variables, such as VNet selection, resource groups, and instance types. This is where you tailor your WEKA deployment.
* **Outputs:** Lists outputs available after running `terraform apply`, such as cluster status, auto-created WEKA password retrieval from KeyVault, and SSH keys.
* **Dependencies:** Describes provider dependencies automatically installed during `terraform init`.
* **Resources:** Lists Azure resources the module may create, depending on user-configured variables.

### Locate the user‚Äôs token in get.weka.io

The user's token in get.weka.io provides access to WEKA binaries and is required during installation.

**Procedure:**

1. Visit get.weka.io, and select the user‚Äôs name in the upper-right corner.

2. From the left-hand menu, select **API Tokens**. The user's API token is displayed on the screen and will be used later in the installation process.

### Select variables and edit the main.tf

To configure the deployment, open the downloaded `main.tf` file in your preferred code editor. Follow these steps to customize the necessary variables for your environment:

1. **Under provider "azurerm":**
   * Replace `subscription_id` with the Azure subscription ID for your deployment environment.
   * Leave the `partner_id` as `f13589d1-f10d-4c3b-ae42-3b1a8337eaf1` (this identifies WEKA as a partner for Azure resource spend).
2. **Under module "weka_deployment":**
   * Set `source = "../../"` to specify the module source location.
   * Update `prefix = "weka"` to define the cluster prefix for Azure resources.
   * Set `rg_name = "weka-rg"` to reference the pre-created Azure resource group.
   * Replace `get_weka_io_token = var.get_weka_io_token` with your unique WEKA software download token from get.weka.io.
   * Ensure `subscription_id = var.subscription_id` is set to your Azure subscription ID.
   * Change `cluster_name = "poc"` to your desired custom cluster name (this will be appended to the prefix).
   * Set `tiering_enable_obs_integration = true` to enable object tiering if required.
   * Adjust `cluster_size = 6` to define the number of WEKA backend members for the cluster.
   * Set `allow_ssh_cidrs = ["0.0.0.0/0"]` to allow SSH access to cluster members from a defined WAN address range (since this is a public network).
   * Set `allow_weka_api_cidrs = ["0.0.0.0/0"]` to allow API access to WEKA from a defined WAN address range.

Several default example variables will be modified, and others will be added to align with this guide's deployment into an existing public network.

Note: **Important note:**\
Many of the Terraform variables listed on the Terraform Registry page for the Azure WEKA module under the Inputs section have pre-set default values. If a variable is not explicitly defined in your `main.tf`, the defaults automatically apply. It is recommended to review these variables to ensure that the defaults meet your deployment needs.

#### **Customized `main.tf` example**

The following is a customized version of the `main.tf` file, which will be used in this guide to deploy a WEKA cluster.

**Variable descriptions of the customized `main.tf` example:**

* **Under provider "azurerm":**
  * `subscription_id = "azure_subscription_id_here"`: Fill this in with the Azure subscription associated with your deployment environment.
* **Under module "weka_deployment":**
  * `source = "weka/weka/azure"`: Specifies the Terraform Registry module source for WEKA on Azure.
  * `version = "4.0.5"`: Sets the version of the WEKA Terraform module.
  * `prefix = "weka"`: Prefix for the Azure resources created (customizable).
  * `rg_name = "bgcadv"`: The name of the existing Azure resource group for deployment.
  * `vnet_name = "bgcadv"`: The existing VNet where WEKA resources will be deployed.
  * `subnet_name = "default"`: The existing subnet within the VNet for WEKA deployment.
  * `get_weka_io_token = "your_token_here"`: WEKA download token.
  * `subscription_id = "azure_subscription_id_here"`: The Azure subscription ID for deployment.
  * `cluster_name = "bgcadv0"`: Name for the deployed cluster (appended to the prefix).
  * `tiering_enable_obs_integration = false`: Disables object integration.
  * `instance_type = "Standard_L8s_v3"`: Specifies the Azure instance type for WEKA backend servers.
  * `cluster_size = 6`: Defines the number of WEKA backends for deployment.
  * `allow_ssh_cidrs = ["0.0.0.0/0"]`: Allows SSH access to the cluster from any WAN address.
  * `allow_weka_api_cidrs = ["0.0.0.0/0"]`: Allows API access from any WAN address.
  * `clients_number = 2`: Specifies the number of WEKA clients to deploy and mount automatically.
  * `client_instance_type = "Standard_D4_v4"`: Sets the instance type for the automatically deployed WEKA clients.
*   The final entry instructs Terraform to output useful information for verifying the WEKA cluster's deployment and connection:

    ```hcl
    output "get-cluster-helpers-commands" {
       value = module.weka_deployment
    }
    ```

### Initialize and run Terraform

Once the `main.tf` file is customized, do the following:

1. Open a terminal window on your local machine (or the machine where Terraform will be run).
2. Navigate to the directory containing the edited `main.tf` file.
3. Run the following command to initialize Terraform:

```bash
terraform init
```

This action downloads the necessary WEKA Azure Terraform modules and provider plugins.

4. After initialization, run the following command to perform a dry run:

```bash
terraform plan
```

This action checks the deployment configuration for issues but cannot account for quota limitations or naming conflicts (for example, KeyVault).

5. Apply the deployment by running:

```bash
terraform apply
```

A successful deployment displays an output similar to the following example, including the `get-cluster-helpers-commands` output for connecting to and verifying the WEKA cluster.

#### Locate and copy the WEKA Cluster SSH Key to Azure Jump Box

The WEKA cluster SSH key created during terraform deployment is required to access the WEKA cluster backend members, as well as any WEKA clients that were deployed via terraform.

**Procedure**

1. Navigate to the `/tmp/` directory.

2. Locate both the public (.pub) and private (.pem) key files.
3. Use SCP to transfer the private key (.pem) file from the local machine‚Äôs `/tmp/` directory to the Azure linux jump box. Text highlighted in purple should be copied and used directly for your specific SCP command. Text in green should be customized to your unique values.\
   \
   `azureuser` is the default user account created when creating a new virtual machine instance in Azure. It is recommended to keep this default. The first path highlighted in green is the path to the private key for your Azure jump box. The second path is for the private key for the newly created WEKA cluster you‚Äôd like to transfer to the Azure jump box. The IP address should be changed to the Azure public IP of your jump box. The WEKA cluster private key should be transferred to the `.ssh` directory in the default azureuser‚Äôs home directory on the jump box.

If the command has been configured correctly, an output similar to below should be printed to the terminal upon completion of private key transfer.

#### Monitor deployment status

When deploying into a customer‚Äôs Azure environment, it‚Äôs likely they‚Äôll already have some means of connectivity to the vnet and subnet into which WEKA has been deployed. This could be by way of a VPN, bastion host, or preconfigured jump box. If the customer doesn‚Äôt have any means of accessing the WEKA cluster on the isolated subnet, they‚Äôll need to configure one of the methods mentioned above. In this example, a preconfigured Ubuntu linux jump box with a publicly assigned IP will be used. The jump box is in the same vnet, network security group, and subnet as the WEKA cluster, though inbound access rules have been applied to the network security group to facilitate access from the outside on SSH port 22.

Some form of access to the WEKA cluster will be crucial for monitoring the deployment progress and ensuring everything completes successfully.

**Procedure**

1. Navigate to Virtual Machines in the Azure portal. Locate the virtual machine instance with the suffix `clusterizing` . The `clusterizing` suffix is only visible in the Azure portal to denote the WEKA backend cluster member that runs post resource deployment clusterization scripts. Note that the virtual machine‚Äôs actual name is `demo-bgc-backend-5`.
2. Identify the local network IP address of the `clusterizing` instance‚Äôs management interface. Take note of the IP address, as it will be used in the next step.

3. SSH to the Azure linux jump box using the applicable private key and public IP address. This private key is **not** the WEKA cluster SSH private key saved to the `/tmp/` directory by terraform. The jump box private key would‚Äôve been specified or created and downloaded at the time the jump box was manually created in the Azure portal.
4. Once connected to the jump box, use the local IP address of the `clusterizing` instance identified in the previous step to SSH into the `clusterizing` instance, also known as `demo-bgc-backend-5`.

Note: The \`clusterizing\` instance will always be the last node of the cluster. For instance, if a 6 node cluster is deployed, the instances will have suffixes \`0-5\`. Instance \`5\` will be the \`clusterizing\` instance. If an 18 node cluster is deployed, the instances will have suffixes \`0-17\`. Instance \`17\` will be the \`clusterizing\` instance.

5. Once connected to the `clusterizing` instance, navigate to the `/var/log` directory. Locate the `cloud-init-output.log` file highlighted in purple. Run the command `tail -f cloud-init-output.log` to tail the logfile to check the status of the deployment. In the example below, the `tail` command was run while WEKA binaries were being downloaded from `get.weka.io`.

The WEKA containers are being configured as the WEKA installation continues.

The install script will start-io. At this point, i-nodes and WEKA buckets will begin coming online.

Finally, the file system group `default` and file system `default` will be created. The date and time of cluster creation completion will be printed in UTC. The number of seconds required to perform clusterization is printed. This signifies that the WEKA installation and clusterization processes are complete.

Note: Stay logged into \`backend-5\` for the next section.

#### Weka Status and Client Status

To confirm that the WEKA cluster is up and running, issue a `weka status` command on the `backend-5` cluster member. If the cluster is indeed up and running, an output identical to the below should be outputted.

Note: Note the \`status\`, \`protection\`, and \`io status\`.

When the terraform `main.tf` file was configured for this deployment, two clients were specified for deployment in addition to the WEKA cluster backend members. Note that when `weka status` is initially run immediately following `cloud-init` script completion, those clients aren‚Äôt acknowledged. Note the entry for `clients: 0 connected`. **This is expected behavior**, as the clients are the last components to initialize. Depending on the number of clients deployed, 15 minutes can elapse before all clients are registered.

The following example shows that the two clients are successfully connected to the cluster three minutes after cluster io starts.

### Cluster helper commands

The cluster helper commands are executed through a terminal and a web browser. The function app, created during deployment, processes these commands, retrieves the necessary information, and returns it to the user.

#### **Retrieve clusterization status**

1. Under the **Get Clusterization Status** section, copy the first line of code.

2. Paste the copied code into the terminal, and run. No output will appear‚Äîthis is expected behavior.

3. Copy the second line of code from the **Get Clusterization Status** section.

4. Paste the copied URL into the terminal, and run. The command returns an error message along with a URL. The curl command is designed to fail, providing the URL needed to check the clusterization status.

5. Copy the URL and paste it into a web browser. The clusterization status of the deployed WEKA cluster displays. Review the returned data to confirm the cluster deployment.

#### **Check cluster status**

1. Follow the same steps as retrieving the clusterization status to check the cluster status. This process returns the same information once clusterization completes.

**Retrieve the WEKA cluster password**

1. Ensure the `jq` tool is installed. `jq` is a lightweight command-line tool for processing JSON data, commonly used in system administration. On a Mac, install it using Homebrew by running the following command:

```
brew install jq
```

The process of installing `jq` begins. Installation completes once a command prompt is returned.

2. After `jq` installation, under the **Fetch WEKA Cluster Password** section, copy the command.

3. Paste the copied command into the terminal, and run it. The WEKA cluster password appears.

#### **Retrieve WEKA backend IP addresses**

Retrieve the IP addresses of the WEKA backend instances. For a public network deployment, WAN addresses appear. If using a private network, LAN IP addresses are retrieved.

1. Copy the helper command for listing the IP addresses of the VMSS (Virtual Machine Scale Set) that contains the WEKA backend instances.

2. Paste the copied command into the terminal, and run it. The public IP addresses display.

## **Access the WEKA web interface**

1. Select one of the backend IP addresses, paste it into the browser's address bar, append `:14000`, and press Enter. The WEKA web interface loads.
2. Log in using the default username `admin` and the password retrieved in the earlier step.

In the examples below, a Windows 10 instance with a public IP address was deployed in the same vnet, subnet, and security group as the WEKA cluster. Network security group rules were added to allow RDP explicit access to the Windows 10 system.

3. Open a browser in the Windows 10 jump box and visit `https://<cluster-backend-ip>:14000`. The WEKA GUI login screen should appear. After changing the default password, login.

4. View the cluster GUI home screen.

5. Review the cluster backends.

6. Review the clients attached to the cluster as part of the terraform deployment process.

7. Review the file system `default` created as part of the terraform deployment process.

8. In the Azure portal Virtual Machines page, view the WEKA cluster instance resources.

<!-- ============================================ -->
<!-- File 42/259: planning-and-installation_weka-installation-on-azure_add-clients.md -->
<!-- ============================================ -->

# Add clients to a WEKA cluster on Azure

When deploying a WEKA cluster, it is possible to create clients using Terraform. After completing this step, you can expand the number of clients in your WEKA system by performing the following procedure.

## Before you begin

* Create a client VM using one of the following methods:
  * **Using the Azure Console:**  Create the client VM that meets the following requirements:
    * The _Accelerated Networking_ feature must be enabled in the NICs.
    * The NICs must be configured with at least MTU 3900.
    * Ensure a supported OFED is installed.
    * Remove the secondary default gateway from the routing table.
    * If working with a different security type than the standard, for example, trusted launch virtual machines, clear the **Enable secure boot** option in the **Configure security features**.
  * **Using a custom image of a WEKA client:**
    * In the Azure console, search for the community image named **weka** with ID `WekaIO-d7d3f308-d5a1-4c45-8e8a-818aed57375a`. The **weka** custom image includes ubuntu 20.04 with kernel 5.4 and ofed 5.8-1.1.2.1.
    * Enable the _Accelerated Networking_ feature in the NICs.
    * Configure the NICs to operate with at least MTU 3900.
* Ensure the client has enough available IP addresses in the selected subnet. Each core allocated to WEKA requires a NIC (and IP address).

## Mount the filesystem

1. Create a mount point (only once):

```
mkdir /mnt/weka
```

1. Install the WEKA agent on your client machine (only once):

```bash
| curl <backend server IP address>:14000/dist/v1/install | sh |
```

Example:

```bash
| curl http://10.0.0.7:14000/dist/v1/install | sh |
```

2. Detect the existing network configuration. Run the command: `ip a`.

<details>

<summary><code>ip a</code> command output example</summary>

<pre class="language-bash" data-overflow="wrap"><code class="lang-bash"><strong>root@jack:~# ip a
</strong>1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 3900 qdisc mq state UP group default qlen 1000
    link/ether 00:0d:3a:8e:3a:67 brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.30/24 brd 10.0.0.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::20d:3aff:fe8e:3a67/64 scope link
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 4038 qdisc mq state UP group default qlen 1000
    link/ether 00:0d:3a:8b:d9:bd brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.31/24 brd 10.0.0.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::20d:3aff:fe8b:d9bd/64 scope link
       valid_lft forever preferred_lft forever
4: enP18334s1np0: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 3900 qdisc mq master eth0 state UP group default qlen 1000
    link/ether 00:0d:3a:8e:3a:67 brd ff:ff:ff:ff:ff:ff
5: enP39539s2np0: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 4038 qdisc mq master eth1 state UP group default qlen 1000
    link/ether 00:0d:3a:8b:d9:bd brd ff:ff:ff:ff:ff:ff
6: dtap0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 4038 qdisc multiq state UNKNOWN group default qlen 1000
    link/ether 00:0d:3a:8b:d9:bd brd ff:ff:ff:ff:ff:ff
    inet6 fe80::20d:3aff:fe8b:d9bd/64 scope link
       valid_lft forever preferred_lft forever

```

</details>

3. Once the WEKA cluster runs, you can mount clients to the filesystem using the following command:

```

```bash
mount -t wekafs <backend-server-IP-address>/<filesystem-name> -o net=<VF interface>/<synthetic network interface IP address>/mask -o mgmt_ip=<Management-IP> /mnt/weka
```

```

Where:

* `<VF interface>/<synthetic network interface IP address>/mask`: The VF interface and synthetic network interface are automatically paired and act as a single interface in most aspects used by applications. The synthetic interface always has a name in the form `eth\<n\>`.\
  You can identify the VF interface and synthetic network interface pair by their common MAC address. In the example above,  the VF interface is `enP39539s2np0` (item 5), and the synthetic network interface is `eth1` (item 3), which has the  IP address and mask 10.0.0.31/24.
* `<Management-IP>`: In the example above, it the `eth0` management IP `10.0.0.30`.

Example:

```

```bash
mount -t wekafs 10.0.0.7/default -o net=enP39539s2np0/10.0.0.31/24 -o mgmt_ip=10.0.0.30 /mnt/weka

```

```

Note: Using the Azure Console, the client instances are provisioned separately from the WEKA cluster.

**Related topics**

<!-- ============================================ -->
<!-- File 43/259: planning-and-installation_weka-installation-on-azure_auto-scale-virtual-machines-in-azure.md -->
<!-- ============================================ -->

# Auto-scale virtual machines in Azure

WEKA provides a resize API to scale up or down the cluster. You must use only this API to resize the cluster. Do not use any other option for resizing.

Note: The cluster name prefix, resource group name, and Azure subscription id appear in the `Resize cluster` section of the [Cluster help commands](../deployment-on-azure-using-terraform#cluster-help-commands) output.

**Procedure:**

1. Get the resize API (function-app).\
   Run the command:

```
function_key=$(az functionapp keys list --name <cluster name prefix>-function-app --resource-group <resource group name> --subscription <Azure subscription id> --query functionKeys -o tsv)

```

2. Resize the cluster.\
   Enter the value of the required number of virtual machines instead of `ENTER_NEW_VALUE_HERE` (the minimum value is `6`), and run the command:

```
curl --fail https://<cluster name prefix>-function-app.azurewebsites.net/api/resize?code=$function_key -H "Content-Type:application/json" -d '{"value":ENTER_NEW_VALUE_HERE}'

```

3. Track the resize progress using the commands provided in the [Check the deployment progress](../deployment-on-azure-using-terraform#check-the-deployment-progress) section.

<!-- ============================================ -->
<!-- File 44/259: planning-and-installation_weka-installation-on-azure_troubleshooting.md -->
<!-- ============================================ -->

# Troubleshooting

During the deployment process, errors may occur. You can use the Azure Console tools to verify the resource status and the Azure quota limitations.

For additional Terraform logs during the WEKA cluster deployment, you can run the Terraform with the option `TF_LOG variable=ERROR`.

The following is a partial list of errors and the corrective actions that may occur during the deployment:

### Error when creating virtual machines

When deploying the WEKA cluster, an error indicates that more virtual machines have been requested than the quota limit in your Azure subscription.

**Resolution:**

Submit a quota increase to Microsoft Azure using the URL specified in the error message.

For more details, see Increase VM-family vCPU quotas on the Microsoft site.

### The storage account name is already taken

Creating a storage account name must be unique across the Azure environment.

**Resolution:**

In the Terraform variables file, ensure the `prefix` and `cluster_name` variables are unique.

The specified resource group in the Terraform variables file was not found because it was not created (as part of the prerequisites).

**Resolution:**

Create the Azure resource group before applying the Terraform file using the Azure console or Azure CLI.

In Azure CLI, use the following command:

`az group create --name <myResourceGroup> --location <region>`

Example:

`az group create --name weka-jack-rg --location eastus`

**Related topics**

#prerequisites

<!-- ============================================ -->
<!-- File 45/259: planning-and-installation_weka-installation-on-gcp.md -->
<!-- ============================================ -->

---
description:
---

# WEKA installation on GCP

## WEKA on GCP overview

Leveraging GCP's advantages, WEKA offers a customizable **terraform-gcp-weka** module for deploying the WEKA cluster on GCP. In GCP, WEKA operates on instances, each capable of using up to eight partitions of drives on the connected physical server (without direct drive usage). These drives can be shared among partitions for other clients on the same server.

WEKA requires either four or seven VPC networks, depending on which instance type is being used for the WEKA backends. This configuration aligns with the four key WEKA processes: Management, Drive, Compute and optionally Frontend, with each process requiring a dedicated network interface as follows:

* eth0: Management vpc-0
* eth1: Drive vpc-1
* eth2: Compute vpc-2
* eth3: Frontend vpc-3

For a WEKA instance using seven NICs, the alignment is as follows:

* eth0: Management vpc-0
* eth1: Drive vpc-1
* eth2: Compute vpc-2
* eth3: Compute vpc-3
* eth4: Compute vpc-4
* eth5: Compute vpc-5
* eth6: Frontend vpc-6

If the frontend container is not deployed, then its core is instead used by the compute process.

VPC peering facilitates communication between the WEKA processes, each using its NIC. The maximum allowable number of peers within a VPC is limited to 25 by GCP (you can request GCP to increase the quota, but it depends on the GCP resources availability).

<details>

<summary>Terraform overview</summary>

Terraform is an open-source project from Hashicorp. It creates and manages resources on cloud platforms and on-premises clouds. Unlike AWS CloudFormation, it works with many APIs from multiple platforms and services.

The GCP Console is already installed with Terraform by default. It is the primary tool for deploying WEKA on GCP. Terraform can be used outside of GCP or independent of GCP Console.

### How does Terraform work?

A deployment with Terraform involves three phases:

* **Write:** Define the infrastructure in configuration files and customize the project variables provided in the Terraform package.
* **Plan**: Review the changes Terraform will make to your infrastructure.
* **Apply:** Terraform provisions the infrastructure, including the VMs and instances, installs the WEKA software, and creates the cluster. Once completed, the WEKA cluster runs on GCP.

**Related information**

Terraform Tutorials

Terraform Installation

</details>

<!-- ============================================ -->
<!-- File 46/259: planning-and-installation_weka-installation-on-gcp_required-services-and-supported-regions.md -->
<!-- ============================================ -->

# Required services and supported regions

The region must support the services used in WEKA on GCP. The following sections list these services and the regions that support them.

## Required services used in WEKA on GCP

* Cloud Build API
* Cloud Deployment Manager V2 API
* Cloud DNS API
* Cloud Functions API
* Cloud Logging API
* Cloud Resource Manager API
* Cloud Scheduler API
* Compute Engine API
* Secret Manager API
* Serverless VPC Access API
* Service Usage API
* Workflow Executions API
* Workflows API

Other services used or enabled:

* App Engine
* IAM
* Google Cloud Storage

## Supported regions

To ensure support for a specific region, it must meet the requirements listed above.

Note: If the region you want to deploy in is not on the supported list, verify the availability of these services in that region. If they are available and your region is not listed, contact the WEKA [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team) for validation.

### Americas

 | Region Name | Region Description |
 | ----------------------- | ----------------------------- |
 | northamerica-northeast1 | Montr√©al, Canada |
 | southamerica-east1 | S√£o Paulo, Brazil |
 | southamerica-west1 | Santiago, Chile |
 | us-central1 | Iowa, United States |
 | us-east1 | South Carolina, United States |
 | us-east4 | Virginia, United States |
 | us-east5 | Columbus, United States |
 | us-west1 | Oregon, United States |
 | us-west2 | Los Angeles, United States |
 | us-west3 | Salt Lake City, United States |
 | us-west4 | Las Vegas, United States |

### Asia Pacific

 | Region name | Region Description |
 | -------------------- | ------------------------ |
 | asia-east1 | Changhua County, Taiwan |
 | asia-east2 | Hong Kong |
 | asia-northeast1 | Tokyo, Japan |
 | asia-northeast2 | Osaka, Japan |
 | asia-northeast3 | Seoul, South Korea |
 | asia-south1 | Mumbai, India |
 | asia-south2 | Delhi, India |
 | asia-southeast1 | Jurong West, Singapore |
 | asia-southeast2 | Jakarta, Indonesia |
 | australia-southeast1 | Sydney, Australia |

### Europe

 | Region Name | Region Description |
 | --------------- | ---------------------- |
 | europe-north1 | Hamina, Finland |
 | europe-west1 | St. Ghislain, Belgium |
 | europe-west2 | London, England |
 | europe-west3 | Frankfurt, Germany |
 | europe-west4 | Eemshaven, Netherlands |
 | europe-west6 | Zurich, Switzerland |
 | europe-central2 | Warsaw, Poland |

**Related information**

Regions and zones

Cloud locations

<!-- ============================================ -->
<!-- File 47/259: planning-and-installation_weka-installation-on-gcp_supported-machine-types-and-storage.md -->
<!-- ============================================ -->

# Supported machine types and storage

## Supported machine types for backends

The following table provides the supported machine types (VM instance) for backends (and clients) applied by the Terraform package:

 | Machine series | Machine types |
 | --- | --- |
 | C2 | c2-standard-8, c2-standard-16 |
 | N2 | n2-standard-8, n2-standard-16 |

Note: * Each machine type supports 1, 2, 4, or 8 local SSD drives. Each drive has 375 GB (maximum 3 TB per instance). These drives are not individual SSDs but partitions locally to the physical server.
* The data in a WEKA cluster is protected with N+2 or N+4. However, use snap-to-object if the data needs further protection from multiple server failures.
* The C2 series may not be available in your chosen GCP region.

## Supported machine types for clients

Explore the two key technologies in network virtualization: **VirtIO in DPDK mode** and **gVNIC in UDP and DPDK modes**.

### Supported machine types for clients over VirtIO in DPDK mode

VirtIO in DPDK mode enables high-performance network interfaces within virtual machines, making it an ideal choice for applications requiring low-latency, high-throughput networking in virtualized environments.

 | Machine series | Machine type |
 | --- | --- |
 | A2 | a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g, a2-ultragpu-1g |
 | C2 | c2-standard-8, c2-standard-16 |
 | C2D | c2d-standard-4, c2d-standard-8, c2d-standard-16, c2d-standard-32, c2d-standard-56, c2d-standard-112, c2d-highmem-56 |
 | E2 | e2-standard-4, e2-standard-8, e2-standard-16, e2-highmem-4, e2-highcpu-8 |
 | N2 | n2-standard-4, n2-standard-8, n2-standard-16, n2-standard-32, n2-standard-48, n2-standard-96, n2-standard-128, n2-highmem-32 |
 | N2D | n2d-standard-32, n2d-standard-64, n2d-highmem-32, n2d-highmem-64 |

### Supported machine types for clients over gVNIC in DPDK and UDP modes

* **gVNIC in DPDK Mode**: Delivers high-performance network interfaces in virtual machines, similar to VirtIO, with optimized performance for demanding network workloads.
* **gVNIC in UDP Mode**: Provides reliable, high-speed network connectivity, leveraging the UDP protocol to balance performance and dependability.

 | Machine series | Machine type |
 | --- | --- |
 | A2 | a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g, a2-ultragpu-1g |
 | A3 | a3-highgpu-8g |
 | C2 | c2-standard-8, c2-standard-16, c2-standard-30, c2-standard-60 |
 | C2D | c2d-standard-4, c2d-standard-8, c2d-standard-16, c2d-standard-32, c2d-standard-56, c2d-standard-112, c2d-highmem-56 |
 | C3 | c3-standard-4, c3-standard-8, c3-standard-22, c3-standard-44, c3-standard-88, c3-standard-176, c3-highcpu-4, c3-highcpu-8, c3-highcpu-22, c3-highcpu-44, c3-highcpu-88, c3-highcpu-176, c3-highmem-4, c3-highmem-8, c3-highmem-22, c3-highmem-44, c3-highmem-88, c3-highmem-176, c3-standard-4-lssd, c3-standard-8-lssd, c3-standard-22-lssd, c3-standard-44-lssd, c3-standard-88-lssd, c3-standard-176-lssd |
 | C3D | c3d-standard-4, c3d-standard-8, c3d-standard-16, c3d-standard-30, c3d-standard-60, c3d-standard-90, c3d-standard-180, c3d-standard-360, c3d-highcpu-4, c3d-highcpu-8, c3d-highcpu-16, c3d-highcpu-30, c3d-highcpu-60, c3d-highcpu-90, c3d-highcpu-180, c3d-highcpu-360, c3d-highmem-4, c3d-highmem-8, c3d-highmem-16, c3d-highmem-30, c3d-highmem-60, c3d-highmem-90, c3d-highmem-180, c3d-highmem-360, c3d-standard-8-lssd, c3d-standard-16-lssd, c3d-standard-30-lssd, c3d-standard-60-lssd, c3d-standard-90-lssd, c3d-standard-180-lssd, c3d-standard-360-lssd, c3d-highmem-8-lssd, c3d-highmem-16-lssd, c3d-highmem-30-lssd, c3d-highmem-60-lssd, c3d-highmem-90-lssd, c3d-highmem-180-lssd, c3d-highmem-360-lssd |
 | C4 | c4-standard-4, c4-standard-8, c4-standard-16, c4-standard-32, c4-standard-48, c4-standard-96, c4-standard-192, c4-highcpu-4, c4-highcpu-8, c4-highcpu-16, c4-highcpu-32, c4-highcpu-48, c4-highcpu-96, c4-highcpu-192, c4-highmem-4, c4-highmem-8, c4-highmem-16, c4-highmem-32, c4-highmem-48, c4-highmem-96, c4-highmem-192 |
 | G2 | g2-standard-4, g2-standard-8, g2-standard-12, g2-standard-16, g2-standard-24, g2-standard-32, g2-standard-48, g2-standard-96 |
 | M3 | m3-ultramem-32, m3-ultramem-64, m3-ultramem-128, m3-megamem-64, m3-megamem-128 |
 | N2 | n2-standard-4, n2-standard-8, n2-standard-16, n2-standard-32, n2-standard-48, n2-standard-64, n2-standard-80, n2-standard-96, n2-standard-128, n2-highmem-4, n2-highmem-8, n2-highmem-16, n2-highmem-32, n2-highmem-48, n2-highmem-64, n2-highmem-80, n2-highmem-96, n2-highmem-128, n2-highcpu-8, n2-highcpu-16, n2-highcpu-32, n2-highcpu-48, n2-highcpu-64, n2-highcpu-80, n2-highcpu-96 |
 | N2D | n2d-standard-4, n2d-standard-8, n2d-standard-16, n2d-standard-32, n2d-standard-48, n2d-standard-64, n2d-standard-80, n2d-standard-96, n2d-standard-224, n2d-highmem-4, n2d-highmem-8, n2d-highmem-16, n2d-highmem-32, n2d-highmem-48, n2d-highmem-64, n2d-highmem-80, n2d-highmem-96, n2d-highcpu-8, n2d-highcpu-16, n2d-highcpu-32, n2d-highcpu-48, n2d-highcpu-64, n2d-highcpu-80, n2d-highcpu-96, n2d-highcpu-128, n2d-highcpu-224 |
 | N4 | n4-standard-4, n4-standard-8, n4-standard-16, n4-standard-32, n4-standard-48, n4-standard-64, n4-standard-80, n4-highcpu-4, n4-highcpu-8, n4-highcpu-16, n4-highcpu-32, n4-highcpu-48, n4-highcpu-64, n4-highcpu-80, n4-highmem-4, n4-highmem-8, n4-highmem-16, n4-highmem-32, n4-highmem-48, n4-highmem-64, n4-highmem-80 |

**Related information**

Machine families resource and comparison guide

<!-- ============================================ -->
<!-- File 48/259: planning-and-installation_weka-installation-on-gcp_weka-project-description.md -->
<!-- ============================================ -->

# WEKA project description

The WEKA project uses internal GCP resources. A basic WEKA project includes a cluster with multiple virtual private clouds (VPCs), virtual machines (VMs), a load balancer, DNS, cloud storage, a secret manager, and other components for managing cluster resizing. Peering between all virtual networks enables functions to run across them, with each VPC connected to every other VPC in a full mesh.

### Resize cloud function operation

A resize cloud function in vpc-0 and a workload listener are deployed for auto-scale instances in GCP. Once a user sends a [request for resizing](auto-scale-instances-in-gcp) the number of instances in the cluster, the workload listener checks the _cluster state_ file in the cloud storage and triggers the resize cloud function if a resize is required. The cluster state file is an essential part of the resizing decision. It indicates states such as:

* Readiness of the cluster.
* The number of existing instances.
* The number of requested instances.

The secret manager retains the user name (usually _admin_) and the Terraform-generated password. The resize cloud function uses the user name and password to operate on the cluster instances.

## GCP internet connectivity

During WEKA deployment, the instances require access to a YUM repository and the WEKA software. This can be achieved through one of the following methods:

* **External IPs:** Allow an external IP address in VPC0 for each instance. This enables direct connection to public internet resources such as a YUM repository and get.weka.io.
* **NAT gateway:** Add a NAT Gateway to VPC0 to allow the instances to connect to the public internet resources through the gateway.
* **Private YUM repository:** If connecting to an external YUM repository and get.weka.io is impossible, specify an internal YUM repository and a private download link for the WEKA software.

<!-- ============================================ -->
<!-- File 49/259: planning-and-installation_weka-installation-on-gcp_gcp-terraform-package-description.md -->
<!-- ============================================ -->

# GCP-WEKA deployment Terraform package description

WEKA provides a ready-to-deploy GCP-WEKA deployment Terraform package you can customize to install the WEKA cluster on GCP.

The Terraform package contains the following modules:

* **setup_network**: includes vpcs, subnets, peering, firewall, and health check.
* **service_account**: includes the service account used for deployment with all necessary permissions.
* **deploy_weka**: includes the actual WEKA deployment, instance template, cloud functions, workflows, job schedulers, secret manager, buckets, and health check.
* **shared_vpcs** (_optional_): includes VPC sharing the WEKA deployment network with another hosting project. For example, when deploying a private network.

Refer to the terraform-gcp-weka module for more details.

## GCP-WEKA deployment Terraform package supported types

The Terraform package supports the following deployment types:

* **Public cloud deployments:** Require passing the `get.weka.io` token to Terraform to download the WEKA release from the public get.weka.io service. The following examples are provided:
  * Public VPC
  * Public VPC with creating a worker pool
  * Public VPC with an existing public network
  * Public VPC with multiple clusters
  * Public VPC with a shared VPC
  * Public VPC with an existing worker pool and VPC
* **Private cloud deployments:** Require placing the WEKA release tar file in a URL-accessible location for internal access (instances can download the WEKA release from this location). The following examples are provided:
  * Private VPC with creating a worker pool
  * Private VPC with an existing network
  * Private VPC with an existing worker pool and VPC
  * Private VPC with multiple clusters
  * Private VPC with a shared VPC

## Terraform example

The following is a basic example where you provide minimal details about your cluster, and the Terraform module completes the remaining required resources, such as cluster size, machine type, and networking parameters.

This example can be used as a reference when creating the main.tf file for a c2-standard-8 with 4 VPCs.

```hcl
provider "google" {
  region  = "europe-west1"
  project = "PROJECT_ID"
}

module "weka_deployment" {
  source                         = "weka/weka/gcp"
  version                        = "4.0.9"
  cluster_name                   = "my_cluster_name"
  project_id                     = "PROJECT_ID"
  prefix                         = "my_prefix"
  region                         = "europe-west1"
  zone                           = "europe-west1-b"
  cluster_size                   = 7
  nvmes_number                   = 2
  get_weka_io_token              = "getwekatoken"
  machine_type                   = "c2-standard-8"
  subnets_range                  = ["10.222.0.0/24", "10.223.0.0/24", "10.224.0.0/24", "10.225.0.0/24"]
  allow_ssh_cidrs                = ["0.0.0.0/0"]
  allow_weka_api_cidrs           = ["0.0.0.0/0"]

}
output "weka_cluster" {
  value = module.weka_deployment
}
```

Note: For the descriptions of the parameters, refer to the GCP-WEKA deployment Terraform package.

This example can be used as a reference when creating the `main.tf` file for a c2-standard-16 with 7 VPCs.

```
provider "google" {
  region  = "europe-west1"
  project = "PROJECT_ID"
}

module "weka_deployment" {
  source                         = "weka/weka/gcp"
  version                        = "4.0.9"
  cluster_name                   = "my_cluster_name"
  project_id                     = "PROJECT_ID"
  prefix                         = "my_prefix"
  region                         = "europe-west1"
  zone                           = "europe-west1-b"
  cluster_size                   = 7
  nvmes_number                   = 2
  get_weka_io_token              = "getwekatoken"
  machine_type                   = "c2-standard-16"
  subnets_range                  = ["10.222.0.0/24", "10.223.0.0/24", "10.224.0.0/24", "10.225.0.0/24", "10.226.0.0/24", "10.227.0.0/24", "10.228.0.0/24"]
  allow_ssh_cidrs                = ["0.0.0.0/0"]
  allow_weka_api_cidrs           = ["0.0.0.0/0"]

}
output "weka_cluster" {
  value = module.weka_deployment
}
```

## Private network considerations

To deploy a private network, the parameter `private_network = true` on the `setup_network` and `deploy_weka` modules level.

Depending on the required network topology, the following parameters are optional for private networking:

* To download the WEKA release from a local bucket, set the local bucket location in the  `install_weka_url` parameter on the `deploy_weka` module level.
* For Centos7 only, a distributive repository is required to download kernel headers and additional build software. To auto-configure yum to use a distributive repository, run `yum_repo_server`.
* If a custom image is required, use `source_image_id`.

## Object store integration

The Terraform package can automate the addition of a Google Cloud Storage bucket for use as object storage.

**Procedure**

1. In the `main.tf` file, add the following fields:
   * `tiering_enable_obs_integration:` Set the value to `true`.
   * `tiering_obs_name:` Match the value to an existing bucket in Google Cloud Storage.
   * `tiering_enable_ssd_percent:` Set the percentage to your desired value.

Example:

```hcl
tiering_enable_obs_integration=true
tiering_obs_name="myBucketName"
tiering_enable_ssd_percent=20
```

Note: If you do not specify the name of an existing bucket using `tiering_obs_name` but specify `tiering_enable_obs_integration=true` then a new Cloud Storage bucket is created automatically.\
The name format of the new bucket is: \
`<project_id>-<prefix>-<cluster_name>-obs`

<!-- ============================================ -->
<!-- File 50/259: planning-and-installation_weka-installation-on-gcp_deployment-on-gcp-using-terraform.md -->
<!-- ============================================ -->

# Deployment on GCP using Terraform

The Terraform package includes a `main.tf` file you create according to your deployment needs.

Applying the created `main.tf` file performs the following:

* Creates VPC networks and subnets on the GCP project.
* Deploys GCP instances.
* Installs the WEKA software.
* Configures the WEKA cluste&#x72;**.**
* Additional GCP objects.

## Prerequisites

Before installing the WEKA software on GCP, the following prerequisites must be met:

* Install the gcloud CLI: It is pre-installed if you use the Cloud Shell.
* Log in to gcloud: `gcloud auth application-default login`
* Install Terraform: It is pre-installed if you use the Cloud Shell. Ensure the Terraform version meets the minimum required version specified in the **Requirements** section of the GCP-WEKA deployment Terraform package.
* Initialize the Terraform module using `terraform init` from the local directory. This command initializes a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, and more.
*   The **Compute Engine** and **Workflows API** services must be enabled to allow the following services:

    ```
    artifactregistry.googleapis.com
    cloudbuild.googleapis.com
    cloudfunctions.googleapis.com
    cloudresourcemanager.googleapis.com
    cloudscheduler.googleapis.com
    compute.googleapis.com
    dns.googleapis.com
    eventarc.googleapis.com
    iam.googleapis.com
    secretmanager.googleapis.com
    servicenetworking.googleapis.com
    serviceusage.googleapis.com
    vpcaccess.googleapis.com
    workflows.googleapis.com
    ```
*   The user running the Terraform module requires the following roles to run the `terraform apply`:

    ```
    roles/cloudfunctions.admin
    roles/cloudscheduler.admin
    roles/compute.admin
    roles/compute.networkAdmin
    roles/compute.serviceAgent
    roles/dns.admin
    roles/iam.serviceAccountAdmin
    roles/iam.serviceAccountUser
    roles/pubsub.editor
    roles/resourcemanager.projectIamAdmin
    roles/secretmanager.admin
    roles/servicenetworking.networksAdmin
    roles/storage.admin
    roles/vpcaccess.admin
    roles/workflows.admin
    ```

## **Create a main.tf file**

The main Terraform configuration settings are included in the `main.tf` file. You can create it by following this procedure or using the WEKA Cloud Deployment Manager. See .

**Procedure**

1. Review the [Terraform-GCP-WEKA example](../gcp-terraform-package-description#terraform-gcp-weka-example) and use it as a reference for creating the `main.tf` according to your deployment specifics on GCP.
2. Tailor the `main.tf` file to create SMB-W or NFS protocol clusters by adding the relevant code snippet. Adjust parameters like the number of gateways, instance types, domain name, and share naming:

* **SMB-W**

<pre><code><strong>smb_protocol_gateways_number = 3
</strong>smb_protocol_gateway_instance_type = "c2-standard-8"
smbw_enabled = true
smb_domain_name = "CUSTOMER_DOMAIN"
smb_share_name = "SPECIFY_SMB_SHARE_NAMING"
smb_setup_protocol = true

```

* **NFS**

```
nfs_protocol_gateways_number = 2
nfs_protocol_gateway_instance_type = "c2-standard-8"
nfs_setup_protocol = true
```

4. Add WEKA POSIX clients (optional)**:** If needed, add [WEKA POSIX clients](../../weka-system-overview/weka-client-and-mount-modes) to support your workload by incorporating the specified variables into the `main.tf` file:

```makefile
clients_number = 2
client_instance_type = "c2-standard-8"
```

## Apply the main.tf file

Once you complete the `main.tf` settings, apply it: Run `terraform apply`.

### **Additional configuration post-Terraform** apply

After applying  the `main.tf`, the Terraform module updates the configuration as follows:

1. **Service account creation:**\
   Format of the service account name: `<prefix>-deployment@<project name>.iam.gserviceaccount.com`\
   Assigned roles:

```
roles/cloudfunctions.developer
roles/compute.serviceAgent
roles/compute.loadBalancerServiceUser
roles/pubsub.subscriber
roles/secretmanager.secretAccessor
roles/vpcaccess.serviceAgent
roles/workflows.invoker
```

2. **Additional roles can be assigned to the created service account (if working with relevant resources):**

*   To create a worker pool:

    ```
    roles/compute.networkAdmin
    roles/servicenetworking.networksAdmin
    roles/cloudbuild.workerPoolOwner
    ```
*   To create a new bucket (for Terraform state and WEKA OBS):

    ```jsx
    roles/storage.admin
    ```
*   To use an existing bucket (for Terraform state and WEKA OBS):

    ```jsx
    roles/storage.objectAdmin
    ```

## Set the license

To run IOs against the cluster, a valid license must be applied. Obtain a valid license and apply it to the WEKA cluster. For details, see .

## **Upgrade the WEKA version**

Upgrading the WEKA version on the cloud is similar to the standard WEKA upgrade process. However, in a cloud configured with auto-scaling, the new instances created by the scale-up must be configured with the new WEKA version.

**Before you begin**

Ensure the cluster does not undergo a scale-up or scale-down process before and during the WEKA version upgrade.

**Procedure**

1. Perform the upgrade process. See .
2. Update the `weka_version` parameter in the `main.tf` file.
3. Run `terraform apply`.

## Removal or rollback of the WEKA cluster

If a rollback is required or the WEKA cluster is no longer required on GCP, first terminate the WEKA cluster and then use the `terraform destroy` action.

The termination of the WEKA cluster can also be used if you need to retain the GCP resources (such as VPCs and cloud functions to save time on the next deployment) and then deploy a new WEKA cluster when you are ready.

Note: If you need to preserve your data, first create a snapshot using [snap-to-object](../../weka-filesystems-and-object-stores/snap-to-obj).

To terminate the WEKA cluster, run the following command (replace the `trigger_url` with the actual trigger URL and `Cluster_Name` with the actual cluster name):

```

```bash
curl -m 70 -X POST ${google_cloudfunctions_function.terminate_cluster_function.https_trigger_url} \
-H "Authorization:bearer $(gcloud auth print-identity-token)" \
-H "Content-Type:application/json" \
-d '{"name":"Cluster_Name"}'
```

```

If you do not know the trigger URL or cluster name, run the `terraform output`command to display them.

Once the WEKA cluster is terminated, you can deploy a new WEKA cluster or run the `terraform destroy` action.

<!-- ============================================ -->
<!-- File 51/259: planning-and-installation_weka-installation-on-gcp_detailed-deployment-tutorial-weka-on-gcp-using-terraform.md -->
<!-- ============================================ -->

---
description:
---

# Detailed deployment tutorial: WEKA on GCP using Terraform

## Introduction

Deploying WEKA on GCP requires proficiency in several technologies, including GCP, Terraform, basic Linux operations, and the WEKA software itself. Recognizing that not all individuals responsible for this deployment are experts in each of these areas, this document aims to provide comprehensive, end-to-end instructions. This ensures that readers with minimal prior knowledge can successfully deploy a functional WEKA cluster on GCP.

### **Document scope**

This document specifically addresses the deployment of WEKA in a GCP environment using Terraform, applicable for both proof-of-concept (POC) and production settings. While no pre-existing GCP elements are necessary beyond an appropriate user account, the guide demonstrates the use of some pre-existing components, as many environments already have these in place.

The reader is guided through:

* General GCP requirements.
* Networking requirements to support WEKA.
* Deployment of WEKA using Terraform.
* Verification of a successful WEKA deployment.

Note: Images embedded in this document can be enlarged with a single click for ease of viewing and a clearer and more detailed examination.

## Terraform preparation and installation

HashiCorp Terraform is a powerful tool that allows you to define, provision, and manage infrastructure as code. You can specify your infrastructure setup in a configuration file using a declarative configuration language, HashiCorp Configuration Language (HCL), or JSON. Terraform then uses this file to automatically create, modify, or delete resources, ensuring consistent and predictable deployment of your infrastructure components such as servers, databases, and networks.

This document outlines the process for automating the deployment of the WEKA Data Platform on Google Cloud Platform (GCP) using Terraform. Terraform's widespread adoption and prominence in the Infrastructure as Code (IaC) domain drive its choice. Organizations of all sizes globally leverage Terraform to deploy persistent infrastructure both on-premises and across public clouds like AWS, Azure, and Google Cloud Platform.

To install Terraform, we recommend following the official installation guides provided by HashiCorp. Additionally, Terraform can be run directly from the GCP Cloud Terminal, which comes with Terraform pre-installed, as illustrated in this guide.

### GCP account

It is essential for the customer to understand their subscription structure for deployments within a WEKA customer environment. If you are deploying internally at WEKA and cannot locate an Account ID or have not been added to the appropriate account, contact the relevant cloud team for assistance.

### User account privileges

Ensure that the GCP IAM user has the permissions outlined in Appendix A to perform the necessary operations for a successful WEKA deployment on GCP using Terraform. The IAM user must be able to create, modify, and delete GCP resources as specified by the Terraform configuration files used in the WEKA deployment.

If the current IAM user lacks the permissions detailed in Appendix A, either update the user's permissions or create a new IAM user with the required privileges.

**Verify GCP IAM user permissions**

1. Navigate to the GCP Management Console.
2. Log in using the account intended for the WEKA deployment.
3. In the GCP Console, go to the Services menu and select **IAM** to access the Identity and Access Management dashboard.

4. Within the IAM dashboard, locate the relevant IAM user by searching for their account.

5. Click on the user's **Security insights** to review their permissions.
6. Ensure that the user possesses the permissions listed in Appendix A, which are necessary for managing GCP resources through Terraform.

Note: While this user has full administrative access to enable Terraform to deploy WEKA, it is recommended to follow the principle of applying the least-privilege permissions. Grant only the specific permissions outlined in Appendix A to ensure security best practices.

### GCP quotas

For a successful WEKA deployment on GCP using Terraform, ensure your GCP project has the necessary quotas for the required resources. When setting up compute instances, such as the c2 type for the WEKA backend cluster, manage quotas based on the CPU count for each compute instance type or family.

Before deploying WEKA, confirm that your compute instance's CPU sizing requirements (determined in partnership with WEKA) can be met within the existing quota limits. If not, increase the quotas in your GCP project before executing the Terraform commands detailed later in this document.

The required minimum quota is the total CPU count for all instances (for example, deploying 10 c2.standard-8 instances requires 80 CPUs just for the cluster). Ensuring sufficient quotas prevents failures during the execution of Terraform commands, as discussed in subsequent sections.

#### Set service quotas

1. Navigate to the GCP Console and search for the service **Quotas & System Limits**.

2. On the Quotas page, search for **CPU** and select the compute instance type family, in this case, **c2**.

3. Locate the region where you intend to deploy WEKA and confirm that there are sufficient available CPUs of the specified family type. If not, adjust the quota accordingly.

## GCP Resource Prerequisites

The WEKA deployment incorporates several GCP components, including VPCs, Subnets, Security Groups, Endpoints, and others. These elements can be either generated by the WEKA Terraform modules or exist beforehand if manually creating components to use.

Four VPCs (Virtual Private Clouds), each with at least one Subnet and a Security Group, are required at minimum. This guide assumes that Terraform generates these items for WEKA within the environment.

### Networking requirements

The Terraform deployment can automatically establish VPC peering connections from the new VPCs to your current VPC, which is utilized by the application servers consuming WEKA storage. Considering how GCP handles compute instances with multiple vNICs, it is advisable to allow Terraform to create the required networking for WEKA.

This guide assumes an already deployed VPC and the necessity of adding four WEKA-specific VPCs. This requirement arises from GCP networking constraints, where each VM instance can only have one vNIC per VPC. However, WEKA mandates a minimum of four vNICs per instance. Ensure that you have the CIDR information for the four subnets created in the new VPCs to prevent conflicts.

## Deploy WEKA in GCP using Terraform

The WEKA Terraform modules establish peering connections between the newly created VPCs for WEKA and an existing VPC within the environment. Therefore, the initial networking step involves identifying the VPC to peer with.

**VPC**

**Subnet (in VPC)**

### Locate the WEKA user token

The WEKA user token provides access to the WEKA binaries and is used to access get.weka.io during installation.

1. In a web browser, navigate to get.weka.io.
2. Select the user's name located in the upper right-hand corner of the page.

2. From the column on the left-hand side of the page, select **API Tokens**. The user‚Äôs API token is displayed. Note it for using it later in the installation process.

### Deploy WEKA in GCP with Terraform: private VPCs example

The following demonstrates deploying WEKA into Virtual Private Clouds (VPCs) and Subnets without exposing the instances to Internet access.

The Terraform module package is designed for deploying various GCP resources essential for WEKA deployment on GCP, including Compute instances, cloud functions, and VPCs.

#### Description of sub-modules in the module package

* **IAM roles, networks, and security groups:** These modules create the necessary IAM roles, networks, and security groups for WEKA deployment. If specific IDs for security groups or subnets are not provided, the modules generate them automatically.
* **Service account:** This module automatically creates a service account that the WEKA deployment functions and services use.
* **Network:** To deploy a private network with Network Address Translation (NAT), certain variables need to be set, such as create_nat_gateway to true and providing a private CIDR range. To prevent instances from obtaining public IPs, set assign_public_ip to false.
* **Shared_VPCs and VPC_Peering:** These modules handle the peering of VPCs to each other and existing VPCs if provided.
* **Clients (optional):** This module automatically mounts clients to the WEKA cluster. Users can specify the number of clients to create along with optional variables such as instance type, number of network interfaces (NICs), and AMI ID.
* **Protocol_Gateways NFS/SMB (optional):** Similar to the client's module, this module allows users to specify the number of protocol gateways per protocol. Additional configuration details, such as instance type and disk size, can also be provided.
* **Worker_Pool:** This module creates a private pool to build GCP cloud functions.

#### Prepare the main.tf file

1. Sign in to Google Cloud Platform and access the **Cloud Shell**.

2. If the Terminal is not associated with the project intended for WEKA deployment, close it, switch to the correct project, and reopen it.

#### Organize the structure of the Terraform configuration files

1. Create a directory specifically for the Terraform configuration files. \
   To maintain state information, it's essential to keep each Terraform deployment separate.\
   Other deployments can be executed by duplicating these instructions and naming the directory appropriately, such as **deploy1** or another unique identifier.

```bash
mkdir deploy
```

2. Navigate to the created directory.

```bash
cd deploy
```

3. To display accessible output data on the screen during the process, create an **output.tf** file in the deploy directory, and insert the following content:

```json
output "weka_deployment_output" {
  value = module.weka_deployment
}
```

4. Save the **output.tf** file.
5. Define the Terraform options by creating the main.tf file using your preferred text editor. Use the following template and replace the placeholders `< >` with the values specific to your deployment environment:

```json
provider "google" {
  project     = "<your_project>"
  region      = "<your_region>"
  zone        = "<your_zone>"
}
module "weka_deployment" {
  source                   = "weka/weka/gcp"
  version                  = "4.0.6"
  cluster_name             = "<name_for_the_cluster>"
  project_id               = "<your_project_id>"
  region                   = "<your_region>"
  zone                     = "<your_zone>"
  cluster_size             = 7
  nvmes_number             = 2
  get_weka_io_token        = "<your_wekaio_token>"
  private_dns_name         = "weka.private.net."
  tiering_enable_obs_integration = true
  assign_public_ip               = false
  create_nat_gateway       = true
  vpcs_to_peer_to_deployment_vpc = ["<your_existing_vpc"]
  subnets_range            = ["<CIDR_for_subnet_for_wekavpc1>", "<CIDR_for_subnet_for_wekavpc2>", "<CIDR_for_subnet_for_wekavpc3>", "<CIDR_for_subnet_for_wekavpc4>"]
```

<details>

<summary>Main.tf file with example values</summary>

This Main.tf file includes example values. Do not copy and paste it directly for your deployment.

```jsx
provider "google" {
  project     = "wekaio-public"
  region      = "us-east1"
  zone        = "us-east1-b"
}
module "weka_deployment" {
  source                   = "weka/weka/gcp"
  version                  = "4.0.6"
  cluster_name             = "gcp-weka"
  project_id               = "wekaio-public"
  region                   = "us-east1"
  zone                     = "us-east1-b"
  cluster_size             = 7
  nvmes_number             = 2
  weka_version             = "4.3.0"
  get_weka_io_token        = "LB9ciQ7aDpHihJXc"
  private_dns_name         = "weka.private.net."
  tiering_enable_obs_integration = true
  assign_public_ip               = false
  create_nat_gateway       = true
  vpcs_to_peer_to_deployment_vpc = ["default-POC"]
  subnets_range            = ["10.100.0.0/24", "10.101.0.0/24", "10.102.0.0/24", "10.103.0.0/24"]
```

</details>

Note: Authentication is managed through the Google Cloud Terminal.

6. After creating and saving the main.tf file, execute the following command in the same directory. This ensures that the required Terraform resource files from GCP are downloaded and accessible to the system.

```json
terraform init
```

7. Before applying or destroying a Terraform configuration file, it's recommended to run the following:

```json
terraform plan
```

If GCP requires authentication, grant permission accordingly.

8. To initiate the deployment of WEKA in GCP, run the following command:

```json
terraform apply
```

This command initiates the creation of GCP resources essential for WEKA. When prompted, confirm the deployment of resources by typing **yes**.

Upon completing the Terraform GCP resource deployment process, a summary of the outcome is displayed. In the event of an unsuccessful deployment, an error message indicating failure is shown instead.

```json
Outputs:

weka_deployment_output = {
  "client_ips" = []
  "cluster_helper_commands" = <<-EOT
  ########################################## get cluster status ##########################################
  curl -m 70 -X POST "https://weka-gcp-weka-status-t6p6clcama-ue.a.run.app" \
  -H "Authorization:bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type:application/json" -d '{"type":"progress"}'
  # for fetching cluster status pass: -d '{"type":"status"}'

  ########################################## resize cluster command ##########################################
  curl -m 70 -X POST "https://weka-gcp-weka-weka-functions-t6p6clcama-ue.a.run.app?action=resize" \
  -H "Authorization:bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type:application/json" \
  -d '{"value":ENTER_NEW_VALUE_HERE}'

  ########################################## pre-terraform destroy, cluster terminate function ################

  # replace CLUSTER_NAME with the actual cluster name, as a confirmation of the destructive action
  # this function needs to be executed before terraform destroy
  curl -m 70 -X POST "https://weka-gcp-weka-weka-functions-t6p6clcama-ue.a.run.app?action=terminate_cluster" \
  -H "Authorization:bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type:application/json" \
  -d '{"name":"CLUSTER_NAME"}'

  ################################# get weka password secret login ############################################

| gcloud secrets versions access 1 --secret=weka-gcp-weka-password --project wekaio-public --format='get(payload.data)' | base64 -d |

  ############################################## get backend ips ##############################################

  gcloud compute instances list --filter="labels.weka_cluster_name=gcp-weka" --format "get(networkInterfaces[0].networkIP)" --project wekaio-public

  EOT
  "cluster_name" = "gcp-weka"
  "functions_url" = {
    "destroy" = {
      "body" = {
        "name" = "gcp-weka"
      }
      "url" = "https://weka-gcp-weka-weka-functions-t6p6clcama-ue.a.run.app?action=terminate_cluster"
    }
    "progressing_status" = {
      "body" = {
        "type" = "progress"
      }
      "url" = "https://weka-gcp-weka-status-t6p6clcama-ue.a.run.app"
    }
    "resize" = {
      "body" = {
        "value" = 7
      }
      "url" = "https://weka-gcp-weka-weka-functions-t6p6clcama-ue.a.run.app?action=resize"
    }
    "status" = {
      "body" = {
        "type" = "status"
      }
      "url" = "https://weka-gcp-weka-status-t6p6clcama-ue.a.run.app"
    }
  }
  "get_cluster_status_uri" = "https://weka-gcp-weka-status-t6p6clcama-ue.a.run.app"
  "lb_url" = "gcp-weka.weka.private.net"
  "nfs_protocol_gateways_ips" = tostring(null)
  "private_ssh_key" = "/tmp/weka-gcp-weka-private-key.pem"
  "project_id" = "wekaio-public"
  "resize_cluster_uri" = "https://weka-gcp-weka-weka-functions-t6p6clcama-ue.a.run.app?action=resize"
  "smb_protocol_gateways_ips" = tostring(null)
  "terminate_cluster_uri" = "https://weka-gcp-weka-weka-functions-t6p6clcama-ue.a.run.app?action=terminate_cluster"
  "vm_username" = "weka"
  "weka_cluster_password_secret_id" = "weka-gcp-weka-password"
}
```

Note: Refer to the `Get WEKA Password Secret Login` section for future reference. This section contains the necessary information to retrieve the WEKA password.

#### Additional commands in the output

The output includes several other commands to allow you to view information or modify the deployment, in addition to a command to look up the WEKA admin password.

* Get cluster status
* Resize cluster command
* Pre-terraform destroy, cluster terminate function
* Get backend ips

Run the `get cluster status` command to verify the state of the WEKA deployment.

```jsx
  curl -m 70 -X POST "https://weka-gcp-weka-status-t6p6clcama-ue.a.run.app" \
  -H "Authorization:bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type:application/json" -d '{"type":"status"}'
```

Here is the output from the example

```

```jsx

{"initial_size":7,"desired_size":8,"clusterized":true,"weka_status":{"hot_spare":1,**"io_status":"STARTED"**,"drives":{"active":16,"total":16},"name":"gcp-weka","io_status_changed_time":"2024-05-13T13:43:50.740546Z","io_nodes":{"active":24,"total":24},"cloud":{"enabled":true,"healthy":true,"proxy":"","url":"https://api.home.weka.io"},"release_hash":"f94d361cf0e465a4f04214064b5f019b9149baf3","hosts":{"active_count":24,"backends":{"active":24,"total":24},"clients":{"active":0,"total":0},"total_count":24},"stripe_data_drives":4,"release":"4.3.0","active_alerts_count":2,"capacity":{"total_bytes":3376488600000,"hot_spare_bytes":482217460000,"unprovisioned_bytes":482217460000},"is_cluster":true,"status":"OK","stripe_protection_drives":2,"guid":"603d3b67-03ce-49af-8833-4f2e6a985b1e","nodes":{"black_listed":0,"total":48},"licensing":{"io_start_eligibility":true,"usage":{"drive_capacity_gb":6442,"usable_capacity_gb":3376,"obs_capacity_gb":0},"mode":"Unlicensed"}}}
```

```

The section **`"io_status":"STARTED"`** shows that the cluster is fully up and running and ready for access.

### Deploy protocol servers (NFS and SMB)

The Terraform deployment simplifies the process of deploying additional compute instances to serve as protocol servers (protocol gateways) for NFS or SMB. These protocol gateways are separate from the instances designated for the WEKA backend cluster.

To deploy protocol gateways, add more information to the `main.tf` file.

The simplest approach is to specify the number of protocol gateways for each type (NFS and SMB) and use default settings for the other parameters. If you plan to distribute your NFS or SMB connections across multiple protocol servers manually, you can adjust these numbers according to your needs. For instance, if you have three projects, each requiring its own bandwidth for NFS, you can deploy three protocol gateways. Assign each project the IP address or DNS entry of its respective gateway to use as the NFS server.

Add the following before the last `‚Äò}‚Äô` of the main.tf file.

```bash
## Protocol gateways ##

## For deploying NFS protocol gateways ##
nfs_protocol_gateways_number = 2 # A minimum of two is required

## For deploying SMB protocol gateways ##
smb_protocol_gateways_number = 3 # A minimum of three is required
```

## Collect login access information about the WEKA cluster

### **Obtain the WEKA cluster IP addresses**

To obtain the IP addresses of your WEKA cluster, follow these steps:

1. Visit the GCP Compute Engine VM instances dashboard.

2. Identify the WEKA backend servers. The instance names follow the format: `<cluster_name>-<Timestamp>`, where `<cluster_name>` corresponds to the value specified in the `main.tf` file.

3. Select any WEKA backend instance and note the IP address of `nic0`.

This IP address is used if your subnet assigns a public IP address to the instance (that is if the VM instance is configured accordingly). WEKA uses only private IPv4 addresses for all interface IP addresses for communication.

#### Retrieve the **WEKA cluster access password**

The WEKA cluster password is securely stored in the Google Cloud Platform (GCP) Secret Manager. You can retrieve it using the `gcloud` command from the Terraform output or through the GCP console. Follow these steps to access the password through the GCP console:

1. Open the GCP console and search for **Secret Manager**.

2. Navigate to the **Secrets** section within the Secret Manager.

3. Locate and select the secret named `weka_<cluster_name>_password` corresponding to your deployment.
4. Select the Actions option and select **View secret value**.

The system displays the randomly generated password assigned to the WEKA user admin.

### Access the WEKA cluster backends

You can access the WEKA cluster backend instances through SSH directly from the GCP browser window. This method allows you to run WEKA CLI commands and gather logs as needed.

Follow these steps to connect to the backend instances:

1. Open the GCP console.
2. Navigate to the Compute Engine section.
3. Select the instance you wish to access.
4. Select the SSH button to open a browser-based SSH session.

## Access and review the WEKA GUI

To access the WEKA GUI, use a jump host with a GUI deployed within the same VPC and subnet as the WEKA cluster.

### Access the WEKA GUI

In the following examples, a Windows 10 instance with a public IP address is deployed in the same VPC, subnet, and security group as the WEKA cluster. The network security group rules are added to allow RDP explicit access to the Windows 10 system.

1. Open a browser in the Windows 10 jump box.
2. Visit https://\<cluster-backend-ip>:14000. The WEKA GUI sign-in screen appears.
3. Sign in as user **admin** and use the password retrieved earlier ( see #retrieve-the-weka-cluster-access-password).

Note: The provided examples are for reference. The values shown below may differ from those of your cluster.

### Review the WEKA GUI

1. View the cluster GUI home screen.

2. Review the cluster backends. Check the status and details of the backend instances.

3. Review the clients, if any, attached to the cluster.

4. Review the filesystems.

## Automated scale-out and scale-in of the WEKA backend cluster

The WEKA backend cluster can be scaled out and scaled in using an API call through the GCP Cloud terminal.

The WEKA backend cluster can be dynamically scaled out and scaled in using API calls through the GCP Cloud terminal. The process is managed by Terraform-created functions that automatically trigger when a new instance is initiated or retired. These functions execute the necessary automation processes to adjust the cluster's computing resources.

To scale out from the initial deployment, use the CLI command provided in the Terraform output.

### Benefits of auto-scaling

* **Replace unhealthy instances:**
  * Auto Scaling automatically initiates the replacement of instances that fail health checks. It launches new instances and incorporates them into the WEKA cluster, ensuring continuous availability and responsiveness.
  * This process mitigates the impact of instance failures by promptly integrating the new instance into the cluster and service.
* **Graceful scaling:**
  * Auto-scaling configurations can be adjusted to perform scaling actions gradually. This prevents sudden spikes in traffic and minimizes application disruptions.
  * This measured approach maintains a balanced and stable environment, effectively adapting to changes in demand without causing abrupt changes.

## **Test WEKA cluster self-healing functionality (optional)**

To validate the self-healing functionality of the WEKA cluster, you can decommission an old instance and allow the Auto Heal feature to launch a new one. Follow this brief guide:

1. **Identify the old instance:** Locate the GCP VM instance you want to decommission. This can be based on factors such as age, outdated configurations, or other criteria.
2. **Terminate the old instance:** Manually terminate the identified GCP VM instance using the GCP Management Console, gcloud CLI, or SDKs. This action triggers the Auto Heal process.
3. **Verify the new instance:** Ensure the new instance is successfully launched, passes the health checks, and joins the cluster. Confirm that the cluster's overall capacity remains unchanged.
4. **Document and monitor:** Record the decommissioning process and monitor the cluster to ensure it continues to operate smoothly with the new instance in place.

## APPENDICES

### Appendix A: Required permissions that Terraform needs

The **Compute Engine** and **Workflows API** services must be enabled to allow the following services:

* artifactregistry.googleapis.com
* cloudbuild.googleapis.com
* cloudfunctions.googleapis.com
* cloudresourcemanager.googleapis.com
* cloudscheduler.googleapis.com
* compute.googleapis.com
* dns.googleapis.com
* eventarc.googleapis.com
* iam.googleapis.com
* secretmanager.googleapis.com
* servicenetworking.googleapis.com
* serviceusage.googleapis.com
* vpcaccess.googleapis.com
* workflows.googleapis.com

The user running the Terraform module requires the following roles to run the `terraform apply`:

* roles/cloudfunctions.admin
* roles/cloudscheduler.admin
* roles/compute.admin
* roles/compute.networkAdmin
* roles/compute.serviceAgent
* roles/dns.admin
* roles/iam.serviceAccountAdmin
* roles/iam.serviceAccountUser
* roles/pubsub.editor
* roles/resourcemanager.projectIamAdmin
* roles/secretmanager.admin
* roles/servicenetworking.networksAdmin
* roles/storage.admin
* roles/vpcaccess.adminroles/workflows.admin

<!-- ============================================ -->
<!-- File 52/259: planning-and-installation_weka-installation-on-gcp_add-clients.md -->
<!-- ============================================ -->

# Add clients to a WEKA cluster on GCP

WEKA supports client instances with at least two NICs, one for management and one for the frontend data. It is possible to add more NICs for redundancy and higher performance.

A client with the same VPC networks and subnets as the cluster can connect without additional configuration. If a client is on another VPC network, peering is required between the VPC networks.

The client instance must be in the same region as the WEKA cluster on GCP.

## Mount the filesystem

1. Create a mount point (only once):

```
mkdir /mnt/weka
```

2. Install the WEKA agent (only once):

```bash
| curl <backend server http address>:14000/dist/v1/install | sh |
```

Example:

```bash
| curl http://10.20.0.2:14000/dist/v1/install | sh |
```

3. Mount a stateless client on the filesystem. In the mount command, specify all the NICs of the client.

* **DPDK mount with four NICs:**

```

```bash
mount -t wekafs -o net=eth1/IP/NETMASK/GATEWAY -o net=eth2/IP/NETMASK/GATEWAY -o net=eth3/IP/NETMASK/GATEWAY -o mgmt_ip=<management IP (eth0)> -o num_cores=4 -o dpdk_base_memory_mb=32 <backend server IP address>/<filesystem name> /mnt/weka
```

```

Example:

```

```bash
mount -t wekafs -o net=eth1/10.20.30.101/24/10.20.30.1 -o net=eth2/10.20.31.102/24/10.20.31.1 -o net=eth3/10.20.32.103/24/10.20.32.1 -o mgmt_ip=10.20.33.100 -o num_cores=4 -o dpdk_base_memory_mb=32 10.20.30.40/fs1 /mnt/weka
```

```

* **UDP mount:**

```

```bash
mount -t wekafs -o net=udp -o num_cores=0 -o mgmt_ip=<management IP (eth0)> <backend server IP address>/<filesystem name> /mnt/weka
```

```

Example:

```

```bash
mount -t wekafs -o net=udp -o num_cores=2 -o mgmt_ip=10.20.30.100 10.20.30.40/fs1 /mnt/weka
```

```

**Related topics**

<!-- ============================================ -->
<!-- File 53/259: planning-and-installation_weka-installation-on-gcp_auto-scale-instances-in-gcp.md -->
<!-- ============================================ -->

# Auto-scale instances in GCP

Once the Terraform modules are applied, two workflows are running every minute. One for scale-up and the other for scale-down.

WEKA provides a cloud function for scale-up or scale-down of the number of compute engine instances (cluster size). Terraform automatically creates the cluster according to the specified target value in a few minutes.

To change the cluster size (up or down), specify the link to the resize cloud function on GCP and the resize target value for the number of compute engine instances in the following command and run it:

```bash
curl -m 70 -X POST  https://<resize_cloud_function_name> \
-H "Authorization:bearer $(gcloud auth print-identity-token)" \
-H "Content-Type:application/json" \
-d '{"value":<Resize_target_value>}'
```

Example:

```bash
curl -m 70 -X POST  https://europe-west1-wekaio-qa.cloudfunctions.net/weka-test \
-H "Authorization:bearer $(gcloud auth print-identity-token)" \
-H "Content-Type:application/json" \
-d '{"value":7}'
```

**Related topics**

#resize-cloud-function-operation

<!-- ============================================ -->
<!-- File 54/259: planning-and-installation_weka-installation-on-gcp_google-kubernetes-engine-and-weka-over-posix-deployment.md -->
<!-- ============================================ -->

---
description:
---

# Google Kubernetes Engine and WEKA over POSIX deployment

## Introduction

Google Kubernetes Engine (GKE) is a managed Kubernetes service for deploying, managing, and scaling containerized applications. WEKA is a high-performance, scalable storage platform that integrates seamlessly with Kubernetes clusters to provide persistent storage for demanding containerized applications and workflows.

Combining GKE and WEKA results in an easily automated and managed Kubernetes environment, delivering best-in-class performance at scale.

## Requirements for WEKA over POSIX with GKE

* GKE must be deployed in Standard mode.
* GKE Worker nodes must be configured with Ubuntu OS.

Note: If GPUDirect-TCPX (supported on GKE only with Google Container Optimized OS) is required, configure WEKA over NFS. For details, see .

## Workflow

1. Deploy GKE in Standard mode with Ubuntu OS.
2. Set up WEKA client on existing GKE worker nodes.
3. Configure automated WEKA setup client on worker nodes.
4. Install and configure the WEKA CSI plugin.
5. Set up WEKA storage for GKE pods.

### 1. Deploy GKE in Standard mode with Ubuntu OS

Follow these steps to deploy GKE in Standard mode with Ubuntu OS for the worker nodes. For complete GKE documentation, visit the GCP documentation.

**Procedure:**

1. Go to the GCP menu, select **Kubernetes Engine**, and then **Clusters**.

2. Click **CREATE** to create a new cluster.

3. If prompted, click **SWITCH TO STANDARD CLUSTER.** This mode also enables SSH access to worker nodes, which is necessary for installing the WEKA POSIX clients.

4. Change from a regional to a zonal setup. Select the zone where the WEKA Cluster management IPs are located to ensure optimal communication and performance. This step ensures seamless communication between GKE and the WEKA cluster.

5. Adjust the node pool settings: Go to **Nodes** within the **default-pool** under **NODE POOLS** in the GKE console, and change the **Image type** to **Ubuntu with containerd (ubuntu_containerd)**.

6. Ensure worker nodes meet a minimum configuration of 8 vCPUs and 32 GB RAM. The WEKA client requires a minimum of 2 vCPUs and 5 GB of RAM.

7. If the GKE cluster was set up in advance, deploy the WEKA cluster to the same networking VPC and subnet. Otherwise, ensure that the GKE cluster networking is configured within the same VPC and subnet as the WEKA management IPs. Aligning networking elements per the recommendation will ensure optimal performance.

8. Click **CREATE** to create the cluster.

8. Wait for the cluster status to indicate **Ready** or **Green** before proceeding with further configuration or deployment tasks.

### 2. Set up WEKA client on existing GKE worker nodes

Perform this procedure for each GKE worker node individually.

Note: Any new GKE worker nodes added later will require these steps for WEKA client installation unless the following automation steps are implemented.

**Before You Begin**

Ensure SSH access to the GKE worker nodes is available to install the WEKA client.

**Procedure:**

1. Identify the names of the GKE worker nodes where the WEKA client will be installed.

2. Go to **Google Cloud Platform > Compute Engine > VM Instance** console. Locate the identified GKE worker node, select **SSH connect** from the dropdown menu, and choose **Open in browser window** to initiate the SSH connection.

3. To avoid CPU pinning conflicts with GKE, start the WEKA client using a stateless client mount. Authorize the SSH connection, then add the WEKA client from the existing WEKA cluster. For details, see the  procedure.

### 3.  Configure automated WEKA setup client on worker nodes

Google Cloud Platform (GCP) allows the addition of startup scripts at the project level, ensuring each new instance runs the script. By using metadata lookups, the WEKA client installation you can restrict to GKE cluster systems.

With auto-scaling enabled, GKE automatically adds and sets up the WEKA client on each new worker node.

**Procedure:**

1. In the GCP **Compute Engine** console, scroll to the bottom of the left-side menu.
2. Select **Metadata** under the **Settings** section.
3. Click **EDIT**, then select **+ ADD ITEM**.
4. Set the key name to **startup-script** (no spaces), and paste the following GKE WEKA client install script into the **Value** field. Replace the following input values according to your environment:
   * WEKA_FS (line 11)
   * WEKA_HOSTS (line 17)
   * GKE_CLUSTER_NAME (line 99)

<details>

<summary>GKE WEKA client install script</summary>

```

```bash
| curl -sS -H 'Metadata-Flavor: Google' 'http://metadata.google.internal/computeMetadata/v1/instance/?recursive=true&alt=json' | jq '.attributes."cluster-name"' -r |

(
    #!/usr/bin/env bash

    set -euo pipefail

    DEBIAN_FRONTEND=noninteractive
    ROOT_MOUNT_DIR="${ROOT_MOUNT_DIR:-/root}"

    export WEKA_FS="default"

    # Mount point for weka filesystem
    export WEKA_MOUNT="/mnt/gkeclient"

    # Its good to add 2-3 servers in case one is not available
    export WEKA_HOSTS="10.0.0.8,10.0.0.9,10.0.0.10"

    # Timeout for how long the client is inaccessible before being removed from the cluster

    # Default is 86400 (24hrs) in a more dynamic environment it can be lower.
    export WEKA_CLIENTTIMEOUNT="3600"

    # Number of cores to add to WEKA FrontEnd.
    export WEKA_FRONTENDCORES=2

    # First IP taken from WEKA_HOSTS list to download the client from.
| export WEKA_DOWNLOADIP=$(echo "$WEKA_HOSTS" | cut -d',' -f1) |

    echo "Installing dependencies"
    apt-get update
    apt-get install -y apt-transport-https curl gnupg lsb-release jq

    echo "Installing gcloud SDK"
    snap install google-cloud-sdk --classic

    echo "Getting node metadata"
    ALL_METADATA="$(curl -sS -H 'Metadata-Flavor: Google' 'http://metadata.google.internal/computeMetadata/v1/instance/?recursive=true&alt=json')"
    NODE_NAME="$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google')"
| ZONE="$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google' | awk -F "/" '{print $4}')" |

    echo "Setting up disks"
    DISK_NAME="$NODE_NAME-wekadir"

| if ! gcloud compute disks list --filter="name:$DISK_NAME" | grep "$DISK_NAME" > /dev/null; then |
        echo "Creating $DISK_NAME"
        gcloud compute disks create "$DISK_NAME" --size=$(( 1024*20 )) --zone="$ZONE"
    else
        echo "$DISK_NAME already exists"
    fi

| if ! gcloud compute instances describe "$NODE_NAME" --zone "$ZONE" --format '(disks[].source)' | grep "$DISK_NAME" > /dev/null; then |
        echo "Attaching $DISK_NAME to $NODE_NAME"
        gcloud compute instances attach-disk "$NODE_NAME" --device-name=sdb --disk "$DISK_NAME" --zone "$ZONE"
    else
        echo "$DISK_NAME is already attached to $NODE_NAME"
    fi
    function create_wekaio_partition() {
        echo "--------------------------------------------"
        echo " Creating local filesystem on WekaIO volume "
        echo "--------------------------------------------"

        wekaiosw_device="/dev/sdb"
| if mount | grep -w $wekaiosw_device | grep -w /opt/weka; then |
          echo "Weka volume is already mounted"
        else
          echo "Formatting and mounting Weka trace volume"
| mkfs.ext4 -L wekaiosw ${wekaiosw_device} |  | return 1 |
| mkdir -p /opt/weka |  | return 1 |
| mount $wekaiosw_device /opt/weka |  | return 1 |
          echo "LABEL=wekaiosw /opt/weka ext4 defaults 0 2" >>/etc/fstab
        fi
    }
    function prepare_weka_env() {
        echo "--------------- ENV ---------------"
        env
        echo "--------------- ENV ---------------"
| create_wekaio_partition |  | logger -s -t weka.install "Failed creating wekaio partition" |
    }

    function start_weka_client() {
        prepare_weka_env
        if ! which weka; then
          echo "Installing agent from ${WEKA_DOWNLOADIP}"
| curl --fail --max-time 10 "http://${WEKA_DOWNLOADIP}:14000/dist/v1/install" | sh |  | logger -s -t weka.install "Failed installing agent from the first backend" |
        else
          echo "Weka seems already installed, skipping agent install"
        fi
        mkdir -p ${WEKA_MOUNT}
| if mount | grep -w ${WEKA_MOUNT}; then |
          echo "Weka filesystem seems already mounted on endpoint, skipping mount"
        else
| mount -t wekafs ${WEKA_HOSTS}/${WEKA_FS} ${WEKA_MOUNT} -o remove_after_secs=${WEKA_CLIENTTIMEOUNT},num_cores=${WEKA_FRONTENDCORES},net=udp |  | logger -s -t weka.install "Error mounting filesystem" |
        fi
    }

## Update to the name of the GKE cluster
GKE_CLUSTER_NAME=my-gke-cloud-name
| GKE_METADATA_CLUSTER_NAME=$(curl -sS -H 'Metadata-Flavor: Google' 'http://metadata.google.internal/computeMetadata/v1/instance/?recursive=true&alt=json' | jq '.attributes."cluster-name"' -r) |

if [ "$GKE_CLUSTER_NAME" != "GKE_METADATA_CLUSTER_NAME" ]; then
    echo "Instance does not belong to GKE cluster $GKE_CLUSTER_NAME. Skipping installation"
else
    echo "Instance belongs to GKE cluster, initializing Weka client installation"
    start_weka_client
fi

) >/root/startup.out 2>/root/startup.err
```

```

</details>

5. After adding the startup script, click **SAVE** at the bottom of the page.
6. Test the script:
   * Increase the Node Pools node count.
   * Check the client list in WEKA UI to verify that the new clients have been added.

### 4. Install and configure the WEKA CSI plugin

To install and configure the WEKA CSI plugin, follow the procedures in the  section.

Note: You may need to adjust the steps according to your specific setup and requirements.

### 5. Set up WEKA storage for GKE pods

To set up WEKA storage for use by GKE pods, follow the procedures in the section, in the CSI Plugin section.

<!-- ============================================ -->
<!-- File 55/259: planning-and-installation_weka-installation-on-gcp_troubleshooting.md -->
<!-- ============================================ -->

# Troubleshooting

The GCP Console has a Logs Explorer interface in which you can view the cloud function logs related to the WEKA cluster activities, such as when scaling instances up or down. In addition, the cluster state file retained in the cloud storage provides you with the status of the operations in the WEKA project.

**Typical troubleshooting flow if the resize cloud function does not resize the cluster**

1. Open the cluster state file and check that the `desired_size` is as expected and the `clusterized` value is `true`. The cluster state file is in the cloud storage, and its name comprises the `prefix` and `cluster_name` provided in the [terraform variables file](../weka-project-description#tf.tfvars-example-public-vpc).
2. Check the scale-up workflow (or scale-down workflow). Check the function that didn't work and its related logs in the Logs Explorer of the GCP Console.

<!-- ============================================ -->
<!-- File 56/259: planning-and-installation_weka-installation-on-oci.md -->
<!-- ============================================ -->

---
description:
---

# WEKA installation on OCI

## Overview

The WEKA Data Platform deployment on OCI follows a process similar to bare-metal installation, with adaptations for cloud-specific architecture. This implementation allows you to leverage WEKA's high-performance storage capabilities within Oracle's cloud environment.

OCI provides the necessary infrastructure components for WEKA deployment, including bare-metal compute shapes, virtual networking, and storage options. However, certain limitations exist compared to on-premises deployments, particularly regarding network configuration flexibility.

## Workflow

The deployment process includes the following main phases:

1. Prepare OCI bare metal infrastructure for WEKA.
2. Install add-ons using templates for OCI HPC Images.
3. Install WEKA on the OCI bare metal infrastructure.
4. Configure the WEKA cluster.
5. Add clients.

Note: WEKA strongly recommends that you coordinate and obtain approval from OCI personnel before deploying any WEKA systems on OCI. This coordination ensures your deployment will be compatible with OCI's architecture and comply with cloud resource management policies.

### 1. Prepare OCI bare metal infrastructure for WEKA

Establish the foundational infrastructure required before installing the WEKA Data Platform software.

**Procedure**

1. **Verify resource compartment access:**
   1. Sign in to https://cloud.oracle.com.
   2. Search for and select **compartments** from the **Services** section.
   3. Locate and click your designated **resource compartment** link.

Note: If you see "Nothing here? Possible reasons..." message, you lack proper access permissions. Contact your cloud team for access before proceeding.

2.  **Verify IAM policy statements:**

    Navigate to https://cloud.oracle.com/identity/domains/policies and verify your login has these permissions:

    * allow group \<identity group> to manage **compute-management-family** in compartment \<resource compartment>
    * allow group \<identity group> to manage **virtual-network-family** in compartment \<resource compartment>
    * allow group \<identity group> to manage **instance-family** in compartment \<resource compartment>
    * allow group \<identity group> to manage **volume-family** in compartment \<resource compartment>
    * allow group \<identity group> to manage **object-family** in compartment \<resource compartment>

    Replace terms in angle brackets with your company's specific names.
3. **Create cloud network:**
   1. Search for and select **VCN** from the **Services** section.
   2. Ensure the designated VCN has:
      * Subnet with sufficient addresses for admin/management access to each server.
      * Subnet with sufficient addresses for high-performance access to each server.
      * Subnet with sufficient addresses for high-performance clients mounting WEKA.

Note: VCN capacity planning must account for both WEKA Data Platform and high-performance clients, as client mount connections cannot traverse firewalls or NAT-gateways.

4. **Deploy bare-metal servers:**
   1. Search for and select **Instances** from the **Services** section.
   2. Select the computer image:
      * Find your preferred OS version on #operating-system.
      * Select a matching image from the OCI instance image gallery.
   3. Select the appropriate server shape. Supported shapes:
      * BM.Optimized3.36
      * BM.DenseIO.E5.128
      * BM.HPC.E5.144
      * BM.GPU.H100, BM.GPU.H200, and BM.GPU.A100
      * VM.Standard.E5.Flex
   4. Configure the boot volume:
      * Access the **Size and Performance** settings panel for the boot volume.
      * Switch to **Custom Configuration** mode.
      * Using the performance slider, set the VPUs/GB ratio to a minimum of **40**. Consider increasing this value beyond 40 VPUs/GB during periods of elevated cluster activity, because performance traces are stored on this boot volume.
   5. Configure network interfaces:
      * Create a primary NIC on the management subnet.
      * Create a secondary NIC on the high-performance subnet.
5. **Install OFED drivers:**
   1. Install drivers compatible with your NIC and OS combination:
      * For production with WEKA 4.4.x, use: https://linux.mellanox.com/public/repo/mlnx_ofed/5.9-0.5.6.0/
      * For widest compatibility across all WEKA releases: https://linux.mellanox.com/public/repo/mlnx_ofed/5.4-3.4.0.0/
   2. Select the appropriate OS version link, then download and install the RPM/DEB package on each bare-metal server running WEKA Data Platform.

### 2. Install add-ons using templates for OCI HPC Images

The **oci-hpc-images** repository provides a set of Packer and Ansible-based templates designed to automate the creation of high-performance computing (HPC) images on Oracle Cloud Infrastructure (OCI). These templates support multiple operating systems and are optimized for OCI environments, enabling users to efficiently deploy consistent and reproducible HPC-ready images.

#### Supported platforms

The templates include specific installation instructions for the following Linux distributions:

* Oracle Linux 8
* Ubuntu 22.04
* Ubuntu 24.04

Each distribution requires the installation of necessary dependencies such as Packer, Python, and Ansible, and the configuration of a Python virtual environment to isolate and manage dependencies.

**Procedure**

1. **Access the repository:** OCI HPC Images Repository.
2. **Install required tools:** Install packer, tmux, python, and supporting packages specified in the repository. Commands vary by OS version and are provided explicitly for each supported platform.
3. **Configure Python environment:**
   1. Create and activate a Python virtual environment (`packer_env`).
   2. Upgrade `pip` and `setuptools`.
   3. Install a specific version of `ansible-core`.
   4. Use `ansible-galaxy` to install required roles as specified in `requirements.yml`.
4. **Configure environment variables:**
   1. Copy the `defaults.pkr.hcl.example` file to `defaults.pkr.hcl`.
   2. Edit the file to specify required variables. For Ubuntu 24.04 or later, explicitly set:\
       `OpenSSH9 = true`
5. **Customize the image:**
   1. Navigate to the required OS-specific directory under `images/`.
   2.  Modify the image `.pkr.hcl` file to include the appropriate image OCID for your region and select the necessary software modules.

       OCIDs for various regions can be found at the Oracle Cloud Infrastructure Image Documentation.
6. **Build the image:**
   1.  Due to the potentially long build time, it is recommended to use a `tmux` session to ensure the process continues if the terminal disconnects:

       `tmux new`
   2. Initialize and build the image using the following commands:\
      Replace `<image-name>` with the specific file name matching your configuration and target OS. The following command is an example for Ubuntu-22.

```
packer init images/Ubuntu-22/<image-name>.pkr.hcl
packer build -var-file="defaults.pkr.hcl" images/Ubuntu-22/<image-name>.pkr.hcl
```

### 3. Install WEKA on the OCI bare metal infrastructure

1. Download the WEKA software. See .
2.  Install the WEKA software.

    * Once the WEKA software tarball is downloaded from get.weka.io, run the untar command.
    * Run the `install.sh` command on each server, following the instructions in the **Install** tab of get.weka.io.

    Once completed, the WEKA software is installed on all the allocated servers and runs in stem mode (no cluster is attached).

### 4. Configure the WEKA cluster

1. Use the resources generator to create configuration files (`drives0.json`, `compute0.json`, and `frontend0.json`) in the **/tmp** directory of each server.
2. Create containers using these configuration files on all cluster servers.
3. Complete essential post-configuration:
   * Apply your license.
   * Activate the IO service.
   * Verify your configuration.
   * Consider enabling event notifications if needed.

Refer to the related topics for detailed instructions on each step.

**Related topics**

.

### 5. Add clients or use converged mode

Depending on your deployment mode, you can choose one of the following options to access the WEKA filesystem:

* **Client-server mode:** In this configuration, client functionality is deployed on dedicated client servers, similar to a bare-metal WEKA cluster. This setup separates client and backend functionality. For detailed instructions, refer to .
* **Converged mode:** In this configuration, client functionality is integrated with the backend servers. You can create a filesystem and mount it directly on each of the WEKA backend servers.

### What to do next

Proceed to , which serves as your entry point for using the WEKA system. Start by familiarizing yourself with the graphical user interface (GUI) and command-line interface (CLI). Once you are comfortable, you can perform your first I/O operations using the WEKA filesystem. This includes creating a filesystem and mounting it on the appropriate client or backend servers, depending on your chosen deployment mode.

<!-- ============================================ -->
<!-- File 57/259: getting-started-with-weka.md -->
<!-- ============================================ -->

# Getting Started with WEKA

## Topics in this section

### Manage the system using the WEKA GUI

WEKA GUI application enables you to configure, administer, and monitor the WEKA system. This page provides an overview of the primary operations, access to the GUI, and system dashboard.

### Manage the system using the WEKA CLI

The overview of the WEKA CLI includes top-level commands, command hierarchy, how to connect to another server, auto-completion, and how to check the status of the cluster.

### Run first IOs with WEKA filesystem

This is a quick reference guide using the CLI to perform the first IO in the WEKA filesystem.

### Getting started with WEKA REST API

### WEKA REST API and equivalent CLI commands

<!-- ============================================ -->
<!-- File 58/259: getting-started-with-weka_performing-the-first-io.md -->
<!-- ============================================ -->

---
description:
---

# Run first IOs with WEKA filesystem

Once the system is installed and you are familiar with the CLI and GUI, you can connect to one of the servers and try it out.

To perform a sanity check that the WEKA cluster is configured and IOs can be performed on it, do the following procedures:

1. Create the first filesystem.
2. Mount the filesystem.
3. Writing to the filesystem.

To validate that the WEKA cluster and IT environment are best configured to benefit from the WEKA filesystem, do the following procedure:

* Validate the configuration.

## Create the first filesystem

1. A filesystem must reside in a filesystem group. Create a filesystem group:

```
# to create a new filesystem group
$ weka fs group create my_fs_group
FSGroupId: 0

# to view existing filesystem groups details in the WEKA system
$weka fs group
| FileSystem Group ID | Name | target-ssd-retention | start-demote |
--------------------+-------------+----------------------+-------------
| FSGroupId: 0 | my_fs_group | 1d 0:00:00h | 0:15:00h |
```

2\. Create a filesystem within that group:

```
# to create a new filesystem
$ weka fs create new_fs my_fs_group 1TiB
FSId: 0

# to view existing filesystems details in the WEKA system
$ weka fs
| Filesystem ID | Filesystem Name | Group | Used SSD (Data) | Used SSD (Meta) | Used SSD | Free SSD | Available SSD (Meta) | Available SSD | Used Total (Data) | Used Total | Free Total | Available Total | Max Files | Status | Encrypted | Object Storages | Auth Required |
--------------+-----------------+-------------+-----------------+-----------------+----------+----------+----------------------+---------------+-------------------+------------+------------+-----------------+-----------+--------+-----------+-----------------+--------------
| 0 | new_fs | my_fs_group | 0 B | 4.09 KB | 4.09 KB | 1.09 TB | 274.87 GB | 1.09 TB | 0 B | 4.09 KB | 1.09 TB | 1.09 TB | 22107463 | READY | False |  | False |
```

Note: In AWS installation via the self-service portal, the default filesystem group and filesystem are created. The `default` filesystem is created with the entire SSD capacity.
For creating an additional filesystem, it is first needed to decrease the `default` filesystem SSD size:
```
# to reduce the size of the default filesystem
$ weka fs update default --total-capacity 1GiB
# to create a new filesystem in the default group
$ weka fs create new_fs default 1GiB
# to view existing filesystems details in the WEKA system
$ weka fs
| Filesystem ID | Filesystem Name | Group | Used SSD (Data) | Used SSD (Meta) | Used SSD | Free SSD | Available SSD (Meta) | Available SSD | Used Total (Data) | Used Total | Free Total | Available Total | Max Files | Status | Encrypted | Object Storages | Auth Required |
--------------+-----------------+---------+-----------------+-----------------+----------+----------+----------------------+---------------+-------------------+------------+------------+-----------------+-----------+--------+-----------+-----------------+--------------
| 0 | default | default | 0 B | 4.09 KB | 4.09 KB | 1.07 GB | 268.43 MB | 1.07 GB | 0 B | 4.09 KB | 1.07 GB | 1.07 GB | 21589 | READY | False |  | False |
| 1 | new_fs | default | 0 B | 4.09 KB | 4.09 KB | 1.09 TB | 274.87 GB | 1.09 TB | 0 B | 4.09 KB | 1.09 TB | 1.09 TB | 22107463 | READY | False |  | False |
```

For more information about filesystems and filesystem groups, see .

## Mount the filesystem

1. To mount a filesystem, create a mount point and call the mount command:

```
$ sudo mkdir -p /mnt/weka
$ sudo mount -t wekafs new_fs /mnt/weka

```

2\. Check that the filesystem is mounted:

```
# using the mount command
| $ mount | grep new_fs |
new_fs on /mnt/weka type wekafs (rw,relatime,writecache,inode_bits=64,dentry_max_age_positive=1000,dentry_max_age_negative=0)
```

Note: In AWS installation via the self-service portal, the `default` filesystem is already mounted under `/mnt/weka.`

For more information about mounting filesystems and mount options, refer to .

## Write to the filesystem

Write data to the filesystem:

```
# to perform random writes
$ sudo dd if=/dev/urandom of=/mnt/weka/my_first_data bs=4096 count=10000
10000+0 records in
10000+0 records out
40960000 bytes (41 MB) copied, 4.02885 s, 10.2 MB/s

# to see the new file creted
$ ll /mnt/weka
total 40000
-rw-r--r-- 1 root root 40960000 Oct 30 11:58 my_first_data

# to check the WekaFS filesystems via the CLI shows the used SSD capacity:
$ weka fs
| Filesystem ID | Filesystem Name | Group | Used SSD (Data) | Used SSD (Meta) | Used SSD | Free SSD | Available SSD (Meta) | Available SSD | Used Total (Data) | Used Total | Free Total | Available Total | Max Files | Status | Encrypted | Object Storages | Auth Required |
--------------+-----------------+---------+-----------------+-----------------+----------+----------+----------------------+---------------+-------------------+------------+------------+-----------------+-----------+--------+-----------+-----------------+--------------
| 0 | default | default | 40.95 MB | 180.22 KB | 41.14 MB | 1.03 GB | 268.43 MB | 1.07 GB | 40.95 MB | 41.14 MB | 1.03 GB | 1.07 GB | 21589 | READY | False |  | False |
```

This has completed the sanity check that the WEKA cluster is configured and IOs can be performed on it.

## Validate the configuration

To ensure that the WEKA cluster and the IT environment are well configured, more complex IO patterns and benchmark tests should be conducted using the FIO utility.

Although results can vary using different servers and networking, it is not expected to be very different than what many other customers and we achieved. A properly configured WEKA cluster and IT environment should yield similar results described in the WEKA performance tests section.

Note: The numbers achieved in the benchmark tests, as described in the WEKA performance tests section, are not just achieved in a controlled environment. Similar numbers should be achieved using a similar configuration if the WEKA cluster and IT environment are properly configured.
If the numbers achieved in your environment significantly vary from those, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team) before running any other workload on the WEKA cluster.

**Related topic**

<!-- ============================================ -->
<!-- File 59/259: getting-started-with-weka_manage-the-system-using-weka-cli.md -->
<!-- ============================================ -->

---
description:
---

# Manage the system using the WEKA CLI

The WEKA CLI is installed on each WEKA server and is available through the `weka` command. It's possible to connect to any of the servers using `ssh` and running the `weka` command. The `weka` command displays a list of all top-level commands.

## Top-level commands

The WEKA CLI is installed on each WEKA server and is available through the `weka` command. Running this command displays a list of all top-level commands:

```
$ weka -h
UUsage:
    weka [--color color] [--help] [--build] [--version] [--legal]

Description:
    The base command for all weka related CLIs

Subcommands:
   access-group      Commands that manage the cluster access-groups
   agent             Commands that control the weka agent (outside the weka containers)
   alerts            List alerts in the Weka cluster
   cloud             Cloud commands. List the cluster's cloud status, if no subcommand supplied.
   cluster           Commands that manage the cluster
   dataservice       Commands that manage dataservice
   diags             Diagnostics commands to help understand the status of the cluster and its environment
   driver            Manage Weka drivers
   events            List all events that conform to the filter criteria
   fs                List filesystems defined in this Weka cluster
   interface-group   List interface groups
   local             Commands that control weka and its containers on the local machine
   mount             Mounts a wekafs filesystem. This is the helper utility installed at /sbin/mount.wekafs.
   nfs               Commands that manage client-groups, permissions and interface-groups
   org               List organizations defined in the Weka cluster
   s3                Commands that manage Weka's S3 container
   security          Security commands.
   smb               Commands that manage Weka's SMB container
   stats             List all statistics that conform to the filter criteria
   status            Get an overall status of the Weka cluster
   umount            Unmounts wekafs filesystems. This is the helper utility installed at /sbin/umount.wekafs.
   upgrade           Commands that control the upgrade precedure of Weka
   user              List users defined in the Weka cluster
   version           When run without arguments, lists the versions available on this machine. Subcommands
                             allow for downloading of versions, setting the current version and other actions to manage
                             versions.

Options:
   --agent         Start the agent service
   --color         Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled')
   -h, --help      Show help message
   --build         Prints the CLI build number and exits
   -v, --version   Prints the CLI version and exits
   --legal         Prints software license information and exits

```

The options that are common to many commands include:

 | Option | Description |
 | --- | --- |
 | -J | --json | Prints the raw JSON value returned by the cluster. |
 | -H | --hostname | Directs the CLI to communicate with the cluster through the specified hostname or IP. |
 | --raw-units | Sets the units such as capacity and bytes to be printed in their raw format, as returned by the cluster. |
 | --UTC | Sets the timestamps to be printed in UTC timezone, instead of the local time of the server running the CLI command. |
 | -f | --format | Specifies the format to output the result (view, csv, markdown, or JSON). |
 | -o | --output | Specifies the columns to include in the output. |
 | -s | --sort | Specifies the order to sort the output. May include a '+' or '-' before the column name to sort by ascending or descending order. |
 | -F | --filter | Specifies the filter values for a member (without forcing it to be in the output). |
 | --no-header | Indicates that the column header should not be shown when printing the output. |
 | -C | --CONNECT-TIMEOUT | Modifies the default timeout used for connecting to the system via the JRPC protocol. |
 | -T | --TIMEOUT | Modifies the default timeout for which the commands wait for a response before giving up. |
 | --color | Controls the usage of color in the outputs. Possible values: enabled, disabled, or auto.Default: auto. It automatically determines whether to enable color based on the output destination. If the output is a terminal that supports color, it is enabled; otherwise, it is disabled. |

Note: Throughout the documentation, the CLI mandatory parameters are marked with an asterisk (*).

## Commands hierarchy

Most WEKA system top-level commands are the default list command for their own collection. Additional sub-commands may be available under them.

**Example:** The `weka fs` command displays a list of all filesystems and is also the top-level command for all filesystems, filesystem groups, and snapshot-related operations. It is possible to use the `-h`/`--help` flags or the `help` command to display a list of available commands at each level, as shown below:

```

```
$ weka fs
 | FileSystem | Name | Group | SSD Bu | Total | Is re | Is creat | Is remov
 | ID |  |  | dget | Budget | ady | ing | ing
+------------+---------+---------+--------+--------+-------+----------+----------
 | FSId: 0 | default | default | 57 GiB | 57 GiB | True | False | False
```

```

```
$ weka fs -h
Usage:
    weka fs [--name name]
            [--color color]
            [--HOST HOST]
            [--PORT PORT]
            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
            [--TIMEOUT TIMEOUT]
            [--profile profile]
            [--format format]
            [--output output]...
            [--sort sort]...
            [--filter filter]...
            [--filter-color filter-color]...
            [--capacities]
            [--force-fresh]
            [--help]
            [--raw-units]
            [--UTC]
            [--no-header]
            [--verbose]

Description:
    List filesystems defined in this Weka cluster

Subcommands:
   add          Create a filesystem
   remove       Delete a filesystem
   download     Download a filesystem from object store
   group        List filesystem groups
   kms-rewrap   Rewrap the key of Filesystem
   protection   Commands used to manage file system protection
   quota        Commands used to control directory quotas
   reserve      Thin provisioning reserve for organizations
   restore      Restore filesystem content from a snapshot
   security     Manage filesystem security
   snapshot     List snapshots
   tier         Show object store connectivity for each node in the cluster
   update       Update a filesystem

Options:
   --name                  Filesystem name
   --color                 Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled')
   -H, --HOST              Specify the host. Alternatively, use the WEKA_HOST env variable
   -P, --PORT              Specify the port. Alternatively, use the WEKA_PORT env variable
   -C, --CONNECT-TIMEOUT   Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w,
                           infinite/unlimited)
   -T, --TIMEOUT           Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w,
                           infinite/unlimited)
   --profile               Name of the connection and authentication profile to use
   -f, --format            Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or
                           'oldview')
   -o, --output            Specify which columns to output. May include any of the following:
                           uid,id,name,group,usedSSD,usedSSDD,usedSSDM,freeSSD,availableSSDM,availableSSD,usedTotal,usedTotalD,freeTotal,availableTotal,maxFiles,status,encrypted,stores,auth,thinProvisioned,thinProvisioningMinSSDBudget,thinProvisioningMaxSSDBudget,usedSSDWD,usedSSDRD,reductionRatio,pendingReduction,dataReduction,reducedProcessedSize,reducedSize,kmsKey,kmsNamespace,kmsRole,processedReductionRatio
                           (may be repeated or comma-separated)
   -s, --sort              Specify which column(s) to take into account when sorting the output. May include a '+' or
                           '-' before the column name to sort in ascending or descending order respectively. Usage:
| [+ | -]column1[,[+ | -]column2[,..]] (may be repeated or comma-separated) |
   -F, --filter            Specify what values to filter by in a specific column. Usage:
                           column1=val1[,column2=val2[,..]] (may be repeated or comma-separated)
   --filter-color          Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated)
   --capacities            Display all capacity columns
   --force-fresh           Refresh the capacities to make sure they are most updated
   -h, --help              Show help message
   -R, --raw-units         Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in
                           human-readable format, e.g 1KiB 234MiB 2GiB.
   -U, --UTC               Print times in UTC. When not set, times are converted to the local time of this host.
   --no-header             Don't show column headers when printing the output
   -v, --verbose           Show all columns in output

```

## Connect to another server

Most WEKA system commands deliver the same result on all cluster servers. However, it is sometimes necessary to run a command on a specific server. To do this, use the `-H/--hostname` option and specify the hostname or IP address of the target server.

## CLI auto-completion

Using `bash` you can use auto-completion for CLI commands and parameters. The auto-completion script is automatically installed.

To disable the auto-completion script, run `weka agent autocomplete uninstall`

To (re-)install the script on a server, run `weka agent autocomplete install` and re-enter your shell session.

You can also use `weka agent autocomplete export` to get the bash completions script and write it to any desired location.

## Standardized CLI command actions and entities

`weka` commands with different names but similar meanings have been standardized. Preferred names are now documented, while aliases remain for backward compatibility. Most commands now accept both singular and plural forms.

#### Standardized commands

The first name in each list is the documented one, followed by its aliases. Aliases ensure existing commands and scripts remain functional.

* **Actions:**
  * `add` (`create`, `new`)
  * `remove` (`destroy`, `delete`)
  * `attach` (`assign`)
  * `detach` (`unassign`)
  * `reset` (`unset`)
  * `update` (`updates`)
* **Entities:**
  * `drive` (`drives`)
  * `driver` (`drivers`)
  * `container` (`containers`)
  * `alerts` (`alert`)
  * `task` (`tasks`)
  * `process` (`node`, `processes`, `nodes`)
  * `resources` (`resource`)
  * `hot-spare` (`hot-spares`, `hotspare`, `hotspares`)
  * `bucket` (`buckets`)
  * `events` (`event`)
  * `denylist` (`blacklist`)
  * `permission` (`permissions`)
  * `client-group` (`client-groups`, `clientgroup`, `client-groups`)
  * `interface-group` (`interface-groups`, `interfacegroup`, `interfacegroups`)
  * `service-account` (`service-accounts`, `serviceaccount`, `serviceaccounts`)
  * `share` (`shares`)
  * `list` (`lists`)
  * `group` (`groups`)
  * `snapshot` (`snapshots`)
  * `user` (`users`)
  * `policy` (`policies`)

## WEKA CLI command output colors

The `weka status` command and various commands that return tables, such as `weka cluster buckets`, support colored output by default when executed in a terminal (tty). You can control the use of colors with the `--color` option or the `WEKA_CLI_COLOR` environment variable.

Colors are used sparingly and consistently to indicate status:

* Green: Indicates that the status is OK.
* Yellow: Represents a warning or a transient state, such as initializing or rebuilding.
* Red: Indicates an error or an issue that needs attention.

Note: Colors are only used when formatting in "human" formats (such as plain text). They are not applied when the output is in machine-readable formats such as JSON, CSV, or Markdown.

### `--color` option usage

The `--color` option controls the usage of color in the outputs. It expects one of the following values:

* **enabled**: Forces color output to be enabled, regardless of the output destination.
* **disabled**: Disables color output entirely.
* **auto**: Automatically determines whether to enable color based on the output destination. If the output is a terminal that supports color, it is enabled; otherwise, it is disabled. The **Default:** `auto`

When the `auto` value is selected, the `NO_COLOR` environment variable is also respected. If `NO_COLOR` is set in the environment, color output is disabled, regardless of the output destination.

### `WEKA_CLI_COLOR` environment variable usage

This environment variable can set the color output with the same possible values as the `--color` parameter (`enabled`, `disabled`, `auto`). However, if the `--color` parameter is specified, it overrides the `WEKA_CLI_COLOR` environment variable.

## Cluster status

The `weka status` command displays the overall status of the WEKA cluster.

Examples:

<!-- ============================================ -->
<!-- File 60/259: getting-started-with-weka_manage-the-system-using-weka-cli_weka-cli-hierarchy.md -->
<!-- ============================================ -->

---
description:
---

# WEKA CLI hierarchy

Note: CLI commands marked with two asterisks (**) are new in version 5.0.2, compared to version 4.4.7.

### weka agent

```
weka agent
 | autocomplete
 | export
 | install
 | uninstall
 | install-agent
 | update-containers
 | uninstall
```

### **weka alerts**

```
weka alerts
 | describe
 | mute
 | types
 | unmute
```

### **weka audit**

```
weka audit **
 | cluster
 | disable
 | enable
 | enhancer
 | disable
 | enable
 | resolve-paths
 | disable
 | enable
 | set-global-operations
 | stats
 | status
 | fs
 | disable
 | enable
 | set-operations
 | status
```

### **weka cloud**

```
weka cloud
 | disable
 | enable
 | proxy
 | status
 | update
 | upload-rate
 | set
```

### **weka cluster**

```
weka cluster
 | bucket
 | client-target-version
 | reset
 | set
 | show
 | container
 | activate
 | add
 | apply
 | auto-remove-timeout
 | bandwidth
 | clear-failure
 | cores
 | deactivate
 | deactivation-chec
 | dedicate
 | failure-domain
 | info-hw
 | join-secret
 | management-ips
 | memory
 | net
 | add
 | remove
 | remove
 | requested-action
 | elective-protection **
 | resources
 | restore
 | add
 | default-net
 | reset
 | set
 | update
 | drive
 | activate
 | add
 | deactivate
 | remove
 | scan
 | failure-domain
 | hot-spare
 | license
 | reset
 | set
 | task
 | pause
 | resume
 | abort
 | limits
 | set
 | mount-defaults
 | reset
 | set
 | show
 | process
 | requested-action **
 | elective-protection
 | set
 | servers
 | list
 | show
 | start-io
 | stop-io
 | task
 | abort
 | bucket
 | limits
 | pause
 | resume
 | throttle
 | update
```

### weka dataservice

```
weka dataservice
 | global-config
 | set
 | show
```

### **weka diags**

```
weka diags
 | collect
 | list
 | rm
 | upload
```

### weka driver

```
weka driver
 | build
 | download
 | export
 | import
 | install
 | kernel
 | pack
 | ready
 | sign
```

### **weka events**

```
weka events
 | list-local
 | list-types
 | trigger-event
```

### **weka fs**

```
weka fs
 | add
 | remove
 | download
 | group
 | add
 | remove
 | update
 | kms-rewrap
 | protection
 | snapshot-policy
 | attach
 | add
 | remove
 | detach
 | duplicate
 | export
 | list
 | run-once
 | show
 | update
 | quota
 | list
 | list-default
 | set
 | set-default
 | reset
 | unset-default
 | reserve
 | set
 | status
 | reset
 | restore
 | security
 | policy
 | attach
 | detach
 | list
 | reset
 | set
 | snapshot
 | access-point-naming-convention
 | status
 | update
 | copy
 | add
 | remove
 | download
 | update
 | upload
 | tier
 | capacity
 | fetch
 | location
 | obs
 | update
 | ops
 | release
 | s3
 | add
 | attach
 | remove
 | detach
 | snapshot
 | list
 | update
 | update
```

### weka interface-group

```
weka interface-group
 | add
 | assignment
 | remove
 | ip-range
 | add
 | remove
 | port
 | add
 | remove
 | update
```

### **weka local**

```
weka local
 | diags
 | disable
 | enable
 | events
 | install-agent
 | monitoring
 | ps
 | reset-data
 | resources
 | apply
 | auto-remove-timeout
 | bandwidth
 | base-port
 | cores
 | dedicate
 | export
 | failure-domain
 | fqdn
 | import
 | join-ips
 | join-secret
 | management-ips
 | memory
 | net
 | add
 | remove
 | restore
 | restart
 | rm
 | run
 | setup
 | client
 | container
 | envoy
 | services
 | taskmon
 | telemetry **
 | weka
 | start
 | status
 | stop
 | upgrade
```

### **weka mount**

```
weka mount
```

### **weka nfs**

```
weka nfs
 | client-group
 | add
 | remove
 | clients
 | show
 | debug-level
 | set
 | show
 | global-config
 | set
 | show
 | interface-group
 | add
 | assignmment
 | remove
 | ip-range
 | add
 | remove
 | port
 | add
 | remove
 | update
 | kerberos
 | registration
 | setup-ad
 | setup-mit
 | show
 | reset
 | service
 | setup
 | show
 | ldap
 | export-openldap
 | import-openldap
 | reset
 | setup-ad
 | setup-ad-nokrb
 | setup-openldap
 | show
 | permission
 | add
 | remove
 | update
 | rules
 | add
 | dns
 | ip
 | remove
 | dns
 | ip
```

### **weka org**

```
weka org
 | add
 | remove
 | rename
 | security
 | policy
 | attach
 | detach
 | list
 | reset
 | set
 | revoke-tokens
 | set-quota
```

### **weka s3**

```
weka s3
 | bucket
 | add
 | remove
 | lifecycle-rule
 | add
 | list
 | remove
 | reset
 | list
 | policy
 | get
 | get-json
 | set
 | set-custom
 | reset
 | quota
 | set
 | reset
 | cluster
 | audit-webhook
 | disable
 | enable
 | show
 | container
 | add
 | list
 | remove
 | add
 | remove
 | status
 | update
 | log-level
 | get
 | policy
 | add
 | attach
 | detach
 | list
 | remove
 | show
 | service-account
 | add
 | list
 | remove
 | show
 | sts
 | assume-role
```

### **weka security**

```
weka security
 | ca-cert
 | download
 | set
 | status
 | reset
 | cors-trusted-sites
 | add
 | list
 | remove
 | remove-all
 | kms
 | rewrap
 | set
 | reset
 | lockout-config
 | reset
 | set
 | show
 | login-banner
 | disable
 | enable
 | reset
 | set
 | show
 | policy
 | add
 | remove
 | duplicate
 | join
 | attach
 | detach
 | list
 | reset
 | set
 | list
 | show
 | test
 | update
 | tls
 | download
 | local
 | set
 | reset
 | set
 | status
 | reset
```

### **weka smb**

```
weka smb
 | cluster
 | container
 | add
 | remove
 | add
 | debug
 | remove
 | status
 | trusted-domains
 | add
 | remove
 | update
 | wait
 | domain
 | join
 | leave
 | share
 | add
 | host-access
 | add
 | list
 | remove
 | reset
 | list
 | add
 | remove
 | reset
 | show
 | remove
 | update
```

### **weka stats**

```
weka stats
 | list-types
 | realtime
 | retention
 | restore-default
 | set
 | status
```

### **weka status**

```
weka status
 | rebuild
```

### weka umount

```
weka unmount
```

### **weka upgrade**

```
weka upgrade
 | pause
 | resume
 | supported-features
```

### **weka user**

```
weka user
 | add
 | change-role
 | remove
 | generate-token
 | ldap
 | disable
 | enable
 | reset
 | setup
 | setup-ad
 | update
 | login
 | logout
 | passwd
 | revoke-tokens
 | update
 | whoami
```

### **weka version**

```
weka version
 | current
 | get
 | prepare
 | rm
 | set
 | reset
```

<!-- ============================================ -->
<!-- File 61/259: getting-started-with-weka_manage-the-system-using-weka-cli_cli-reference-guide.md -->
<!-- ============================================ -->

---
description:
---

# CLI reference guide

## weka

The base command for all weka related CLIs

```sh
weka [--color color] [--help] [--build] [--version] [--legal]

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `--agent` | Start the agent service |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |
 | `--build` | Prints the CLI build number and exits |
 | `-v`, `--version` | Prints the CLI version and exits |
 | `--legal` | Prints software license information and exits |

### weka agent

Commands that control the weka agent (outside the weka containers)

```sh
weka agent [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka agent autocomplete

Bash autocompletion utilities

```sh
weka agent autocomplete [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka agent autocomplete export**

Export bash autocompletion script

```sh
weka agent autocomplete export [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka agent autocomplete install**

Locally install bash autocompletion utility

```sh
weka agent autocomplete install [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka agent autocomplete uninstall**

Locally uninstall bash autocompletion utility

```sh
weka agent autocomplete uninstall [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka agent install-agent

Installs Weka agent on the machine the command is executed from

```sh
weka agent install-agent [--color color] [--no-update] [--no-start] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--no-update` | Don't update the locally installed containers |
 | `--no-start` | Do not register the weka-agent service and start it after its creation |
 | `-h`, `--help` | Show help message |

#### weka agent uninstall

Deletes all Weka files, drivers, shared memory and any other remainder from the machine this command is executed from. WARNING - This action is destructive and might cause a loss of data!

```sh
weka agent uninstall [--color color] [--force] [--ignore-wekafs-mounts] [--keep-files] [--help]

```

 | Parameter | Description |
 | ------------------------ | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--force` | Force the action to actually happen |
 | `--ignore-wekafs-mounts` | Proceed even with active wekafs mounts |
 | `--keep-files` | Do not remove Weka version images and keep in installation directory |
 | `-h`, `--help` | Show help message |

#### weka agent update-containers

Update the currently available containers and version specs to the current agent version. This command does not update weka, only the container's representation on the local machine.

```sh
weka agent update-containers [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

### weka alerts

List alerts in the Weka cluster

```sh
weka alerts [--severity severity]
            [--color color]
            [--HOST HOST]
            [--PORT PORT]
            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
            [--TIMEOUT TIMEOUT]
            [--profile profile]
            [--format format]
            [--output output]...
            [--sort sort]...
            [--filter filter]...
            [--filter-color filter-color]...
            [--muted]
            [--inactive]
            [--help]
            [--no-header]
            [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--severity` | Include event with equal and higher severity, default: WARNING (format: 'debug', 'warning', 'minor', 'major' or 'critical') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: muted,type,severity,time,endTime,activeDuration,count,title,description,action (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--muted` | List muted alerts alongside the unmuted ones |
 | `--inactive` | List alerts that became inactive recently |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka alerts describe

Describe all the alert types that might be returned from the weka cluster (including explanations and how to handle them)

```sh
weka alerts describe [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--format format]
                     [--output output]...
                     [--sort sort]...
                     [--filter filter]...
                     [--filter-color filter-color]...
                     [--help]
                     [--no-header]
                     [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: type,title,action,severity (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka alerts mute

Mute an alert-type. Muted alerts will not appear in the list of active alerts. It is required to specify a duration for the mute. Once the set duration concludes, the alert-type will automatically be unmuted.

```sh
weka alerts mute <alert-type>
                 <duration>
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `alert-type`* | An alert-type to mute, use `weka alerts types` to list types |
 | `duration`* | How long to mute this alert type for (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka alerts types

List all alert types that can be returned from the Weka cluster

```sh
weka alerts types [--color color]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--help]
                  [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka alerts unmute

Unmute an alert-type which was previously muted.

```sh
weka alerts unmute <alert-type>
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `alert-type`* | An alert-type to unmute, use `weka alerts types` to list types |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

### weka audit

Commands used for audit in a weka cluster

```sh
weka audit [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka audit cluster

Audit cluster CLI

```sh
weka audit cluster [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka audit cluster disable**

Disable audit logging cluster-wide

```sh
weka audit cluster disable [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--help]
                           [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit cluster enable**

Enable audit logging cluster-wide

```sh
weka audit cluster enable [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]
                          [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit cluster enhancer**

Audit cluster enhancer CLI

```sh
weka audit cluster enhancer [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka audit cluster enhancer disable**

Disable audit logging enhancement cluster-wide

```sh
weka audit cluster enhancer disable [--color color]
                                    [--HOST HOST]
                                    [--PORT PORT]
                                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                    [--TIMEOUT TIMEOUT]
                                    [--profile profile]
                                    [--help]
                                    [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit cluster enhancer enable**

Enable audit logging enhancement cluster-wide

```sh
weka audit cluster enhancer enable [--color color]
                                   [--HOST HOST]
                                   [--PORT PORT]
                                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                   [--TIMEOUT TIMEOUT]
                                   [--profile profile]
                                   [--help]
                                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit cluster resolve-paths**

Resolve full file paths in audit telemetry

```sh
weka audit cluster resolve-paths [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka audit cluster resolve-paths disable**

Disable resolving full file paths in audit telemetry

```sh
weka audit cluster resolve-paths disable [--color color]
                                         [--HOST HOST]
                                         [--PORT PORT]
                                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                         [--TIMEOUT TIMEOUT]
                                         [--profile profile]
                                         [--help]
                                         [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit cluster resolve-paths enable**

Enable resolving full file paths in audit telemetry

```sh
weka audit cluster resolve-paths enable [--color color]
                                        [--HOST HOST]
                                        [--PORT PORT]
                                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                        [--TIMEOUT TIMEOUT]
                                        [--profile profile]
                                        [--help]
                                        [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit cluster set-global-operations**

Define which operations to audit globally (can be 'All' or any subset)

```sh
weka audit cluster set-global-operations [--color color]
                                         [--HOST HOST]
                                         [--PORT PORT]
                                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                         [--TIMEOUT TIMEOUT]
                                         [--profile profile]
                                         [--help]
                                         [--json]
                                         [<operations>]...

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `operations`... | Audit operations to enable (e.g. 'All' or any from \[Open, Create, Read, ...]), replaces any previously enabled operations (format: 'none', 'open', 'create', 'read', 'modify', 'delete', 'rename', 'close', 'sessionmanagement' or 'all') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit cluster stats**

Audit logging cluster-wide stats

```sh
weka audit cluster stats [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--format format]
                         [--output output]...
                         [--sort sort]...
                         [--filter filter]...
                         [--filter-color filter-color]...
                         [--help]
                         [--raw-units]
                         [--UTC]
                         [--no-header]
                         [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: node,category,timestamp,stat,unit,value,containerId,container,hostname,roles (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka audit cluster status**

Audit logging cluster-wide status

```sh
weka audit cluster status [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]
                          [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka audit fs

Audit filesystems CLI

```sh
weka audit fs [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka audit fs disable**

Disable audit on a filesystem

```sh
weka audit fs disable <name>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka audit fs enable**

Enable audit on a filesystem

```sh
weka audit fs enable <name>
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka audit fs set-operations**

Override audit operations for a specific filesystem

```sh
weka audit fs set-operations <name>
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--help]
                             [--json]
                             [<operations>]...

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `operations`... | Audit operations to enable (e.g. 'All' or any subset of \[Open, Create, Read, Modify, Delete, Rename, Close, SessionManagement], replaces any previously enabled operations) (format: 'none', 'open', 'create', 'read', 'modify', 'delete', 'rename', 'close', 'sessionmanagement' or 'all') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka audit fs status**

List filesystems audit status

```sh
weka audit fs status [--name name]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--format format]
                     [--output output]...
                     [--sort sort]...
                     [--filter filter]...
                     [--filter-color filter-color]...
                     [--help]
                     [--raw-units]
                     [--UTC]
                     [--no-header]
                     [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Filesystem name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,name,audit,audit_string (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

### weka cloud

Cloud commands. List the cluster's cloud status, if no subcommand supplied.

```sh
weka cloud [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka cloud disable

Turn cloud features off

```sh
weka cloud disable [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka cloud enable

Turn cloud features on

```sh
weka cloud enable [--cloud-url cloud]
                  [--cloud-stats on/off]
                  [--color color]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--cloud-url` | The base url of the cloud service |
 | `--cloud-stats` | Enable or disable uploading stats to the cloud (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka cloud proxy

Get or set the HTTP proxy used to connect to cloud services

```sh
weka cloud proxy [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--set url]
                 [--help]
                 [--json]
                 [--unset]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-s`, `--set` | Set a new proxy setting |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-u`, `--unset` | Remove the HTTP proxy setting |

#### weka cloud status

Show cloud connectivity status

```sh
weka cloud status [--color color]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--format format]
                  [--output output]...
                  [--sort sort]...
                  [--filter filter]...
                  [--filter-color filter-color]...
                  [--help]
                  [--no-header]
                  [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: host,health (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka cloud upload-rate

Get the cloud upload rate

```sh
weka cloud upload-rate [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--help]
                       [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka cloud upload-rate set**

Set the cloud upload rate

```sh
weka cloud upload-rate set [--bytes-per-second bps]
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--bytes-per-second` | Maximum uploaded bytes per second |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

### weka cluster

Commands that manage the cluster

```sh
weka cluster [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka cluster bucket

List the cluster buckets, logical compute units used to divide the workload in the cluster

```sh
weka cluster bucket [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--format format]
                    [--output output]...
                    [--sort sort]...
                    [--filter filter]...
                    [--filter-color filter-color]...
                    [--help]
                    [--raw-units]
                    [--UTC]
                    [--no-header]
                    [--verbose]
                    [<bucket-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `bucket-ids`... | Only return these bucket IDs. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: id,leader,term,lastActiveTerm,state,council,uptime,leaderVersionSig,electableMode,sourceMembers,nonSourceMembers,fillLevel (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka cluster client-target-version

Commands that manage the clients target version

```sh
weka cluster client-target-version [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka cluster client-target-version reset**

Clear cluster's client target version value

```sh
weka cluster client-target-version reset [--color color]
                                         [--HOST HOST]
                                         [--PORT PORT]
                                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                         [--TIMEOUT TIMEOUT]
                                         [--profile profile]
                                         [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster client-target-version set**

Determine clients target version to be used in case of upgrade or a new mount (stateless client).

```sh
weka cluster client-target-version set <version-name>
                                       [--color color]
                                       [--HOST HOST]
                                       [--PORT PORT]
                                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                       [--TIMEOUT TIMEOUT]
                                       [--profile profile]
                                       [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `version-name`* | The version to set |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster client-target-version show**

Show clients target version to be used in case of upgrade or a new mount (stateless client).

```sh
weka cluster client-target-version show [--color color]
                                        [--HOST HOST]
                                        [--PORT PORT]
                                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                        [--TIMEOUT TIMEOUT]
                                        [--profile profile]
                                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka cluster container

List the cluster containers

```sh
weka cluster container [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--format format]
                       [--output output]...
                       [--sort sort]...
                       [--filter filter]...
                       [--filter-color filter-color]...
                       [--local]
                       [--backends]
                       [--clients]
                       [--leadership]
                       [--leader]
                       [--help]
                       [--raw-units]
                       [--UTC]
                       [--no-header]
                       [--verbose]
                       [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | Only return these container IDs. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,hostname,container,machineIdentifier,ips,status,requestedAction,software,release,mode,fd,fdName,fdType,fdId,cores,feCores,driveCores,coreIds,memory,bw,scrubberLimit,dedicated,autoRemove,leadership,uptime,failureText,failure,failureTime,failureCode,requestedActionFailureText,requestedActionFailure,requestedActionFailureTime,requestedActionFailureCode,added,cloudProvider,availabilityZone,instanceType,instanceId,hypervisorType,kernelName,kernelRelease,kernelVersion,platform,tlsStrictnessLevel (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--local` | Get result from local weka host |
 | `-b`, `--backends` | Only return backend containers |
 | `-c`, `--clients` | Only return client containers |
 | `-l`, `--leadership` | Only return containers that are part of the cluster leadership |
 | `-L`, `--leader` | Only return the cluster leader |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka cluster container activate**

Activate the supplied containers, or all containers (if none supplied)

```sh
weka cluster container activate [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--no-wait]
                                [--skip-resource-validation]
                                [--skip-activate-drives]
                                [--help]
                                [<container-ids>]...

```

 | Parameter | Description |
 | ---------------------------- | -------------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids to activate |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--no-wait` |  |
 | `--skip-resource-validation` | Skip verifying that the cluster will still have enough RAM and SSD resources after deactivating the containers |
 | `--skip-activate-drives` | Do not activate the drives of the container |
 | `-h`, `--help` | Show help message |

**weka cluster container add**

Add a container to the cluster

```sh
weka cluster container add <hostname>
                           [--ip ip]
                           [--host-fqdn host-fqdn]
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--no-wait]
                           [--help]
                           [--json]
                           [--raw-units]
                           [--UTC]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `hostname`* | Management network hostname |
 | `--ip` | Management IP; If both FQDN and IP are empty, the hostname is resolved; If container is highly-available or mixed-networking, use IP set '++...+'; |
 | `--host-fqdn` | Management fully qualified domain name; If both FQDN and IP are empty, hostnames will be resolved; |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--no-wait` | Skip waiting for the container to be added to the cluster |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka cluster container apply**

Apply the staged resources of the supplied containers, or all containers

```sh
weka cluster container apply [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--skip-resource-validation]
                             [--all]
                             [--force]
                             [--help]
                             [<container-ids>]...

```

 | Parameter | Description |
 | ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids for which to apply resources config |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--skip-resource-validation` | Skip verifying that the cluster will still have enough RAM and SSD resources after deactivating the containers |
 | `--all` | Apply resources on all the containers in the cluster. This will cause all backend containers in the entire cluter to restart simultaneously! |
 | `-f`, `--force` | Force this action without further confirmation. |
 | `-h`, `--help` | Show help message |

**weka cluster container auto-remove-timeout**

Set how long to wait before removing this container if it disconnects from the cluster (for clients only)

```sh
weka cluster container auto-remove-timeout <container-id>
                                           <auto-remove-timeout>
                                           [--color color]
                                           [--HOST HOST]
                                           [--PORT PORT]
                                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                           [--TIMEOUT TIMEOUT]
                                           [--profile profile]
                                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `auto-remove-timeout`* | Minimum value is 60, use 0 to disable automatic removal |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container bandwidth**

Limit weka's bandwidth for the container

```sh
weka cluster container bandwidth <container-id>
                                 <bandwidth>
                                 [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `bandwidth`* | New bandwidth limitation per second (format: either "unlimited" or bandwidth per second in binary or decimal values: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container clear-failure**

Clear the last failure fields for all supplied containers

```sh
weka cluster container clear-failure [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--help]
                                     [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids for which to clear the last failure |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container cores**

Dedicate container's cores to weka

```sh
weka cluster container cores <container-id>
                             <cores>
                             [--frontend-dedicated-cores frontend-dedicated-cores]
                             [--drives-dedicated-cores drives-dedicated-cores]
                             [--compute-dedicated-cores compute-dedicated-cores]
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--core-ids core-ids]...
                             [--no-frontends]
                             [--only-drives-cores]
                             [--only-compute-cores]
                             [--only-frontend-cores]
                             [--allow-mix-setting]
                             [--help]

```

 | Parameter | Description |
 | ---------------------------- | --------------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `cores`* | Number of CPU cores dedicated to weka - If set to 0 - no drive could be added to this container |
 | `--frontend-dedicated-cores` | Number of cores dedicated to weka frontend (out of the total ) |
 | `--drives-dedicated-cores` | Number of cores dedicated to weka drives (out of the total ) |
 | `--compute-dedicated-cores` | Number of cores dedicated to weka compute (out of the total ) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--core-ids`... | Specify the ids of weka dedicated cores. (may be repeated or comma-separated) |
 | `--no-frontends` | Do not create any processes with a frontend role |
 | `--only-drives-cores` | Create only processes with a drives role |
 | `--only-compute-cores` | Create only processes with a compute role |
 | `--only-frontend-cores` | Create only processes with a frontend role |
 | `--allow-mix-setting` | Allow specified core-ids even if there are running containers with AUTO core-ids allocation on the same server. |
 | `-h`, `--help` | Show help message |

**weka cluster container deactivate**

Deactivate the supplied container(s)

```sh
weka cluster container deactivate [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--no-wait]
                                  [--skip-resource-validation]
                                  [--allow-unavailable]
                                  [--help]
                                  [<container-ids>]...

```

 | Parameter | Description |
 | ---------------------------- | -------------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids to deactivate |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--no-wait` |  |
 | `--skip-resource-validation` | Skip verifying that the cluster will still have enough RAM and SSD resources after deactivating the containers |
 | `--allow-unavailable` | Allow the container to be unavailable while it is deactivated which skips setting its local resources |
 | `-h`, `--help` | Show help message |

**weka cluster container deactivation-check**

Check if the provided containers can be deactivated

```sh
weka cluster container deactivation-check [--color color]
                                          [--HOST HOST]
                                          [--PORT PORT]
                                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                          [--TIMEOUT TIMEOUT]
                                          [--profile profile]
                                          [--help]
                                          [--json]
                                          [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids for which to set flow |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka cluster container dedicate**

Set the container as dedicated to weka. For example it can be rebooted whenever needed, and configured by weka for optimal performance and stability

```sh
weka cluster container dedicate <container-id>
                                <on>
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `on`* | Set the container as weka dedicated, off unsets container as weka dedicated (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container failure-domain**

Set the container failure-domain

```sh
weka cluster container failure-domain <container-id>
                                      [--name name]
                                      [--color color]
                                      [--HOST HOST]
                                      [--PORT PORT]
                                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                      [--TIMEOUT TIMEOUT]
                                      [--profile profile]
                                      [--auto]
                                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `--name` | Add this container to a named failure-domain. A failure-domain will be created if it doesn't exist yet. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--auto` | Set this container to be a failure-domain of its own |
 | `-h`, `--help` | Show help message |

**weka cluster container info-hw**

Show hardware information about one or more containers

```sh
weka cluster container info-hw [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--info-type info-type]...
                               [--help]
                               [--json]
                               [--raw-units]
                               [--UTC]
                               [<hostnames>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `hostnames`... | A list of containers to query (by hostnames or IPs). If no container is supplied, all of the cluster containers will be queried |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--info-type`... | Specify what information to query: version |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka cluster container join-secret**

Set secret this container will use when joining or validating other backends

```sh
weka cluster container join-secret <container-id>
                                   <secret>
                                   [--color color]
                                   [--HOST HOST]
                                   [--PORT PORT]
                                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                   [--TIMEOUT TIMEOUT]
                                   [--profile profile]
                                   [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `secret`* | Cluster join secret |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container management-ips**

Set the container's management process IPs. Setting 2 IPs will turn this containers networking into highly-available mode

```sh
weka cluster container management-ips <container-id>
                                      [--color color]
                                      [--HOST HOST]
                                      [--PORT PORT]
                                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                      [--TIMEOUT TIMEOUT]
                                      [--profile profile]
                                      [--help]
                                      [<management-ips>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `management-ips`... | New IPs for the management processes |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container memory**

Dedicate a set amount of RAM to weka

```sh
weka cluster container memory <container-id>
                              <memory>
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `memory`* | Memory dedicated to weka in bytes, set to 0 to let the system decide (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container net**

List Weka dedicated networking devices in a container

```sh
weka cluster container net [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--format format]
                           [--output output]...
                           [--sort sort]...
                           [--filter filter]...
                           [--filter-color filter-color]...
                           [--help]
                           [--raw-units]
                           [--UTC]
                           [--no-header]
                           [--verbose]
                           [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | Container IDs to get the network devices of |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,name,id,host,hostname,device,ips,netmask,gateway,cores,owner,vlan,netlabel (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka cluster container net add**

Allocate a dedicated networking device on a container (to the cluster).

```sh
weka cluster container net add <container-id>
                               <device>
                               [--ips-type ips-type]
                               [--gateway gateway]
                               [--netmask netmask]
                               [--name name]
                               [--label label]
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--ips ips]...
                               [--help]
                               [--json]
                               [--raw-units]
                               [--UTC]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `container-id`* | The container's id |
 | `device`* | Network device pci-slot/mac-address/interface-name(s) |
 | `--ips-type` | IPs type: POOL: IPs from the default data networking IP pool would be used, USER: configured by the user (format: 'pool' or 'user') |
 | `--gateway` | Default gateway IP. In AWS this value is auto-detected, otherwise the default data networking gateway will be used. |
 | `--netmask` | Netmask in bits number. In AWS this value is auto-detected, otherwise the default data networking netmask will be used. |
 | `--name` | If empty, a name will be auto generated. |
 | `--label` | The name of the switch or network group to which this network device is attached |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--ips`... | IPs to be allocated to cores using the device. If not given - IPs may be set automatically according the interface's IPs, or taken from the default networking IPs pool (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka cluster container net remove**

Undedicate a networking device in a container.

```sh
weka cluster container net remove <container-id>
                                  <name>
                                  [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-id`* | The container's id |
 | `name`* | Net device name, e.g. container0net0 |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster container remove**

Remove a container from the cluster

```sh
weka cluster container remove <container-id>
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--no-wait]
                              [--no-unimprint]
                              [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-id`* | The container ID of the container to be removed |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--no-wait` | Don't wait for the container removal to complete, return immediately |
 | `--no-unimprint` | Don't remotely unimprint the container, just remove it from the cluster configuration |
 | `-h`, `--help` | Show help message |

**weka cluster container resources**

Get the resources of the supplied container

```sh
weka cluster container resources <container-id>
                                 [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--stable]
                                 [--help]
                                 [--json]
                                 [--raw-units]
                                 [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `container-id`* | Container ID as shown in `weka cluster container` |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--stable` | List the resources from the last successfull container boot |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka cluster container restore**

Restore staged resources of the supplied containers, or all containers, to their stable state

```sh
weka cluster container restore [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--all]
                               [--help]
                               [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids for which to apply resources config |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--all` | Apply resources on all the containers in the cluster. This will cause all backend containers in the entire cluter to restart simultaneously! |
 | `-h`, `--help` | Show help message |

#### weka cluster add

Form a Weka cluster from hosts that just had Weka installed on them

```sh
weka cluster add [--admin-password admin-password]
                 [--join-secret join-secret]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--host-ips host-ips]...
                 [--host-fqdn host-fqdn]...
                 [--overwrite_resource_ips]
                 [--help]
                 [--json]
                 [<host-hostnames>]...

```

 | Parameter | Description |
 | -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `host-hostnames`... | A list of hostnames to be included in the new cluster |
 | `--admin-password` | The password for the cluster admin user; will be set to the default password if not provided |
 | `--join-secret` | Initial join secret |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--host-ips`... | Management IP addresses; If both FQDN and IPs are empty, the hostnames will be resolved; If hosts are highly-available or mixed-networking, use IP set '++...+'; (may be repeated or comma-separated) |
 | `--host-fqdn`... | Management fully qualified domain name; If both FQDNs and IPs are empty, hostnames will be resolved; (may be repeated or comma-separated) |
 | `--overwrite_resource_ips` | overwrite pre-set ips in host's resource file with --host-ips set up provided by user or resolved from DNS |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka cluster default-net

List the default data networking configuration

```sh
weka cluster default-net [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]
                         [--json]
                         [--raw-units]
                         [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka cluster default-net reset**

Reset the default data networking configuration

```sh
weka cluster default-net reset [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster default-net set**

Set the default data networking configuration

```sh
weka cluster default-net set [--range range]
                             [--gateway gateway]
                             [--netmask-bits netmask-bits]
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--range` | IP range |
 | `--gateway` | Default gateway IP |
 | `--netmask-bits` | Subnet mask bits (0..32) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster default-net update**

Update the default data networking configuration

```sh
weka cluster default-net update [--range range]
                                [--gateway gateway]
                                [--netmask-bits netmask-bits]
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--range` | IP range |
 | `--gateway` | Default gateway IP |
 | `--netmask-bits` | Subnet mask bits (0..32) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka cluster drive

List the cluster's drives

```sh
weka cluster drive [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--format format]
                   [--container container]...
                   [--output output]...
                   [--sort sort]...
                   [--filter filter]...
                   [--filter-color filter-color]...
                   [--show-removed]
                   [--help]
                   [--raw-units]
                   [--UTC]
                   [--no-header]
                   [--verbose]
                   [<uuids>]...

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `uuids`... | A list of drive IDs or UUIDs to list. If no ID is specified, all drives are listed. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--container`... | Only return the drives of these container IDs, if not specified, all drives are listed (may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,uuid,host,hostname,node,path,size,status,stime,fdName,fdId,writable,used,nvkvused,attachment,vendor,firmware,serial,model,added,removed,block,remain,threshold,pool,drive_status_message,pci_vid,pci_id,pci_ssvid,pci_ssid (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--show-removed` | Show drives that were removed from the cluster |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka cluster drive activate**

Activate the supplied drive, or all drives (if none supplied)

```sh
weka cluster drive activate [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]
                            [--raw-units]
                            [--UTC]
                            [<uuids>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `uuids`... | A list of drive IDs or UUIDs to activate. If no ID is supplied, all inactive drives will be activated. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka cluster drive add**

Add the given drive

```sh
weka cluster drive add <container-id>
                       [--pool pool]
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--format format]
                       [--output output]...
                       [--sort sort]...
                       [--filter filter]...
                       [--filter-color filter-color]...
                       [--force]
                       [--allow-format-non-wekafs-drives]
                       [--help]
                       [--raw-units]
                       [--UTC]
                       [--no-header]
                       [--verbose]
                       [<device-paths>]...

```

 | Parameter | Description |
 | ---------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `container-id`* | The container the drive attached to (given by ids) |
 | `device-paths`... | Device paths of the drives to add |
 | `--pool` | Specify disk pool to add into (format: 'auto', 'iu4k', 'iubig' or 'legacy') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: path,uuid (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--force` | Force formatting the drive for weka, avoiding all safety checks! |
 | `--allow-format-non-wekafs-drives` | Allow reuse of drives formatted by another versions |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka cluster drive deactivate**

Deactivate the supplied drive(s)

```sh
weka cluster drive deactivate [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--skip-resource-validation]
                              [--force]
                              [--help]
                              [<uuids>]...

```

 | Parameter | Description |
 | ---------------------------- | ------------------------------------------------------------------------------------------------------------------ |
 | `uuids`... | A list of drive IDs or UUIDs to deactivate. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--skip-resource-validation` | Skip verifying that the configured hot spare capacity will remain available after deactivating the drives |
 | `-f`, `--force` | Force this action without further confirmation. This action may impact performance while the drive is phasing out. |
 | `-h`, `--help` | Show help message |

**weka cluster drive remove**

Remove the supplied drive(s)

```sh
weka cluster drive remove [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--force]
                          [--help]
                          [<uuids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
 | `uuids`... | A list of drive UUIDs to remove. A UUID is a hex string formatted as 8-4-4-4-12 e.g. 'abcdef12-1234-abcd-1234-1234567890ab' |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. To undo the removal, add the drive back and re-scan the drives on the host local to the drive. |
 | `-h`, `--help` | Show help message |

**weka cluster drive scan**

Scan for provisioned drives on the cluster's containers

```sh
weka cluster drive scan [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]
                        [--raw-units]
                        [--UTC]
                        [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids to scan for drives |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka cluster failure-domain

List the Weka cluster failure domains

```sh
weka cluster failure-domain [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--format format]
                            [--output output]...
                            [--sort sort]...
                            [--filter filter]...
                            [--filter-color filter-color]...
                            [--show-removed]
                            [--help]
                            [--raw-units]
                            [--UTC]
                            [--no-header]
                            [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,fd,active_drives,failed_drives,total_drives,removed_drives,containers,total_containers,drive_proces,total_drive_proces,compute_proces,total_compute_proces,capacity (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--show-removed` | Show drives that were removed from the cluster |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka cluster hot-spare

Get or set the number of hot-spare failure-domains in the cluster. If param is not given, the current number of hot-spare FDs will be listed

```sh
weka cluster hot-spare [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--skip-resource-validation]
                       [--help]
                       [--json]
                       [--raw-units]
                       [--UTC]
                       [<count>]...

```

 | Parameter | Description |
 | ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `count`... | The number of failure-domains |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--skip-resource-validation` | Skip verifying that the cluster has enough RAM and SSD resources allocated for the hot-spare |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka cluster license

Get information about the current license status, how much resources are being used in the cluster and whether or not your current license is valid.

```sh
weka cluster license [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--help]
                     [--json]
                     [--raw-units]
                     [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka cluster license reset**

Removes existing license information, returning the cluster to an unlicensed mode

```sh
weka cluster license reset [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster license set**

Set the cluster license

```sh
weka cluster license set <license>
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `license`* | The new license to set to the system |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka cluster mount-defaults

Commands for editing default mount options

```sh
weka cluster mount-defaults [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka cluster mount-defaults reset**

Reset default mount options

```sh
weka cluster mount-defaults reset [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--qos-max-throughput]
                                  [--qos-preferred-throughput]
                                  [--qos-max-ops]
                                  [--help]

```

 | Parameter | Description |
 | ---------------------------- | ----------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--qos-max-throughput` | qos-max-throughput is the maximum throughput allowed for the client for either receive or transmit traffic. |
 | `--qos-preferred-throughput` | qos-preferred-throughput is the throughput that gets preferred state (NORMAL instead of LOW) in QoS. |
 | `--qos-max-ops` | qos-max-ops is the maximum number of operations of any kind for the client |
 | `-h`, `--help` | Show help message |

**weka cluster mount-defaults set**

Set default mount options.

```sh
weka cluster mount-defaults set [--qos-max-throughput qos-max-throughput]
                                [--qos-preferred-throughput qos-preferred-throughput]
                                [--qos-max-ops qos-max-ops]
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]

```

 | Parameter | Description |
 | ---------------------------- | ----------------------------------------------------------------------------------------------------------- |
 | `--qos-max-throughput` | qos-max-throughput is the maximum throughput allowed for the client for either receive or transmit traffic. |
 | `--qos-preferred-throughput` | qos-preferred-throughput is the throughput that gets preferred state (NORMAL instead of LOW) in QoS. |
 | `--qos-max-ops` | qos-max-ops is the maximum number of operations of any kind for the client |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster mount-defaults show**

View default mount options

```sh
weka cluster mount-defaults show [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--json]
                                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-J`, `--json` | Format output as JSON |
 | `-h`, `--help` | Show help message |

#### weka cluster process

List the cluster processes

```sh
weka cluster process [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--format format]
                     [--container container]...
                     [--role role]...
                     [--output output]...
                     [--sort sort]...
                     [--filter filter]...
                     [--filter-color filter-color]...
                     [--local]
                     [--backends]
                     [--clients]
                     [--leadership]
                     [--leader]
                     [--help]
                     [--raw-units]
                     [--UTC]
                     [--no-header]
                     [--verbose]
                     [<process-ids>]...

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `process-ids`... | Only return these processes IDs. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--container`... | Only return the processes of these container IDs, if not specified the weka-processes for all the containers will be returned (may be repeated or comma-separated) |
 | `--role`... | Only return processes with these roles (format: 'management', 'frontend', 'compute', 'drives' or 'dataserv', may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,containerId,slot,hostname,container,ips,status,software,release,role,mode,netmode,cpuId,core,socket,numa,cpuModel,memory,uptime,fdName,fdId,traceHistory,fencingReason,joinRejectReason,failureText,failure,failureTime,failureCode,blackListingReason,blackListingTime (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--local` | Get result from local weka host |
 | `-b`, `--backends` | Only return backend containers |
 | `-c`, `--clients` | Only return client containers |
 | `-l`, `--leadership` | Only return containers that are part of the cluster leadership |
 | `-L`, `--leader` | Only return the cluster leader |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka cluster requested-action

Requested Action CLI

```sh
weka cluster requested-action [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka cluster requested-action elective-protection**

Set the elective protection level of the cluster

```sh
weka cluster requested-action elective-protection <value>
                                                  [--color color]
                                                  [--HOST HOST]
                                                  [--PORT PORT]
                                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                                  [--TIMEOUT TIMEOUT]
                                                  [--profile profile]
                                                  [--help]
                                                  [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `value`* | The elective protection level to set. Must be a positive integer (default: 2) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka cluster requested-action set**

Set the requested action of the supplied containers to one of: STOP, RESTART, APPLY_RESOURCES to gracefully stop, restart or apply resources to the containers.

```sh
weka cluster requested-action set <requested_action>
                                  [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--help]
                                  [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------ |
 | `requested_action`* | The requested action to set the containers to (format: 'none', 'stop', 'restart', 'apply_resources' or 'upgrade') |
 | `container-ids`... | A list of container ids for which to set flow |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka cluster servers

Commands for physical servers

```sh
weka cluster servers [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka cluster servers list**

List the cluster servers

```sh
weka cluster servers list [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--format format]
                          [--role role]...
                          [--output output]...
                          [--sort sort]...
                          [--filter filter]...
                          [--filter-color filter-color]...
                          [--help]
                          [--raw-units]
                          [--UTC]
                          [--no-header]
                          [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--role`... | Only list machines with specified roles. Possible roles: (format: 'backend', 'client', 'nfs', 'smb' or 's3', may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: hostname,uid,ip,roles,status,up_since,cores,memory,drives,nodes,load,versions,architecture (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka cluster servers show**

Show a single server overview according to given server uid

```sh
weka cluster servers show <uid>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--json]
                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `uid`* | The Server UID |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-J`, `--json` | Format output as JSON |
 | `-h`, `--help` | Show help message |

#### weka cluster start-io

Start IO services

```sh
weka cluster start-io [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka cluster stop-io

Stop IO services

```sh
weka cluster stop-io [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--brutal-no-flush]
                     [--keep-external-containers]
                     [--force]
                     [--help]
                     [--json]

```

 | Parameter | Description |
 | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--brutal-no-flush` | Force stopping IO services immediately without graceful flushing of ongoing operations. Using this flag may cause data-loss if used without explicit guidance from WekaIO customer support. |
 | `--keep-external-containers` | Keep external containers(S3, SMB, NFS) running |
 | `-f`, `--force` | Force this action without further confirmation. This action will disrupt operation of all connected clients. To restore IO service run 'weka cluster start-io'. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka cluster task

List the currently running background tasks and their status

```sh
weka cluster task [--color color]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--format format]
                  [--output output]...
                  [--sort sort]...
                  [--filter filter]...
                  [--filter-color filter-color]...
                  [--show-waiting]
                  [--help]
                  [--raw-units]
                  [--UTC]
                  [--no-header]
                  [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,type,state,phase,progress,paused,desc,time,throttle (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--show-waiting` | Show waiting tasks |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka cluster task abort**

Abort a currently running background task

```sh
weka cluster task abort <task-id>
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `task-id`* | Id of the task to abort |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster task bucket**

List the status of a background task on specific buckets

```sh
weka cluster task bucket [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--format format]
                         [--task task]...
                         [--bucket bucket]...
                         [--output output]...
                         [--sort sort]...
                         [--filter filter]...
                         [--filter-color filter-color]...
                         [--help]
                         [--raw-units]
                         [--UTC]
                         [--no-header]
                         [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--task`... | Only return these task IDs. (may be repeated or comma-separated) |
 | `-b`, `--bucket`... | Only return these bucket IDs. (may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: bucket_id,task_id,type,phase,progress,lastProgress,pausedAtGeneration (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka cluster task limits**

List the current limits for background tasks

```sh
weka cluster task limits [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]
                         [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka cluster task limits set**

Set the limits for background tasks

```sh
weka cluster task limits set [--cpu-limit cpu-limit]
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--cpu-limit` | Percent of the CPU resources to dedicate to background tasks (format: 0..100) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster task pause**

Pause a currently running background task

```sh
weka cluster task pause <task-id>
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `task-id`* | Id of the task to pause |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster task resume**

Resume a currently paused background task

```sh
weka cluster task resume <task-id>
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `task-id`* | Id of the task to resume |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka cluster task throttle**

Slow down the rate of progress of a currently running background task

```sh
weka cluster task throttle <task-id>
                           [--throttle throttle]
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `task-id`* | Id of the task to throttle |
 | `--throttle` | Percentage by which to throttle the task (between 0 and 100) (format: 0..100) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka cluster update

Update cluster configuration

```sh
weka cluster update [--cluster-name cluster-name]
                    [--data-drives data-drives]
                    [--parity-drives parity-drives]
                    [--scrubber-bytes-per-sec scrubber-bytes-per-sec]
                    [--bucket-raft-size bucket-raft-size]
                    [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--help]

```

 | Parameter | Description |
 | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--cluster-name` | Cluster name |
 | `--data-drives` | Number of RAID data drives |
 | `--parity-drives` | Number of RAID protection parity drives |
 | `--scrubber-bytes-per-sec` | Rate of RAID scrubbing in units per second (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--bucket-raft-size` | Number of members in the buckets raft group |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

### weka dataservice

Commands that manage dataservice

```sh
weka dataservice [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka dataservice global-config

Dataservice Global Configuration

```sh
weka dataservice global-config [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka dataservice global-config set**

Set Dataservice global configuration options

```sh
weka dataservice global-config set [--config-fs config-fs]
                                   [--color color]
                                   [--HOST HOST]
                                   [--PORT PORT]
                                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                   [--TIMEOUT TIMEOUT]
                                   [--profile profile]
                                   [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--config-fs` | config filesystem name, use "" to invalidate |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka dataservice global-config show**

Show the Dataservice global configuration

```sh
weka dataservice global-config show [--color color]
                                    [--HOST HOST]
                                    [--PORT PORT]
                                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                    [--TIMEOUT TIMEOUT]
                                    [--profile profile]
                                    [--help]
                                    [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

### weka diags

Diagnostics commands to help understand the status of the cluster and its environment

```sh
weka diags [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka diags collect

Collect diags from all cluster containers to a directory on the container running this command

```sh
weka diags collect [--id id]
                   [--timeout timeout]
                   [--output-dir output-dir]
                   [--core-limit core-limit]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--color color]
                   [--container-id container-id]...
                   [--clients]
                   [--backends]
                   [--tar]
                   [--verbose]
                   [--help]
                   [--raw-units]
                   [--UTC]
                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `-i`, `--id` | Optional ID for this dump, if not specified a random ID is generated |
 | `-m`, `--timeout` | How long to wait when downloading diags from all containers. Default is 10 minutes, 0 means indefinite (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-d`, `--output-dir` | Directory to save the diags dump to, default: /opt/weka/diags |
 | `-c`, `--core-limit` | Limit to processing this number of core dumps, if found (default: 1) |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--container-id`... | Container IDs to collect diags from, can be used multiple times. This flag causes --clients to be ignored. (may be repeated or comma-separated) |
 | `--clients` | Collect diags from client containers only (by default diags are only collected from backends) |
 | `--backends` | Collect diags from backend containers (to be used in combination with --clients to collect from all containers) |
 | `-t`, `--tar` | Create a TAR of all collected diags |
 | `-v`, `--verbose` | Print results of all diags, including successful ones |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `-J`, `--json` | Format output as JSON |

#### weka diags list

Prints results of a previously collected diags report

```sh
weka diags list [--color color] [--verbose] [--help] [<id>]...

```

 | Parameter | Description |
 | ----------------- | ------------------------------------------------------------------------------------------------------------ |
 | `id`... | ID of the dump to show or a path to the diags dump. If not specified a list of all collected diags is shown. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-v`, `--verbose` | Print results of all diags, including successful ones |
 | `-h`, `--help` | Show help message |

#### weka diags rm

Stop a running instance of diags, and cancel its uploads.

```sh
weka diags rm [--HOST HOST]
              [--PORT PORT]
              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
              [--TIMEOUT TIMEOUT]
              [--profile profile]
              [--color color]
              [--all]
              [--help]
              [<id>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `id`... | ID of the diags to cancel. Must be specified unless the all option is set. |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--all` | Delete all. |
 | `-h`, `--help` | Show help message |

#### weka diags upload

Collect and upload diags from all cluster containers to Weka's support cloud

```sh
weka diags upload [--timeout timeout]
                  [--core-limit core-limit]
                  [--dump-id dump-id]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--color color]
                  [--container-id container-id]...
                  [--clients]
                  [--backends]
                  [--tar]
                  [--help]
                  [--json]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
 | `-m`, `--timeout` | How long to wait for diags to upload. Default is 10 minutes, 0 means indefinite (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-c`, `--core-limit` | Limit to processing this number of core dumps, if found (default: 1) |
 | `--dump-id` | ID of an existing dump to upload. This dump ID has to exist on this local server. If an ID is not specified, a new dump is created. |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--container-id`... | Container IDs to collect diags from, can be used multiple times. This flag causes --clients to be ignored. (may be repeated or comma-separated) |
 | `--clients` | Collect diags from client containers only (by default diags are only collected from backends) |
 | `--backends` | Collect diags from backend containers (to be used in combination with --clients to collect from all containers) |
 | `--tar` | Tar and compress local files |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

### weka driver

Manage Weka drivers

```sh
weka driver [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka driver build

Compiles drivers for the machine where this command is executed.

```sh
weka driver build [--color color] [--version version] [--help]

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-V`, `--version` | Weka version for drivers. |
 | `-h`, `--help` | Show help message |

#### weka driver download

Downloads drivers from a distribution server.

```sh
weka driver download [--color color] [--version version] [--kernel-signature kernel-signature] [--from from]... [--help]

```

 | Parameter | Description |
 | -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-V`, `--version` | Weka version for drivers. |
 | `-K`, `--kernel-signature` | Kernel signature for drivers. |
 | `--from`... | Download from this distribution server (can be given multiple times). Otherwise distribution servers are taken from the $WEKA_DIST_SERVERS environment variable, the /etc/wekaio/dist-servers file, or /etc/wekaio/service.conf in that order of precedence. (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

#### weka driver export

Exports drivers from this machine to an archive.

```sh
weka driver export <path> [--color color] [--version version] [--kernel-signature kernel-signature] [--help]

```

 | Parameter | Description |
 | -------------------------- | -------------------------------------------------------------------------------- |
 | `path`* | Path of the output archive, will be in .zip format. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-V`, `--version` | Weka version for drivers. |
 | `-K`, `--kernel-signature` | Kernel signature for drivers. |
 | `-h`, `--help` | Show help message |

#### weka driver import

Imports drivers from a previously exported archive to this machine.

```sh
weka driver import <path> [--color color] [--overwrite] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `path`* | The path of the importing tar file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--overwrite` | Overwrite existing drivers |
 | `-h`, `--help` | Show help message |

#### weka driver install

Installs drivers on the machine where this command is executed.

```sh
weka driver install [--color color] [--version version] [--help]

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-V`, `--version` | Weka version for drivers. |
 | `-h`, `--help` | Show help message |

#### weka driver kernel

Shows the kernel signature of the system. This signature is used to identify the specific kernel.

```sh
weka driver kernel [--color color] [--help] [--json]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka driver pack

Creates driver package.

```sh
weka driver pack [--color color] [--version version] [--kernel-signature kernel-signature] [--help]

```

 | Parameter | Description |
 | -------------------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-V`, `--version` | Weka version for drivers. |
 | `-K`, `--kernel-signature` | Kernel signature for drivers. |
 | `-h`, `--help` | Show help message |

#### weka driver ready

Checks if kernel drivers are loaded and ready for Weka.

```sh
weka driver ready [--color color] [--version version] [--help] [--json] [--quiet]

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-V`, `--version` | Weka version for drivers. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-q`, `--quiet` | Checks quietly (exit status 0 if drivers are ready, 1 otherwise). |

#### weka driver sign

Signs drivers with a private key.

```sh
weka driver sign <key>
                 [cert]
                 [--hash hash]
                 [--color color]
                 [--version version]
                 [--kernel-signature kernel-signature]
                 [--pack]
                 [--help]

```

 | Parameter | Description |
 | -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `key`* | Path to the private key file. |
 | `cert` | Path to the certificate file. |
 | `passwd`* | Password for private key file. (may also be supplied by $WEKA_SIGNING_KEY_PASSWORD, or in the file named by $WEKA_SIGNING_KEY_PASSWORD_FILE) |
 | `--hash` | Hash algorithm used for signing modules. (format: 'sha256', 'sha384' or 'sha512') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-V`, `--version` | Weka version for drivers. |
 | `-K`, `--kernel-signature` | Kernel signature for drivers. |
 | `--pack` | Sign driver package. |
 | `-h`, `--help` | Show help message |

### weka events

List all events that conform to the filter criteria

```sh
weka events [--num-results num-results]
            [--start-time <start>]
            [--end-time <end>]
            [--severity severity]
            [--direction direction]
            [--color color]
            [--HOST HOST]
            [--PORT PORT]
            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
            [--TIMEOUT TIMEOUT]
            [--profile profile]
            [--format format]
            [--type-list type-list]...
            [--exclude-type-list exclude-type-list]...
            [--category-list category-list]...
            [--output output]...
            [--show-internal]
            [--cloud-time]
            [--help]
            [--raw-units]
            [--UTC]
            [--no-header]
            [--verbose]

```

 | Parameter | Description |
 | ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `-n`, `--num-results` | Get up to this number of events, default: 50 |
 | `--start-time` | Include events occurred in this time point and later (format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00) |
 | `--end-time` | Include events occurred not later then this time point (format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00) |
 | `--severity` | Include event with equal and higher severity, default: INFO (format: 'debug', 'info', 'warning', 'minor', 'major' or 'critical') |
 | `-d`, `--direction` | Fetch events from the first available event (forward) or the latest created event (backward), default: backward (format: 'forward' or 'backward') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-t`, `--type-list`... | Filter events by type, can be used multiple times (use 'weka events list-types' to see available types) (may be repeated or comma-separated) |
 | `-x`, `--exclude-type-list`... | Remove events by type, can be used multiple times (use 'weka events list-types' to see available types) (may be repeated or comma-separated) |
 | `-c`, `--category-list`... | Include only events matches to the category_list. Category can be Events, Node, Raid, Drive, ObjectStorage, System, Resources, Clustering, Network, Filesystem, Upgrade, NFS, Config, Cloud, InterfaceGroup, Org, User, Alerts, Licensing, Custom, Kms, Smb, Telemetry, Traces, S3, Security, Agent or KDriver (may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: time,cloudTime,node,category,severity,type,entity,desc (may be repeated or comma-separated) |
 | `-i`, `--show-internal` | Show internal events |
 | `-l`, `--cloud-time` | Sort by cloud time instead of local timestamp |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka events list-local

List recent events that happened on the machine running this command

```sh
weka events list-local [--start-time <start>]
                       [--end-time <end>]
                       [--next next]
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--format format]
                       [--output output]...
                       [--sort sort]...
                       [--filter filter]...
                       [--filter-color filter-color]...
                       [--stem-mode]
                       [--show-internal]
                       [--help]
                       [--raw-units]
                       [--UTC]
                       [--no-header]
                       [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--start-time` | Include events occurred in this time point and later (format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00) |
 | `--end-time` | Include events occurred not later then this time point (format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00) |
 | `--next` | Token for the next page of events |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: time,category,severity,permission,type,entity,node,hash (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--stem-mode` | List stem mode events |
 | `--show-internal` | Show internal events |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka events list-types

Show the event type definition information

```sh
weka events list-types [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--format format]
                       [--category category]...
                       [--type type]...
                       [--output output]...
                       [--sort sort]...
                       [--filter filter]...
                       [--filter-color filter-color]...
                       [--show-internal]
                       [--help]
                       [--no-header]
                       [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-c`, `--category`... | List only the events that fall under one of the following categories: Events, Node, Raid, Drive, ObjectStorage, System, Resources, Clustering, Network, Filesystem, Upgrade, NFS, Config, Cloud, InterfaceGroup, Org, User, Alerts, Licensing, Custom, Kms, Smb, Telemetry, Traces, S3, Security, Agent or KDriver (may be repeated or comma-separated) |
 | `-t`, `--type`... | List only events of the specified types (may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: type,category,severity,description,format,permission,parameters,dedup,dedupParams (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--show-internal` | Show internal events |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka events trigger-event

Trigger a custom event with a user defined parameter

```sh
weka events trigger-event <message>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `message`* | User defined text to trigger as the events parameter |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

### weka fs

List filesystems defined in this Weka cluster

```sh
weka fs [--name name]
        [--color color]
        [--HOST HOST]
        [--PORT PORT]
        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
        [--TIMEOUT TIMEOUT]
        [--profile profile]
        [--format format]
        [--output output]...
        [--sort sort]...
        [--filter filter]...
        [--filter-color filter-color]...
        [--capacities]
        [--force-fresh]
        [--help]
        [--raw-units]
        [--UTC]
        [--no-header]
        [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Filesystem name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,name,group,usedSSD,usedSSDD,usedSSDM,freeSSD,availableSSDM,availableSSD,usedTotal,usedTotalD,freeTotal,availableTotal,maxFiles,status,encrypted,stores,auth,thinProvisioned,thinProvisioningMinSSDBudget,thinProvisioningMaxSSDBudget,usedSSDWD,usedSSDRD,reductionRatio,pendingReduction,dataReduction,reducedProcessedSize,reducedSize,kmsKey,kmsNamespace,kmsRole,processedReductionRatio (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--capacities` | Display all capacity columns |
 | `--force-fresh` | Refresh the capacities to make sure they are most updated |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka fs add

Create a filesystem

```sh
weka fs add <name>
            <group-name>
            <total-capacity>
            [--obs-name obs-name]
            [--ssd-capacity ssd-capacity]
            [--thin-provision-min-ssd thin-provision-min-ssd]
            [--thin-provision-max-ssd thin-provision-max-ssd]
            [--audit]
            [--kms-key-identifier kms-key-identifier]
            [--kms-namespace kms-namespace]
            [--kms-role-id kms-role-id]
            [--kms-secret-id kms-secret-id]
            [--auth-required auth-required]
            [--color color]
            [--HOST HOST]
            [--PORT PORT]
            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
            [--TIMEOUT TIMEOUT]
            [--profile profile]
            [--encrypted]
            [--allow-no-kms]
            [--data-reduction]
            [--help]
            [--json]
            [--raw-units]
            [--UTC]

```

 | Parameter | Description |
 | -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `group-name`* | Group name |
 | `total-capacity`* | Total capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--obs-name` | Object Store bucket name. Mandatory for tiered filesystems |
 | `--ssd-capacity` | SSD capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--thin-provision-min-ssd` | Thin provisioned minimum SSD capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--thin-provision-max-ssd` | Thin provisioned maximum SSD capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--audit` | Forwards this filesystem's audit logs to a configured events monitoring platform, provided that cluster-wide auditing is also enabled |
 | `--kms-key-identifier` | Customize KMS key identifier for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-namespace` | Customize KMS namespace for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-role-id` | Customize KMS role-id for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-secret-id` | Customize KMS secret-id for this filesystem (currently only for HashiCorp Vault) |
 | `--auth-required` | Require the mounting user to be authenticated for mounting this filesystem. This flag is only effective in the root organization, users in non-root organizations must be authenticated to perform a mount operation. (format: 'yes' or 'no') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--encrypted` | Creates an encrypted filesystem |
 | `--allow-no-kms` | Allow (insecurely) creating an encrypted filesystem without a KMS configured |
 | `--data-reduction` | Enable data reduction |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka fs remove

Delete a filesystem

```sh
weka fs remove <name>
               [--color color]
               [--HOST HOST]
               [--PORT PORT]
               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
               [--TIMEOUT TIMEOUT]
               [--profile profile]
               [--purge-from-obs]
               [--force]
               [--help]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--purge-from-obs` | Delete filesystem's objects from the local writable Object Store, making all locally uploaded snapshots unusable |
 | `-f`, `--force` | Force this action without further confirmation. This action DELETES ALL DATA in the filesystem and cannot be undone. |
 | `-h`, `--help` | Show help message |

#### weka fs download

Download a filesystem from object store

```sh
weka fs download <name>
                 <group-name>
                 <total-capacity>
                 <ssd-capacity>
                 <obs-bucket>
                 <locator>
                 [--audit Enable audit logging for this filesystem. Takes effect only if audit is enabled cluster-wide]
                 [--auth-required auth-required]
                 [--additional-obs-bucket additional-obs-bucket]
                 [--snapshot-name snapshot-name]
                 [--access-point access-point]
                 [--kms-key-identifier kms-key-identifier]
                 [--kms-namespace kms-namespace]
                 [--kms-role-id kms-role-id]
                 [--kms-secret-id kms-secret-id]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--skip-resource-validation]
                 [--help]
                 [--json]
                 [--raw-units]
                 [--UTC]

```

 | Parameter | Description |
 | ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `group-name`* | Group name |
 | `total-capacity`* | Total capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `ssd-capacity`* | SSD capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `obs-bucket`* | Object Store bucket |
 | `locator`* | Locator |
 | `--audit` | Enable audit |
 | `--auth-required` | Require the mounting user to be authenticated for mounting this filesystem. This flag is only effective in the root organization, users in non-root organizations must be authenticated to perform a mount operation. (format: 'yes' or 'no') |
 | `--additional-obs-bucket` | Additional Object Store bucket |
 | `--snapshot-name` | Downloaded snapshot name (default: uploaded name) |
 | `--access-point` | Downloaded snapshot access point (default: uploaded access-point) |
 | `--kms-key-identifier` | Customize KMS key name for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-namespace` | Customize KMS namespace for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-role-id` | Customize KMS role-id for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-secret-id` | Customize KMS secret-id for this filesystem (currently only for HashiCorp Vault) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--skip-resource-validation` | Skip verifying that the cluster has enough RAM and SSD resources allocated for the downloaded filesystem |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka fs group

List filesystem groups

```sh
weka fs group [--color color]
              [--HOST HOST]
              [--PORT PORT]
              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
              [--TIMEOUT TIMEOUT]
              [--profile profile]
              [--format format]
              [--output output]...
              [--sort sort]...
              [--filter filter]...
              [--filter-color filter-color]...
              [--help]
              [--raw-units]
              [--UTC]
              [--no-header]
              [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,group,name,retention,demote (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs group add**

Create a filesystem group

```sh
weka fs group add <name>
                  [--target-ssd-retention target-ssd-retention]
                  [--start-demote start-demote]
                  [--color color]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--help]
                  [--json]
                  [--raw-units]
                  [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | The filesystem group name to be created |
 | `--target-ssd-retention` | Period of time to keep an SSD copy of the data (format: 3s, 2h, 4m, 1d, 1d5h, 1w) |
 | `--start-demote` | Period of time to wait before copying data to the Object Store (format: 3s, 2h, 4m, 1d, 1d5h, 1w) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs group remove**

Delete a filesystem group

```sh
weka fs group remove <name>
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | The name of the filesystem group to be deleted |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka fs group update**

Update a filesystem group

```sh
weka fs group update <name>
                     [--new-name new-name]
                     [--target-ssd-retention target-ssd-retention]
                     [--start-demote start-demote]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | The filesystem group name to be created |
 | `--new-name` | Updated name of the specified filesystem group |
 | `--target-ssd-retention` | Period of time to keep an SSD copy of the data (format: 3s, 2h, 4m, 1d, 1d5h, 1w) |
 | `--start-demote` | Period of time to wait before copying data to the Object Store (format: 3s, 2h, 4m, 1d, 1d5h, 1w) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka fs kms-rewrap

Rewrap the key of Filesystem

```sh
weka fs kms-rewrap <name>
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--help]
                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka fs protection

Commands used to manage file system protection

```sh
weka fs protection [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka fs protection snapshot-policy**

Snapshot policy management commands

```sh
weka fs protection snapshot-policy [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka fs protection snapshot-policy attach**

Attach existing filesystems to snapshot policy

```sh
weka fs protection snapshot-policy attach <name>
                                          [--color color]
                                          [--HOST HOST]
                                          [--PORT PORT]
                                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                          [--TIMEOUT TIMEOUT]
                                          [--profile profile]
                                          [--help]
                                          [--raw-units]
                                          [--UTC]
                                          [<filesystems>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | The snapshot policy name |
 | `filesystems`... | A list of filesystems you want to attach to the policy |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs protection snapshot-policy add**

Create a new snapshot policy

```sh
weka fs protection snapshot-policy add <name>
                                       <path>
                                       [--description description]
                                       [--enabled enabled]
                                       [--color color]
                                       [--HOST HOST]
                                       [--PORT PORT]
                                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                       [--TIMEOUT TIMEOUT]
                                       [--profile profile]
                                       [--help]
                                       [--json]
                                       [--raw-units]
                                       [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | The snapshot policy name (up to 12 alphanumeric characters, hyphens (-), underscores (_), and periods (.)) |
 | `path`* | The path to the snapshot policy file, must be in JSON format |
 | `--description` | Policy description (up to 128 characters) |
 | `--enabled` | Set snapshot policy status, can be true/false , default is true |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs protection snapshot-policy remove**

Delete a snapshot policy

```sh
weka fs protection snapshot-policy remove <name>
                                          [--color color]
                                          [--HOST HOST]
                                          [--PORT PORT]
                                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                          [--TIMEOUT TIMEOUT]
                                          [--profile profile]
                                          [--force]
                                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------- |
 | `name`* | Existing snapshot policy name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action deletes the snapshot policy and cannot be undone. |
 | `-h`, `--help` | Show help message |

**weka fs protection snapshot-policy detach**

Detach existing filesystems from the snapshot policy

```sh
weka fs protection snapshot-policy detach <name>
                                          [--color color]
                                          [--HOST HOST]
                                          [--PORT PORT]
                                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                          [--TIMEOUT TIMEOUT]
                                          [--profile profile]
                                          [--remove-waiting-tasks]
                                          [--help]
                                          [--raw-units]
                                          [--UTC]
                                          [--force]
                                          [<filesystems>]...

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | The snapshot policy name |
 | `filesystems`... | A list of filesystems you want to detach from the policy |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--remove-waiting-tasks` | Allow to delete all waiting tasks corresponding to the filesystems. |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `-f`, `--force` | Force this action without further confirmation. This action detach existing filesystem from the snapshot policy and cannot be undone. |

**weka fs protection snapshot-policy duplicate**

Duplicates an existing snapshot policy, creating a new one.

```sh
weka fs protection snapshot-policy duplicate <policyName>
                                             <name>
                                             [--description description]
                                             [--include-attached-filesystems include-attached-filesystems]
                                             [--color color]
                                             [--HOST HOST]
                                             [--PORT PORT]
                                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                             [--TIMEOUT TIMEOUT]
                                             [--profile profile]
                                             [--help]
                                             [--json]

```

 | Parameter | Description |
 | -------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
 | `policyName`* | Name of the snapshot policy to duplicate. |
 | `name`* | Name of the new snapshot policy. (up to 12 alphanumeric characters, hyphens (-), underscores (_), and periods (.)) |
 | `--description` | Policy description (up to 128 characters) |
 | `--include-attached-filesystems` | Define whether or not to include the attached filesystems |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka fs protection snapshot-policy export**

Export snapshot policy configuration, use policy sys-default to export the cluster default configuration

```sh
weka fs protection snapshot-policy export <name>
                                          <path>
                                          [--color color]
                                          [--HOST HOST]
                                          [--PORT PORT]
                                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                          [--TIMEOUT TIMEOUT]
                                          [--profile profile]
                                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | The snapshot policy you want to export |
 | `path`* | The path where export policy file will be located |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka fs protection snapshot-policy list**

List snapshot policies

```sh
weka fs protection snapshot-policy list [--color color]
                                        [--HOST HOST]
                                        [--PORT PORT]
                                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                        [--TIMEOUT TIMEOUT]
                                        [--profile profile]
                                        [--format format]
                                        [--output output]...
                                        [--sort sort]...
                                        [--filter filter]...
                                        [--filter-color filter-color]...
                                        [--help]
                                        [--raw-units]
                                        [--UTC]
                                        [--no-header]
                                        [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,name,enable,description,filesystems (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs protection snapshot-policy run-once**

Runs the snapshot policy schedule once immediately.

```sh
weka fs protection snapshot-policy run-once <name>
                                            <schedule-type>
                                            [--color color]
                                            [--HOST HOST]
                                            [--PORT PORT]
                                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                            [--TIMEOUT TIMEOUT]
                                            [--profile profile]
                                            [--help]
                                            [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Name of the snapshot policy |
 | `schedule-type`* | Schedule type to be run once (format: 'periodic', 'hourly', 'daily', 'weekly' or 'monthly') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka fs protection snapshot-policy show**

Show snapshot policy configuration

```sh
weka fs protection snapshot-policy show <name>
                                        [--color color]
                                        [--HOST HOST]
                                        [--PORT PORT]
                                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                        [--TIMEOUT TIMEOUT]
                                        [--profile profile]
                                        [--help]
                                        [--raw-units]
                                        [--UTC]
                                        [--json]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Policy name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `-J`, `--json` | Format output as JSON |

**weka fs protection snapshot-policy update**

Update a snapshot policy

```sh
weka fs protection snapshot-policy update <name>
                                          [--new-name new-name]
                                          [--description description]
                                          [--path path]
                                          [--enabled enabled]
                                          [--color color]
                                          [--HOST HOST]
                                          [--PORT PORT]
                                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                          [--TIMEOUT TIMEOUT]
                                          [--profile profile]
                                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Existing snapshot policy name |
 | `--new-name` | New policy name (up to 12 alphanumeric characters, hyphens (-), underscores (_), and periods (.)) |
 | `--description` | New policy description (up to 128 characters) |
 | `--path` | The path to the new/modified snapshot policy file, must be in JSON format |
 | `--enabled` | Set snapshot policy status, can be true/false |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka fs quota

Commands used to control directory quotas

```sh
weka fs quota [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka fs quota list**

List filesystem quotas (by default, only exceeding ones)

```sh
weka fs quota list [filesystem]
                   [--snap-name snap-name]
                   [--path path]
                   [--under under]
                   [--over over]
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--format format]
                   [--output output]...
                   [--sort sort]...
                   [--filter filter]...
                   [--filter-color filter-color]...
                   [--all]
                   [--quick]
                   [--help]
                   [--raw-units]
                   [--UTC]
                   [--no-header]
                   [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem` | Filesystem name |
 | `--snap-name` | Optional snapshot name |
 | `-p`, `--path` | Show this path only |
 | `-u`, `--under` | List under (and including) this path only |
 | `--over` | Show only quotas over this percentage of usage (format: 0..100) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: quotaId,path,used,dblk,mblk,soft,hard,usage,owner,grace_seconds,time_over_soft_limit,status (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--all` | Show all (not only exceeding) quotas |
 | `-q`, `--quick` | Skip resolving inodes to paths |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs quota list-default**

List filesystem default quotas

```sh
weka fs quota list-default [filesystem]
                           [--snap-name snap-name]
                           [--path path]
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--format format]
                           [--output output]...
                           [--sort sort]...
                           [--filter filter]...
                           [--filter-color filter-color]...
                           [--help]
                           [--raw-units]
                           [--UTC]
                           [--no-header]
                           [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem` | Filesystem name |
 | `--snap-name` | Optional snapshot name |
 | `-p`, `--path` | Show this path only |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: inodeId,path,soft,hard,owner,grace (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs quota set**

Set a directory quota in a filesystem

```sh
weka fs quota set <path>
                  [--soft soft]
                  [--hard hard]
                  [--grace grace]
                  [--owner owner]
                  [--filesystem filesystem]
                  [--snap-name snap-name]
                  [--color color]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--help]
                  [--json]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `path`* | Path in the filesystem |
 | `--soft` | Soft limit for the directory, or 0 for unlimited (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--hard` | Hard limit for the directory, or 0 for unlimited (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--grace` | Soft limit grace period (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--owner` | Quota owner (e.g., email) |
 | `--filesystem` | Filesystem name |
 | `--snap-name` | Name of the writable snapshot |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka fs quota set-default**

Set a default directory quota in a filesystem

```sh
weka fs quota set-default <path>
                          [--soft soft]
                          [--hard hard]
                          [--grace grace]
                          [--owner owner]
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `path`* | Path in the filesystem |
 | `--soft` | Soft limit for the directory (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--hard` | Hard limit for the directory (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--grace` | Soft limit grace period (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--owner` | Quota owner (e.g., email) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka fs quota reset**

Unsets a directory quota in a filesystem

```sh
weka fs quota reset <path>
                    [--generation generation]
                    [--filesystem filesystem]
                    [--snap-name snap-name]
                    [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--help]
                    [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `path`* | Path in the filesystem |
 | `--generation` | Remove a specific generation of quota |
 | `--filesystem` | Filesystem name |
 | `--snap-name` | Name of the writable snapshot |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka fs quota unset-default**

Unsets a default directory quota in a filesystem

```sh
weka fs quota unset-default <path>
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `path`* | Path in the filesystem |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka fs reserve

Thin provisioning reserve for organizations

```sh
weka fs reserve [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka fs reserve set**

Set an organization's thin provisioning SSD reserve

```sh
weka fs reserve set <ssd-capacity>
                    [--org org]
                    [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--help]
                    [--json]
                    [--raw-units]
                    [--UTC]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
 | `ssd-capacity`* | SSD capacity to reserve (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--org` | Organization name or ID |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs reserve status**

Thin provisioning reserve for organizations

```sh
weka fs reserve status [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--format format]
                       [--output output]...
                       [--sort sort]...
                       [--filter filter]...
                       [--filter-color filter-color]...
                       [--help]
                       [--raw-units]
                       [--UTC]
                       [--no-header]
                       [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: id,name,ssdReserve (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs reserve reset**

Unset an organization's thin provisioning SSD's reserve

```sh
weka fs reserve reset [--org org]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]
                      [--raw-units]
                      [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--org` | Organization name or ID |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka fs restore

Restore filesystem content from a snapshot

```sh
weka fs restore <filesystem>
                <source-name>
                [--preserved-overwritten-snapshot-name preserved-overwritten-snapshot-name]
                [--preserved-overwritten-snapshot-access-point preserved-overwritten-snapshot-access-point]
                [--color color]
                [--HOST HOST]
                [--PORT PORT]
                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                [--TIMEOUT TIMEOUT]
                [--profile profile]
                [--force]
                [--help]
                [--json]

```

 | Parameter | Description |
 | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `filesystem`* | The name of the filesystem to be restored |
 | `source-name`* | The name of the source snapshot |
 | `--preserved-overwritten-snapshot-name` | Name of a snapshot to create with the old content of the filesystem |
 | `--preserved-overwritten-snapshot-access-point` | Access point of the preserved overwritten snapshot |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action replaces all data in the filesystem with the content of the snapshot and cannot be undone. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka fs security

Manage filesystem security

```sh
weka fs security [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka fs security policy**

Manages filesystem security policies.

```sh
weka fs security policy [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka fs security policy attach**

Attaches new security policies to a filesystem, adding them to the existing policies.

```sh
weka fs security policy attach <filesystem>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]
                               [--json]
                               [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Filesystem name. |
 | `policies`... | Security policies to add for filesystem. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka fs security policy detach**

Removes security policies from a filesystem.

```sh
weka fs security policy detach <filesystem>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]
                               [--json]
                               [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Filesystem name. |
 | `policies`... | Security policies to remove from filesystem. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka fs security policy list**

Lists filesystem security policies.

```sh
weka fs security policy list <filesystem>
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--format format]
                             [--output output]...
                             [--sort sort]...
                             [--filter filter]...
                             [--filter-color filter-color]...
                             [--help]
                             [--raw-units]
                             [--UTC]
                             [--no-header]
                             [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Filesystem name. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: position,uid,id,name (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs security policy reset**

Removes all security policies from a filesystem.

```sh
weka fs security policy reset <filesystem>
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--help]
                              [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Filesystem name. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka fs security policy set**

Sets security policies for a filesystem, replacing the existing list of policies.

```sh
weka fs security policy set <filesystem>
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]
                            [--json]
                            [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Filesystem name. |
 | `policies`... | Security Policies to set for filesystem. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka fs snapshot

List snapshots

```sh
weka fs snapshot [--filesystem filesystem]
                 [--name name]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--format format]
                 [--output output]...
                 [--sort sort]...
                 [--filter filter]...
                 [--filter-color filter-color]...
                 [--help]
                 [--raw-units]
                 [--UTC]
                 [--no-header]
                 [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--filesystem` | Filesystem name |
 | `--name` | Snapshot name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,filesystem,name,access,writeable,created,local_upload_size,remote_upload_size,local_object_status,local_object_progress,local_object_locator,remote_object_status,remote_object_progress,remote_object_locator,removing,prefetched,est_reclaimable_size,metadata_size (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs snapshot access-point-naming-convention**

Access point naming convention

```sh
weka fs snapshot access-point-naming-convention [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka fs snapshot access-point-naming-convention status**

Show access point naming convention

```sh
weka fs snapshot access-point-naming-convention status [--color color]
                                                       [--HOST HOST]
                                                       [--PORT PORT]
                                                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                                       [--TIMEOUT TIMEOUT]
                                                       [--profile profile]
                                                       [--help]
                                                       [--json]
                                                       [--raw-units]
                                                       [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs snapshot access-point-naming-convention update**

Update access point naming convention

```sh
weka fs snapshot access-point-naming-convention update <access-point-naming-convention>
                                                       [--color color]
                                                       [--HOST HOST]
                                                       [--PORT PORT]
                                                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                                       [--TIMEOUT TIMEOUT]
                                                       [--profile profile]
                                                       [--help]

```

 | Parameter | Description |
 | ---------------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `access-point-naming-convention`* | access point naming configuration (format: 'date' or 'name') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka fs snapshot copy**

Copy one snapshot over another

```sh
weka fs snapshot copy <filesystem>
                      <source-name>
                      <destination-name>
                      [--preserved-overwritten-snapshot-name preserved-overwritten-snapshot-name]
                      [--preserved-overwritten-snapshot-access-point preserved-overwritten-snapshot-access-point]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]
                      [--raw-units]
                      [--UTC]

```

 | Parameter | Description |
 | ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Source filesystem name |
 | `source-name`* | Source snapshot name |
 | `destination-name`* | Destination snapshot name |
 | `--preserved-overwritten-snapshot-name` | Name of a snapshot to create with the old content of the destination |
 | `--preserved-overwritten-snapshot-access-point` | Access point of the preserved overwritten snapshot |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs snapshot add**

Create a snapshot

```sh
weka fs snapshot add <filesystem>
                     <name>
                     [--access-point access-point]
                     [--source-snapshot source-snapshot]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--format format]
                     [--output output]...
                     [--sort sort]...
                     [--filter filter]...
                     [--filter-color filter-color]...
                     [--is-writable]
                     [--help]
                     [--raw-units]
                     [--UTC]
                     [--no-header]
                     [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Source filesystem name |
 | `name`* | Target snapshot name |
 | `--access-point` | Access point |
 | `--source-snapshot` | Source snapshot |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: id,name,access,writeable,created (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--is-writable` | Writable |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs snapshot remove**

Delete a snapshot

```sh
weka fs snapshot remove <filesystem>
                        <name>
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--force]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Source filesystem name |
 | `name`* | Snapshot name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action deletes all data stored by the snapshot and cannot be undone. |
 | `-h`, `--help` | Show help message |

**weka fs snapshot download**

Download a snapshot into an existing filesystem

```sh
weka fs snapshot download <filesystem>
                          <locator>
                          [--name name]
                          [--access-point access-point]
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--allow-non-chronological]
                          [--allow-divergence]
                          [--help]
                          [--json]
                          [--raw-units]
                          [--UTC]

```

 | Parameter | Description |
 | --------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Filesystem name |
 | `locator`* | Locator |
 | `--name` | Snapshot name (default: uploaded name) |
 | `--access-point` | Access point (default: uploaded access point) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--allow-non-chronological` | Allow downloading snapshots in non-chronological order. This is not recommended, as it will incur high data overhead. |
 | `--allow-divergence` | Allow downloading snapshots which are not descendants of the last downloaded snapshot. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs snapshot update**

Update snapshot parameters

```sh
weka fs snapshot update <filesystem>
                        <name>
                        [--new-name new-name]
                        [--access-point access-point]
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]
                        [--json]
                        [--raw-units]
                        [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Source filesystem name |
 | `name`* | Snapshot name |
 | `--new-name` | Updated snapshot name |
 | `--access-point` | Access point |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs snapshot upload**

Upload a snapshot to object store

```sh
weka fs snapshot upload <filesystem>
                        <snapshot>
                        [--site site]
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--allow-non-chronological]
                        [--help]
                        [--json]
                        [--raw-units]
                        [--UTC]

```

 | Parameter | Description |
 | --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
 | `filesystem`* | Filesystem name |
 | `snapshot`* | Snapshot name |
 | `--site` | The site of the Object Store to upload to (format: 'local' or 'remote') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--allow-non-chronological` | Allow uploading snapshots to remote object-store in non-chronological order. This is not recommended, as it will incur high data overhead. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka fs tier

Show object store connectivity for each node in the cluster

```sh
weka fs tier [--color color]
             [--HOST HOST]
             [--PORT PORT]
             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
             [--TIMEOUT TIMEOUT]
             [--profile profile]
             [--format format]
             [--output output]...
             [--sort sort]...
             [--filter filter]...
             [--filter-color filter-color]...
             [--help]
             [--raw-units]
             [--UTC]
             [--no-header]
             [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: obsBucket,statusUpload,statusDownload,statusRemove,nodesDown,errors (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs tier capacity**

List capacities for object store buckets attached to filesystems

```sh
weka fs tier capacity [--filesystem filesystem]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--format format]
                      [--output output]...
                      [--sort sort]...
                      [--filter filter]...
                      [--filter-color filter-color]...
                      [--force-fresh]
                      [--help]
                      [--raw-units]
                      [--UTC]
                      [--no-header]
                      [--verbose]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--filesystem` | Filesystem name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: fsUid,fsName,bucketUid,bucketName,totalConsumedCapacity,UsedCapacity,reclaimable,reclaimableThreshold,reclaimableLowThreshold,reclaimableHighThreshold (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--force-fresh` | Refresh the capacities to make sure they are most updated |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs tier fetch**

Fetch object-stored files to SSD storage

```sh
weka fs tier fetch [--non-existing non-existing]
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--verbose]
                   [--help]
                   [--raw-units]
                   [--UTC]
                   [<path>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `path`... | A file path to fetch to SSD storage. Multiple paths can be passed, e.g. \`find ... |
 | `--non-existing` | Behavior for non-existing files (default: error) (format: 'error', 'warn' or 'ignore') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-v`, `--verbose` | Verbose output, showing fetch requests as they are submitted |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs tier location**

Show data storage location for a given path

```sh
weka fs tier location <path>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--format format]
                      [--output output]...
                      [--sort sort]...
                      [--filter filter]...
                      [--filter-color filter-color]...
                      [--help]
                      [--raw-units]
                      [--UTC]
                      [--no-header]
                      [--verbose]
                      [<paths>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `path`* | Path to get information about |
 | `paths`... | Extra paths to get information about |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: path,type,size,ssdWrite,ssdRead,obsBytes,remoteBytes (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs tier obs**

List object stores configuration and status

```sh
weka fs tier obs [--name name]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--format format]
                 [--output output]...
                 [--sort sort]...
                 [--filter filter]...
                 [--filter-color filter-color]...
                 [--help]
                 [--raw-units]
                 [--UTC]
                 [--no-header]
                 [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Name of the Object Store |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,name,site,bucketsCount,uploadBucketsUp,downloadBucketsUp,removeBucketsUp,protocol,hostname,port,auth,region,access,secret,downloadBandwidth,uploadBandwidth,remove3Bandwidth,downloads,uploads,removals,maxUploadExtents,maxUploadSize,enableUploadTags,maxUploadRam,stsOperationType,stsRoleArn,stsRoleSessionName,stsDuration (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs tier obs update**

Edit an existing object store

```sh
weka fs tier obs update <name>
                        [--new-name new-name]
                        [--hostname hostname]
                        [--port port]
                        [--protocol protocol]
                        [--auth-method auth-method]
                        [--region region]
                        [--access-key-id access-key-id]
                        [--secret-key secret-key]
                        [--bandwidth bandwidth]
                        [--download-bandwidth download-bandwidth]
                        [--upload-bandwidth upload-bandwidth]
                        [--remove-bandwidth remove-bandwidth]
                        [--max-concurrent-downloads max-concurrent-downloads]
                        [--max-concurrent-uploads max-concurrent-uploads]
                        [--max-concurrent-removals max-concurrent-removals]
                        [--max-extents-in-data-blob max-extents-in-data-blob]
                        [--max-data-blob-size max-data-blob-size]
                        [--upload-memory-limit upload-memory-limit]
                        [--enable-upload-tags enable-upload-tags]
                        [--sts-operation-type sts-operation-type]
                        [--sts-role-arn sts-role-arn]
                        [--sts-role-session-name sts-role-session-name]
                        [--sts-session-duration sts-session-duration]
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]
                        [--raw-units]
                        [--UTC]

```

 | Parameter | Description |
 | ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Name of the Object Store |
 | `--new-name` | New name |
 | `--hostname` | Hostname (or IP) of the entrypoint to the bucket |
 | `--port` | Port of the entrypoint to S3 (single Accesser or Load-Balancer) |
 | `--protocol` | One of: HTTP (default), HTTPS, HTTPS_UNVERIFIED |
 | `--auth-method` | Authentication method. S3AuthMethod can be None, AWSSignature2 or AWSSignature4 |
 | `--region` | Name of the region we are assigned to work with (usually empty) |
 | `--access-key-id` | Access Key ID for AWS Signature authentications |
 | `--secret-key` | Secret Key for AWS Signature authentications |
 | `--bandwidth` | Bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--download-bandwidth` | Download bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--upload-bandwidth` | Upload bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--remove-bandwidth` | Remove bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--max-concurrent-downloads` | Maximum number of downloads we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-concurrent-uploads` | Maximum number of uploads we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-concurrent-removals` | Maximum number of removals we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-extents-in-data-blob` | Maximum number of extents' data to upload to an object store data blob |
 | `--max-data-blob-size` | Maximum size to upload to an object store data blob (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--upload-memory-limit` | Maximum RAM to allocate for concurrent uploads to this object store (per node) (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--enable-upload-tags` | Enable tagging of uploaded objects |
 | `--sts-operation-type` | AWS STS operation type to use. Default: none (format: 'assume_role' or 'none') |
 | `--sts-role-arn` | The Amazon Resource Name (ARN) of the role to assume. Mandatory when setting sts-operation to ASSUME_ROLE |
 | `--sts-role-session-name` | An identifier for the assumed role session. Length constraints: Minimum length of 2, maximum length of 64. |
 | `owed characters: upper and lo` | wer-case alphanumeric characters with no spaces. |

**weka fs tier ops**

List all the operations currently running on an object store from all the hosts in the cluster

```sh
weka fs tier ops [name]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--format format]
                 [--output output]...
                 [--sort sort]...
                 [--filter filter]...
                 [--filter-color filter-color]...
                 [--help]
                 [--raw-units]
                 [--UTC]
                 [--no-header]
                 [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name` | Name of the Object Store bucket |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: node,obsBucket,key,type,execution,phase,previous,start,size,results,errors,lastHTTP,concurrency,inode (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs tier release**

Release object-stored files from SSD storage

```sh
weka fs tier release [--non-existing non-existing]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--verbose]
                     [--help]
                     [--raw-units]
                     [--UTC]
                     [<path>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `path`... | A file path to release from SSD storage. Multiple paths can be passed, e.g. \`find ... |
 | `--non-existing` | Behavior for non-existing files (default: error) (format: 'error', 'warn' or 'ignore') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-v`, `--verbose` | Verbose output, showing release requests as they are submitted |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs tier s3**

List S3 object store buckets configuration and status

```sh
weka fs tier s3 [--obs-name obs-name]
                [--name name]
                [--color color]
                [--HOST HOST]
                [--PORT PORT]
                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                [--TIMEOUT TIMEOUT]
                [--profile profile]
                [--format format]
                [--output output]...
                [--sort sort]...
                [--filter filter]...
                [--filter-color filter-color]...
                [--help]
                [--raw-units]
                [--UTC]
                [--no-header]
                [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--obs-name` | Name of the Object Store |
 | `--name` | Name of the Object Store bucket |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,obsId,obsName,id,name,site,statusUpload,statusDownload,statusRemove,nodesUpForUpload,nodesUpForDownload,nodesUpForRemove,nodesDownForUpload,nodesDownForDownload,nodesDownForRemove,nodesUnknownForUpload,nodesUnknownForDownload,nodesUnknownForRemove,errors,protocol,hostname,port,bucket,auth,region,access,secret,status,up,downloadBandwidth,uploadBandwidth,removeBandwidth,errorsTimeout,prefetch,downloads,uploads,removals,maxUploadExtents,maxUploadSize,enableUploadTags,dataStorageClass,metadataStorageClass,stsOperationType,stsRoleArn,stsRoleSessionName,stsDuration (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs tier s3 add**

Create a new S3 object store bucket connection

```sh
weka fs tier s3 add <name>
                    [--site site]
                    [--obs-name obs-name]
                    [--hostname hostname]
                    [--port port]
                    [--bucket bucket]
                    [--auth-method auth-method]
                    [--region region]
                    [--access-key-id access-key-id]
                    [--secret-key secret-key]
                    [--protocol protocol]
                    [--obs-type obs-type]
                    [--bandwidth bandwidth]
                    [--download-bandwidth download-bandwidth]
                    [--upload-bandwidth upload-bandwidth]
                    [--remove-bandwidth remove-bandwidth]
                    [--errors-timeout errors-timeout]
                    [--prefetch-mib prefetch-mib]
                    [--max-concurrent-downloads max-concurrent-downloads]
                    [--max-concurrent-uploads max-concurrent-uploads]
                    [--max-concurrent-removals max-concurrent-removals]
                    [--max-extents-in-data-blob max-extents-in-data-blob]
                    [--max-data-blob-size max-data-blob-size]
                    [--enable-upload-tags enable-upload-tags]
                    [--data-storage-class data-storage-class]
                    [--metadata-storage-class metadata-storage-class]
                    [--sts-operation-type sts-operation-type]
                    [--sts-role-arn sts-role-arn]
                    [--sts-role-session-name sts-role-session-name]
                    [--sts-session-duration sts-session-duration]
                    [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--dry-run]
                    [--skip-verification]
                    [--verbose-errors]
                    [--help]
                    [--json]
                    [--raw-units]
                    [--UTC]

```

 | Parameter | Description |
 | ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Name of the Object Store bucket |
 | `--site` | The site of the Object Store, default: local (format: 'local' or 'remote') |
 | `--obs-name` | Name of the Object Store to associate this new bucket to |
 | `--hostname` | Hostname (or IP) of the entrypoint to the storage |
 | `--port` | Port of the entrypoint to S3 (single Accesser or Load-Balancer) |
 | `--bucket` | Name of the bucket we are assigned to work with |
 | `--auth-method` | Authentication method. S3AuthMethod can be None, AWSSignature2 or AWSSignature4 |
 | `--region` | Name of the region we are assigned to work with (usually empty) |
 | `--access-key-id` | Access Key ID for AWS Signature authentications |
 | `--secret-key` | Secret Key for AWS Signature authentications |
 | `--protocol` | One of: HTTP (default), HTTPS, HTTPS_UNVERIFIED |
 | `--obs-type` | One of: AWS (default), AZURE |
 | `--bandwidth` | Bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--download-bandwidth` | Download bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--upload-bandwidth` | Upload bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--remove-bandwidth` | Remove bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--errors-timeout` | If the Object Store bucket link is down for longer than this, all IOs that need data return with an error (format: duration between 1 minute and 15 minutes) |
 | `--prefetch-mib` | How many MiB of data to prefetch when reading a whole MiB on object store. Default Value is 128MiB (format: 0..600) |
 | `--max-concurrent-downloads` | Maximum number of downloads we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-concurrent-uploads` | Maximum number of uploads we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-concurrent-removals` | Maximum number of removals we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-extents-in-data-blob` | Maximum number of extents' data to upload to an object store data blob |
 | `--max-data-blob-size` | Maximum size to upload to an object store data blob (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--enable-upload-tags` | Enable tagging of uploaded objects |
 | `--data-storage-class` | The AWS storage class / Azure access tier to use for uploaded data blobs |
 | `--metadata-storage-class` | The AWS storage class / Azure access tier to use for uploaded data blobs |
 | `--sts-operation-type` | AWS STS operation type to use. Default: none (format: 'assume_role' or 'none') |
 | `--sts-role-arn` | The Amazon Resource Name (ARN) of the role to assume. Mandatory when setting sts-operation to ASSUME_ROLE |
 | `--sts-role-session-name` | An identifier for the assumed role session. Length constraints: Minimum length of 2, maximum length of 64. |
 | `owed characters: upper and lo` | wer-case alphanumeric characters with no spaces. |

**weka fs tier s3 attach**

Attach a filesystem to an existing Object Store

```sh
weka fs tier s3 attach <filesystem>
                       <obs-name>
                       [--mode mode]
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--help]
                       [--raw-units]
                       [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Name of the Filesystem |
 | `obs-name`* | Name of the Object Store bucket to attach |
 | `--mode` | The operation mode for the Object Store bucket (format: 'writable' or 'remote') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka fs tier s3 remove**

Delete an existing S3 object store connection

```sh
weka fs tier s3 remove <name>
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Name of the Object Store bucket |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka fs tier s3 detach**

Detach a filesystem from an attached object store

```sh
weka fs tier s3 detach <filesystem>
                       <obs-name>
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--help]
                       [--force]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | Name of the Filesystem |
 | `obs-name`* | Name of the Object Store bucket to detach |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. This process might take a while to complete and it cannot be aborted. The data will remain intact on the object store, and you can still use the uploaded snapshots for recovery. |

**weka fs tier s3 snapshot**

Commands used to display info about uploaded snapshots

```sh
weka fs tier s3 snapshot [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

\####### weka fs tier s3 snapshot list

List and show info about snapshots uploaded to Object Storage

```sh
weka fs tier s3 snapshot list <name>
                              [--locator locator]
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--format format]
                              [--output output]...
                              [--sort sort]...
                              [--filter filter]...
                              [--filter-color filter-color]...
                              [--help]
                              [--raw-units]
                              [--UTC]
                              [--no-header]
                              [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Name of the Object Store bucket |
 | `--locator` | Locator |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: guid,fsId,snapId,origFsId,fsName,snapName,accessPoint,totalMetaData,totalSize,ssdCapacity,totalCapacity,maxFiles,numGuids,compatibleVersion (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka fs tier s3 update**

Edit an existing S3 object store bucket connection

```sh
weka fs tier s3 update <name>
                       [--new-name new-name]
                       [--new-obs-name new-obs-name]
                       [--hostname hostname]
                       [--port port]
                       [--protocol protocol]
                       [--bucket bucket]
                       [--auth-method auth-method]
                       [--region region]
                       [--access-key-id access-key-id]
                       [--secret-key secret-key]
                       [--bandwidth bandwidth]
                       [--download-bandwidth download-bandwidth]
                       [--upload-bandwidth upload-bandwidth]
                       [--remove-bandwidth remove-bandwidth]
                       [--prefetch-mib prefetch-mib]
                       [--errors-timeout errors-timeout]
                       [--max-concurrent-downloads max-concurrent-downloads]
                       [--max-concurrent-uploads max-concurrent-uploads]
                       [--max-concurrent-removals max-concurrent-removals]
                       [--max-extents-in-data-blob max-extents-in-data-blob]
                       [--max-data-blob-size max-data-blob-size]
                       [--enable-upload-tags enable-upload-tags]
                       [--data-storage-class data-storage-class]
                       [--metadata-storage-class metadata-storage-class]
                       [--sts-operation-type sts-operation-type]
                       [--sts-role-arn sts-role-arn]
                       [--sts-role-session-name sts-role-session-name]
                       [--sts-session-duration sts-session-duration]
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--format format]
                       [--output output]...
                       [--sort sort]...
                       [--filter filter]...
                       [--filter-color filter-color]...
                       [--dry-run]
                       [--skip-verification]
                       [--verbose-errors]
                       [--help]
                       [--raw-units]
                       [--UTC]
                       [--no-header]
                       [--verbose]

```

 | Parameter | Description |
 | ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Name of the Object Store bucket |
 | `--new-name` | New name |
 | `--new-obs-name` | New Object Store name |
 | `--hostname` | Hostname (or IP) of the entrypoint to the storage |
 | `--port` | Port of the entrypoint to S3 (single Accesser or Load-Balancer) |
 | `--protocol` | One of: HTTP (default), HTTPS, HTTPS_UNVERIFIED |
 | `--bucket` | Name of the bucket we are assigned to work with |
 | `--auth-method` | Authentication method. S3AuthMethod can be None, AWSSignature2 or AWSSignature4 |
 | `--region` | Name of the region we are assigned to work with (usually empty) |
 | `--access-key-id` | Access Key ID for AWS Signature authentications |
 | `--secret-key` | Secret Key for AWS Signature authentications |
 | `--bandwidth` | Bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--download-bandwidth` | Download bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--upload-bandwidth` | Upload bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--remove-bandwidth` | Remove bandwidth limitation per core (Mbps) (format: 1..4294967295) |
 | `--prefetch-mib` | How many MiB of data to prefetch when reading a whole MiB on object store. Default Value is 128MiB (format: 0..600) |
 | `--errors-timeout` | If the Object Store bucket link is down for longer than this, all IOs that need data return with an error (format: duration between 1 minute and 15 minutes) |
 | `--max-concurrent-downloads` | Maximum number of downloads we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-concurrent-uploads` | Maximum number of uploads we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-concurrent-removals` | Maximum number of removals we concurrently perform on this object store in a single IO node (format: 1..64) |
 | `--max-extents-in-data-blob` | Maximum number of extents' data to upload to an object store data blob |
 | `--max-data-blob-size` | Maximum size to upload to an object store data blob (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--enable-upload-tags` | Enable tagging of uploaded objects |
 | `--data-storage-class` | The AWS storage class / Azure access tier to use for uploaded data blobs |
 | `--metadata-storage-class` | The AWS storage class / Azure access tier to use for uploaded data blobs |
 | `--sts-operation-type` | AWS STS operation type to use. Default: none (format: 'assume_role' or 'none') |
 | `--sts-role-arn` | The Amazon Resource Name (ARN) of the role to assume. Mandatory when setting sts-operation to ASSUME_ROLE |
 | `--sts-role-session-name` | An identifier for the assumed role session. Length constraints: Minimum length of 2, maximum length of 64. |
 | `owed characters: upper and lo` | wer-case alphanumeric characters with no spaces. |

#### weka fs update

Update a filesystem

```sh
weka fs update <name>
               [--new-name new-name]
               [--total-capacity total-capacity]
               [--ssd-capacity ssd-capacity]
               [--thin-provision-min-ssd thin-provision-min-ssd]
               [--thin-provision-max-ssd thin-provision-max-ssd]
               [--audit]
               [--data-reduction data-reduction]
               [--auth-required auth-required]
               [--kms-key-identifier kms-key-identifier]
               [--kms-namespace kms-namespace]
               [--kms-role-id kms-role-id]
               [--kms-secret-id kms-secret-id]
               [--color color]
               [--HOST HOST]
               [--PORT PORT]
               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
               [--TIMEOUT TIMEOUT]
               [--profile profile]
               [--use-cluster-kms-key-identifier]
               [--help]

```

 | Parameter | Description |
 | ---------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Filesystem name |
 | `--new-name` | New name |
 | `--total-capacity` | Total capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--ssd-capacity` | SSD capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--thin-provision-min-ssd` | Thin provision minimum SSD capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--thin-provision-max-ssd` | Thin provision maximum SSD capacity (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--audit` | Forwards this filesystem's audit logs to a configured events monitoring platform, provided that cluster-wide auditing is also enabled |
 | `--data-reduction` | Enable data reduction |
 | `--auth-required` | Require the mounting user to be authenticated for mounting this filesystem. This flag is only effective in the root organization, users in non-root organizations must be authenticated to perform a mount operation. (format: 'yes' or 'no') |
 | `--kms-key-identifier` | Customize KMS key identifier for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-namespace` | Customize KMS namespace for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-role-id` | Customize KMS role-id for this filesystem (currently only for HashiCorp Vault) |
 | `--kms-secret-id` | Customize KMS secret-id for this filesystem (currently only for HashiCorp Vault) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--use-cluster-kms-key-identifier` | Use cluster KMS configuration for this filesystem, removes the custom KMS configuration for this filesystem |
 | `-h`, `--help` | Show help message |

### weka interface-group

List interface groups

```sh
weka interface-group [--name name]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--format format]
                     [--output output]...
                     [--sort sort]...
                     [--filter filter]...
                     [--filter-color filter-color]...
                     [--help]
                     [--no-header]
                     [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,name,mask,gateway,type,status,ips,ports,allowManageGids (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka interface-group add

Create an interface group

```sh
weka interface-group add <name>
                         <type>
                         [--subnet subnet]
                         [--gateway gateway]
                         [--allow-manage-gids allow-manage-gids]
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--format format]
                         [--output output]...
                         [--sort sort]...
                         [--filter filter]...
                         [--filter-color filter-color]...
                         [--help]
                         [--no-header]
                         [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `type`* | Group type |
 | `--subnet` | subnet mask in the 255.255.0.0 format |
 | `--gateway` | gateway ip |
 | `--allow-manage-gids` | Allow to use manage-gids in exports. With manage-gids, the list of group ids received from the client will be replaced by a list of group ids determined by an appropriate lookup on the server (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: name,mask,gateway,type,status,ips,ports,allowManageGids (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka interface-group assignment

List the currently assigned interface for each floating-IP address in the given interface-group. If is not supplied, assignments for all floating-IP addresses will be listed

```sh
weka interface-group assignment [--name name]
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--format format]
                                [--output output]...
                                [--sort sort]...
                                [--filter filter]...
                                [--filter-color filter-color]...
                                [--help]
                                [--no-header]
                                [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: ip,host,port,group (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka interface-group remove

Delete an interface group

```sh
weka interface-group remove <name>
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--force]
                            [--help]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected clients and can be undone by re-creating the interface group. |
 | `-h`, `--help` | Show help message |

#### weka interface-group ip-range

Commands that manage interface-groups' ip-ranges

```sh
weka interface-group ip-range [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka interface-group ip-range add**

Add an ip range to an interface group

```sh
weka interface-group ip-range add <name>
                                  <ips>
                                  [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `ips`* | IP range |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka interface-group ip-range remove**

Delete an ip range from an interface group

```sh
weka interface-group ip-range remove <name>
                                     <ips>
                                     [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--force]
                                     [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `ips`* | IP range |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected clients and can be undone by re-creating the IP range. |
 | `-h`, `--help` | Show help message |

#### weka interface-group port

Commands that manage interface-groups' ports

```sh
weka interface-group port [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka interface-group port add**

Add a server port to an interface group

```sh
weka interface-group port add <name>
                              <server-id>
                              <port>
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `server-id`* | Server ID on which the port resides |
 | `port`* | Port's device. (e.g. eth1) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka interface-group port remove**

Delete a server port from an interface group

```sh
weka interface-group port remove <name>
                                 <server-id>
                                 <port>
                                 [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--force]
                                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `server-id`* | Server ID on which the port resides |
 | `port`* | Port's device. (e.g. eth1) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected clients and can be undone by re-adding the port. |
 | `-h`, `--help` | Show help message |

#### weka interface-group update

Update an interface group

```sh
weka interface-group update <name>
                            [--subnet subnet]
                            [--gateway gateway]
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `--subnet` | subnet mask in the 255.255.0.0 format |
 | `--gateway` | gateway ip |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

### weka local

Commands that control weka and its containers on the local machine

```sh
weka local [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka local diags

Collect diagnostics from the local machine

```sh
weka local diags [--id id]
                 [--output-dir output-dir]
                 [--core-dump-limit core-dump-limit]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--collect-cluster-info]
                 [--tar]
                 [--verbose]
                 [--help]

```

 | Parameter | Description |
 | ------------------------------ | ---------------------------------------------------------------------------------------------------------------------- |
 | `-i`, `--id` | A unique identifier for this dump |
 | `-d`, `--output-dir` | Directory to save the diags dump to, default: /opt/weka/diags |
 | `-c`, `--core-dump-limit` | Limit to processing this number of core dumps, if found (default: 1) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-s`, `--collect-cluster-info` | Collect cluster-related information. Warning: Use this flag on one container at a time to avoid straining the cluster. |
 | `-t`, `--tar` | Create a TAR of all collected diags |
 | `-v`, `--verbose` | Print results of all diags, including successful ones |
 | `-h`, `--help` | Show help message |

#### weka local disable

Disable containers by not launching them on machine boot. This does not affect the current running status of the container. In order to change the current status, use the "weka local start/stop" commands. If no container names are specified, this command runs on all containers.

```sh
weka local disable [--color color] [--type type]... [--help] [<container>]...

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `container`... | The container to disable |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-t`, `--type`... | The container types to disable (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

#### weka local enable

Enable monitoring for the requested containers so they automaticlly start on machine boot. This does not affect the current running status of the container. In order to change the current status, use the "weka local start/stop" commands. If no container names are specified, this command runs on all containers.

```sh
weka local enable [--color color] [--type type]... [--help] [<container>]...

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `container`... | The container to enable |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-t`, `--type`... | The container types to enable (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

#### weka local events

List the events saved to the local drive. This command does not require authentication and can be used when Weka is turned off.

```sh
weka local events [--path path]
                  [--container-name container-name]
                  [--color color]
                  [--format format]
                  [--output output]...
                  [--sort sort]...
                  [--filter filter]...
                  [--filter-color filter-color]...
                  [--help]
                  [--raw-units]
                  [--UTC]
                  [--no-header]
                  [--verbose]

```

 | Parameter | Description |
 | ------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--path` | Path to where local events are stored |
 | `--container-name` | Name of the container whose events will be collected (default default) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: time,uuid,category,severity,permission,type,entity,node,parameters,hash (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka local install-agent

Installs Weka agent on the machine the command is executed from

```sh
weka local install-agent [--color color] [--no-update] [--no-start] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--no-update` | Don't update the locally installed containers |
 | `--no-start` | Do not register the weka-agent service and start it after its creation |
 | `-h`, `--help` | Show help message |

#### weka local monitoring

Turn monitoring on/off for the given containers, or all containers if none are specified. When a container is started, it's always monitored. When a container is monitored, it will be restarted if it exits without being stopped through the CLI.

```sh
weka local monitoring <enabled> [--color color] [--type type]... [--help] [<container>]...

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `enabled`* | Whether monitoring should be on or off (format: 'on' or 'off') |
 | `container`... | The container to disable |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-t`, `--type`... | The container types to disable (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

#### weka local ps

List the Weka containers running on the machine this command is executed from

```sh
weka local ps [--color color]
              [--format format]
              [--output output]...
              [--sort sort]...
              [--filter filter]...
              [--filter-color filter-color]...
              [--help]
              [--raw-units]
              [--UTC]
              [--no-header]
              [--verbose]

```

 | Parameter | Description |
 | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: name,state,running,disabled,uptime,monitoring,persistent,port,pid,status,managementIps,versionName,failureText,failure,failureTime,upgradeState (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka local reset-data

Resets the data directory for a given container, making the host no longer aware of the rest of the cluster

```sh
weka local reset-data [--container container] [--color color] [--clean-unused] [--force] [--help] [<version-name>]...

```

 | Parameter | Description |
 | ------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
 | `version-name`... | The versions to remove |
 | `-C`, `--container` | The container to run in |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--clean-unused` | Delete all container data directories for versions which aren't the current set version |
 | `-f`, `--force` | Force this action without further confirmation. This action is destructive and can potentially lose all data in the cluster. |
 | `-h`, `--help` | Show help message |

#### weka local resources

List and control container resources

```sh
weka local resources [--container container] [--color color] [--stable] [--help] [--json] [--raw-units] [--UTC]

```

 | Parameter | Description |
 | ------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--stable` | List the resources from the last successful container boot |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

**weka local resources apply**

Apply changes to resources locally

```sh
weka local resources apply [--container container] [--color color] [--graceful] [--help] [--force]

```

 | Parameter | Description |
 | ------------------- | ------------------------------------------------------------------------------------------------------------------------- |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-g`, `--graceful` | Restart the container gracefully |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. This action will restart the container on this host and cannot be undone. |

**weka local resources auto-remove-timeout**

Configure the auto-remove-timeout (in seconds) to remove inactive client containers.

```sh
weka local resources auto-remove-timeout <auto-remove-timeout>
                                         [--container container]
                                         [--color color]
                                         [--force]
                                         [--help]

```

 | Parameter | Description |
 | ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
 | `auto-remove-timeout`* | The auto-remove timeout in seconds to remove inactive client containers. |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-f`, `--force` | Force this action without further confirmation. This would cause a non-client container to become a client and this action is irreversible.. |
 | `-h`, `--help` | Show help message |

**weka local resources bandwidth**

Limit weka's bandwidth for the host

```sh
weka local resources bandwidth <bandwidth> [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `bandwidth`* | New bandwidth limitation per second (format: either "unlimited" or bandwidth per second in binary or decimal values: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local resources base-port**

Change the port-range used by the container. Weka containers require 100 ports to operate.

```sh
weka local resources base-port <base-port> [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | ------------------------------------------------------------------------------------ |
 | `base-port`* | The first port that will be used by the Weka container, out of a total of 100 ports. |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local resources cores**

Change the core configuration of the host

```sh
weka local resources cores <cores>
                           [--container container]
                           [--frontend-dedicated-cores frontend-dedicated-cores]
                           [--drives-dedicated-cores drives-dedicated-cores]
                           [--compute-dedicated-cores compute-dedicated-cores]
                           [--color color]
                           [--core-ids core-ids]...
                           [--no-frontends]
                           [--only-drives-cores]
                           [--only-compute-cores]
                           [--only-frontend-cores]
                           [--allow-mix-setting]
                           [--help]

```

 | Parameter | Description |
 | ---------------------------- | --------------------------------------------------------------------------------------------------------------- |
 | `cores`* | Number of CPU cores dedicated to weka - If set to 0 - no drive could be added to this host |
 | `-C`, `--container` | The container name |
 | `--frontend-dedicated-cores` | Number of cores dedicated to weka frontend (out of the total ) |
 | `--drives-dedicated-cores` | Number of cores dedicated to weka drives (out of the total ) |
 | `--compute-dedicated-cores` | Number of cores dedicated to weka compute (out of the total ) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--core-ids`... | Specify the ids of weka dedicated cores (may be repeated or comma-separated) |
 | `--no-frontends` | Don't allocate frontend nodes |
 | `--only-drives-cores` | Create only nodes with a drives role |
 | `--only-compute-cores` | Create only nodes with a compute role |
 | `--only-frontend-cores` | Create only nodes with a frontend role |
 | `--allow-mix-setting` | Allow specified core-ids even if there are running containers with AUTO core-ids allocation on the same server. |
 | `-h`, `--help` | Show help message |

**weka local resources dedicate**

Set the host as dedicated to weka. For example it can be rebooted whenever needed, and configured by weka for optimal performance and stability

```sh
weka local resources dedicate <on> [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | ----------------------------------------------------------------------------------------- |
 | `on`* | Set the host as weka dedicated, off unsets host as weka dedicated (format: 'on' or 'off') |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local resources export**

Export stable resources to file

```sh
weka local resources export <path> [--container container] [--color color] [--staging] [--stable] [--help]

```

 | Parameter | Description |
 | ------------------- | ----------------------------------------------------------------------------------------------- |
 | `path`* | Path to export resources |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--staging` | List the resources from the currently staged resources that were not yet applied |
 | `--stable` | List the resources from the currently stable resources, which are the last known good resources |
 | `-h`, `--help` | Show help message |

**weka local resources failure-domain**

Set the host failure-domain

```sh
weka local resources failure-domain [--container container] [--name name] [--color color] [--auto] [--help]

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------------------------- |
 | `-C`, `--container` | The container name |
 | `--name` | Add this host to a named failure-domain. A failure-domain will be created if it doesn't exist yet. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--auto` | Set this host to be a failure-domain of its own |
 | `-h`, `--help` | Show help message |

**weka local resources fqdn**

Configure the fqdn to be used by other containers for TLS hostname verification when interacting with the cluster.

```sh
weka local resources fqdn <fqdn> [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
 | `fqdn`* | The Fully Qualified Domain Name (FQDN) to be used by other containers for TLS hostname verification when interacting with the cluster. |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local resources import**

Import resources from file

```sh
weka local resources import <path> [--container container] [--color color] [--with-identifiers] [--help] [--force]

```

 | Parameter | Description |
 | -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `path`* | Path of file to import resources from |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--with-identifiers` | Import net device unique identifiers |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. This action will override any resource changes that have not been applied, and cannot be undone. |

**weka local resources join-ips**

Set the IPs and ports of all hosts in the cluster. This will enable the host to join the cluster using these IPs.

```sh
weka local resources join-ips [--container container]
                              [--color color]
                              [--join-fqdns join-fqdns]...
                              [--restricted]
                              [--help]
                              [<management-ips>]...

```

 | Parameter | Description |
 | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `management-ips`... | New IP:port pairs for the management processes. If no port is used the command will use the default Weka port |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--join-fqdns`... | FQDN:port pairs for the management processes. If no port is used the command will use the default Weka port (may be repeated or comma-separated) |
 | `--restricted` | Join using restricted client port |
 | `-h`, `--help` | Show help message |

**weka local resources join-secret**

Configure the secret used when joining a cluster as a backend

```sh
weka local resources join-secret <secret> [--container container] [--color color] [--purge] [--help]

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------- |
 | `secret`* | Cluster join secret |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--purge` | Purge previous join secrets |
 | `-h`, `--help` | Show help message |

**weka local resources management-ips**

Set the host's management node IPs. Setting 2 IPs will turn this hosts networking into highly-available mode

```sh
weka local resources management-ips [--container container] [--color color] [--help] [<management-ips>]...

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------- |
 | `management-ips`... | New IPs for the management nodes |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local resources memory**

Dedicate a set amount of RAM to weka

```sh
weka local resources memory <memory> [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `memory`* | Memory dedicated to weka in bytes, set to 0 to let the system decide (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local resources net**

List and control container resources

```sh
weka local resources net [--container container] [--color color] [--stable] [--help] [--json]

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------- |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--stable` | List the resources from the last successful container boot |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka local resources net add**

Allocate a dedicated networking device on a host (to the cluster).

```sh
weka local resources net add <device>
                             [--container container]
                             [--gateway gateway]
                             [--netmask netmask]
                             [--name name]
                             [--label label]
                             [--vfs vfs]
                             [--vlan vlan]
                             [--color color]
                             [--ips ips]...
                             [--help]

```

 | Parameter | Description |
 | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `device`* | Network device pci-slot/mac-address/interface-name(s) |
 | `-C`, `--container` | The container name |
 | `--gateway` | Default gateway IP. In AWS this value is auto-detected, otherwise the default data networking gateway will be used. |
 | `--netmask` | Netmask in bits number. In AWS this value is auto-detected, otherwise the default data networking netmask will be used. |
 | `--name` | If empty, a name will be auto generated. |
 | `--label` | The name of the switch or network group to which this network device is attached |
 | `--vfs` | The number of VFs to preallocate (default is all supported by NIC) |
 | `--vlan` | The VLAN to use (802.1Q, Ethernet only, 1-4094) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--ips`... | IPs to be allocated to cores using the device. If not given - IPs may be set automatically according the interface's IPs, or taken from the default networking IPs pool (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

**weka local resources net remove**

Undedicate a networking device in a host.

```sh
weka local resources net remove <name> [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------- |
 | `name`* | Net device name or identifier as appears in `weka local resources net` |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local resources restore**

Restore resources from Stable resources

```sh
weka local resources restore [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------- |
 | `-C`, `--container` | The container name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka local restart

Restart a Weka container

```sh
weka local restart [--wait-time wait-time]
                   [--color color]
                   [--type type]...
                   [--dont-restart-dependent-containers]
                   [--force]
                   [--help]
                   [<container>]...

```

 | Parameter | Description |
 | ------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
 | `container`... | The container to restart |
 | `-w`, `--wait-time` | How long to wait for the container to start (default: 15m) (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-t`, `--type`... | The container types to restart (may be repeated or comma-separated) |
 | `--dont-restart-dependent-containers` | Do not restart dependent containers |
 | `-f`, `--force` | Skip the check for active mounts and perform an ungraceful restart |
 | `-h`, `--help` | Show help message |

#### weka local rm

Delete a Weka container from the machine this command is executed from (this removed the data associated with the container, but retains the downloaded software)

```sh
weka local rm [--color color] [--all] [--force] [--help] [<containers>]...

```

 | Parameter | Description |
 | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `containers`... | The containers to remove |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--all` | Remove all containers |
 | `-f`, `--force` | Force this action without further confirmation. This would delete all data associated with the container(s) and can potentially lose all data in the cluster. |
 | `-h`, `--help` | Show help message |

#### weka local run

Execute a command inside a new container that has the same mounts as the given container. If no container is specified, either "default" or the only defined container is selected. If no command is specified, opens an interactive shell.

```sh
weka local run [--container container] [--in in] [--color color] [--environment environment]... [--help] [<command>]...

```

 | Parameter | Description |
 | ------------------------ | -------------------------------------------------------------------------------- |
 | `-C`, `--container` | The container to run in |
 | `--in` | The container version to run the command in |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-e`, `--environment`... | Environment variable to add (may be repeated) |
 | `-h`, `--help` | Show help message |

#### weka local setup

Container setup commands

```sh
weka local setup [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka local setup client**

Setup a local weka client container

```sh
weka local setup client [--name name]
                        [--cores cores]
                        [--memory memory]
                        [--bandwidth bandwidth]
                        [--timeout timeout]
                        [--base-port base-port]
                        [--weka-version weka-version]
                        [--fqdn fqdn]
                        [--dedicated-mode dedicated-mode]
                        [--color color]
                        [--core-ids core-ids]...
                        [--management-ips management-ips]...
                        [--join-ips join-ips]...
                        [--net net]...
                        [--disable]
                        [--no-start]
                        [--allow-mix-setting]
                        [--restricted]
                        [--help]

```

 | Parameter | Description |
 | --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `-n`, `--name` | The name to give the container |
 | `--cores` | Number of CPU cores dedicated to weka |
 | `--memory` | Memory dedicated to weka in bytes, set to 0 to let the system decide (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--bandwidth` | bandwidth limitation per second (format: either "unlimited" or bandwidth per second in binary or decimal values: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `-t`, `--timeout` | Join command timeout in seconds (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--base-port` | The first port that will be used by the Weka container, out of a total of 100 ports. |
 | `--weka-version` | Use the specified version to start the container in |
 | `--fqdn` | The Fully Qualified Domain Name (FQDN) to be used by other containers for TLS hostname verification when interacting with the cluster |
 | `--dedicated-mode` | Determine whether DPDK networking dedicates a core (full) or not (none). none can only be set when the NIC driver supports it. (format: 'full' or 'none') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--core-ids`... | Specify the ids of weka dedicated cores (may be repeated or comma-separated) |
 | `--management-ips`... | New IPs for the management nodes (may be repeated or comma-separated) |
 | `--join-ips`... | New IP:port pairs for the management processes. If no port is used the command will use the default Weka port (may be repeated or comma-separated) |
 | `--net`... | Network specification - /\[ip]/\[bits]/\[gateway]. Or: 'udp' to enforce UDP and avoid an attempt of auto deduction (may be repeated or comma-separated) |
 | `--disable` | Should the container be created as disabled |
 | `--no-start` | Do not start the container after its creation |
 | `--allow-mix-setting` | Allow specified core-ids even if there are running containers with AUTO core-ids allocation on the same server. |
 | `--restricted` | Restricted client mode functionality only |
 | `-h`, `--help` | Show help message |

**weka local setup container**

Setup a local weka container

```sh
weka local setup container [--name name]
                           [--cores cores]
                           [--frontend-dedicated-cores frontend-dedicated-cores]
                           [--drives-dedicated-cores drives-dedicated-cores]
                           [--compute-dedicated-cores compute-dedicated-cores]
                           [--memory memory]
                           [--bandwidth bandwidth]
                           [--failure-domain failure-domain]
                           [--timeout timeout]
                           [--container-id container-id]
                           [--base-port base-port]
                           [--resources-path resources-path]
                           [--weka-version weka-version]
                           [--fqdn fqdn]
                           [--auto-remove-timeout auto-remove-timeout]
                           [--dedicated-mode dedicated-mode]
                           [--color color]
                           [--core-ids core-ids]...
                           [--management-ips management-ips]...
                           [--join-ips join-ips]...
                           [--join-fqdns join-fqdns]...
                           [--net net]...
                           [--management-net management-net]...
                           [--disable]
                           [--no-start]
                           [--no-frontends]
                           [--only-drives-cores]
                           [--only-compute-cores]
                           [--only-frontend-cores]
                           [--only-dataserv-cores]
                           [--allow-mix-setting]
                           [--dedicate]
                           [--force]
                           [--ignore-used-ports]
                           [--skip-management-ips-check]
                           [--client]
                           [--restricted]
                           [--help]

```

 | Parameter | Description |
 | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `-n`, `--name` | The name to give the container |
 | `--cores` | Number of CPU cores dedicated to weka |
 | `--frontend-dedicated-cores` | Number of cores dedicated to weka frontend (out of the total ) |
 | `--drives-dedicated-cores` | Number of cores dedicated to weka drives (out of the total ) |
 | `--compute-dedicated-cores` | Number of cores dedicated to weka compute (out of the total ) |
 | `--memory` | Memory dedicated to weka in bytes, set to 0 to let the system decide (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--bandwidth` | bandwidth limitation per second (format: either "unlimited" or bandwidth per second in binary or decimal values: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--failure-domain` | Add this container to a named failure-domain. A failure-domain will be created if it doesn't exist yet. If not specified, an automatic failure domain will be assigned. |
 | `-t`, `--timeout` | Join command timeout in seconds (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--container-id` | Designate a container-id that will be used when the container joins the cluster |
 | `--base-port` | The first port that will be used by the Weka container, out of a total of 100 ports. |
 | `--resources-path` | Import the container's resources from a file (additional command-line flags specified will override the resources in the file) |
 | `--weka-version` | Use the specified version to start the container in |
 | `--fqdn` | The Fully Qualified Domain Name (FQDN) to be used by other containers for TLS hostname verification when interacting with the cluster |
 | `--auto-remove-timeout` | Set the timeout (in seconds) to remove inactive client containers. Applies only with the --client flag. |
 | `--dedicated-mode` | Determine whether DPDK networking dedicates a core (full) or not (none). none can only be set when the NIC driver supports it. (format: 'full' or 'none') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--core-ids`... | Specify the ids of weka dedicated cores (may be repeated or comma-separated) |
 | `--management-ips`... | New IPs for the management nodes (may be repeated or comma-separated) |
 | `--join-ips`... | New IP:port pairs for the management processes. If no port is used the command will use the default Weka port (may be repeated or comma-separated) |
 | `--join-fqdns`... | FQDN:port pairs for the management processes. If no port is used the command will use the default Weka port (may be repeated or comma-separated) |
 | `--net`... | Network specification - /\[ip]/\[bits]/\[gateway]. Or: 'udp' to enforce UDP and avoid an attempt of auto deduction (may be repeated or comma-separated) |
 | `--management-net`... | Net interface used to auto configure management IPs (may be repeated or comma-separated) |
 | `--disable` | Should the container be created as disabled |
 | `--no-start` | Do not start the container after its creation |
 | `--no-frontends` | Don't allocate frontend nodes |
 | `--only-drives-cores` | Create only nodes with a drives role |
 | `--only-compute-cores` | Create only nodes with a compute role |
 | `--only-frontend-cores` | Create only nodes with a frontend role |
 | `--only-dataserv-cores` | Create only nodes with a dataserv role |
 | `--allow-mix-setting` | Allow specified core-ids even if there are running containers with AUTO core-ids allocation on the same server. |
 | `--dedicate` | Set the host as weka dedicated |
 | `--force` | Create a new container even if a container with the same name exists, disregarding all safety checks! |
 | `--ignore-used-ports` | Allow container to start even if the required ports are used by other processes |
 | `--skip-management-ips-check` | Skip The enforcement of management IPs |
 | `--client` | Create persistent client container |
 | `--restricted` | Restricted client mode functionality only |
 | `-h`, `--help` | Show help message |

**weka local setup envoy**

Setup a local envoy container

```sh
weka local setup envoy [--name name] [--color color] [--disable] [--no-start] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `-n`, `--name` | The name to give the container |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--disable` | Should the container be created as disabled |
 | `--no-start` | Do not start the container after its creation |
 | `-h`, `--help` | Show help message |

**weka local setup taskmon**

Setup a local taskmon container

```sh
weka local setup taskmon [--color color] [--disable] [--no-start] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--disable` | Should the container be created as disabled |
 | `--no-start` | Do not start the container after its creation |
 | `-h`, `--help` | Show help message |

**weka local setup telemetry**

Setup a local telemetry container

```sh
weka local setup telemetry [--dependent-container-name dependent-container-name]
                           [--color color]
                           [--disable]
                           [--no-start]
                           [--help]

```

 | Parameter | Description |
 | ------------------------------------ | -------------------------------------------------------------------------------- |
 | `-dep`, `--dependent-container-name` | The name of the container that the telemetry container depends on |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--disable` | Should the container be created as disabled |
 | `--no-start` | Do not start the container after its creation |
 | `-h`, `--help` | Show help message |

**weka local setup weka**

Setup a local weka container

```sh
weka local setup weka [--name name] [--color color] [--disable] [--no-start] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `-n`, `--name` | The name to give the container |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--disable` | Should the container be created as disabled |
 | `--no-start` | Do not start the container after its creation |
 | `-h`, `--help` | Show help message |

#### weka local start

Start a Weka container

```sh
weka local start [--wait-time wait-time]
                 [--color color]
                 [--type type]...
                 [--skip-start-and-enable-dependent]
                 [--help]
                 [<container>]...

```

 | Parameter | Description |
 | ----------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
 | `container`... | The container to start |
 | `-w`, `--wait-time` | How long to wait for the container to start (default: 15m) (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-t`, `--type`... | The container types to start (may be repeated or comma-separated) |
 | `--skip-start-and-enable-dependent` | Skip starting and enabling dependent containers when starting containers by name |
 | `-h`, `--help` | Show help message |

#### weka local status

Show the status of a Weka container

```sh
weka local status [--color color] [--type type]... [--verbose] [--help] [--json] [<container>]...

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------- |
 | `container`... | The container to display it's status |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-t`, `--type`... | The container types to show (may be repeated or comma-separated) |
 | `-v`, `--verbose` | Verbose mode |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka local stop

Stop a Weka container

```sh
weka local stop [--reason reason]
                [--color color]
                [--type type]...
                [--skip-stop-and-disable-dependent]
                [--force]
                [--help]
                [<container>]...

```

 | Parameter | Description |
 | ----------------------------------- | -------------------------------------------------------------------------------- |
 | `container`... | The container to stop |
 | `--reason` | The reason weka was stopped, will be presented to the user during 'weka status' |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-t`, `--type`... | The container types to stop (may be repeated or comma-separated) |
 | `--skip-stop-and-disable-dependent` | Skip stopping and disabling dependent containers |
 | `-f`, `--force` | Skip the check for active mounts and perform an ungraceful stop |
 | `-h`, `--help` | Show help message |

#### weka local upgrade

Upgrade a Weka Host Container to its cluster version

```sh
weka local upgrade [--container container]
                   [--target-version target-version]
                   [--upgrade-container-timeout upgrade-container-timeout]
                   [--prepare-container-timeout prepare-container-timeout]
                   [--container-action-timeout container-action-timeout]
                   [--color color]
                   [--allow-not-ready]
                   [--dont-upgrade-agent]
                   [--upgrade-dependents]
                   [--all]
                   [--help]

```

 | Parameter | Description |
 | ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
 | `-C`, `--container` | The container name |
 | `-t`, `--target-version` | Specify a specific target version for upgrade, instead of upgrading to the backend's version. |
 | `NOTE - This parameter is` | DANGEROUS, use with caution. Incorrect usage may cause upgrade failure. |
 | `--upgrade-container-timeout` | How long to wait for the container to upgrade. default is 120s (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--prepare-container-timeout` | How long to wait for the container to prepare for upgrade. default is 120s (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--container-action-timeout` | How long to wait for the container action to run before timing out and retrying 30s (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--allow-not-ready` | Allow starting local upgrade while the container is not fully up |
 | `--dont-upgrade-agent` | Don't upgrade the weka agent |
 | `--upgrade-dependents` | Upgrade dependent containers |
 | `--all` | Upgrade all containers |
 | `-h`, `--help` | Show help message |

### weka mount

Mounts a wekafs filesystem. This is the helper utility installed at /sbin/mount.wekafs.

```sh
weka mount <source>
           <target>
           [--option option]
           [--type type]
           [--color color]
           [--no-mtab]
           [--sloppy]
           [--fake]
           [--verbose]
           [--help]
           [--raw-units]
           [--UTC]

```

 | Parameter | Description |
 | ------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `source`* | Source filesystem to mount |
 | `target`* | Location to mount the source filesystem on |
 | `-o`, `--option` | Mount options |
 | `-t`, `--type` | The filesystem type |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-n`, `--no-mtab` | Mount without writing in /etc/mtab. This is necessary for example when /etc is on a read-only filesystem |
 | `-s`, `--sloppy` | Tolerate sloppy mount options rather than failing |
 | `-f`, `--fake` | Causes everything to be done except for the actual system call |
 | `-v`, `--verbose` | Verbose mode |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

### weka nfs

Commands that manage client-groups, permissions and interface-groups

```sh
weka nfs [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka nfs client-group

Lists NFS client groups

```sh
weka nfs client-group [--name name]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--format format]
                      [--output output]...
                      [--sort sort]...
                      [--filter filter]...
                      [--filter-color filter-color]...
                      [--help]
                      [--no-header]
                      [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,name,rules (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka nfs client-group add**

Create an NFS client group

```sh
weka nfs client-group add <name>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--format format]
                          [--output output]...
                          [--sort sort]...
                          [--filter filter]...
                          [--filter-color filter-color]...
                          [--help]
                          [--no-header]
                          [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: name,rules (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka nfs client-group remove**

Delete an NFS client group

```sh
weka nfs client-group remove <name>
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--force]
                             [--help]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected NFS clients and can be undone by re-creating the client group. |
 | `-h`, `--help` | Show help message |

#### weka nfs clients

NFS Clients usage information

```sh
weka nfs clients [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs clients show**

Show NFS Clients usage information. If no options are given, all NFS Ganesha containers will be selected.

```sh
weka nfs clients show [--interface-group interface-group]
                      [--container-id container-id]
                      [--fip fip]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--format format]
                      [--fsnames fsnames]...
                      [--output output]...
                      [--sort sort]...
                      [--filter filter]...
                      [--filter-color filter-color]...
                      [--help]
                      [--no-header]
                      [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `--interface-group` | interface-group-name |
 | `--container-id` | container-id |
 | `--fip` | floating-ip |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--fsnames`... | A comma-separated list of filesystems. If no options are given, all NFS exported filesystems will be selected. (may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: hostid,fsname,client_ip,idle_time,num_v3_ops,num_v4_ops,num_v4_open_ops,num_v4_close_ops (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka nfs debug-level

Manage debug level for nfs servers.

```sh
weka nfs debug-level [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs debug-level set**

Set debug level for nfs servers. Return to default (EVENT) when finish debugging.

```sh
weka nfs debug-level set <level>
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--nfs-hosts nfs-hosts]...
                         [--help]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------- |
 | `level`* | The debug level, can be one of this options: EVENT, INFO, DEBUG, MID_DEBUG, FULL_DEBUG |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--nfs-hosts`... | Hosts to set debug level (pass weka's host id as a number). All hosts as default (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

**weka nfs debug-level show**

Get debug level for nfs servers.

```sh
weka nfs debug-level show [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--format format]
                          [--nfs-hosts nfs-hosts]...
                          [--output output]...
                          [--sort sort]...
                          [--filter filter]...
                          [--filter-color filter-color]...
                          [--help]
                          [--no-header]
                          [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--nfs-hosts`... | Only return these host IDs (pass weka's host id as a number). All hosts as default (may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: host,debugLevel,component (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka nfs global-config

NFS Global Configuration

```sh
weka nfs global-config [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs global-config set**

Set NFS global configuration options

```sh
weka nfs global-config set [--mountd-port mountd-port]
                           [--config-fs config-fs]
                           [--lockmgr-port lockmgr-port]
                           [--statmon-port statmon-port]
                           [--notify-port notify-port]
                           [--acl acl]
                           [--default-acl-type default-acl-type]
                           [--extended-stats extended-stats]
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--default-supported-versions default-supported-versions]...
                           [--enable-auth-types enable-auth-types]...
                           [--no-restart]
                           [--help]
                           [--force]

```

 | Parameter | Description |
 | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--mountd-port` | Configure the port number of the mountd service |
 | `--config-fs` | config filesystem name, use "" to invalidate |
 | `--lockmgr-port` | The alternate port for the nfs lock manager (default: 0 means any available port) |
 | `--statmon-port` | The alternate port for the nfs status monitor (default: 0 means any available port) |
 | `--notify-port` | The alternate port for notification used in NFSv3 (default: 0 means any available port) |
 | `--acl` | Enables or disables NFSv4 ACL support (default: On) (format: 'on' or 'off') |
 | `--default-acl-type` | Specifies the default ACL type. Options are none, posix, nfsv4, or hybrid (default: posix) (format: 'none', 'posix', 'nfsv4' or 'hybrid') |
 | `--extended-stats` | enable or disable NFS stats collection for each client and permission (default: On) (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--default-supported-versions`... | A comma-separated list of the default supported NFS versions for new permissions (format: 'v3' or 'v4', may be repeated or comma-separated) |
 | `--enable-auth-types`... | A comma-separated list of NFS authentication types -- none, sys, krb5, krb5i, krb5p for permissions (format: 'none', 'sys', 'krb5', 'krb5i' or 'krb5p', may be repeated or comma-separated) |
 | `--no-restart` | Prevents the restart of NFS-W containers when applying changes (default: false) |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. This may cause a temporary disruption in the NFS service. |

**weka nfs global-config show**

Show the NFS global configuration

```sh
weka nfs global-config show [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]
                            [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka nfs interface-group

List interface groups

```sh
weka nfs interface-group [--name name]
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--format format]
                         [--output output]...
                         [--sort sort]...
                         [--filter filter]...
                         [--filter-color filter-color]...
                         [--help]
                         [--no-header]
                         [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,name,mask,gateway,type,status,ips,ports,allowManageGids (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka nfs interface-group add**

Create an interface group

```sh
weka nfs interface-group add <name>
                             <type>
                             [--subnet subnet]
                             [--gateway gateway]
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--format format]
                             [--output output]...
                             [--sort sort]...
                             [--filter filter]...
                             [--filter-color filter-color]...
                             [--help]
                             [--no-header]
                             [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `type`* | Group type. cli subnet type can be NFS |
 | `--subnet` | subnet mask in the 255.255.0.0 format |
 | `--gateway` | gateway ip |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: name,mask,gateway,type,status,ips,ports,allowManageGids (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka nfs interface-group assignment**

List the currently assigned interface for each floating-IP address in the given interface-group. If is not supplied, assignments for all floating-IP addresses will be listed

```sh
weka nfs interface-group assignment [--name name]
                                    [--color color]
                                    [--HOST HOST]
                                    [--PORT PORT]
                                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                    [--TIMEOUT TIMEOUT]
                                    [--profile profile]
                                    [--format format]
                                    [--output output]...
                                    [--sort sort]...
                                    [--filter filter]...
                                    [--filter-color filter-color]...
                                    [--help]
                                    [--no-header]
                                    [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--name` | Group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: ip,host,port,group (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka nfs interface-group remove**

Delete an interface group

```sh
weka nfs interface-group remove <name>
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--force]
                                [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `name`* | Interface group name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected NFS clients and can be undone by re-creating the interface group. |
 | `-h`, `--help` | Show help message |

**weka nfs interface-group ip-range**

Commands that manage nfs interface-groups' ip-ranges

```sh
weka nfs interface-group ip-range [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs interface-group ip-range add**

Add an ip range to an interface group

```sh
weka nfs interface-group ip-range add <name>
                                      <ips>
                                      [--color color]
                                      [--HOST HOST]
                                      [--PORT PORT]
                                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                      [--TIMEOUT TIMEOUT]
                                      [--profile profile]
                                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `ips`* | IP range |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka nfs interface-group ip-range remove**

Delete an ip range from an interface group

```sh
weka nfs interface-group ip-range remove <name>
                                         <ips>
                                         [--color color]
                                         [--HOST HOST]
                                         [--PORT PORT]
                                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                         [--TIMEOUT TIMEOUT]
                                         [--profile profile]
                                         [--force]
                                         [--help]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `ips`* | IP range |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected NFS clients and can be undone by re-creating the IP range. |
 | `-h`, `--help` | Show help message |

**weka nfs interface-group port**

Commands that manage nfs interface-groups' ports

```sh
weka nfs interface-group port [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs interface-group port add**

Add a server port to an interface group

```sh
weka nfs interface-group port add <name>
                                  <server-id>
                                  <port>
                                  [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `server-id`* | Server ID on which the port resides |
 | `port`* | Port's device. (e.g. eth1) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka nfs interface-group port remove**

Delete a server port from an interface group

```sh
weka nfs interface-group port remove <name>
                                     <server-id>
                                     <port>
                                     [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--force]
                                     [--help]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `server-id`* | Server ID on which the port resides |
 | `port`* | Port's device. (e.g. eth1) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected NFS clients and can be undone by re-adding the port. |
 | `-h`, `--help` | Show help message |

**weka nfs interface-group update**

Update an interface group

```sh
weka nfs interface-group update <name>
                                [--subnet subnet]
                                [--gateway gateway]
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Interface group name |
 | `--subnet` | subnet mask in the 255.255.0.0 format |
 | `--gateway` | gateway ip |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka nfs kerberos

NFS Kerberos Commands

```sh
weka nfs kerberos [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs kerberos registration**

NFS Kerberos service registration

```sh
weka nfs kerberos registration [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs kerberos registration setup-ad**

Register NFS Kerberos service with Microsoft Active Directory. Running this command with the `restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs kerberos registration setup-ad <nfs-service-name>
                                        <realm-admin-name>
                                        [realm-admin-passwd]
                                        [--color color]
                                        [--HOST HOST]
                                        [--PORT PORT]
                                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                        [--TIMEOUT TIMEOUT]
                                        [--profile profile]
                                        [--force]
                                        [--restart]
                                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `nfs-service-name`* | NFS FQDN Service Name (Maximum 20 characters for hostname in FQDN |
 | `realm-admin-name`* | KDC Realm Admin Name |
 | `realm-admin-passwd` | KDC Realm Admin password |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--force` | Force this action when the service is already registered (default: false) |
 | `--restart` | Don't restart the NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs kerberos registration setup-mit**

Register NFS Kerberos service with MIT KDC. Running this command with the `restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs kerberos registration setup-mit <nfs-service-name>
                                         <keytab-file>
                                         [--color color]
                                         [--HOST HOST]
                                         [--PORT PORT]
                                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                         [--TIMEOUT TIMEOUT]
                                         [--profile profile]
                                         [--force]
                                         [--restart]
                                         [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `nfs-service-name`* | NFS FQDN Service Name |
 | `keytab-file`* | Path to keytab file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--force` | Force this action when the service is already registered (default: false) |
 | `--restart` | Don't restart the NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs kerberos registration show**

Show NFS Kerberos service registration information

```sh
weka nfs kerberos registration show [--color color]
                                    [--HOST HOST]
                                    [--PORT PORT]
                                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                    [--TIMEOUT TIMEOUT]
                                    [--profile profile]
                                    [--format format]
                                    [--output output]...
                                    [--sort sort]...
                                    [--filter filter]...
                                    [--filter-color filter-color]...
                                    [--help]
                                    [--no-header]
                                    [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: nfs_service_name,kdc_type,generation_id,registration_status (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka nfs kerberos reset**

Wipe out NFS Kerberos Service configuration information. Running this command without the `no-restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs kerberos reset [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--force]
                        [--no-restart]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--force` | Force this action without further confirmation (default: false) |
 | `--no-restart` | Don't restart the NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs kerberos service**

NFS Kerberos service

```sh
weka nfs kerberos service [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs kerberos service setup**

Setup the NFS Kerberos Service information. Running this command with the `restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs kerberos service setup <kdc-realm-name>
                                <kdc-primary-server>
                                <kdc-admin-server>
                                [--kdc-secondary-server kdc-secondary-server]
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--force]
                                [--restart]
                                [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `kdc-realm-name`* | KDC Realm Name |
 | `kdc-primary-server`* | KDC Primary Server |
 | `kdc-admin-server`* | KDC Admin Server |
 | `--kdc-secondary-server` | KDC Secondary Server |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--force` | Force this action when the service is already configured (default: false) |
 | `--restart` | Restart the NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs kerberos service show**

Show NFS Kerberos service setup information

```sh
weka nfs kerberos service show [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--format format]
                               [--output output]...
                               [--sort sort]...
                               [--filter filter]...
                               [--filter-color filter-color]...
                               [--help]
                               [--no-header]
                               [--verbose]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: realm_name,primary_server,secondary_server,admin_server,generation_id,service_status (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka nfs ldap

NFS LDAP Commands

```sh
weka nfs ldap [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs ldap export-openldap**

Export in use configuration information for NFS to use OpenLDAP.

```sh
weka nfs ldap export-openldap <server-name>
                              <ldap-domain>
                              <sssd-conf-file>
                              <idmapd-conf-file>
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `server-name`* | OpenLDAP Server Name |
 | `ldap-domain`* | OpenLDAP Domain |
 | `sssd-conf-file`* | Path to sssd configuration file |
 | `idmapd-conf-file`* | Path to idmapd configuration file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka nfs ldap import-openldap**

Import configuration information for NFS to use OpenLDAP. Running this command without the `no-restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs ldap import-openldap <server-name>
                              <ldap-domain>
                              <sssd-conf-file>
                              <idmapd-conf-file>
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--verify]
                              [--force]
                              [--no-restart]
                              [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `server-name`* | OpenLDAP Server Name |
 | `ldap-domain`* | OpenLDAP Domain |
 | `sssd-conf-file`* | Path to sssd configuration file |
 | `idmapd-conf-file`* | Path to idmapd configuration file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--verify` | verify connectivity with the OpenLDAP server (default: false) |
 | `--force` | Force this action when OpenLDAP client is already setup (default: false) |
 | `--no-restart` | Don't restart NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs ldap reset**

Wipe out NFS LDAP configuration information, This action may disrupt IO service for connected NFS clients if used without no-restart option

```sh
weka nfs ldap reset [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--force]
                    [--no-restart]
                    [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--force` | Force this action without further confirmation (default: false) |
 | `--no-restart` | Don't restart the NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs ldap setup-ad**

Setup configuration information for NFS to use Active Directory LDAP. Running this command without the `no-restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs ldap setup-ad [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--force]
                       [--no-restart]
                       [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--force` | Force this action when Active Directory LDAP client is already setup (default: false) |
 | `--no-restart` | Don't restart the NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs ldap setup-ad-nokrb**

Setup configuration information for NFS to use Active Directory LDAP for ACL only when kerberos is not used. Running this command without the `no-restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs ldap setup-ad-nokrb <server-name>
                             <ldap-domain>
                             <nfs-service-name>
                             <admin-user-name>
                             [admin-user-password]
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--force]
                             [--no-restart]
                             [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `server-name`* | AD Server Name |
 | `ldap-domain`* | AD Domain |
 | `nfs-service-name`* | NFS FQDN Service Name (Maximum 20 characters for hostname in FQDN |
 | `admin-user-name`* | AD Admin Name |
 | `admin-user-password` | AD Admin password |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--force` | Force this action when Active Directory LDAP client is already setup (default: false) |
 | `--no-restart` | Don't restart the NFS-W containers to apply changes (default: false) |
 | `-h`, `--help` | Show help message |

**weka nfs ldap setup-openldap**

Setup configuration information for NFS to use OpenLDAP. Running this command without the `no-restart` option can disrupt IO service for connected NFS clients.

```sh
weka nfs ldap setup-openldap <server-name>
                             <ldap-domain>
                             <reader-user-name>
                             [reader-user-password]
                             [--base-dn base-dn]
                             [--ldap-port-number ldap-port-number]
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--force]
                             [--no-restart]
                             [--help]

```

 | Parameter | Description |
 | --------------------------- | ----------------------------------------------------------------------------------------- |
 | `server-name`* | OpenLDAP Server Name |
 | `ldap-domain`* | OpenLDAP Domain |
 | `reader-user-name`* | OpenLDAP Reader User Name (DN based name e.g. cn=readonly-user,dc=test,dc=example,dc=com) |
 | `reader-user-password` | OpenLDAP Reader User password |
 | `--base-dn` | OpenLDAP Base DN (e.g. dc=myldapdomain,dc=example,dc=com) |
 | `--ldap-port-number O` | penLDAP Port Number (default: 389) |
 | `--color S` | pecify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST S` | pecify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT S` | pecify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT T` | imeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, |
 | `i` | nfinite/unlimited) |
 | `-T`, `--TIMEOUT T` | imeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, |
 | `--profile N` | ame of the connection and authentication profile to use |
 | `--force F` | orce this action when OpenLDAP client is already setup (default: false) |
 | `--no-restart D` | on't restart NFS-W containers to apply changes (default: false) |
 | `-h`, `--help S` | how help message |

**weka nfs ldap show**

Show NFS LDAP setup information

```sh
weka nfs ldap show [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--format format]
                   [--output output]...
                   [--sort sort]...
                   [--filter filter]...
                   [--filter-color filter-color]...
                   [--help]
                   [--no-header]
                   [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: server_type,domain,server_name,server_port,base_dn,reader_name,reader_password,generation_id,setup_status (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka nfs permission

List NFS permissions for a filesystem

```sh
weka nfs permission [--filesystem filesystem]
                    [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--format format]
                    [--output output]...
                    [--sort sort]...
                    [--filter filter]...
                    [--filter-color filter-color]...
                    [--help]
                    [--no-header]
                    [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--filesystem` | File system name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,filesystem,group,path,type,squash,auid,agid,obsdirect,manageGids,options,customOptions,privilegedPort,priority,supportedVersions,enabledAuthTypes,aclType (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka nfs permission add**

Allow a client group to access a file system

```sh
weka nfs permission add <filesystem>
                        <group>
                        [--path path]
                        [--permission-type permission-type]
                        [--root-squashing root-squashing]
                        [--squash squash]
                        [--anon-uid anon-uid]
                        [--anon-gid anon-gid]
                        [--obs-direct obs-direct]
                        [--manage-gids manage-gids]
                        [--privileged-port privileged-port]
                        [--acl-type acl-type]
                        [--force-acl-type force-acl-type]
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--supported-versions supported-versions]...
                        [--enable-auth-types enable-auth-types]...
                        [--no-restart]
                        [--force]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | File system name |
 | `group`* | Client group name |
 | `--path` | path \[default: /] |
 | `--permission-type` | Permission type (format: 'ro' or 'rw') |
 | `--root-squashing` | Root squashing (format: 'on' or 'off') |
 | `--squash` | Permission squashing. NOTE - The option 'all' can be used only on interface groups with --allow-manage-gids=on (format: 'none', 'root' or 'all') |
 | `--anon-uid` | Anonymous UID to be used instead of root when root squashing is enabled |
 | `--anon-gid` | Anonymous GID to be used instead of root when root squashing is enabled |
 | `--obs-direct` | Obs direct (format: 'on' or 'off') |
 | `--manage-gids` | the list of group ids received from the client will be replaced by a list of group ids determined by an appropriate lookup on the server. NOTE - this only works with a interface group which allows manage-gids (format: 'on' or 'off') |
 | `--privileged-port` | Privileged port (format: 'on' or 'off') |
 | `--acl-type` | Specifies the ACL type. Options include none, posix, nfsv4, and hybrid. Default is determined by the NFS global configuration (format: 'none', 'posix', 'nfsv4' or 'hybrid') |
 | `--force-acl-type` | Forces a change to the ACL type for existing permissions on the same filesystem (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--supported-versions`... | A comma-separated list of supported NFS versions (format: 'v3' or 'v4', may be repeated or comma-separated) |
 | `--enable-auth-types`... | A comma-separated list of NFS authentication types -- none, sys, krb5, krb5i, krb5p (format: 'none', 'sys', 'krb5', 'krb5i' or 'krb5p', may be repeated or comma-separated) |
 | `--no-restart` | Prevents the restart of NFS-W containers when applying changes (default: false), null, No.HideFromUsage |
 | `-f`, `--force` | Force this action without further confirmation. This action will affect all NFS users of this permission/export, Use it with caution and consult the Weka Customer Success team at need. |
 | `-h`, `--help` | Show help message |

**weka nfs permission remove**

Delete a file system permission

```sh
weka nfs permission remove <filesystem>
                           <group>
                           [--path path]
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--force]
                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `filesystem`* | File system name |
 | `group`* | Client group name |
 | `--path` | path \[default: /] |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected NFS clients and can be undone by re-creating the filesystem permission. |
 | `-h`, `--help` | Show help message |

**weka nfs permission update**

Edit a file system permission

```sh
weka nfs permission update <filesystem>
                           <group>
                           [--path path]
                           [--permission-type permission-type]
                           [--root-squashing root-squashing]
                           [--squash squash]
                           [--anon-uid anon-uid]
                           [--anon-gid anon-gid]
                           [--obs-direct obs-direct]
                           [--manage-gids manage-gids]
                           [--custom-options custom-options]
                           [--privileged-port privileged-port]
                           [--acl-type acl-type]
                           [--force-acl-type force-acl-type]
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--supported-versions supported-versions]...
                           [--enable-auth-types enable-auth-types]...
                           [--force]
                           [--no-restart]
                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `filesystem`* | File system name |
 | `group`* | Client group name |
 | `--path` | path \[default: /] |
 | `--permission-type` | Permission type (format: 'ro' or 'rw') |
 | `--root-squashing` | Root squashing (format: 'on' or 'off') |
 | `--squash` | Permission squashing. NOTE - The option 'all' can be used only on interface groups with --allow-manage-gids=on (format: 'none', 'root' or 'all') |
 | `--anon-uid` | Anonymous UID to be used instead of root when root squashing is enabled |
 | `--anon-gid` | Anonymous GID to be used instead of root when root squashing is enabled |
 | `--obs-direct` | Obs direct (format: 'on' or 'off') |
 | `--manage-gids` | the list of group ids received from the client will be replaced by a list of group ids determined by an appropriate lookup on the server. NOTE - this only works with a interface group which allows manage-gids (format: 'on' or 'off') |
 | `--custom-options` | Custom export options |
 | `--privileged-port` | Privileged port (format: 'on' or 'off') |
 | `--acl-type` | Specifies the ACL type. Options include none, posix, nfsv4, and hybrid. Default is determined by the NFS global configuration (format: 'none', 'posix', 'nfsv4' or 'hybrid') |
 | `--force-acl-type` | Forces a change to the ACL type for existing permissions on the same filesystem (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--supported-versions`... | A comma-separated list of supported NFS versions (format: 'v3' or 'v4', may be repeated or comma-separated) |
 | `--enable-auth-types`... | A comma-separated list of NFS authentication types -- none, sys, krb5, krb5i, krb5p (format: 'none', 'sys', 'krb5', 'krb5i' or 'krb5p', may be repeated or comma-separated) |
 | `--force` | Force this action without further confirmation |
 | `--no-restart` | Prevents the restart of NFS-W containers when applying changes (default: false), null, No.HideFromUsage |
 | `-h`, `--help` | Show help message |

#### weka nfs rules

Commands that manage NFS-rules

```sh
weka nfs rules [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs rules add**

Commands that add NFS-rules

```sh
weka nfs rules add [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs rules add dns**

Add a DNS rule to an NFS client group

```sh
weka nfs rules add dns <name>
                       <dns>
                       [--ip ip]
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Group name |
 | `dns`* | DNS rule with *?\[] wildcards rule |
 | `--ip` | IP with netmask or CIDR rule, in the 1.1.1.1/255.255.0.0 or 1.1.1.1/16 format |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka nfs rules add ip**

Add an IP rule to an NFS client group

```sh
weka nfs rules add ip <name>
                      <ip>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Group name |
 | `ip`* | IP with netmask or CIDR rule, in the 1.1.1.1/255.255.0.0 or 1.1.1.1/16 format |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka nfs rules remove**

Commands for deleting NFS-rules

```sh
weka nfs rules remove [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka nfs rules remove dns**

Delete a DNS rule from an NFS client group

```sh
weka nfs rules remove dns <name>
                          <dns>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Group name |
 | `dns`* | DNS rule with *?\[] wildcards rule |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka nfs rules remove ip**

Delete an IP rule from an NFS client group

```sh
weka nfs rules remove ip <name>
                         <ip>
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | Group name |
 | `ip`* | IP with netmask or CIDR rule, in the 1.1.1.1/255.255.0.0 or 1.1.1.1/16 format |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

### weka org

List organizations defined in the Weka cluster

```sh
weka org [--color color]
         [--HOST HOST]
         [--PORT PORT]
         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
         [--TIMEOUT TIMEOUT]
         [--profile profile]
         [--format format]
         [--output output]...
         [--sort sort]...
         [--filter filter]...
         [--filter-color filter-color]...
         [--help]
         [--raw-units]
         [--UTC]
         [--no-header]
         [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,name,allocSSD,quotaSSD,allocTotal,quotaTotal,policyNames,policyIds (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka org add

Create a new organization in the Weka cluster

```sh
weka org add <name>
             <username>
             [--ssd-quota ssd-quota]
             [--total-quota total-quota]
             [--color color]
             [--HOST HOST]
             [--PORT PORT]
             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
             [--TIMEOUT TIMEOUT]
             [--profile profile]
             [--help]
             [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Organization name |
 | `username`* | Username of organization admin |
 | `password`* | Password of organization admin |
 | `--ssd-quota` | SSD quota (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--total-quota` | Total quota (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka org remove

Delete an organization

```sh
weka org remove <org>
                [--color color]
                [--HOST HOST]
                [--PORT PORT]
                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                [--TIMEOUT TIMEOUT]
                [--profile profile]
                [--force]
                [--help]
                [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `org`* | Organization name or ID |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action will DELETE ALL DATA stored in this organization's filesystems and cannot be undone. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka org rename

Change an organization name

```sh
weka org rename <org>
                <new-name>
                [--color color]
                [--HOST HOST]
                [--PORT PORT]
                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                [--TIMEOUT TIMEOUT]
                [--profile profile]
                [--help]
                [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `org`* | Current organization name or ID |
 | `new-name`* | New organization name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka org security

Manages organization security

```sh
weka org security [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka org security policy**

Manages organization security policies.

```sh
weka org security policy [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka org security policy attach**

Attaches new security policies to an organization, adding them to the existing policies.

```sh
weka org security policy attach <org>
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]
                                [--json]
                                [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `org`* | Organization name or ID. |
 | `policies`... | Security policy names or IDs to attach to the organization. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka org security policy detach**

Removes security policies from an organization.

```sh
weka org security policy detach <org>
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]
                                [--json]
                                [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `org`* | Organization name or ID. |
 | `policies`... | Security policy names or IDs to remove from the organization. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka org security policy list**

List organization security policies.

```sh
weka org security policy list <org>
                              [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--format format]
                              [--output output]...
                              [--sort sort]...
                              [--filter filter]...
                              [--filter-color filter-color]...
                              [--help]
                              [--raw-units]
                              [--UTC]
                              [--no-header]
                              [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `org`* | Organization name or ID. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: position,uid,id,name (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka org security policy reset**

Removes all security policies from an organization.

```sh
weka org security policy reset <org>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]
                               [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `org`* | Organization name or ID. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka org security policy set**

Sets security policies for an organization, replacing the existing list of policies.

```sh
weka org security policy set <org>
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--help]
                             [--json]
                             [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `org`* | Organization name or ID. |
 | `policies`... | Security policy names or IDs to assign to the organization. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka org security revoke-tokens**

Revokes all API tokens issued for this organization.

```sh
weka org security revoke-tokens <org>
                                [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]
                                [--force]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `org`* | Organization name or ID. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. This action will log all users out of the organization. |

#### weka org set-quota

Set an organization's SSD and/or total quotas

```sh
weka org set-quota <org>
                   [--ssd-quota ssd-quota]
                   [--total-quota total-quota]
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--help]
                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
 | `org`* | Organization name or ID |
 | `--ssd-quota` | SSD quota (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--total-quota` | Total quota (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

### weka s3

Commands that manage Weka's S3 container

```sh
weka s3 [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka s3 bucket

S3 Cluster Bucket Commands

```sh
weka s3 bucket [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 bucket add**

Create an S3 bucket

```sh
weka s3 bucket add <name>
                   [--policy policy]
                   [--policy-json policy-json]
                   [--hard-quota hard-quota]
                   [--existing-path existing-path]
                   [--fs-name fs-name]
                   [--fs-id fs-id]
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--force]
                   [--help]
                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `name`* | bucket name to create |
 | `--policy` | Set an existing S3 policy for a bucket |
 | `--policy-json` | Get S3 policy for bucket in JSON format |
 | `--hard-quota` | Hard limit for the directory (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--existing-path` | existing path |
 | `--fs-name` | file system name |
 | `--fs-id` | file system id |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force when existing-path has quota |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket remove**

Destroy an S3 bucket

```sh
weka s3 bucket remove <name>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--unlink]
                      [--help]
                      [--json]
                      [--force]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------- |
 | `name`* | bucket name to destroy |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--unlink` | unlinks the bucket, but leave the data directory in place |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected S3 clients.. |

**weka s3 bucket lifecycle-rule**

S3 Bucket Lifecycle

```sh
weka s3 bucket lifecycle-rule [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 bucket lifecycle-rule add**

Add a lifecycle rule to an S3 Bucket

```sh
weka s3 bucket lifecycle-rule add <bucket>
                                  <expiry-days>
                                  [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--prefix prefix]
                                  [--tags tags]
                                  [--help]
                                  [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket`* | S3 Bucket Name |
 | `expiry-days`* | expiry days |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--prefix` | prefix |
 | `--tags` | object tags |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket lifecycle-rule list**

List all lifecycle rules of an S3 bucket

```sh
weka s3 bucket lifecycle-rule list <bucket>
                                   [--color color]
                                   [--HOST HOST]
                                   [--PORT PORT]
                                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                   [--TIMEOUT TIMEOUT]
                                   [--profile profile]
                                   [--format format]
                                   [--output output]...
                                   [--sort sort]...
                                   [--filter filter]...
                                   [--filter-color filter-color]...
                                   [--help]
                                   [--no-header]
                                   [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `bucket`* | S3 Bucket Name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,expiry_days,prefix,tags (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka s3 bucket lifecycle-rule remove**

Remove a lifecycle rule from an S3 bucket

```sh
weka s3 bucket lifecycle-rule remove <bucket>
                                     <name>
                                     [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--help]
                                     [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket`* | S3 Bucket Name |
 | `name`* | rule name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket lifecycle-rule reset**

Reset all lifecycle rules of an S3 bucket

```sh
weka s3 bucket lifecycle-rule reset <bucket>
                                    [--color color]
                                    [--HOST HOST]
                                    [--PORT PORT]
                                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                    [--TIMEOUT TIMEOUT]
                                    [--profile profile]
                                    [--help]
                                    [--json]
                                    [--force]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket`* | S3 Bucket Name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-f`, `--force` | Force this action without further confirmation. This action will delete the existing S3 bucket rules. |

**weka s3 bucket list**

Show all the buckets on the S3 cluster

```sh
weka s3 bucket list [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--format format]
                    [--output output]...
                    [--sort sort]...
                    [--filter filter]...
                    [--filter-color filter-color]...
                    [--help]
                    [--raw-units]
                    [--UTC]
                    [--no-header]
                    [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: name,hard,used,path,fs (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka s3 bucket policy**

S3 bucket policy commands

```sh
weka s3 bucket policy [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 bucket policy get**

Get S3 policy for bucket

```sh
weka s3 bucket policy get <bucket-name>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]
                          [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket-name`* | full path to bucket to get policy for |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket policy get-json**

Get S3 policy for bucket in JSON format

```sh
weka s3 bucket policy get-json <bucket-name>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]
                               [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket-name`* | full path to bucket to get policy for |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket policy set**

| Set an existing S3 policy for a bucket, Available predefined options are : none | download | upload | public |

```sh
weka s3 bucket policy set <bucket-name>
                          <bucket-policy>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]
                          [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket-name`* | full path to bucket to set policy for |
 | `bucket-policy`* | Set an existing S3 policy. Available predefined options are: none |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket policy set-custom**

Set a custom S3 policy for bucket

```sh
weka s3 bucket policy set-custom <bucket-name>
                                 <policy-file>
                                 [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--help]
                                 [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket-name`* | Full path to bucket to set policy for |
 | `policy-file`* | Path of the file containing the policy rules |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket policy reset**

Unset the configured S3 policy for bucket

```sh
weka s3 bucket policy reset <bucket-name>
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]
                            [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `bucket-name`* | full path to bucket to unset the policy for |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 bucket quota**

S3 Bucket Quota, configure the hard limit of bucket disk usage

```sh
weka s3 bucket quota [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 bucket quota set**

Set the hard limit of bucket's disk usage

```sh
weka s3 bucket quota set <name>
                         <hard-quota>
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `name`* | bucket name |
 | `hard-quota`* | Hard limit for the directory (format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka s3 bucket quota reset**

Remove the hard limit on bucket's disk usage

```sh
weka s3 bucket quota reset <name>
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `name`* | bucket name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka s3 cluster

View info about the S3 cluster managed by weka

```sh
weka s3 cluster [--color color]
                [--HOST HOST]
                [--PORT PORT]
                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                [--TIMEOUT TIMEOUT]
                [--profile profile]
                [--verbose]
                [--help]
                [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-v`, `--verbose` | Verbose mode |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 cluster audit-webhook**

S3 Cluster Audit Webhook Commands

```sh
weka s3 cluster audit-webhook [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 cluster audit-webhook disable**

Disable the Audit Webhook

```sh
weka s3 cluster audit-webhook disable [--color color]
                                      [--HOST HOST]
                                      [--PORT PORT]
                                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                      [--TIMEOUT TIMEOUT]
                                      [--profile profile]
                                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka s3 cluster audit-webhook enable**

Enable/Disable the S3 audit webhook on the S3 Cluster

```sh
weka s3 cluster audit-webhook enable [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--endpoint endpoint]
                                     [--auth-token auth-token]
                                     [--help]
                                     [--verify]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--endpoint` | The webhook endpoint |
 | `--auth-token` | The webhook authentication token |
 | `-h`, `--help` | Show help message |
 | `--verify` | verification to apply configuration |

**weka s3 cluster audit-webhook show**

Show the S3 Audit Webhook configuration

```sh
weka s3 cluster audit-webhook show [--color color]
                                   [--HOST HOST]
                                   [--PORT PORT]
                                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                   [--TIMEOUT TIMEOUT]
                                   [--profile profile]
                                   [--help]
                                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 cluster container**

Commands that manage Weka's S3 cluster's containers

```sh
weka s3 cluster container [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 cluster container add**

Add S3 containers to S3 cluster

```sh
weka s3 cluster container add [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--help]
                              [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | The containers to add to the S3 cluster |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka s3 cluster container list**

Lists containers in S3 cluster

```sh
weka s3 cluster container list [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]
                               [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 cluster container remove**

Remove S3 containers from S3 cluster

```sh
weka s3 cluster container remove [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--help]
                                 [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | The containers to remove from the S3 cluster |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka s3 cluster add**

Create an S3 cluster managed by weka

```sh
weka s3 cluster add <default-fs-name>
                    <config-fs-name>
                    [--port port]
                    [--key key]
                    [--secret secret]
                    [--max-buckets-limit max-buckets-limit]
                    [--anonymous-posix-uid anonymous-posix-uid]
                    [--anonymous-posix-gid anonymous-posix-gid]
                    [--domain domain]
                    [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--container container]...
                    [--all-servers]
                    [--force]
                    [--help]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `default-fs-name`* | S3 default filesystem name |
 | `config-fs-name`* | S3 config filesystem name |
 | `--port` | S3 service port |
 | `--key` | S3 service key |
 | `--secret` | S3 service secret |
 | `--max-buckets-limit` | Limit the number of buckets that can be created |
 | `--anonymous-posix-uid` | POSIX UID for anonymous users |
 | `--anonymous-posix-gid` | POSIX GID for anonymous users |
 | `--domain` | Virtual host-style comma seperated domains |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--container`... | The containers that will serve via the S3 protocol (pass weka's container ID as a number) (may be repeated or comma-separated) |
 | `--all-servers` | Install S3 on all servers |
 | `-f`, `--force` | Force this action without further confirmation. Be aware that this will impact all S3 buckets within the S3 service. Exercise caution and consult the WEKA Customer Success team if assistance is required. |
 | `-h`, `--help` | Show help message |

**weka s3 cluster remove**

Destroy the S3 cluster managed by weka. This will not delete the data, just stop exposing it via S3

```sh
weka s3 cluster remove [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--force]
                       [--help]

```

 | Parameter | Description |
 | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. |
 | `troying the S3 cluster r` | emoves the S3 service and its associated configuration, including IAM policies, buckets, and ILM rules. access will no longer be available for clients. |
 | `s operation does not aut` | omatically delete the data stored within the buckets. |
 | `ever`, `internal users wit` | h S3 roles will be permanently removed from the system.. |
 | `-h`, `--help` | Show help message |

**weka s3 cluster status**

Show which of the containers are ready.

```sh
weka s3 cluster status [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--format format]
                       [--output output]...
                       [--sort sort]...
                       [--filter filter]...
                       [--filter-color filter-color]...
                       [--help]
                       [--no-header]
                       [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: id,hostname,statusTitle,ip,port,versions,uptime,requests,lastError,failureTime (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka s3 cluster update**

Update an S3 cluster

```sh
weka s3 cluster update [--key key]
                       [--secret secret]
                       [--port port]
                       [--anonymous-posix-uid anonymous-posix-uid]
                       [--anonymous-posix-gid anonymous-posix-gid]
                       [--domain domain]
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--container container]...
                       [--all-servers]
                       [--force]
                       [--help]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--key` | S3 service key |
 | `--secret` | S3 service secret |
 | `--port` | S3 service port |
 | `--anonymous-posix-uid` | POSIX UID for anonymous users |
 | `--anonymous-posix-gid` | POSIX GID for anonymous users |
 | `--domain` | Virtual host-style comma seperated domains. Empty to disable |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--container`... | The containers that will serve via the S3 protocol (may be repeated or comma-separated) |
 | `--all-servers` | Install S3 on all servers |
 | `-f`, `--force` | Force this action without further confirmation. Be aware that this will impact all S3 buckets within the S3 service. Exercise caution and consult the WEKA Customer Success team if assistance is required. |
 | `-h`, `--help` | Show help message |

#### weka s3 policy

S3 policy commands

```sh
weka s3 policy [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 policy add**

Add an S3 IAM policy

```sh
weka s3 policy add <policy-name>
                   <policy-file>
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--help]
                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policy-name`* | The policy name |
 | `policy-file`* | Path of the file containing the policy rules |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 policy attach**

Attach an S3 policy to a user

```sh
weka s3 policy attach <policy>
                      <user>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policy`* | Policy name to attach |
 | `user`* | User name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 policy detach**

Detach an S3 policy from a user

```sh
weka s3 policy detach <user>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `user`* | User name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 policy list**

Print a list of the existing S3 IAM policies

```sh
weka s3 policy list [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--format format]
                    [--output output]...
                    [--sort sort]...
                    [--filter filter]...
                    [--filter-color filter-color]...
                    [--help]
                    [--no-header]
                    [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: name (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka s3 policy remove**

Remove an S3 IAM policy

```sh
weka s3 policy remove <policy>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policy`* | Policy name to remove |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 policy show**

Show the details of an S3 IAM policy

```sh
weka s3 policy show <policy-name>
                    [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--help]
                    [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policy-name`* | Policy name to show |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka s3 service-account

S3 service account commands. Should be run only with an S3 user role

```sh
weka s3 service-account [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 service-account add**

Add an S3 service account

```sh
weka s3 service-account add [--policy-file policy-file]
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]
                            [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--policy-file` | Policy file path |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 service-account list**

Print a list of the user's S3 service accounts

```sh
weka s3 service-account list [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--format format]
                             [--output output]...
                             [--sort sort]...
                             [--filter filter]...
                             [--filter-color filter-color]...
                             [--help]
                             [--no-header]
                             [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: accessKey (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka s3 service-account remove**

Remove an S3 service account

```sh
weka s3 service-account remove <access_key>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]
                               [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `access_key`* | Access key of the service account to remove |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka s3 service-account show**

Show the details of an S3 service account

```sh
weka s3 service-account show <access_key>
                             [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--help]
                             [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `access_key`* | Access key of the service account to show |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka s3 sts

S3 security token commands

```sh
weka s3 sts [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka s3 sts assume-role**

Generate a temporary security token with an assumed role using existing user credentials

```sh
weka s3 sts assume-role [--access-key access-key]
                        [--secret-key secret-key]
                        [--policy-file policy-file]
                        [--duration duration]
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]
                        [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--access-key` | Access key |
 | `--secret-key` | Secret key |
 | `--policy-file` | Policy file path |
 | `--duration` | Duration, valid values: 15 minutes to 52 weeks and 1 day (format: 3s, 2h, 4m, 1d, 1d5h, 1w) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

### weka security

Security commands.

```sh
weka security [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka security ca-cert

Commands handling custom CA signed certificate

```sh
weka security ca-cert [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security ca-cert download**

Download the Weka cluster custom certificate, if such certificate was set

```sh
weka security ca-cert download <path>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `path`* | Path to output file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security ca-cert set**

Add a custom certificate to the certificates list. If a custom certificate is already set, this command updates it.

```sh
weka security ca-cert set [--cert-file cert-file]
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--cert-file` | Path to certificate file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security ca-cert status**

Show the Weka cluster CA-cert status and certificate

```sh
weka security ca-cert status [--color color]
                             [--HOST HOST]
                             [--PORT PORT]
                             [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                             [--TIMEOUT TIMEOUT]
                             [--profile profile]
                             [--help]
                             [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security ca-cert reset**

Unsets custom CA signed certificate from cluster

```sh
weka security ca-cert reset [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka security cors-trusted-sites

Commands for handling Cross Origin Resource Sharing weka apis

```sh
weka security cors-trusted-sites [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security cors-trusted-sites add**

Add a trusted site to list, provide url with http or https prefix and port number if not a standard port.

```sh
weka security cors-trusted-sites add <site>
                                     [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `site`* | Trusted site |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security cors-trusted-sites list**

Lists the set of trusted sites where CORS in configured

```sh
weka security cors-trusted-sites list [--color color]
                                      [--HOST HOST]
                                      [--PORT PORT]
                                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                      [--TIMEOUT TIMEOUT]
                                      [--profile profile]
                                      [--help]
                                      [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security cors-trusted-sites remove**

Remove the specified site from the trusted list.

```sh
weka security cors-trusted-sites remove <site>
                                        [--color color]
                                        [--HOST HOST]
                                        [--PORT PORT]
                                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                        [--TIMEOUT TIMEOUT]
                                        [--profile profile]
                                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `site`* | Site to remove from the trusted list |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security cors-trusted-sites remove-all**

Removes all trusted sites for Cross Origin Resource Sharing

```sh
weka security cors-trusted-sites remove-all [--color color]
                                            [--HOST HOST]
                                            [--PORT PORT]
                                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                            [--TIMEOUT TIMEOUT]
                                            [--profile profile]
                                            [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka security kms

List the currently configured key management service settings

```sh
weka security kms [--color color]
                  [--HOST HOST]
                  [--PORT PORT]
                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                  [--TIMEOUT TIMEOUT]
                  [--profile profile]
                  [--help]
                  [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security kms rewrap**

Rewraps all the master filesystem keys using the configured KMS. This can be used to rewrap with a rotated KMS key, or to change wrapping to the newly-configured KMS.

```sh
weka security kms rewrap [--new-key-uid new-key-uid]
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--all]
                         [--convert-to-cluster-key-on-fs]
                         [--help]
                         [--json]

```

 | Parameter | Description |
 | -------------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--new-key-uid` | (KMIP-only) Unique identifier for the new key to be used to wrap filesystem keys |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--all` | rewrap all the filesystem keys |
 | `--convert-to-cluster-key-on-fs` | Convert all encrypted filesystems to use cluster key |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security kms set**

Configure the active KMS

```sh
weka security kms set <type>
                      <address>
                      <key-identifier>
                      [--token token]
                      [--namespace namespace]
                      [--client-cert client-cert]
                      [--client-key client-key]
                      [--ca-cert ca-cert]
                      [--role-id role-id]
                      [--secret-id secret-id]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--convert-to-cluster-key-on-fs]
                      [--help]

```

 | Parameter | Description |
 | -------------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `type`* | KMS type, one of \["vault", "kmip"] |
 | `address`* | Server address, usually a hostname:port or a URL |
 | `key-identifier`* | Key to secure the filesystem keys with, e.g a key name (for Vault) or a key uid (for KMIP) |
 | `--token` | API token to access the KMS |
 | `--namespace` | Namespace (Vault, optional) |
 | `--client-cert` | Path to the client certificate PEM file |
 | `--client-key` | Path to the client key PEM file |
 | `--ca-cert` | Path to the CA certificate PEM file |
 | `--role-id` | Role Id to access the KMS |
 | `--secret-id` | Secret Id to access the KMS |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--convert-to-cluster-key-on-fs` | Convert all encrypted filesystems to use cluster key |
 | `-h`, `--help` | Show help message |

**weka security kms reset**

Remove external KMS configurations. This will fail if there are any encrypted filesystems that rely on the KMS.

```sh
weka security kms reset [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--allow-downgrade]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--allow-downgrade` | Allows downgrading existing encrypted filesystems to local encryption instead of a KMS |
 | `-h`, `--help` | Show help message |

#### weka security lockout-config

Commands used to interact with the account lockout config parameters

```sh
weka security lockout-config [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security lockout-config reset**

Reset the number of failed attempts before lockout and the duration of lock to their defaults

```sh
weka security lockout-config reset [--color color]
                                   [--HOST HOST]
                                   [--PORT PORT]
                                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                   [--TIMEOUT TIMEOUT]
                                   [--profile profile]
                                   [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security lockout-config set**

Configure the number of failed attempts before lockout and the duration of lock

```sh
weka security lockout-config set [--failed-attempts failed-attempts]
                                 [--lockout-duration lockout-duration]
                                 [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
 | `--failed-attempts` | Number of consecutive failed logins before user account locks out |
 | `--lockout-duration` | How long the account should be locked out for after failed logins (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security lockout-config show**

Show the current number of attempts needed to lockout and how long the lockout is for

```sh
weka security lockout-config show [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--help]
                                  [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka security login-banner

Commands used to view and edit the login banner

```sh
weka security login-banner [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security login-banner disable**

Disable the login banner

```sh
weka security login-banner disable [--color color]
                                   [--HOST HOST]
                                   [--PORT PORT]
                                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                   [--TIMEOUT TIMEOUT]
                                   [--profile profile]
                                   [--help]
                                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security login-banner enable**

Enable the login banner

```sh
weka security login-banner enable [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--help]
                                  [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security login-banner reset**

Resets the login banner back to the default state (empty)

```sh
weka security login-banner reset [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security login-banner set**

Set the login banner

```sh
weka security login-banner set <login-banner>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `login-banner`* | Text banner to be displayed before the user logs into the web UI |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security login-banner show**

Show the current login banner

```sh
weka security login-banner show [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]
                                [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka security policy

Manages security policies.

```sh
weka security policy [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security policy add**

Creates a new security policy.

```sh
weka security policy add <name>
                         [--description description]
                         [--action action]
                         [--read-only read-only]
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--ips ips]...
                         [--roles roles]...
                         [--help]
                         [--json]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | Name of the new security policy. (up to 64 alphanumeric characters, hyphens (-), underscores (_), and periods (.), starting with a letter) |
 | `--description` | Description of the security policy. (up to 256 characters) |
 | `--action` | Whether access is granted or denied when the security policy matches. (format: 'allow' or 'deny') |
 | `--read-only` | The security policy allows read-only mounts only. (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--ips`... | IP address ranges to which the security policy applies. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | `--roles`... | User roles to which the security policy applies. (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi', may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security policy remove**

Deletes a security policy.

```sh
weka security policy remove <policy>
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]
                            [--force]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------- |
 | `policy`* | Name or ID of security policy. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. Security policy details will be lost with no chance for recovery. |

**weka security policy duplicate**

Duplicates an existing security policy, creating a new one.

```sh
weka security policy duplicate <policy>
                               <name>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--help]
                               [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
 | `policy`* | Name or ID of the security policy to duplicate. |
 | `name`* | Name of the new security policy. (up to 64 alphanumeric characters, hyphens (-), underscores (_), and periods (.), starting with a letter) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security policy join**

Manages security policies related to cluster joining.

```sh
weka security policy join [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security policy join attach**

Adds new security policies applied when joining cluster, adding them to the existing policies.

```sh
weka security policy join attach [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--client]
                                 [--backend]
                                 [--force]
                                 [--help]
                                 [--json]
                                 [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policies`... | Security policy names or IDs to attach to cluster join process. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-c`, `--client` | Apply policies to clients. |
 | `-b`, `--backend` | Apply policies to backends. |
 | `-f`, `--force` | Force update, bypassing safeguards (may disrupt cluster members!) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security policy join detach**

Removes security policies applied when joining cluster.

```sh
weka security policy join detach [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--client]
                                 [--backend]
                                 [--force]
                                 [--help]
                                 [--json]
                                 [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policies`... | Security policy names or IDs to remove from cluster join process. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-c`, `--client` | Apply policies to clients. |
 | `-b`, `--backend` | Apply policies to backends. |
 | `-f`, `--force` | Force update, bypassing safeguards (may disrupt cluster members!) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security policy join list**

Lists security policies applied when joining containers.

```sh
weka security policy join list [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--format format]
                               [--output output]...
                               [--sort sort]...
                               [--filter filter]...
                               [--filter-color filter-color]...
                               [--client]
                               [--backend]
                               [--help]
                               [--raw-units]
                               [--UTC]
                               [--no-header]
                               [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: mode,policyNames,policyIds (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-c`, `--client` | List policies for clients. |
 | `-b`, `--backend` | List policies for backends. |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka security policy join reset**

Removes all security policies applied when joining cluster.

```sh
weka security policy join reset [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--client]
                                [--backend]
                                [--help]
                                [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-c`, `--client` | Reset policies applied to clients. |
 | `-b`, `--backend` | Reset policies applied to backends. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security policy join set**

Sets security policies for joining cluster, replacing the existing set of policies.

```sh
weka security policy join set [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--client]
                              [--backend]
                              [--force]
                              [--help]
                              [--json]
                              [<policies>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policies`... | Security policy names or IDs applied to cluster join process. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-c`, `--client` | Apply policies to clients. |
 | `-b`, `--backend` | Apply policies to backends. |
 | `-f`, `--force` | Force update, bypassing safeguards (may disrupt cluster members!) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security policy list**

List security policies defined in the Weka cluster.

```sh
weka security policy list [--action action]
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--format format]
                          [--roles roles]...
                          [--ips ips]...
                          [--output output]...
                          [--sort sort]...
                          [--filter filter]...
                          [--filter-color filter-color]...
                          [--help]
                          [--raw-units]
                          [--UTC]
                          [--no-header]
                          [--verbose]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | `--action` | Lists security policies that match a specific action. (format: 'allow' or 'deny') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--roles`... | Lists security policies that include specific roles. (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi', may be repeated or comma-separated) |
 | `--ips`... | Lists security policies that include specific IP address ranges. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,name,refCount,description,action,roles,ips,readonly,createdBy,createdAt,modifiedBy,modifiedAt (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka security policy show**

Displays information about a specific security policy.

```sh
weka security policy show <policy>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--json]
                          [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `policy`* | Name or ID of security policy. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-J`, `--json` | Format output as JSON |
 | `-h`, `--help` | Show help message |

**weka security policy test**

Simulates the effect of one or more security policies.

```sh
weka security policy test [--role role]
                          [--ip ip]
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--join]
                          [--help]
                          [--json]
                          [<policy>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
 | `policy`... | Policies to evaluate, with access verified in the order listed. |
 | `--role` | Simulate effect of policies on API access from the given user role. (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi') |
 | `--ip` | IP address to evaluate as the source address. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--join` | Simulate effect of policies when joining the cluster. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security policy update**

Updates the settings of an existing security policy.

```sh
weka security policy update <policy>
                            [--description description]
                            [--action action]
                            [--new-name new-name]
                            [--read-only read-only]
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--roles roles]...
                            [--add-roles add-roles]...
                            [--remove-roles remove-roles]...
                            [--ips ips]...
                            [--add-ips add-ips]...
                            [--remove-ips remove-ips]...
                            [--force]
                            [--help]
                            [--json]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `policy`* | Name or ID of security policy. |
 | `--description` | Updates the description of the security policy. (up to 256 characters) |
 | `--action` | Changes whether access is granted when the security policy matches. (format: 'allow' or 'deny') |
 | `--new-name` | New name of the security policy. (up to 64 alphanumeric characters, hyphens (-), underscores (_), and periods (.), starting with a letter) |
 | `--read-only` | The security policy allows read-only mounts only. (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--roles`... | User roles to which the security policy applies. (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi', may be repeated or comma-separated) |
 | `--add-roles`... | User roles to append to the security policy. (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi', may be repeated or comma-separated) |
 | `--remove-roles`... | User roles to remove from the security policy. (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi', may be repeated or comma-separated) |
 | `--ips`... | IP address ranges to which the security policy applies. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | `--add-ips`... | IP address ranges to append to the security policy. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | `--remove-ips`... | IP address ranges to remove from the security policy. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | `-f`, `--force` | Force update, bypassing safeguards (may disrupt cluster members!) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka security tls

TLS commands.

```sh
weka security tls [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security tls download**

Download the Weka cluster TLS certificate

```sh
weka security tls download <path>
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `path`* | Path to output file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security tls local**

TLS local configuration commands

```sh
weka security tls local [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka security tls local set**

Make HTTP server use local TLS configuration. If local TLS already configured, updates the configuration.

```sh
weka security tls local set [--private-key private-key]
                            [--certificate certificate]
                            [--ca-cert ca-cert]
                            [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--all]
                            [--help]
                            [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids to apply local TLS configuration |
 | `--private-key` | Path to TLS private key pem file |
 | `--certificate` | Path to TLS certificate pem file |
 | `--ca-cert` | Path to TLS CA certificate pem file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--all` | Apply TLS configuration on all the backend containers in the cluster |
 | `-h`, `--help` | Show help message |

**weka security tls local reset**

Removes the local TLS configuration.

```sh
weka security tls local reset [--color color]
                              [--HOST HOST]
                              [--PORT PORT]
                              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                              [--TIMEOUT TIMEOUT]
                              [--profile profile]
                              [--private-key]
                              [--ca-cert]
                              [--all]
                              [--help]
                              [<container-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `container-ids`... | A list of container ids to apply local TLS configuration |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--private-key` | Remove local TLS private key and associated certificate |
 | `--ca-cert` | Remove local TLS CA certificate |
 | `--all` | Apply TLS configuration on all the backend containers in the cluster |
 | `-h`, `--help` | Show help message |

**weka security tls set**

Make Ngnix use TLS when accessing UI. If TLS already set this command updates the key and certificate.

```sh
weka security tls set [--private-key private-key]
                      [--certificate certificate]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--private-key` | Path to TLS private key pem file |
 | `--certificate` | Path to TLS certificate pem file |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

**weka security tls status**

Show the Weka cluster TLS status and certificate

```sh
weka security tls status [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]
                         [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka security tls reset**

Make Ngnix not use TLS when accessing UI

```sh
weka security tls reset [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

### weka smb

Commands that manage Weka's SMB container

```sh
weka smb [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka smb cluster

View info about the SMB cluster managed by weka

```sh
weka smb cluster [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--help]
                 [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb cluster container**

Update an SMB cluster containers

```sh
weka smb cluster container [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka smb cluster container add**

Update an SMB cluster

```sh
weka smb cluster container add [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--container-ids container-ids]...
                               [--help]
                               [--force]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--container-ids`... | The SMB containers being added (pass weka's host id as a number) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected SMB clients. |

**weka smb cluster container remove**

Update an SMB cluster

```sh
weka smb cluster container remove [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--container-ids container-ids]...
                                  [--help]
                                  [--force]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--container-ids`... | The SMB containers being removed (pass weka's container id as a number) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected SMB clients. |

**weka smb cluster add**

Create a SMB cluster managed by weka

```sh
weka smb cluster add <netbios-name>
                     <domain>
                     <config-fs-name>
                     [--symlink symlink]
                     [--domain-netbios-name domain-netbios-name]
                     [--idmap-backend idmap-backend]
                     [--default-domain-mapping-from-id default-domain-mapping-from-id]
                     [--default-domain-mapping-to-id default-domain-mapping-to-id]
                     [--joined-domain-mapping-from-id joined-domain-mapping-from-id]
                     [--joined-domain-mapping-to-id joined-domain-mapping-to-id]
                     [--encryption encryption]
                     [--scale-out-mode scale-out-mode]
                     [--smb-conf-extra smb-conf-extra]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--container-ids container-ids]...
                     [--smb-ips-pool smb-ips-pool]...
                     [--smb-ips-range smb-ips-range]...
                     [--help]

```

 | Parameter | Description |
 | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `netbios-name`* | The netbios name to give to the SMB cluster |
 | `domain`* | The domain to join the SMB cluster to |
 | `config-fs-name`* | SMB config filesystem name |
 | `--symlink` | Enable or disable symbolic link (symlink) support for the SMB-W cluster. (format: 'on' or 'off') |
 | `--domain-netbios-name` | The domain netbios name; If not given, the default will be the first part of the given domain name |
 | `--idmap-backend` | The SMB domain backend type (rid, rfc2307, etc.). Note that rfc2307 requires uid/gid configuration on the Active Directory and is persistent, while rid does not require any Active Directory configuration but in case of range changes uids/gids could break. |
 | `--default-domain-mapping-from-id` | The SMB default domain first id |
 | `--default-domain-mapping-to-id` | The SMB default domain last id |
 | `--joined-domain-mapping-from-id` | The joined domain first id |
 | `--joined-domain-mapping-to-id` | The joined domain last id |
 | `--encryption` | Encryption (format: 'enabled', 'disabled', 'desired' or 'required') |
 | `--scale-out-mode` | Controls the sync level between the SMB-W servers. Possible values: 'full' (Default), 'partial'. The default value is 'full,' but you can use 'partial' if shared access to files is unnecessary. Be cautious when deviating from the default setting; consult the documentation or Customer Success to avoid misuse. (format: 'none', 'full' or 'partial') |
 | `--smb-conf-extra` | Extra smb configuration options |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--container-ids`... | The containers that will serve via the SMB protocol (pass weka's container id as a number) (may be repeated or comma-separated) |
 | `--smb-ips-pool`... | IPs used as floating IPs for samba to server SMB in a HA manner. Then should not be assigned to any container on the network (may be repeated or comma-separated) |
 | `--smb-ips-range`... | IPs used as floating IPs for samba to server SMB in a HA manner. Then should not be assigned to any container on the network (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

**weka smb cluster debug**

Set debug level in an SMB container

```sh
weka smb cluster debug <level>
                       [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--container-ids container-ids]...
                       [--help]
                       [--json]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------- |
 | `level`* | The debug level |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--container-ids`... | Hosts to set debug level (pass weka's host id as a number). All hosts as default (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb cluster remove**

Destroy the SMB cluster managed by weka. This will not delete the data, just stop exposing it via SMB

```sh
weka smb cluster remove [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--force]
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected SMB clients. |
 | `-h`, `--help` | Show help message |

**weka smb cluster status**

Show which of the containers are ready.

```sh
weka smb cluster status [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]
                        [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb cluster trusted-domains**

List all trusted domains

```sh
weka smb cluster trusted-domains [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--format format]
                                 [--output output]...
                                 [--sort sort]...
                                 [--filter filter]...
                                 [--filter-color filter-color]...
                                 [--help]
                                 [--no-header]
                                 [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: id,domain,idmap,from,to (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka smb cluster trusted-domains add**

Add a new trusted domain

```sh
weka smb cluster trusted-domains add <domain-name>
                                     <from-id>
                                     <to-id>
                                     [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--force]
                                     [--help]
                                     [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
 | `domain-name`* | The name of the domain being added |
 | `from-id`* | The first id |
 | `to-id`* | The last id |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected SMB clients and modify existing uids/gids. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb cluster trusted-domains remove**

Remove a trusted domain

```sh
weka smb cluster trusted-domains remove <trusteddomain-id>
                                        [--color color]
                                        [--HOST HOST]
                                        [--PORT PORT]
                                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                        [--TIMEOUT TIMEOUT]
                                        [--profile profile]
                                        [--force]
                                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
 | `trusteddomain-id`* | The id of the domain to remove |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected SMB clients and modify existing uids/gids. |
 | `-h`, `--help` | Show help message |

**weka smb cluster update**

Update an SMB cluster

```sh
weka smb cluster update [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--encryption encryption]
                        [--smb-ips-pool smb-ips-pool]...
                        [--smb-ips-range smb-ips-range]...
                        [--help]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--encryption` | Encryption (format: 'enabled', 'disabled', 'desired' or 'required') |
 | `--smb-ips-pool`... | IPs used as floating IPs for SMB to serve in a HA manner. Then should not be assigned to any host on the network (may be repeated or comma-separated) |
 | `--smb-ips-range`... | IPs used as floating IPs for SMB to serve in a HA manner. Then should not be assigned to any host on the network (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |

**weka smb cluster wait**

Wait for SMB cluster to become ready

```sh
weka smb cluster wait [--timeout timeout]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `-t`, `--timeout` | Timeout (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka smb domain

View info about the domain

```sh
weka smb domain [--color color]
                [--HOST HOST]
                [--PORT PORT]
                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                [--TIMEOUT TIMEOUT]
                [--profile profile]
                [--help]
                [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb domain join**

Join cluster to Active Directory domain

```sh
weka smb domain join <username>
                     [password]
                     [--server server]
                     [--create-computer create-computer]
                     [--extra-options extra-options]
                     [--timeout timeout]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--debug]
                     [--help]
                     [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `username`* | The name of the administrator user to join the domain using it |
 | `password` | The administrator user password |
 | `--server` | Specifies the remote domain controller for SMB-W domain join commands. |
 | `--create-computer` | Creates an SMB cluster computer account in AD under a specified OU. |
 | `--extra-options` | Consult with SMB 'net ads join' manual for extra options |
 | `-t`, `--timeout` | Join command timeout in seconds (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--debug` | Run the command in debug mode |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb domain leave**

Leave Active Directory domain

```sh
weka smb domain leave <username>
                      [password]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--debug]
                      [--force]
                      [--help]
                      [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `username`* | The name of the administrator user to leave the domain using it |
 | `password` | The administrator user password |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--debug` | Run the command in debug mode |
 | `-f`, `--force` | Force to leave the domain. Use when Active Directory is unresponsive |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka smb share

List all shares exposed via SMB

```sh
weka smb share [--color color]
               [--HOST HOST]
               [--PORT PORT]
               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
               [--TIMEOUT TIMEOUT]
               [--profile profile]
               [--format format]
               [--output output]...
               [--sort sort]...
               [--filter filter]...
               [--filter-color filter-color]...
               [--help]
               [--no-header]
               [--verbose]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: id,share,filesystem,description,path,fmask,dmask,acls,options,additional,direct,Sensitivity,encryption,validUsers,invalidUsers,readonlyUsers,readwriteUsers,readonlyShare,allowGuestAccess,hidden,vfsZerocopyRead,namedStreams (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka smb share add**

Add a new share to be exposed by SMB

```sh
weka smb share add <share-name>
                   <fs-name>
                   [--description description]
                   [--internal-path internal-path]
                   [--file-create-mask file-create-mask]
                   [--directory-create-mask directory-create-mask]
                   [--acl acl]
                   [--map-acls map-acls]
                   [--case-sensitivity case-sensitivity]
                   [--obs-direct obs-direct]
                   [--encryption encryption]
                   [--read-only read-only]
                   [--user-list-type user-list-type]
                   [--allow-guest-access allow-guest-access]
                   [--enable-ADS enable-ADS]
                   [--hidden hidden]
                   [--vfs-zerocopy-read vfs-zerocopy-read]
                   [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--users users]...
                   [--force]
                   [--help]
                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `share-name`* | The name of the share being added |
 | `fs-name`* | Filesystem name to share |
 | `--description` | A description for SMB to show regarding the share |
 | `--internal-path` | The path inside the filesystem to share |
 | `--file-create-mask` | POSIX mode mask files will be created with. E.g. "0744" |
 | `--directory-create-mask` | POSIX mode mask directories will be created with. E.g. "0755" |
 | `--acl` | Enable Windows ACLs on the share. Will also be translated (as possible) to POSIX ACLs. (format: 'on' or 'off') |
 | `--map-acls` | Map ACL (format: 'posix', 'windows', 'hybrid' or 'none') |
 | `--case-sensitivity` | Enable or disable case sensitivity for the specified SMB share. When enabled, the share distinguishes between files with the same name but different capitalization. This option applies exclusively to SMB-W cluster. (format: 'on' or 'off') |
 | `--obs-direct` | Mount share in obs-direct mode (format: 'on' or 'off') |
 | `--encryption` | Encryption (format: 'cluster_default', 'desired' or 'required') |
 | `--read-only` | Mount share as read-only (format: 'on' or 'off') |
 | `--user-list-type` | The list type to which users are added to (format: 'read_only', 'read_write', 'valid' or 'invalid') |
 | `--allow-guest-access` | Allow guests to access the share (format: 'on' or 'off') |
 | `--enable-ADS` | Enables the use of Alternate Data Streams (ADS) on a specified SMB share. (format: 'yes' or 'no') |
 | `--hidden` | Hidden (format: 'on' or 'off') |
 | `--vfs-zerocopy-read` | Enable zero-copy reads if supported. Default: true (format: 'on' or 'off') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--users`... | Users to add (may be repeated or comma-separated) |
 | `-f`, `--force` | Force this action without further confirmation. This action will affect all SMB users of this share, Use it with caution and consult the Weka Customer Success team at need. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb share host-access**

Show host access help

```sh
weka smb share host-access [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka smb share host-access add**

Add hosts IPs to host access list

```sh
weka smb share host-access add <share-id>
                               <mode>
                               [--color color]
                               [--HOST HOST]
                               [--PORT PORT]
                               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                               [--TIMEOUT TIMEOUT]
                               [--profile profile]
                               [--ips ips]...
                               [--help]
                               [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `share-id`* | The id of the share |
 | `mode`* | allow/deny host access (format: 'allow' or 'deny') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--ips`... | ips to add (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb share host-access list**

Show host access list

```sh
weka smb share host-access list [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--format format]
                                [--output output]...
                                [--sort sort]...
                                [--filter filter]...
                                [--filter-color filter-color]...
                                [--help]
                                [--no-header]
                                [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,share,mode,IP (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka smb share host-access remove**

Remove hosts IPs from a user list

```sh
weka smb share host-access remove <share_id>
                                  [--color color]
                                  [--HOST HOST]
                                  [--PORT PORT]
                                  [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                  [--TIMEOUT TIMEOUT]
                                  [--profile profile]
                                  [--help]
                                  [--json]
                                  [<hosts>]...

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `share_id`* | The id of the share being removed from |
 | `hosts`... | Hosts IPs to remove |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb share host-access reset**

Reset host access lists

```sh
weka smb share host-access reset <share-id>
                                 <mode>
                                 [--color color]
                                 [--HOST HOST]
                                 [--PORT PORT]
                                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                 [--TIMEOUT TIMEOUT]
                                 [--profile profile]
                                 [--force]
                                 [--help]
                                 [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `share-id`* | The id of the share |
 | `mode`* | allow/deny host access (format: 'allow' or 'deny') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action will delete all host access ips. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb share list**

Show lists help

```sh
weka smb share list [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka smb share list add**

Add users to a user list

```sh
weka smb share list add <share-id>
                        <user-list-type>
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--users users]...
                        [--help]
                        [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `share-id`* | The id of the share |
 | `user-list-type`* | The list type (format: 'read_only', 'read_write', 'valid' or 'invalid') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--users`... | Users to add (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb share list remove**

Remove users from a user list

```sh
weka smb share list remove <share_id>
                           <user-list-type>
                           [--color color]
                           [--HOST HOST]
                           [--PORT PORT]
                           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                           [--TIMEOUT TIMEOUT]
                           [--profile profile]
                           [--users users]...
                           [--help]
                           [--json]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------- |
 | `share_id`* | The id of the share being removed from |
 | `user-list-type`* | The list type from which users are removed from (format: 'read_only', 'read_write', 'valid' or 'invalid') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--users`... | Users to remove (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb share list reset**

Reset a user list

```sh
weka smb share list reset <share-id>
                          <user-list-type>
                          [--color color]
                          [--HOST HOST]
                          [--PORT PORT]
                          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                          [--TIMEOUT TIMEOUT]
                          [--profile profile]
                          [--help]
                          [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `share-id`* | The id of the share |
 | `user-list-type`* | The list type (format: 'read_only', 'read_write', 'valid' or 'invalid') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka smb share list show**

Show user lists

```sh
weka smb share list show [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--format format]
                         [--output output]...
                         [--sort sort]...
                         [--filter filter]...
                         [--filter-color filter-color]...
                         [--help]
                         [--no-header]
                         [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,id,share,readonly,validusers,invalidusers,readonlyusers,readwriteusers (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

**weka smb share remove**

Remove a share exposed by SMB

```sh
weka smb share remove <share-id>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--force]
                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------- |
 | `share-id`* | The id of the share to remove |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This action may disrupt IO service for connected SMB clients. |
 | `-h`, `--help` | Show help message |

**weka smb share update**

Update an SMB share

```sh
weka smb share update <share-id>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--encryption encryption]
                      [--read-only read-only]
                      [--allow-guest-access allow-guest-access]
                      [--hidden hidden]
                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `share-id`* | The id of the share to update |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--encryption` | Encryption (format: 'cluster_default', 'desired' or 'required') |
 | `--read-only` | Mount as read-only (format: 'on' or 'off') |
 | `--allow-guest-access` | Allow Guest Access (format: 'on' or 'off') |
 | `--hidden` | Hidden (format: 'on' or 'off') |
 | `-h`, `--help` | Show help message |

### weka stats

List all statistics that conform to the filter criteria

```sh
weka stats [--start-time <start>]
           [--end-time <end>]
           [--interval interval]
           [--resolution-secs <secs>]
           [--role role]
           [--aggregate-by aggregate-by]
           [--color color]
           [--HOST HOST]
           [--PORT PORT]
           [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
           [--TIMEOUT TIMEOUT]
           [--profile profile]
           [--format format]
           [--category category]...
           [--stat stat]...
           [--process-ids process-ids]...
           [--param param]...
           [--exclude-process-ids exclude-process-ids]...
           [--output output]...
           [--sort sort]...
           [--filter filter]...
           [--filter-color filter-color]...
           [--accumulated]
           [--per-process]
           [--per-role]
           [--no-zeros]
           [--show-internal]
           [--skip-validations]
           [--help]
           [--raw-units]
           [--UTC]
           [--no-header]
           [--verbose]

```

 | Parameter | Description |
 | -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--start-time` | Query for stats starting at this time (format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00) |
 | `--end-time` | Query for stats up to this time point (format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00) |
 | `--interval` | Period (in seconds) of time of the report |
 | `--resolution-secs` | Length of each interval in the report period |
 | `--role` | Limit the report to processes with the specified role |
 | `--aggregate-by` | Aggregate statistics by the specified component (format: 'none', 'process', 'container' or 'server') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `--category`... | Retrieve only statistics of the specified categories (may be repeated or comma-separated) |
 | `--stat`... | Retrieve only the specified statistics (may be repeated or comma-separated) |
 | `--process-ids`... | Limit the report to the specified processes (may be repeated or comma-separated) |
 | `--param`... | For parameterized statistics, retrieve only the instantiations where the specified parameter is of the specified value. Multiple values can be supplied for the same key, e.g. '--param method:putBlocks --param method:initBlock'. (format: key:value, may be repeated or comma-separated) |
 | `--exclude-process-ids`... | Limit the report to all processes except the specified ones (may be repeated or comma-separated) |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: node,category,timestamp,stat,unit,value,containerId,container,hostname,roles (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--accumulated` | Show accumulated statistics, not rate statistics |
 | `--per-process` | Do not aggregate statistics across processes |
 | `--per-role` | Aggregate statistics by role |
 | `-Z`, `--no-zeros` | Do not retrieve results where the value is 0 |
 | `--show-internal` | Show internal statistics |
 | `--skip-validations` | Skip category/stat name validations |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka stats list-types

Show the statistics definition information

```sh
weka stats list-types [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--format format]
                      [--output output]...
                      [--sort sort]...
                      [--filter filter]...
                      [--filter-color filter-color]...
                      [--show-internal]
                      [--help]
                      [--no-header]
                      [--verbose]
                      [<name-or-category>]...

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name-or-category`... | Filter by these names or categories |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: category,clabel,identifier,description,label,type,unit,params,realted,permission,ntype,accumulate,histogram,histogramUnit (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `--show-internal` | Show internal statistics |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka stats realtime

Get performance related stats which are updated in a one-second interval.

```sh
weka stats realtime [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--format format]
                    [--output output]...
                    [--sort sort]...
                    [--filter filter]...
                    [--filter-color filter-color]...
                    [--help]
                    [--raw-units]
                    [--UTC]
                    [--show-total]
                    [--no-header]
                    [--verbose]
                    [<process-ids>]...

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `process-ids`... | Only show realtime stats of these processes |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: node,hostname,role,mode,writeps,writebps,wlatency,readps,readbps,rlatency,ops,cpu,l6recv,l6send,upload,download,rdmarecv,rdmasend (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |
 | `--show-total` | Show each column's sum of values in the real-time statistics output |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka stats retention

Configure retention for statistics

```sh
weka stats retention [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

**weka stats retention restore-default**

Restore default retention for statistics

```sh
weka stats retention restore-default [--color color]
                                     [--HOST HOST]
                                     [--PORT PORT]
                                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                     [--TIMEOUT TIMEOUT]
                                     [--profile profile]
                                     [--dry-run]
                                     [--help]
                                     [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--dry-run` | Only test the command, don't affect the system |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka stats retention set**

Choose how long to keep statistics for

```sh
weka stats retention set [--days days]
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--dry-run]
                         [--help]
                         [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--days` | Number of days to keep the statistics |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--dry-run` | Only test the command, don't affect the system |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka stats retention status**

Show configured statistics retention

```sh
weka stats retention status [--color color]
                            [--HOST HOST]
                            [--PORT PORT]
                            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                            [--TIMEOUT TIMEOUT]
                            [--profile profile]
                            [--help]
                            [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

### weka status

Get an overall status of the Weka cluster

```sh
weka status [--color color]
            [--HOST HOST]
            [--PORT PORT]
            [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
            [--TIMEOUT TIMEOUT]
            [--profile profile]
            [--detailed-capacity]
            [--help]
            [--json]
            [--raw-units]
            [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `--detailed-capacity` | Include more detailed capacity information |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka status rebuild

Show the cluster phasing in/out progress, and protection per fault-level

```sh
weka status rebuild [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--help]
                    [--json]
                    [--raw-units]
                    [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

#### weka status reduction

Show cluster data reduction information'

```sh
weka status reduction [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]
                      [--raw-units]
                      [--UTC]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |
 | `-R`, `--raw-units` | Print values in raw units (bytes, seconds, etc.). When not set, sizes are printed in human-readable format, e.g 1KiB 234MiB 2GiB. |
 | `-U`, `--UTC` | Print times in UTC. When not set, times are converted to the local time of this host. |

### weka umount

Unmounts wekafs filesystems. This is the helper utility installed at /sbin/umount.wekafs.

```sh
weka umount <target>
            [--type type]
            [--color color]
            [--verbose]
            [--no-mtab]
            [--lazy-unmount]
            [--force]
            [--readonly]
            [--help]

```

 | Parameter | Description |
 | ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
 | `target`* | The target mount point to unmount |
 | `-t`, `--type` | Indicate that the actions should only be taken on file systems of the specified type |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-v`, `--verbose` | Verbose mode |
 | `-n`, `--no-mtab` | Unmount without writing in /etc/mtab |
 | `-l`, `--lazy-unmount` | Detach the filesystem from the filesystem hierarchy now, and cleanup all references to the filesystem as soon as it is not busy anymore |
 | `-f`, `--force` | Force unmount |
 | `-r`, `--readonly` | In case unmounting fails, try to remount read-only |
 | `-h`, `--help` | Show help message |

### weka upgrade

Commands that control the upgrade precedure of Weka

```sh
weka upgrade [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka upgrade pause

Pause the upgrade process

```sh
weka upgrade pause [--color color]
                   [--HOST HOST]
                   [--PORT PORT]
                   [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                   [--TIMEOUT TIMEOUT]
                   [--profile profile]
                   [--help]
                   [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka upgrade resume

Resume the upgrade process

```sh
weka upgrade resume [--color color]
                    [--HOST HOST]
                    [--PORT PORT]
                    [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                    [--TIMEOUT TIMEOUT]
                    [--profile profile]
                    [--help]
                    [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka upgrade supported-features

List upgrade features supported by the running cluster

```sh
weka upgrade supported-features [--color color]
                                [--HOST HOST]
                                [--PORT PORT]
                                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                                [--TIMEOUT TIMEOUT]
                                [--profile profile]
                                [--help]
                                [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

### weka user

List users defined in the Weka cluster

```sh
weka user [--color color]
          [--HOST HOST]
          [--PORT PORT]
          [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
          [--TIMEOUT TIMEOUT]
          [--profile profile]
          [--format format]
          [--output output]...
          [--sort sort]...
          [--filter filter]...
          [--filter-color filter-color]...
          [--help]
          [--no-header]
          [--verbose]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: uid,user,source,role,s3Policy,posix_uid,posix_gid (may be repeated or comma-separated) |
 | `-s`, `--sort`... | Specify which column(s) to take into account when sorting the output. May include a '+' or '-' before the column name to sort in ascending or descending order respectively. Usage: \[+ |
 | `-F`, `--filter`... | Specify what values to filter by in a specific column. Usage: column1=val1\[,column2=val2\[,..]] (may be repeated or comma-separated) |
 | `--filter-color`... | Filter rows with specific colors (red/yellow/green) (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

#### weka user add

Create a new user in the Weka cluster

```sh
weka user add <username>
              <role>
              [--posix-uid posix-uid]
              [--posix-gid posix-gid]
              [--color color]
              [--HOST HOST]
              [--PORT PORT]
              [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
              [--TIMEOUT TIMEOUT]
              [--profile profile]
              [--help]
              [--json]

```

 | Parameter | Description |
 | ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `username`* | Username of the new user to create |
 | `role`* | The role of the new user (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi') |
 | `password`* | Password for the new user: must contain at least 8 characters, and have at least one uppercase letter, one lowercase letter, and one number or special character. Typing special characters as arguments to this command might require escaping |
 | `--posix-uid` | POSIX UID for user (S3 Only) |
 | `--posix-gid` | POSIX GID for user (S3 Only) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka user change-role

Change the role of an existing user.

```sh
weka user change-role <username>
                      <role>
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `username`* | Username of user to change the role of |
 | `role`* | New role to set for the user (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka user remove

Delete user from the Weka cluster

```sh
weka user remove <username>
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `username`* | User's name |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka user generate-token

Generate an access token for the current logged in user for use with REST API

```sh
weka user generate-token [--access-token-timeout access-token-timeout]
                         [--color color]
                         [--HOST HOST]
                         [--PORT PORT]
                         [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                         [--TIMEOUT TIMEOUT]
                         [--profile profile]
                         [--help]
                         [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--access-token-timeout` | In how long should the access token expire (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka user ldap

Show current LDAP configuration used for authenticating users

```sh
weka user ldap [--color color]
               [--HOST HOST]
               [--PORT PORT]
               [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
               [--TIMEOUT TIMEOUT]
               [--profile profile]
               [--help]
               [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka user ldap disable**

Disable authentication through the configured LDAP server

```sh
weka user ldap disable [--color color]
                       [--HOST HOST]
                       [--PORT PORT]
                       [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                       [--TIMEOUT TIMEOUT]
                       [--profile profile]
                       [--force]
                       [--help]
                       [--json]

```

 | Parameter | Description |
 | ------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This would prevent all LDAP users from logging-in until LDAP is enabled again. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka user ldap enable**

Enable authentication through the configured LDAP server (has no effect if LDAP server is already enabled)

```sh
weka user ldap enable [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka user ldap reset**

Delete all LDAP settings from the cluster

```sh
weka user ldap reset [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--force]
                     [--help]
                     [--json]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--force` | Force this action without further confirmation. This would prevent all LDAP users from logging-in until LDAP is configured again. |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka user ldap setup**

Setup an LDAP server for user authentication

```sh
weka user ldap setup <server-uri>
                     <base-dn>
                     <user-object-class>
                     <user-id-attribute>
                     <group-object-class>
                     <group-membership-attribute>
                     <group-id-attribute>
                     <reader-username>
                     [--cluster-admin-group cluster-admin-group]
                     [--org-admin-group org-admin-group]
                     [--regular-group regular-group]
                     [--readonly-group readonly-group]
                     [--start-tls start-tls]
                     [--ignore-start-tls-failure ignore-start-tls-failure]
                     [--server-timeout-secs server-timeout-secs]
                     [--protocol-version protocol-version]
                     [--user-revocation-attribute user-revocation-attribute]
                     [--color color]
                     [--HOST HOST]
                     [--PORT PORT]
                     [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                     [--TIMEOUT TIMEOUT]
                     [--profile profile]
                     [--help]
                     [--json]

```

 | Parameter | Description |
 | ------------------------------ | ------------------------------------------------------------------------------------------------------------------------- |
 | `server-uri`* | LDAP server URI (\[ldap://]hostname\[:port] or ldaps://hostname\[:port]) |
 | `base-dn`* | Base DN |
 | `user-object-class`* | User object class |
 | `user-id-attribute`* | User ID attribute |
 | `group-object-class`* | Group object class |
 | `group-membership-attribute`* | Group membership attribute |
 | `group-id-attribute`* | Group ID attribute |
 | `reader-username`* | Reader username |
 | `reader-password`* | Reader password |
 | `--cluster-admin-group` | LDAP group of users that should get ClusterAdmin role (this role is only available for the root tenant to configure) |
 | `--org-admin-group` | LDAP group of users that should get OrgAdmin role |
 | `--regular-group` | LDAP group of users that should get Regular role |
 | `--readonly-group` | LDAP group of users that should get ReadOnly role |
 | `--start-tls` | Issue StartTLS after connecting (should not be used with ldaps://) (format: 'yes' or 'no') |
 | `--ignore-start-tls-failure` | Ignore start TLS failure (format: 'yes' or 'no') |
 | `--server-timeout-secs` | LDAP connection timeout in seconds |
 | `--protocol-version` | LDAP protocol version |
 | `--user-revocation-attribute` | User revocation attribute: If provided, updating this attribute in the LDAP server automatically revokes all user tokens. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka user ldap setup-ad**

Setup an Active Directory server for user authentication

```sh
weka user ldap setup-ad <server-uri>
                        <domain>
                        <reader-username>
                        [--cluster-admin-group cluster-admin-group]
                        [--org-admin-group org-admin-group]
                        [--regular-group regular-group]
                        [--readonly-group readonly-group]
                        [--start-tls start-tls]
                        [--ignore-start-tls-failure ignore-start-tls-failure]
                        [--server-timeout-secs server-timeout-secs]
                        [--user-revocation-attribute user-revocation-attribute]
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]
                        [--json]

```

 | Parameter | Description |
 | ----------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
 | `server-uri`* | LDAP server URI (\[ldap://]hostname\[:port] or ldaps://hostname\[:port]) |
 | `domain`* | Domain |
 | `reader-username`* | Reader username |
 | `reader-password`* | Reader password |
 | `--cluster-admin-group` | LDAP group of users that should get ClusterAdmin role (this role is only available for the root tenant to configure) |
 | `--org-admin-group` | LDAP group of users that should get OrgAdmin role |
 | `--regular-group` | LDAP group of users that should get Regular role |
 | `--readonly-group` | LDAP group of users that should get ReadOnly role |
 | `--start-tls` | Issue StartTLS after connecting (should not be used with ldaps://) (format: 'yes' or 'no') |
 | `--ignore-start-tls-failure` | Ignore start TLS failure (format: 'yes' or 'no') |
 | `--server-timeout-secs` | LDAP connection timeout in seconds |
 | `--user-revocation-attribute` | User revocation attribute: If provided, updating this attribute in the LDAP server automatically revokes all user tokens. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

**weka user ldap update**

Edit LDAP server configuration

```sh
weka user ldap update [--server-uri server-uri]
                      [--base-dn base-dn]
                      [--user-object-class user-object-class]
                      [--user-id-attribute user-id-attribute]
                      [--group-object-class group-object-class]
                      [--group-membership-attribute group-membership-attribute]
                      [--group-id-attribute group-id-attribute]
                      [--reader-username reader-username]
                      [--reader-password reader-password]
                      [--cluster-admin-group cluster-admin-group]
                      [--org-admin-group org-admin-group]
                      [--regular-group regular-group]
                      [--readonly-group readonly-group]
                      [--start-tls start-tls]
                      [--certificate certificate]
                      [--ignore-start-tls-failure ignore-start-tls-failure]
                      [--server-timeout-secs server-timeout-secs]
                      [--protocol-version protocol-version]
                      [--user-revocation-attribute user-revocation-attribute]
                      [--color color]
                      [--HOST HOST]
                      [--PORT PORT]
                      [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                      [--TIMEOUT TIMEOUT]
                      [--profile profile]
                      [--help]
                      [--json]

```

 | Parameter | Description |
 | ------------------------------ | ------------------------------------------------------------------------------------------------------------------------- |
 | `--server-uri` | LDAP server URI (\[ldap://]hostname\[:port] or ldaps://hostname\[:port]) |
 | `--base-dn` | Base DN |
 | `--user-object-class` | User object class |
 | `--user-id-attribute` | User ID attribute |
 | `--group-object-class` | Group object class |
 | `--group-membership-attribute` | Group membership attribute |
 | `--group-id-attribute` | Group ID attribute |
 | `--reader-username` | Reader username |
 | `--reader-password` | Reader password |
 | `--cluster-admin-group` | LDAP group of users that should get ClusterAdmin role (this role is only available for the root tenant to configure) |
 | `--org-admin-group` | LDAP group of users that should get OrgAdmin role |
 | `--regular-group` | LDAP group of users that should get Regular role |
 | `--readonly-group` | LDAP group of users that should get ReadOnly role |
 | `--start-tls` | Issue StartTLS after connecting (should not be used with ldaps://) (format: 'yes' or 'no') |
 | `--certificate` | Certificate or certificate chain for the LDAP server |
 | `--ignore-start-tls-failure` | Ignore certificate verification errors (format: 'yes' or 'no') |
 | `--server-timeout-secs` | LDAP connection timeout in seconds |
 | `--protocol-version` | LDAP protocol version |
 | `--user-revocation-attribute` | User revocation attribute: If provided, updating this attribute in the LDAP server automatically revokes all user tokens. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka user login

Logs a user into the Weka cluster. If login is successful, the user credentials are saved to the user homedir.

```sh
weka user login [username]
                [password]
                [--org org]
                [--path path]
                [--color color]
                [--HOST HOST]
                [--PORT PORT]
                [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                [--TIMEOUT TIMEOUT]
                [--profile profile]
                [--help]

```

 | Parameter | Description |
 | ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `username` | User's username |
 | `password` | User's password |
 | `-g`, `--org` | Organization name or ID |
 | `-p`, `--path` | The path where the login token will be saved (default: \~/.weka/auth-token.json). This path can also be specified using the WEKA_TOKEN environment variable. After logging-in, use the WEKA_TOKEN environment variable to specify where the login token is located. |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka user logout

Logs the current user out of the Weka cluster by removing the user credentials from WEKA_TOKEN if exists, or otherwise from the user homedir

```sh
weka user logout [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka user passwd

Set a user's password. If the currently logged-in user is an admin, it can change the password for all other users in the organization.

```sh
weka user passwd [--username username]
                 [--current-password current-password]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `password`* | New password: must contain at least 8 characters, and have at least one uppercase letter, one lowercase letter, and one number or special character. Typing special characters as arguments to this command might require escaping |
 | `--username` | Username to change the password for, by default password is changed for the current user |
 | `--current-password` | User's current password. Only necessary if changing current user's password |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka user revoke-tokens

Revoke all existing login tokens of an internal user

```sh
weka user revoke-tokens <username>
                        [--color color]
                        [--HOST HOST]
                        [--PORT PORT]
                        [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                        [--TIMEOUT TIMEOUT]
                        [--profile profile]
                        [--help]
                        [--json]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `username`* | Username of user to revoke the tokens for |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka user update

Change parameters of an existing user.

```sh
weka user update <username>
                 [--posix-uid posix-uid]
                 [--posix-gid posix-gid]
                 [--role role]
                 [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--help]

```

 | Parameter | Description |
 | ------------------------- | ---------------------------------------------------------------------------------------------------------- |
 | `username`* | Username of user to update |
 | `--posix-uid` | POSIX UID for user (S3 Only) |
 | `--posix-gid` | POSIX GID for user (S3 Only) |
 | `--role` | New role to set for the user (format: 'clusteradmin', 'orgadmin', 'regular', 'readonly', 's3' or 'csi') |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-h`, `--help` | Show help message |

#### weka user whoami

Get information about currently logged-in user

```sh
weka user whoami [--color color]
                 [--HOST HOST]
                 [--PORT PORT]
                 [--CONNECT-TIMEOUT CONNECT-TIMEOUT]
                 [--TIMEOUT TIMEOUT]
                 [--profile profile]
                 [--format format]
                 [--output output]...
                 [--help]
                 [--no-header]
                 [--verbose]

```

 | Parameter | Description |
 | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-H`, `--HOST` | Specify the host. Alternatively, use the WEKA_HOST env variable |
 | `-P`, `--PORT` | Specify the port. Alternatively, use the WEKA_PORT env variable |
 | `-C`, `--CONNECT-TIMEOUT` | Timeout for connecting to cluster, default: 10 secs (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `-T`, `--TIMEOUT` | Timeout to wait for response, default: 1 minute (format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited) |
 | `--profile` | Name of the connection and authentication profile to use |
 | `-f`, `--format` | Specify in what format to output the result (format: 'view', 'csv', 'markdown', 'json' or 'oldview') |
 | `-o`, `--output`... | Specify which columns to output. May include any of the following: orgId,orgName,user,source,role (may be repeated or comma-separated) |
 | `-h`, `--help` | Show help message |
 | `--no-header` | Don't show column headers when printing the output |
 | `-v`, `--verbose` | Show all columns in output |

### weka version

When run without arguments, lists the versions available on this machine. Subcommands allow for downloading of versions, setting the current version and other actions to manage versions.

```sh
weka version [--color color] [--full] [--help] [--json]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--full` | Show only fully installed versions |
 | `-h`, `--help` | Show help message |
 | `-J`, `--json` | Format output as JSON |

#### weka version current

Prints the current version. If no version is set, a failure exit status is returned.

```sh
weka version current [--container container] [--color color] [--help]

```

 | Parameter | Description |
 | ------------------- | -------------------------------------------------------------------------------- |
 | `-C`, `--container` | Get the version for a specific container |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka version get

Download a Weka version to the machine this command is executed from

```sh
weka version get <version>
                 [--color color]
                 [--from from]...
                 [--set-current]
                 [--no-progress-bar]
                 [--set-dist-servers]
                 [--client-only]
                 [--driver-only]
                 [--help]

```

 | Parameter | Description |
 | -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `version`* | Version to download |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--from`... | Download from this distribution server (can be given multiple times). Otherwise distribution servers are taken from the $WEKA_DIST_SERVERS environment variable, the /etc/wekaio/dist-servers file, or /etc/wekaio/service.conf in that order of precedence (may be repeated or comma-separated) |
 | `--set-current` | Set the downloaded version as the current version. Will fail if any containers are currently running. |
 | `--no-progress-bar` | Don't render download progress bar |
 | `--set-dist-servers` | Override the default distribution servers upon successful download |
 | `--client-only` | Only download components required for client |
 | `--driver-only` | Only download components required for compiling drivers |
 | `-h`, `--help` | Show help message |

#### weka version prepare

Prepare the version for use. This includes things like compiling the version drivers for the local machine.

```sh
weka version prepare <version-name> [--color color] [--help] [<containers>]...

```

 | Parameter | Description |
 | ---------------- | -------------------------------------------------------------------------------- |
 | `version-name`* | The version to prepare |
 | `containers`... | The containers to prepare the version for |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

#### weka version rm

Delete a version from the machine this command is executed from

```sh
weka version rm [--color color] [--clean-unused] [--force] [--help] [<version-name>]...

```

 | Parameter | Description |
 | ----------------- | -------------------------------------------------------------------------------------------------------- |
 | `version-name`... | The versions to remove |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--clean-unused` | Delete all versions which aren't the current set version, or the version of any of the containers |
 | `-f`, `--force` | Force this action without further confirmation. This action may be undone by re-downloading the version. |
 | `-h`, `--help` | Show help message |

#### weka version set

Set the current version. Containers must be stopped before setting the current version and the new version must have already been downloaded.

```sh
weka version set <version>
                 [--container container]
                 [--color color]
                 [--allow-running-containers]
                 [--default-only]
                 [--agent-only]
                 [--set-dependent]
                 [--client-only]
                 [--help]

```

 | Parameter | Description |
 | ---------------------------- | -------------------------------------------------------------------------------- |
 | `version`* | The version name to use |
 | `-C`, `--container` | The container to set the version for |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `--allow-running-containers` | Do not verify that all containers are stopped |
 | `--default-only` | Only set the default version used for creating containers |
 | `--agent-only` | Only set the agent version |
 | `--set-dependent` | Set the version for all containers depending on the specified container |
 | `--client-only` | Only set the client version |
 | `-h`, `--help` | Show help message |

#### weka version reset

Unset the current version. Containers must be stopped before setting the current version and the new version must have already been downloaded.

```sh
weka version reset [--color color] [--help]

```

 | Parameter | Description |
 | -------------- | -------------------------------------------------------------------------------- |
 | `--color` | Specify whether to use color in output (format: 'auto', 'disabled' or 'enabled') |
 | `-h`, `--help` | Show help message |

<!-- ============================================ -->
<!-- File 62/259: getting-started-with-weka_manage-the-system-using-weka-gui.md -->
<!-- ============================================ -->

---
description:
---

# Manage the system using the WEKA GUI

## WEKA GUI overview

The WEKA GUI application is the administration tool for your WEKA system. Use this tool for system configuration, filesystems management, user management, and investigation of alarms, events, and statistics.

WEKA GUI application supports the following functions:

* **Configuration**:
  * Configure the cluster, such as data availability, license, security, and central monitoring.
  * Configure the backend containers and expose the data in different protocols.
  * Manage local users and set up the user directory.
  * Create and manage organizations and their quotas.
* **Management**:
  * Manage the filesystems, including tiering, thin provisioning, and encryption.
  * Manage snapshots.
  * Manage the object store buckets.
  * Manage the filesystem protocols: SMB, S3, and NFS.
  * Manage directory quotas.
* **Investigation**:
  * Investigate events.
  * Investigate overtime statistics, such as total operations, R/W throughput, CPU usage, and read or write latency.
* **Monitoring**:
  * View the cluster protection and availability.
  * View the R/W throughput.
  * View the backend and client top consumers.
  * View alarms.
  * View the used, provisioned, and total capacity.
  * View the frontend, compute, and drive cores usage.
  * View the hardware components (active/total).

## Access the WEKA GUI

WEKA GUI is a web application you can access using an already configured account and has the appropriate rights to configure, administer, or view.

You can access the WEKA GUI with any standard browser using the address:\
`https://<weka system or server name>:14000`

For example: `https://WekaProd:14000` or `https://weka01:14000`.

Note: On AWS installations, you can access the WEKA GUI from the self-service portal. In the **Outputs** tab of the **CloudFormation** stack, click the **GUI** link.

**Before you begin**

Make sure that port 14000 is open in the firewall of your organization.

**Procedure**

1. In your browser, go to `https://<weka system or server name>:14000`.\
   The sign-in page opens.

2. Sign in with the username and password of an account with cluster administration or\
   organization administration privileges. For details about the account types, see\
   &#xNAN;_&#x55;ser management_ in the related topics.

The system dashboard opens.

Note: The initial default username and password are _admin_ and _admin_[.](../operation-guide/user-management) In the first sign-in, WEKA GUI enforces changing the admin password.

**Related topics**

## System Dashboard

The system dashboard contains widgets that provide an overview of the WEKA system, including an overall status, R/W throughput, top consumers, alerts, capacity, core usage, and hardware.

The system dashboard opens by default when you sign in. If you select another menu and want to display the dashboard again, select **Monitor > System Dashboard**, or click the **WEKA** logo.

### Cluster Protection and Availability widget

This widget shows the overall status of the system's health and protection state.

The overall status widget includes the following indications:

* **Protection state:** The possible protection states include:
  * OK: The system operates properly.
  * UNKNOWN: The protection state is unknown.
  * UNINITIALIZED: The system still needs to complete the cluster configuration and run the first IOs.
  * REBUILDING: When a failure occurs, the data rebuild process reads all the stripes where the failure occurred, rebuilds the data, and returns the system to full protection.
  * PARTIALLY_PROTECTED: Some or all of the data is not fully protected. The reported number of protections indicates the cluster's failure resilience.
  * UNPROTECTED: The data is not protected against any failure.
  * UNAVAILABLE: Too many parallel failures occur in the system that can cause system unavailability.
  * REDISTRIBUTING: The system redistributes the data between servers and drives due to scale-up or scale-down.
* **Service Uptime**: The elapsed time since the I/O services started.
* **Data Protection**: The number of data drives and protection parity drives. The color of the protection parity drives indicates their status.
* **Virtual (Hot) Spares**: The number of failure domains the system can lose and still complete the data rebuild while maintaining the same net capacity.

### R/W Throughput widget

This widget shows the current performance statistics aggregated across the cluster.

The R/W Throughput widget includes the following indications:

* **Throughput**: The total throughput.
* **Total Ops**: The number of cluster operations.
* **Latency**: The average latency of R/W operations.
* **Active clients**: The number of clients connected to the cluster.

Note: Selecting one of the R/W Throughput, Latency, and Total Ops titles displays the statistics page.
Selecting the Active clients title displays the clients tab.

### Top Consumers widget

This widget shows the top 5 backend servers and clients in the system. You can sort the list of servers by total IO operations per second or total throughput.

### Alerts widget

This widget shows the alerts that are not muted.

### Capacity widget

This widget shows an overview of the managed capacity.

The top bar indicates the total capacity provisioned for all filesystems and the used capacity. For tiered filesystems, the total capacity also includes the Object Store part.

The bottom bar indicates the total SSD capacity available in the system, the provisioned capacity, and the used capacity.

Note: Selecting the Capacity title displays the filesystems page.

### Core Usage widget

This widget shows the average usage and the maximum load level of the Frontend, Compute, and Drive cores. Hovering the maximum value displays the most active server and the NodeID number.

### Hardware widget

This widget shows an overview of the hardware components (active/total).

The hardware components include:

* **Backends**: The number of backend servers.
* **Cores**: The number of cores configured for running processes in the backend servers.
* **Drives**: The number of drives.
* **OBS Buckets**: The number of the object store buckets.

Note: Selecting one of the Backends, Cores, or Drives titles displays the **backend servers** page.
Selecting the OBS Buckets title displays the **object store buckets** page.

## Switch the display time

Timestamps in events and statistics are logged internally in UTC. Weka GUI displays the timestamps in local or system time. You can switch between the local and system time.

Switching the display time may be required when the customer, WEKA support, and the WEKA system are in different time zones. In this situation, the customer and WEKA support can switch the display to system time instead of local time so both view the identical timestamps.

**Procedure**

1. On the top bar, point to the timestamp.
2. Depending on the displayed time, select **Switch to System Time** or **Switch to Local Time**.

## Switch the GUI between light and dark modes

You can switch the GUI between light and dark modes according to your preferences. The dark mode is a user interface for content that displays light text on a dark background. The dark mode is beneficial for viewing screens at night. The reduced brightness can reduce eye strain in low-light conditions.

**Procedure**

1. Depending on the current display mode, point to the sun or moon symbol on the top bar.
2. Select **Switch to the light mode** or **Switch to dark mode**.

## Display servers in 3D view

You can switch the view of the servers to 3D for the backend servers, NFS servers, S3 servers, and SMB servers.

The 3D view provides the server components' status at a glance, including the drives, cores, protocols, and load. The colors indicate, for example, if the drives or processes failed or the container is down.

## Display tables

When managing filesystems, snapshots, and object stores, the displayed tables listing the rows have two behaviors in common.

* The table title also specifies the table's number of rows and the maximum number of rows the table can display.
* You can customize the columns displayed on the table using the column selector.

## Switch display units between Base 2 and Base 10

You can switch the display units for numeric values in the GUI between Base 2 (binary) and Base 10 (decimal). This option lets you view values, such as capacity sizes and metrics, in your preferred unit format.

To switch the display units, open the user profile menu and select either **Base 2 units** or **Base 10 units**.

<!-- ============================================ -->
<!-- File 63/259: getting-started-with-weka_getting-started-with-weka-rest-api.md -->
<!-- ============================================ -->

# Getting started with WEKA REST API

The WEKA system provides a RESTful API, enabling you to automate interactions with the WEKA system and integrate them into your workflows or monitoring systems.

Note: It is essential to have a solid understanding of the WEKA CLI commands and parameters related to the REST API services.
For example, to create a filesystem using the `POST /fileSystems` service, see the related documentation in #create-a-filesystem (using the CLI).

## Access the REST API

You can access the REST API using one of the following methods:

Using port 14000 and the URL `/api/v2`.

By browsing to: `https://<cluster name>:14000/api/v2/docs`

Select the three dots on the upper right menu and select **REST API**.

Browse to api.docs.weka.io and select the REST API version from the definition selector.

In addition, you can create a client code using the OpenAPI client generator and the `.json` file.

## Explore the REST API through the GUI

## Obtain an access token

To use the WEKA REST API, provide an access or refresh token.

You can generate an access or refresh for the REST API usage through the CLI or the GUI.\
See .

You can also call the login API to obtain access or refresh tokens through the API, providing it with a `username` and `password`.

If you already obtained a refresh token, you can use the `login/refresh` API to refresh the access token.

```

```python
import requests

url = "https://weka01:14000/api/v2/login"

payload="{\
    \"username\": \"admin\",\
    \"password\": \"admin\"\
}"
headers = {
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)

```

```

```

```python
import requests

url = "https://weka01:14000/api/v2/login/refresh"

payload="{\
    \"refresh_token\": \"REPLACE-WITH-REFRESH-TOKEN\"\
}"
headers = {
  'Content-Type': 'application/json'
}

response = requests.request("POST", url, headers=headers, data=payload)

print(response.text)

```

```

The response includes the access token (valid for 5 minutes) to use in the other APIs requiring token authentication, along with the refresh token (valid for 1 year), for getting additional access tokens without using the username/password.

```

```python
{
  "data": [
    {
      "access_token": "ACCESS-TOKEN",
      "token_type": "Bearer",
      "expires_in": 300,
      "refresh_token": "REFRESH-TOKEN"
    }
  ]
}
```

```

## Call the REST API

Once you obtain an access token, you can call WEKA REST API commands with it. For example, you can query the cluster status:

```

```python
import requests

url = "https://weka01:14000/api/v2/cluster"

payload={}
headers = {
  'Authorization': 'Bearer REPLACE-WITH-ACCESS-TOKEN'
}

response = requests.request("GET", url, headers=headers, data=payload)

print(response.text)

```

```

**Related topics**

REST API Reference Guide

<!-- ============================================ -->
<!-- File 64/259: getting-started-with-weka_weka-rest-api-and-equivalent-cli-commands.md -->
<!-- ============================================ -->

# WEKA REST API and equivalent CLI commands

To maximize your success with the REST API, it's essential to familiarize yourself with the comprehensive documentation. This valuable resource provides in-depth insights into the subject matter. Moreover, each REST API method corresponds to a CLI command. Additionally, many parameters accessible through the CLI are equally accessible when using the REST API. Run the CLI command help for details. This ensures a smooth and consistent experience across both interfaces.

Note: New REST APIs in version 5.0.2, compared to 4.4.7, are marked with two asterisks (**).

## Active directory

Related information:

 | Task | REST API | CLI |
 | ---------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------------------- |
 | **Update Active Directory:** Change the cluster's configuration to use a different Active Directory server or modify its settings. | PUT ‚Äã/activeDirectory | `weka user ldap setup-ad` |

## Alerts

Related information:

 | Task | REST API | CLI |
 | ---------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |
 | **View all alerts:** Get a complete list of active alerts, including silenced ones. | GET ‚Äã/alerts | `weka alerts` |
 | **List possible alerts:** View all types of alerts the cluster can generate. | GET ‚Äã/alerts‚Äã/types | `weka alerts types` |
 | **List alert types with actions:** View different alert types and their recommended troubleshooting steps. | GET ‚Äã/alerts‚Äã/description | `weka alerts describe` |
 | **Mute alerts by type:** Silence specific types of alerts. | PUT ‚Äã/alerts‚Äã/{alert_type}‚Äã/mute | `weka alerts mute <alert-type> <duration>` |
 | **Unmute alerts by type:** Reactivate specific types of alerts. | PUT ‚Äã/alerts‚Äã/{alert_type}‚Äã/unmute | `weka alerts unmute <alert-type>` |

## WEKA Home

Related information:

 | Task | REST API | CLI |
 | --------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
 | **View cloud WEKA Home configuration:** View the existing settings for the cloud WEKA Home service. | GET ‚Äã/wekaHome | `weka cloud status` |
 | **View cloud WEKA Home proxy URL:** Get the existing URL to access cloud services. | GET ‚Äã/wekaHome‚Äã/proxy | `weka cloud proxy` |
 | **Set cloud WEKA Home proxy URL:** Change the URL used to access cloud services. | POST ‚Äã/wekaHome‚Äã/proxy | `weka cloud proxy --set ` |
 | **View cloud WEKA Home upload rate:** View the existing data upload speed to the cloud service. | GET ‚Äã/wekaHome‚Äã/uploadRate | `weka cloud upload-rate` |
 | **Set cloud WEKA Home upload rate:** Define the preferred data upload speed to the cloud service. | PUT ‚Äã/wekaHome‚Äã/uploadRate | `weka cloud upload-rate set --bytes-per-second <bps>` |
 | **View cloud WEKA Home URL:** Get the URL for accessing the cloud WEKA Home service. | GET ‚Äã/wekaHome‚Äã/url | `weka cloud status` |
 | **Enable cloud WEKA Home:** Start using the cloud WEKA Home service. | POST ‚Äã/wekaHome‚Äã/enable | `weka cloud enable --cloud-url <cloud> --cloud-stats <on/off>` |
 | **Disable cloud WEKA Home:** Stop using the cloud WEKA Home service. | POST ‚Äã/wekaHome‚Äã/disable | `weka cloud disable` |

## Cluster

Related information:

 | Task | REST API | CLI |
 | --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------- |
 | **Create a cluster:** Start a new cluster with chosen configurations. | POST ‚Äã/cluster | `weka cluster add <host-hostnames>` |
 | **Update cluster configuration:** Modify settings for an existing cluster. | PUT ‚Äã/cluster | `weka cluster update` |
 | **View cluster status:** Check the overall health and performance of the cluster. | GET ‚Äã/cluster | `weka status --json` |
 | **View cluster capacity reduction information** | GET /cluster/capacity_reduction ** | `weka status reduction` |
 | **Set the container's requested action** | POST /cluster/requestedAction ** | `weka cluster requested-action set` |
 | **Set the cluster's elective protection level** | POST /cluster/requestedAction/electiveProtection ** | `weka cluster requested-action elective-protection` |

## Containers

Related information:

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- |
 | **List containers:** View all containers running in the cluster. | GET ‚Äã/containers | `weka cluster container` |
 | **Add a container:** Introduce a new container to the cluster (apply afterward to activate). | POST ‚Äã/containers | `weka cluster container add <hostname>` |
 | **View container details:** Get information about a specific container (resources, state). | GET ‚Äã/containers‚Äã/{uid} | `weka cluster container <container-ids>` |
 | **Update container configuration:** Change settings for a container (cores, memory). | PUT ‚Äã/containers‚Äã/{uid} | `weka cluster container <container-ids> <subcommand>` |
 | **Manage protocol containers:** Restart the containers. | POST‚Äã/containers‚Äã/manageProtoContainers | `weka local restart <container>` |
 | **Remove a container:** Stop and delete a container from the cluster. | DELETE ‚Äã/containers‚Äã/{uid} | `weka cluster container remove <container-ids>` |
 | **Apply configuration updates:** Implement changes to all containers. | POST ‚Äã/containers‚Äã/apply | `weka cluster container apply` |
 | **Apply configuration updates:** Implement changes to specific containers. | POST ‚Äã/containers‚Äã/{uid}‚Äã/apply | `weka cluster container apply <container-ids>` |
 | **Clear container failure:** Reset the error record for a container. | DELETE ‚Äã/containers‚Äã/lastFailureReason‚Äã/{uid} | `weka cluster container clear-failure<container-ids>` |
 | **Monitor container resources:** Track resource usage (CPU, memory) for containers. | GET ‚Äã/containers‚Äã/{uid}‚Äã/resources | `weka cluster container resources <container-ids>` |
 | **Start all containers:** Bring all inactive containers online and running. | POST ‚Äã/containers‚Äã/activate | `weka cluster container activate` |
 | **Start a specific container:** Activate an individual container by name or identifier. | POST ‚Äã/containers‚Äã/{uid}‚Äã/activate | `weka cluster container activate <container-ids>` |
 | **Stop all containers:** Gracefully shut down all running containers. | POST ‚Äã/containers‚Äã/deactivate | `weka cluster container deactivate` |
 | **Check the container deactivation:** Simulate container deactivation to assess whether the operation can be performed safely without impacting the cluster. | POST‚Äã/containers‚Äã/deactivation-check | `weka cluster container deactivation-check <container-ids>` |
 | **Stop a specific container:** Deactivate an individual container by name or identifier. | POST ‚Äã/containers‚Äã/{uid}‚Äã/deactivate | `weka cluster container deactivate <container-ids>` |
 | **View network details for all containers:** View the network configuration and connectivity information for each container within the cluster. | GET ‚Äã/containers‚Äã/netdevs | `weka cluster container net` |
 | **View network details for a specific container:** View the network configuration and connectivity information for a single container specified by its name or identifier. | GET ‚Äã/containers‚Äã/{uid}‚Äã/netdevs | `weka cluster container net <container-ids>` |
 | **Assign dedicated network:** Give a container its network device (apply afterward to activate). | POST ‚Äã/containers‚Äã/{uid}‚Äã/netdevs | `weka cluster container net add <container-ids>` |
 | **Remove dedicated network:** Take away a container's dedicated network device (apply afterward to activate). | DELETE ‚Äã/containers‚Äã/{uid}‚Äã/netdevs‚Äã/{netdev_uid} | `weka cluster container net remove <container-ids>` |
 | **View container hardware:** View hardware details (IP addresses) for containers. | POST ‚Äã/containers‚Äã/infos | `weka cluster container info-hw` |

## DataService

Related information:

 | Task | REST API | CLI |
 | --------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | --- |
 | **Get Data Service features:** Retrieves a list of background tasks managed by the Data Services container. Currently, this includes only quota coloring. | GET‚Äã/dataService‚Äã/features | N/A |

## Default network

Related information: #id-6.-configure-default-data-networking-optional

 | Task | REST API | CLI |
 | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ | --------------------------------- |
 | **Check default network setup:** Review the predefined network properties for container deployments. | GET ‚Äã/defaultNet | `weka cluster default-net` |
 | **Define new network defaults:** Define the IP address range, gateway address, and subnet mask to be used for future container network assignments. | POST ‚Äã/defaultNet | `weka cluster default-net set` |
 | **Modify existing network defaults:** Change the parameters like IP range, gateway, or subnet mask used for future container network assignments. | PUT ‚Äã/defaultNet | `weka cluster default-net update` |
 | **Clear custom network defaults:** Remove any modifications to the standard network settings and return to the initial baseline. | DELETE ‚Äã/defaultNet | `weka cluster default-net reset` |

## Drive

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------ |
 | **View a list of all SSD drives in the cluster:** Get information about all available SSD drives within the cluster, including size, UUID, status, and more. drive | GET ‚Äã/drives | `weka cluster drive` |
 | **Add a new SSD drive to a container:** Attach an additional SSD drive to a specific container within the cluster to expand its available resources. | POST ‚Äã/drives | `weka cluster drive add <container-id> <device-paths>` |
 | **View a specific SSD drive in the cluster:** Get detailed information about a particular SSD drive in the cluster. | GET ‚Äã/drives‚Äã/{uid} | `weka cluster drive <uuids>` |
 | **Remove an SSD drive from the cluster:** Detach an SSD drive from the cluster, making it unavailable for further use. | DELETE ‚Äã/drives‚Äã/{uid} | `weka cluster drive remove <uuids>` |
 | **Activate SSD drives in the cluster:** Bring one or more SSD drives online and make them available for use in the cluster. | POST ‚Äã/drives‚Äã/activate | `weka cluster drive activate <uuids>` |
 | **Deactivate SSD drives in the cluster:** Temporarily take one or more SSD drives offline, preventing their use in the cluster while preserving the stored data. | POST ‚Äã/drives‚Äã/deactivate | `weka cluster drive deactivate <uuids>` |

## Events

Related information:

 | **Filter and explore events:** Find specific events in the cluster by applying filters based on criteria like severity, category, and time range. | GET ‚Äã/events | `weka events` |
 | ------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
 | **Get event details:** View a detailed description of a specific event type, including its meaning and potential causes. | GET ‚Äã/events‚Äã/describe | `weka events list-types` |
 | **Analyze event trends:** View how events occur over time by aggregating them within a specific time interval. | GET ‚Äã/events‚Äã/aggregate | `weka events --start-time <start> --end-time <end> --show-internal` |
 | **Trace events by server:** Focus on events generated by a specific server in the cluster for deeper troubleshooting. | GET ‚Äã/events‚Äã/local | `weka events list-local` |
 | **Create custom events:** Trigger and record your custom events with additional user-defined parameters for enhanced monitoring and logging. | POST /events/custom | `weka events trigger-event` |

## Failure domains

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
 | **View all failure domains:** Get a list of all available failure domains within the cluster. | GET ‚Äã/failureDomains | `weka cluster failure-domain` |
 | **View details of a specific failure domain:** View information about a single failure domain, including its resources and capacity. | GET ‚Äã/failureDomains‚Äã/{uid} | `weka cluster container <container-ids>` |

## Filesystem

Related information:

*
*
*

 | Task | REST API | CLI |
 | --- | --- | --- |
 | List all filesystems: Get a complete list of all defined filesystems in the cluster. | GET ‚Äã/fileSystems | weka fs |
 | Create a new filesystem: Configure and establish a new filesystem within the cluster. | POST ‚Äã/fileSystems | weka fs add |
 | View details of a specific filesystem: Obtain specific information about a specified filesystem, like its size, quota, and usage. | GET ‚Äã/fileSystems‚Äã/{uid} | weka fs --name <name> |
 | Modify a filesystem: Change the settings or properties of an existing filesystem. | PUT ‚Äã/fileSystems‚Äã/{uid} | weka fs update <name> |
 | Remove a filesystem: Remove a chosen filesystem and its data from the cluster. | DELETE ‚Äã/fileSystems‚Äã/{uid} | weka fs remove <name> |
 | Attach an object store bucket: Link an object store bucket to a filesystem, allowing data access from both locations. | POST ‚Äã/fileSystems‚Äã/{uid}‚Äã/objectStoreBuckets | weka fs tier s3 attach <fs-name> |
 | Get filesystem parameters by inode ID | GET ‚Äã/fileSystems‚Äã/{inode_id}‚Äã/getPath ** Replaced GET‚Äã/fileSystems‚Äã/{inode_context}‚Äã/getPath | weka debug fs resolve-inode |
 | Detach an object store bucket: Disconnect an object store bucket from a filesystem, separating their data access. | DELETE ‚Äã/fileSystems‚Äã/{uid}‚Äã/objectStoreBuckets‚Äã/{obs_uid} | weka fs tier s3 detach <fs-name> <obs-name> |
 | Restore a filesystem from a snapshot: Create a new filesystem based on a saved snapshot stored in an object store bucket. | POST ‚Äã/fileSystems‚Äã/download | weka fs download |
 | View thin-provisioning status: Check the existing allocated thin-provisioning space reserved for your organization within the cluster. | GET ‚Äã/fileSystems‚Äã/thinProvisionReserve | weka fs reserve status |
 | Reserve guaranteed SSD for your organization: Set the thin-provisioning space for your organization's filesystems. | PUT ‚Äã/fileSystems‚Äã/thinProvisionReserve‚Äã/{org_uid} | weka fs reserve set <ssd-capacity> |
 | Release dedicated SSD space for your organization: Remove the existing reserved thin-provisioning space allocated for your organization's filesystems. | DELETE ‚Äã/fileSystems‚Äã/thinProvisionReserve‚Äã/{org_uid} | weka fs reserve unset --org <org> |
 | Remove all security policies from a filesystem | DELETE‚Äã/fileSystems‚Äã/{uid}‚Äã/securityPolicy | weka fs security policy reset |
 | Get security policies for a filesystem | GET‚Äã/fileSystems‚Äã/{uid}‚Äã/securityPolicy | weka fs security policy list |
 | Set filesystem security policies | PUT‚Äã/fileSystems‚Äã/{uid}‚Äã/securityPolicy | weka fs security policy set |
 | Attach security policies to a filesystem | POST‚Äã/fileSystems‚Äã/{uid}‚Äã/securityPolicy‚Äã/attach | weka fs security policy attach |
 | Detach security policies from a filesystem | POST‚Äã/fileSystems‚Äã/{uid}‚Äã/securityPolicy‚Äã/detach | weka fs security policy detach |

## Quota

Related information

 | Task | REST API | CLI |
 | ----------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
 | **View quotas:** View a list of the existing quota settings for all directories within the filesystem. | GET ‚Äã/fileSystems‚Äã/{uid}‚Äã/quota | `weka fs quota list <fs-name>` |
 | **View default quotas:** Check the default quota configuration applied to new directories. | GET ‚Äã/fileSystems‚Äã/{uid}‚Äã/quota‚Äã/default | `weka fs quota list-default` |
 | **View/list the parameters of a specific directory quota** | GET ‚Äã/fileSystems‚Äã/{uid}‚Äã/quota‚Äã/{inode_id} | `weka fs quota list <fs-name> --path ` |
 | **Set/update a directory quota:** Specify disk space limits for an individual directory. | PUT ‚Äã/fileSystems‚Äã/{uid}‚Äã/quota‚Äã/{inode_id} | `weka fs quota set ` |
 | **Update directory quota parameters:** Modify specific settings (like grace period) for an existing directory quota. | PATCH ‚Äã/fileSystems‚Äã/{uid}‚Äã/quota‚Äã/{inode_id} | `weka fs quota set --soft <soft> --hard <hard> --grace <grace> --owner <owner>` |
 | **Remove a directory quota:** Disable the quota restrictions for a directory. | DELETE ‚Äã/fileSystems‚Äã/{uid}‚Äã/quota‚Äã/{inode_id} | `weka fs quota unset ` |
 | **Set/update default quota:** Establish or change the default quota applied to all newly created directories. | <a href="https://api.docs.weka.io/?urls.primaryName=5.0#/Quota/putDefaultQuota">PUT ‚Äã/fileSystems‚Äã/quota‚Äã/{inode_id}</a> ** Replaced PUT‚Äã/fileSystems/quota‚Äã/{inode_context} | `weka fs quota set-default ` |
 | **Unset a default directory quota:** Disable the pre-defined quota restrictions automatically applied to new directories within the filesystem. | DELETE ‚Äã/fileSystems‚Äã/quota‚Äã/{inode_id}‚Äã/default | `weka fs quota unset-default ` |
 | **Resolve path to Inode** | GET‚Äã/fileSystems‚Äã/{uid}‚Äã/resolvePath | N/A |

## Filesystem group

Related information:

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------- |
 | **View filesystem groups:** View the list of all existing filesystem groups. | GET ‚Äã/fileSystemGroups | `weka fs group` |
 | **Create/add a filesystem group:** Establish a new group to share and manage access control for certain filesystems. | POST ‚Äã/fileSystemGroups | `weka fs group add` |
 | **View filesystem group details:** Get specific information about a particular filesystem group. | GET ‚Äã/fileSystemGroups‚Äã/{uid} | N/A |
 | **Update a filesystem group:** Modify the properties of an existing filesystem group. | PUT ‚Äã/fileSystemGroups‚Äã/{uid} | `weka fs group update <name>` |
 | **Remove a filesystem group:** Remove a filesystem group and its associated permissions. | DELETE ‚Äã/fileSystemGroups‚Äã/{uid} | `weka fs group remove <name>` |

## Health

Related information: #cluster-protection-and-availability-widget

 | Task | REST API | CLI |
 | ---------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | --- |
 | **Check REST API status:** Verify the existing functionality and availability of the REST API used for programmatic system access. | GET ‚Äã/healthcheck | N/A |
 | **Check GUI status:** Confirm the proper operation and responsiveness of the graphical user interface. | GET ‚Äã/ui‚Äã/healthcheck | N/A |

## Interface group

Related information:

 | Task | REST API | CLI |
 | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------- |
 | **View interface groups:** View the list of all interface groups configured in the system. | GET ‚Äã/interfaceGroups | `weka nfs interface-group` |
 | **Create/add an interface group:** Set up a new interface group to manage network configuration for specific P addresses and ports. | POST ‚Äã/interfaceGroups | `weka nfs interface-group add` |
 | **View interface group details:** View specific information about a particular interface group. | GET ‚Äã/interfaceGroups‚Äã/{uid} | `weka nfs interface-group --name <name>` |
 | **Delete an interface group:** Remove an interface group and its associated network definitions. | DELETE ‚Äã/interfaceGroups‚Äã/{uid} | `weka nfs interface-group remove <name>` |
 | **Update an interface group:** Modify the settings of an existing interface group. | PUT ‚Äã/interfaceGroups‚Äã/{uid} | `weka nfs interface-group update <name>` |
 | **Add an IP range to an interface group:** Define a specific range of IP addresses within the existing interface group for network access. | POST ‚Äã/interfaceGroups‚Äã/{uid}‚Äã/ips | `weka nfs interface-group ip-range add <name> <ips>` |
 | **Add a port to an interface group:** Assign a specific port number to the interface group, making it accessible through that port. | POST ‚Äã/interfaceGroups‚Äã/{uid}‚Äã/ports‚Äã/{container_uid} | `weka nfs interface-group port add <name> <server-id> ` |
 | **Remove an IP range from an interface group:** Delete a previously defined IP range from the interface group, disabling its access. | DELETE ‚Äã/interfaceGroups‚Äã/{uid}‚Äã/ports‚Äã/{container_uid}‚Äã/{port} | `weka nfs interface-group port remove <name> ` |
 | **Remove a port from an interface group:** Unassign a specific port from the interface group, making it no longer accessible through that port. | DELETE ‚Äã/interfaceGroups‚Äã/{uid}‚Äã/ips‚Äã/{ips} | `weka nfs interface-group ip-range remove <name> <ips>` |
 | **View floating IPs:** View the list of all allocated floating IPs and their existing assignments. | GET ‚Äã/interfaceGroups‚Äã/listAssignment | `weka nfs interface-group assignment` |
 | **Add port for all interface groups:** Assign a port to be accessible by the specified interface group. | POST ‚Äã/interfaceGroups‚Äã/port | weka nfs interface-group port add <name> <server-id> <port> |

## KMS

Related information:

 | Task | REST API | CLI |
 | ----------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | --------------------------------------------------------- |
 | **View KMS configuration:** View the existing Key Management Service (KMS) settings for encrypting filesystems. | GET ‚Äã/kms | `weka security kms` |
 | **Set configuration (new KMS):** Establish a new KMS configuration with details like type, address, and key identifier. | POST ‚Äã/kms | `weka security kms set <type> <address> <key-identifier>` |
 | **Delete configuration (unused only):** Remove the KMS configuration if no encrypted filesystems rely on it. | DELETE ‚Äã/kms | `weka security kms unset` |
 | **View existing KMS type:** Find out whether HashiCorp Vault or KMIP is used for KMS. | GET ‚Äã/kms‚Äã/type | `weka security kms` |
 | **Re-encrypt filesystems:** Update the encryption keys for existing filesystems using the new KMS master key. | POST ‚Äã/kms‚Äã/rewrap | `weka security kms rewrap` |

## LDAP

Related information:

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | ------------------------ |
 | **View LDAP configuration:** Get detailed information about the configured settings for connecting to your LDAP server. This includes information like the server address, port, base DN, and authentication method. | GET ‚Äã/ldap | `weka user ldap` |
 | **Update LDAP configuration:** Modify the existing settings used for connecting to your LDAP server. This may involve changing the server details, authentication credentials, or other relevant parameters. | PUT ‚Äã/ldap | `weka user ldap setup` |
 | **Disable LDAP:** Deactivate the integration with your LDAP server for user authentication. | DELETE ‚Äã/ldap | `weka user ldap disable` |

## License

Related information:

 | Task | REST API | CLI |
 | ---------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------ |
 | **View license details:** Get information about the configured cluster license, including resource usage and validity. | GET ‚Äã/license | `weka cluster license` |
 | **Set license:** Install a new cluster license for continued operation. | POST ‚Äã/license | `weka cluster license set <license>` |
 | **Remove license:** Deactivate the existing license and return the cluster to unlicensed mode. | DELETE ‚Äã/license | `weka cluster license reset` |

## Lockout policy

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------- | ------------------------------------ |
 | **View policy:** View the configured settings for the lockout policy, including attempt limits and duration. | GET ‚Äã/lockoutPolicy | `weka security lockout-config show` |
 | **Update policy:** Modify the parameters of the lockout policy to adjust login security. | PUT ‚Äã/lockoutPolicy | `weka security lockout-config set` |
 | **Reset lockout:** Clear the failed login attempts counter and unlock any currently locked accounts. | DELETE ‚Äã/lockoutPolicy | `weka security lockout-config reset` |

## Login

Related information: \

 | Task | REST API | CLI |
 | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ----------------- |
 | **Log in to the cluster:** Authenticate and grant access to the cluster using valid credentials. Securely save user credentials in the user's home directory upon successful login. | POST ‚Äã/login | `weka user login` |
 | **Retrieve access token:** Obtain a new access token using an existing refresh token. The system creates an authentication token file and saves it in `~/.weka/auth-token.json`. The token file contains both the access token and the refresh token. | POST ‚Äã/login‚Äã/refresh | `weka user login` |

## Mounts Defaults

Related information:

*
* #set-mount-option-default-values

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------- |
 | **View cluster-wide mount options:** View the configured mount options applied to all filesystems across the cluster. | GET ‚Äã/mountDefaults | `weka cluster mount-defaults show` |
 | **Set cluster-wide mount options:** Configure default options for mounting filesystems across the cluster. | PUT ‚Äã/mountDefaults | `weka cluster mount-defaults set` |
 | **Reset cluster-wide mount options:** Revert default mount options to initial settings for all filesystems in the cluster. | DELETE ‚Äã/mountDefaults | `weka cluster mount-defaults reset` |

## NFS

Related information:

 | Task | REST API | CLI |
 | --- | --- | --- |
 | View all permissions: View all defined NFS permissions for client groups across all filesystems. | GET ‚Äã/nfs‚Äã/permissions | weka nfs permission |
 | Grant permission: Grant a client group access to an NFS share by creating a new permission rule. | POST ‚Äã/nfs‚Äã/permissions | weka nfs permission add <fs_name> <client-group-name> |
 | View specific permission: View the specific access control rules for a single NFS permission entry identified by its UID. | GET ‚Äã/nfs‚Äã/permissions‚Äã/{uid} | weka nfs permission --filesystem <fs_name> |
 | Modify permission: Modify an existing NFS permission to change access controls for a client group. | PUT ‚Äã/nfs‚Äã/permissions‚Äã/{uid} | weka nfs permission update <fs_name> <client-group-name> |
 | Revoke permission: Revoke a client group's access to a filesystem by removing a specific NFS permission | DELETE ‚Äã/nfs‚Äã/permissions‚Äã/{uid} | weka nfs permission delete <fs_name> <client-group-name> |
 | View all client groups: View all defined client groups used for managing NFS access controls. | GET ‚Äã/nfs‚Äã/clientGroups | weka nfs client-group |
 | Create client group: Create a new client group to use for managing access controls for NFS mounts. | POST ‚Äã/nfs‚Äã/clientGroups | weka nfs client-group add <group-name> |
 | View specific client group: View the details of a specific NFS client group | GET ‚Äã/nfs‚Äã/clientGroups‚Äã/{uid} | weka nfs client-group --name <client-group-name> |
 | Delete client group: Delete an existing NFS client group. | DELETE ‚Äã/nfs‚Äã/clientGroups‚Äã/{uid} | weka nfs client-group delete <client-group-name> |
 | Add DNS rule: Add a DNS-based rule to a client group, which grants access to clients matching the DNS hostname. | POST ‚Äã/nfs‚Äã/clientGroups‚Äã/{uid}‚Äã/rules | weka nfs rules add dns <client-group-name> <dns-rule> |
 | Remove DNS rule: Remove a DNS-based rule from an NFS client group. | DELETE ‚Äã/nfs‚Äã/clientGroups‚Äã/{uid}‚Äã/rules‚Äã/{rule_uid} | weka nfs rules delete dns <client-group-name> <dns-rule> |
 | Configure global settings: Manage global parameters for NFS operations, including the mountd service port, configuration filesystem for NFSv4, supported NFS versions, and ACL settings. | PUT ‚Äã/nfs‚Äã/globalConfig | weka nfs global-config set |
 | View cluster-wide NFS configuration: Get the global parameters for NFS operations, including the mountd service port, configuration filesystem for NFSv4, and supported NFS versions. | GET ‚Äã/nfs‚Äã/globalConfig | weka nfs global-config show |
 | View logging level: Check the current logging level for container processes involved in the NFS cluster. | GET ‚Äã/nfs‚Äã/debug | weka nfs debug-level show |
 | Set logging level: Adjust the logging verbosity for container processes in the NFS cluster to control the level of detail in logs. | POST ‚Äã/nfs‚Äã/debug | weka nfs debug-level set <debug-level> |
 | Integrate Kerberos: Set up secure network communication by defining KDC details, administrative credentials, and other parameters for robust Kerberos authentication. | PUT‚Äã/nfs‚Äã/kerberosService | weka nfs kerberos service setup |
 | View Kerberos configuration: View the current NFS-Kerberos service configuration details, including the realm name and KDC servers. | GET‚Äã/nfs‚Äã/kerberosService | weka nfs kerberos service show |
 | Register with MIT Kerberos: Register the NFS service with a MIT Kerberos Key Distribution Center (KDC) using a pre-generated keytab file | PUT‚Äã/nfs‚Äã/kerberosMitRegistration | weka nfs kerberos registration setup-mit |
 | Register with AD Kerberos: Register the NFS service with a Microsoft Active Directory domain controller. | PUT‚Äã/nfs‚Äã/kerberosActiveDirectoryRegistration | weka nfs kerberos registration setup-ad |
 | View Kerberos registration: View the current NFS Kerberos registration status and type (AD or MIT). | GET‚Äã/nfs‚Äã/kerberosRegistration | weka nfs kerberos registration show |
 | Reset Kerberos configuration: Remove the entire Kerberos service configuration data from the system, which allows for a new integration to be set up. | PUT‚Äã/nfs‚Äã/kerberosReset | weka nfs kerberos reset |
 | Set OpenLDAP configuration: Configure NFS to use an OpenLDAP service for user and group information, typically in conjunction with a MIT Kerberos integration. | PUT‚Äã/nfs‚Äã/openLdapService | weka nfs ldap setup-openldap |
 | Set AD LDAP configuration: Configure NFS to use the LDAP service within Active Directory for user and group lookups.S | PUT‚Äã/nfs‚Äã/activeDirectoryLdapService | weka nfs ldap setup-ad |
 | Set AD LDAP for ACLs (no Kerberos): Configure NFS to use LDAP for ACLs when Kerberos authentication is not in use. | PUT‚Äã/nfs‚Äã/activeDirectoryNoKrbLdapService | weka nfs ldap setup-ad-nokrb |
 | View LDAP configuration: View the current LDAP configuration for the NFS service. | GET‚Äã/nfs‚Äã/ldapService | weka nfs ldap show |
 | Reset LDAP configuration: Clear the entire NFS LDAP configuration from the system. | PUT‚Äã/nfs‚Äã/ldapReset | weka nfs ldap reset |
 | Import LDAP configuration: Import an OpenLDAP configuration from a file and apply it to the NFS service. | PUT‚Äã/nfs‚Äã/ldapImport | weka nfs ldap import-openldap |
 | Export LDAP configuration: Export the current OpenLDAP configuration for the NFS service to a file for backup or migration. | POST‚Äã/nfs‚Äã/ldapExport | weka nfs ldap export-openldap |

## Object store

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | ------------------------------------ |
 | **Update object store connection:** Update details for an existing object store connection. | PUT ‚Äã/objectStores‚Äã/{uid} | `weka fs tier obs update <obs-name>` |

## Object store bucket

Related information:

 | Task | REST API | CLI |
 | ----------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
 | **View S3 configurations:** View the list of connection and status details for all S3 object store buckets. | GET ‚Äã/objectStoreBuckets | `weka fs tier s3` |
 | **Create an S3 connection:** Establish a new S3 object store bucket connection. | POST ‚Äã/objectStoreBuckets | `weka fs tier s3 add <obs-name>` |
 | **View an S3 connection:** View the list of connections and status details for a specific S3 object store bucket. | GET ‚Äã/objectStoreBuckets‚Äã/{uid} | `weka fs tier s3 --obs-name <obs-name> --name <bucket-name>` |
 | **Delete an S3 connection:** Remove an existing S3 object store connection. | DELETE ‚Äã/objectStoreBuckets‚Äã/{uid} | `weka fs tier s3 delete <obs-name>` |
 | **Update an S3 connection:** Modify an existing S3 object store bucket connection. | PUT ‚Äã/objectStoreBuckets‚Äã/{uid} | `weka fs tier s3 update <bucket-name>` |
 | **View snapshots:** List and view details about uploaded snapshots within an object store. | GET ‚Äã/objectStoreBuckets‚Äã/{uid}‚Äã/operations | `weka fs tier s3 snapshot list <bucket-name>` |

## Organization

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
 | **Check for multiple organizations:** Verify if multiple organizations exist within the cluster. | GET ‚Äã/organizations‚Äã/multipleOrgsExist | `weka org` |
 | **View organizations:** View the list of all organizations defined in the cluster. | GET ‚Äã/organizations | `weka org` |
 | **Add organization:** Create a new organization within the cluster. | POST ‚Äã/organizations | `weka org add` |
 | **View organization details:** View information about an existing organization. | GET ‚Äã/organizations‚Äã/{uid} | `weka org <org name or ID>` |
 | **Delete organization:** Remove an organization from the cluster. | DELETE ‚Äã/organizations‚Äã/{uid} | `weka org delete <org name or ID>` |
 | **Update organization name:** Change the name of an existing organization. | PUT ‚Äã/organizations‚Äã/{uid} | `weka org rename <org name or ID> <new-org-name>` |
 | **Set organization quotas:** Define SSD and total storage quotas for an organization. | PUT ‚Äã/organizations‚Äã/{uid}‚Äã/limits | `weka org set-quota <org name or ID>` |
 | **Remove all security policies from an organization** | DELETE‚Äã/organizations‚Äã/{uid}‚Äã/securityPolicy | `weka org security reset` |
 | **Get security policies for an organization** | GET‚Äã/organizations‚Äã/{uid}‚Äã/securityPolicy | `weka org security list` |
 | **Set organization security policies** | PUT‚Äã/organizations‚Äã/{uid}‚Äã/securityPolicy | `weka org security set` |
 | **Attach security policies to an organization** | POST‚Äã/organizations‚Äã/{uid}‚Äã/securityPolicy‚Äã/attach | `weka org security attach` |
 | **Detach security policies from an organizatio**n | POST‚Äã/organizations‚Äã/{uid}‚Äã/securityPolicy‚Äã/detach | `weka org security detach` |

## Processes

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | -------------------------------------- |
 | **View all processes' details:** View information about all running processes within the cluster. | GET ‚Äã/processes | `weka cluster processes` |
 | **View process details:** View information about a specific process based on its ID. | GET ‚Äã/processes‚Äã/{uid} | `weka cluster processes ` |

## S3

Related information:

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------- |
 | **View S3 cluster information:** View details about the S3 cluster managed by WEKA. | GET ‚Äã/s3 | `weka s3 cluster` |
 | **Create an S3 cluster:** Establish a new S3 cluster. | POST ‚Äã/s3 | `weka s3 cluster add` |
 | **Update an S3 cluster:** Modify the configuration of an existing S3 cluster. | PUT ‚Äã/s3 | `weka s3 cluster update` |
 | **Delete an S3 cluster:** Remove an S3 cluster. | DELETE ‚Äã/s3 | `weka s3 cluster destroy` |
 | **View buckets:** View the list of all buckets within an S3 cluster. | GET ‚Äã/s3‚Äã/buckets | `weka s3 bucket list` |
 | **Add an S3 bucke**t: Establish a new bucket within an S3 cluster. | POST ‚Äã/s3‚Äã/buckets | `weka s3 bucket add` |
 | **View S3 user policies:** View the list of S3 user policies. | GET ‚Äã/s3‚Äã/userPolicies | `weka s3 bucket policy` |
 | **Delete an S3 bucket:** Delete a specified S3 bucket. | DELETE ‚Äã/s3‚Äã/buckets‚Äã/{bucket} | `weka s3 bucket delete <bucket-name>` |
 | **View S3 IAM policies:** View the list of S3 IAM policies. | GET ‚Äã/s3‚Äã/policies | `weka s3 policy list` |
 | **Add an S3 IAM policy:** Add a new S3 IAM policy. | POST ‚Äã/s3‚Äã/policies | `weka s3 policy add` |
 | **View S3 IAM policy details:** View details about a specific S3 IAM policy. | GET ‚Äã/s3‚Äã/policies‚Äã/{policy} | `weka s3 policy show ` |
 | **Remove an S3 IAM policy:** Delete an S3 IAM policy. | DELETE ‚Äã/s3‚Äã/policies‚Äã/{policy} | `weka s3 policy remove ` |
 | **Attach an S3 IAM policy to a user:** Assign an S3 IAM policy to a user. | POST ‚Äã/s3‚Äã/policies‚Äã/attach | `weka s3 policy attach <user>‚Äå` |
 | **Detach an S3 IAM policy from a user:** Remove an S3 IAM policy from a user. | POST ‚Äã/s3‚Äã/policies‚Äã/detach | `weka s3 policy detach <user>‚Äå‚Äå` |
 | **View service accounts:** View the list of S3 service accounts. | GET ‚Äã/s3‚Äã/serviceAccounts | `weka s3 service-account list` |
 | **Add an S3 service account:** Establish a new S3 service account. | POST ‚Äã/s3‚Äã/serviceAccounts | `weka s3 service-account add ` |
 | **View service account details:** View details about a specific S3 service account. | GET ‚Äã/s3‚Äã/serviceAccounts‚Äã/{access_key} | `weka s3 service-account show <access-key>` |
 | **Delete an S3 service account:** Remove an S3 service account. | DELETE ‚Äã/s3‚Äã/serviceAccounts‚Äã/{access_key} | `weka s3 service-account remove <access-key>` |
 | **Create an S3 STS token:** Create an S3 STS token with an assumed role. | POST ‚Äã/s3‚Äã/sts | `weka s3 sts assume-role` |
 | **Add lifecycle rule:** Create a new lifecycle rule for an S3 bucket. | POST ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/lifecycle‚Äã/rules | `weka s3 bucket lifecycle-rule add <bucket-name>` |
 | **Reset lifecycle rules:** Reset all lifecycle rules for an S3 bucket to their default settings. | DELETE ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/lifecycle‚Äã/rules | `weka s3 bucket lifecycle-rule reset <bucket-name>` |
 | **View lifecycle rules:** View the list of all lifecycle rules for an S3 bucket. | GET ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/lifecycle‚Äã/rules | `weka s3 bucket lifecycle-rule list <bucket-name>` |
 | **Delete lifecycle rule:** Remove a lifecycle rule from an S3 bucket. | DELETE ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/lifecycle‚Äã/rules‚Äã/{rule} | `weka s3 bucket lifecycle-rule remove <bucket-name> <rule-name>` |
 | **View S3 bucket policy:** View the policy attached to an S3 bucket. | GET ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/policy | `weka s3 bucket policy get <bucket-name>` |
 | **Set S3 bucket policy:** Assign a policy to an S3 bucket. | PUT ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/policy | `weka s3 bucket policy set <bucket-name> <bucket-policy>` |
 | **View S3 bucket policy (JSON):** View the bucket policy in JSON format. | GET ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/policyJson | `weka s3 bucket policy get-json <bucket-name>` |
 | **Set S3 bucket policy (JSON):** Set the bucket policy using a JSON file. | PUT ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/policyJson | `weka s3 bucket policy set-custom <bucket-name> ` |
 | **Set S3 bucket quota:** Define a storage quota for an S3 bucket. | PUT ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/quota | `weka s3 bucket quota set <bucket-name> <hard-quota>` |
 | **Unset S3 bucket quota:** Remove a storage quota from an S3 bucket. | DELETE ‚Äã/s3‚Äã/buckets‚Äã/{bucket}‚Äã/quota | `weka s3 bucket quota unset <bucket-name>` |
 | **View container readiness:** Check the readiness status of containers within the S3 cluster. | GET ‚Äã/s3‚Äã/containersAreReady | `weka s3 cluster status` |
 | **Add container to S3 cluster:** Add a container to the S3 cluster. | POST ‚Äã/s3‚Äã/containers | `weka s3 cluster container add <container-ids>` |
 | **Remove containers:** Remove containers from the S3 cluster. | DELETE ‚Äã/s3‚Äã/containers | `weka s3 cluster container remove <container-ids>` |
 | **View logging verbosity:** View the logging level for container processes within the S3 cluster. | GET ‚Äã/s3‚Äã/debug | `weka s3 log-level get` |
 | **Set logging verbosity:** Adjust the logging level for container processes within the S3 cluster. | POST ‚Äã/s3‚Äã/debug | `weka s3 log-level set <log-level>` |
 | **Enable S3 audit webhook:** Activate the S3 audit webhook. | POST ‚Äã/s3‚Äã/auditWebhook‚Äã/enable | `weka s3 cluster audit-webhook enable` |
 | **Disable S3 audit webhook:** Deactivate the S3 audit webhook. | POST ‚Äã/s3‚Äã/auditWebhook‚Äã/disable | `weka s3 cluster audit-webhook disable` |
 | **View S3 audit webhook configuration:** View details about the S3 audit webhook configuration. | GET ‚Äã/s3‚Äã/auditWebhook | `weka s3 cluster audit-webhook show` |

## SMB

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------- |
 | **View SMB cluster configuration:** View details about the existing SMB cluster configuration. | GET ‚Äã/smb | `weka smb cluster` |
 | **Add SMB cluster:** Establish a new SMB cluster managed by WEKA. | POST ‚Äã/smb | `weka smb cluster add <netbios-name> <domain> <config-fs-name>` |
 | **Update SMB cluster configuration:** Modify the existing configuration of an SMB cluster. | PUT ‚Äã/smb | `weka smb cluster update` |
 | **Remove SMB cluster configuration:** Disable SMB access to data without affecting the data itself. | DELETE ‚Äã/smb | `weka smb cluster destroy` |
 | **View trusted domains (SMB):** View the list of trusted domains recognized by the SMB cluster. | GET ‚Äã/smb‚Äã/domains | `weka smb cluster trusted-domains` |
 | **Add trusted domain (SMB):** Add a new trusted domain to the SMB cluster (triggers a background restart for SMB-W). | POST ‚Äã/smb‚Äã/domains | `weka smb cluster trusted-domains add` |
 | **View SMB mount options:** View the list of mount options used by the existing SMB cluster. | GET ‚Äã/smb‚Äã/mount | N/A |
 | **View SMB shares:** View the list of all shares available within the SMB cluster. | GET ‚Äã/smb‚Äã/shares | `weka smb share` |
 | **Add SMB share:** Add a new share within the SMB cluster. | POST ‚Äã/smb‚Äã/shares | `weka smb share add <share-name> <fs-name>` |
 | **Join Active Directory:** Integrate the SMB cluster with an Active Directory domain. | POST ‚Äã/smb‚Äã/activeDirectory | `weka smb domain join <username> ` |
 | **Leave Active Directory:** Disconnect the SMB cluster from the Active Directory domain. | PUT ‚Äã/smb‚Äã/activeDirectory | `weka smb domain leave <username>` |
 | **Set SMB container logging verbosity:** Adjust the logging level for container processes in the SMB cluster. | POST ‚Äã/smb‚Äã/debug | `weka smb cluster debug <level>` |
 | **Update SMB share:** Modify the configuration of an existing SMB share. | PUT ‚Äã/smb‚Äã/shares‚Äã/{uid} | `weka smb share update <share-id>` |
 | **Delete SMB share:** Remove an SMB share from the cluster. | DELETE ‚Äã/smb‚Äã/shares‚Äã/{uid} | `weka smb share remove <share-id>` |
 | **Remove trusted domain (SMB):** Remove a trusted domain from the SMB cluster (triggers a background restart for SMB-W). | DELETE ‚Äã/smb‚Äã/domains‚Äã/{uid} | `weka smb cluster trusted-domains remove` |
 | **Add SMB share users:** Add users associated with a specific SMB share. | POST ‚Äã/smb‚Äã/users‚Äã/{share_uid}‚Äã/{user_type} | `weka smb share lists add <share-id> <user-list-type> --users <users>` |
 | **Remove SMB share users:** Remove users associated with a specific SMB share. | DELETE ‚Äã/smb‚Äã/users‚Äã/reset‚Äã/{share_uid}‚Äã/{user_type} | `weka smb share lists reset <share-id> <user-list-type>` |
 | **Remove specific SMB share users:** Remove specific users associated with a specific SMB share. | DELETE ‚Äã/smb‚Äã/users‚Äã/{share_uid}‚Äã/{user_type}‚Äã/{user} | `weka smb share lists remove <share-id> <user-list-type> --users <users>` |
 | **View SMB container status:** Check the status of containers participating in the SMB cluster. | GET ‚Äã/smb‚Äã/containersAreReady | `weka smb cluster status` |
 | **Add SMB cluster containers:** Add containers to the SMB cluster. | PUT ‚Äã/smb‚Äã/servers | `weka smb cluster containers add --containers-id <containers-id>` |
 | **Remove SMB cluster containers:** Remove containers from the SMB cluster. | DELETE ‚Äã/smb‚Äã/servers | `weka smb cluster containers remove --containers-id <containers-id>` |

## Security

Related information:

 | Task | REST API | CLI |
 | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
 | **View token expiry:** View the default and maximum expiration times for access and refresh tokens. | GET ‚Äã/security‚Äã/defaultTokensExpiry | `weka security token-expiry show` |
 | **Set token expiry:** Set the default and maximum expiration times for access and refresh tokens. | POST ‚Äã/security‚Äã/defaultTokensExpiry | `weka security token-expiry set` |
 | **View login banner:** View the existing login banner displayed on the sign-in page. | GET ‚Äã/security‚Äã/banner | `weka security login-banner show` |
 | **Set login banner:** Create or modify the login banner containing a security statement or legal message. | PUT ‚Äã/security‚Äã/banner | `weka security login-banner set <login-banner>` |
 | **Show login banner:** Show the login banner on the sign-in page. | POST ‚Äã/security‚Äã/banner‚Äã/enable | `weka security login-banner enable` |
 | **Hide login banner:** Hide the login banner from the sign-in page. | POST ‚Äã/security‚Äã/banner‚Äã/disable | `weka security login-banner disable` |
 | **Add or update custom CA certificate:** Upload a custom CA certificate to be used for authentication. If a certificate is already present, this command replaces it. | PUT ‚Äã/security‚Äã/caCert | `weka security ca-cert set` |
 | **Delete custom CA certificate:** Remove the currently configured custom CA certificate from the cluster. | DELETE ‚Äã/security‚Äã/caCert | `weka security ca-cert unset` |
 | **View cluster CA certificate:** View the status and details of the cluster's CA certificate. | GET ‚Äã/security‚Äã/caCert | `weka security ca-cert status` |
 | **Create a security policy:** Add a new security policy to control access based on client IP address ranges. | POST‚Äã/security‚Äã/policies | `weka security policy create` |
 | **List all security policies**: List the security policies defined in the WEKA cluster. | GET‚Äã/security‚Äã/policies | `weka security policy list` |
 | **Display information about a security policy:** Display information for a specific security policy. | GET‚Äã/security‚Äã/policies‚Äã/{policy} | `weka security policy show` |
 | **Delete a security policy:** Remove a security policy from the system. | DELETE‚Äã/security‚Äã/policies‚Äã/{policy} | `weka security policy delete` |
 | **Update a security policy:** Update the settings of an existing security policy. | PATCH‚Äã/security‚Äã/policies‚Äã/{policy} | `weka security policy update` |
 | **Duplicate a security policy:** Duplicate an existing security policy to create a new one.**y** | POST‚Äã/security‚Äã/policies‚Äã/{policy}‚Äã/duplicate | `weka security policy duplicate` |
 | **Test the effect of security policies:** Simulate the effect of one or more security policies on API access or cluster joining. | GET‚Äã/security‚Äã/policies‚Äã/test | `weka security policy test` |
 | **Remove all security policies for joining containers:** Remove all security policies applied when a container joins the cluster. | DELETE‚Äã/security‚Äã/join‚Äã/{mode}‚Äã/securityPolicy | `weka security policy join reset` |
 | **List security policies for joining containers:** List the security policies applied when containers join the cluster. | GET‚Äã/security‚Äã/join‚Äã/{mode}‚Äã/securityPolicy | `weka security policy join list` |
 | **Set security policies for joining containers:** Set the security policies for a container joining the cluster, which replaces the existing policy set. | PUT‚Äã/security‚Äã/join‚Äã/{mode}‚Äã/securityPolicy | `weka security policy join set` |
 | **Attach security policies for joining containers:** Attach security policies for a container joining the cluster, adding them to the existing list of policies. | POST‚Äã/security‚Äã/join‚Äã/{mode}‚Äã/securityPolicy‚Äã/attach | `weka security policy join attach` |
 | **Detach security policies for joining containers:** Detach (remove) specific security policies applied when a container joins the cluster. | POST‚Äã/security‚Äã/join‚Äã/{mode}‚Äã/securityPolicy‚Äã/detach | `weka security policy join detach` |

## Servers

Related information:

 | Task | REST API | CLI |
 | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | --------------------------- |
 | **View cluster servers:** View the list of all servers within the cluster. | GET ‚Äã/servers | `weka cluster servers list` |
 | **View server details:** View specific information about an individual server based on its UID. | GET ‚Äã/servers‚Äã/{uid} | `weka cluster servers show` |

## Snapshot policy

Related information:

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------- |
 | **Add snapshot policy:** Create a new snapshot policy with defined rules and schedules for creating and managing point-in-time copies of data. | POST‚Äã/snapshotPolicy | `weka fs protection snapshot-policy add` |
 | **View snapshot policies**: Get a complete list of all snapshot policies defined in the cluster | GET‚Äã/snapshotPolicy | `weka fs protection snapshot-policy list` |
 | **View snapshot policy details**: Obtain the detailed configuration for a specific snapshot policy, including its schedules, retention settings, and attached filesystems. | GET‚Äã/snapshotPolicy‚Äã/{uid} | `weka fs protection snapshot-policy show` |
 | **Update snapshot policy**: Modify the settings of an existing snapshot policy, such as its name, description, schedules, or retention rules. | PUT‚Äã/snapshotPolicy‚Äã/{uid} | `weka fs protection snapshot-policy update` |
 | **Delete snapshot policy:** Remove a snapshot policy from the system. Ensure no filesystems are attached before deleting the policy. | DELETE‚Äã/snapshotPolicy‚Äã/{uid} | `weka fs protection snapshot-policy remove` |
 | **List filesystems attached to a snapshot policy**: View a list of all filesystems that are assigned to a specific snapshot policy. | GET‚Äã/snapshotPolicy‚Äã/{uid}‚Äã/filesystems | `weka fs protection snapshot-policy show` |
 | **Attach filesystems to snapshot policy**: Assign a snapshot policy to one or more filesystems to apply its data protection rules. | POST‚Äã/snapshotPolicy‚Äã/{uid}‚Äã/filesystems‚Äã/attach | `weka fs protection snapshot-policy attach` |
 | **Detach filesystems from snapshot policy**: Remove the association between a snapshot policy and specific filesystems. | POST‚Äã/snapshotPolicy‚Äã/{uid}‚Äã/filesystems‚Äã/detach | `weka fs protection snapshot-policy detach` |

## Snapshots

Related information:

*
*

 | Task | REST API | CLI |
 | ---------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
 | **View snapshots:** View the list of all snapshots currently available. | GET ‚Äã/snapshots | `weka fs snapshot` |
 | **Add snapshot:** Establish a new snapshot of a filesystem. | POST ‚Äã/snapshots | `weka fs snapshot add <file-system> <snapshot-name>` |
 | **View snapshot details:** View specific information about an existing snapshot. | GET ‚Äã/snapshots‚Äã/{uid} | `weka fs snapshot --name <snapshot-name>` |
 | **Update snapshot:** Modify the configuration of an existing snapshot. | PUT ‚Äã/snapshots‚Äã/{uid} | `weka fs snapshot update <file-system> <snapshot-name>` |
 | **Remove snapshot:** Remove a snapshot from the system. | DELETE ‚Äã/snapshots‚Äã/{uid} | `weka fs snapshot remove <file-system> <snapshot-name>` |
 | **Copy snapshot:** Copy a snapshot from the same filesystem to a different location. | POST ‚Äã/snapshots‚Äã/{uid}‚Äã/copy | `weka fs snapshot copy <file-system> <source-name> <destination-name>` |
 | **Upload snapshot to object store:** Transfer a snapshot to an object storage. | POST ‚Äã/snapshots‚Äã/{uid}‚Äã/upload | `weka fs snapshot upload <file-system> <snapshot-name>` |
 | **Download snapshot:** Download a snapshot from an object storage system. | POST ‚Äã/snapshots‚Äã/download | `weka fs snapshot download` |
 | **Restore filesystem from snapshot**: Restore a filesystem using a previously created snapshot. | POST ‚Äã/snapshots‚Äã/{fs_uid}‚Äã/{uid}‚Äã/restore | `weka fs snapshot download <file-system> <snapshot-locator>` |
 | **Get snapshot detailed information from the object store**. | POST‚Äã/snapshots‚Äã/locatorInfo | N/A |
 | **Prepare for snapshot differences** | POST/snapshots‚Äã/diff‚Äã/prepare ** | N/A |
 | **Get snapshot differences:** Retrieve snapshot differences to support efficient paging of data changes between specified points in time | POST/snapshots/diff/getResults ** | N/A |

## Stats

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
 | **View stats:** View the list of various statistics related to the cluster's performance and resource usage. | GET ‚Äã/stats | `weka stats` |
 | **View stats description:** Get detailed explanations of the available statistics. | GET ‚Äã/stats‚Äã/description | `weka stats list-types` |
 | **View real-time stats:** Monitor live statistics for the cluster. | GET ‚Äã/stats‚Äã/realtime | `weka stats realtime` |
 | **View stats retention and disk usage:** View how long statistics are retained and estimate disk space used for storage. | GET ‚Äã/stats‚Äã/retention | `weka stats retention status` |
 | **Set stats retention:** Define the duration for which statistics are stored. | POST ‚Äã/stats‚Äã/retention | `weka stats retention set --days <num-of-days>` |

## System IO

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | ----------------------- |
 | **Start cluster IO services:** Enable the cluster-wide IO services. | POST ‚Äã/io‚Äã/start | `weka cluster start-io` |
 | **Stop cluster IO services:** Disable the cluster-wide IO services. | POST ‚Äã/io‚Äã/stop | `weka cluster stop-io` |

## Tasks

Related information:

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | ------------------------------------ |
 | **View background tasks:** View the list of all currently running background tasks within the cluster. | GET ‚Äã/tasks | `weka cluster task‚Äå` |
 | **Resume a background task:** Re-initiate a paused background task, allowing execution to continue. | POST ‚Äã/tasks‚Äã/{uid}‚Äã/resume | `weka cluster task resume <task-id>` |
 | **Pause a background task:** Temporarily halt the execution of a running background task. The task can be resumed later. | POST ‚Äã/tasks‚Äã/{uid}‚Äã/pause | `weka cluster task pause <task-id>` |
 | **Abort a background task:** Terminate a running background task, permanently stopping its execution. Any unfinished work associated with the task will be discarded. | POST ‚Äã/tasks‚Äã/{uid}‚Äã/abort | `weka cluster task abort <task-id>` |
 | **View background task limits:** View the existing limitations on the number of background tasks running concurrently within the system. This information helps you understand the capacity for handling background processes. | GET ‚Äã/tasks‚Äã/limits | `weka cluster task limits` |
 | **Set background task limits:** Adjust the maximum number of background tasks allowed to run simultaneously. This allows you to control the system's resource allocation and potential performance impact from concurrent tasks. | PUT ‚Äã/tasks‚Äã/limits | `weka cluster task limits set` |

## Telemetry

Related information: Audit and forwarding management

 | **View telemetry status:** View the current cluster-wide status of the audit and forwarding functionality, including whether it is enabled. | GET /telemetry | `weka audit cluster status` |
 | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- |
 | **Update telemetry status:** Enable or disable the audit and forwarding functionality at the cluster level. | POST /telemetry | weka audit cluster enable weka audit cluster disable |
 | **List telemetry exports:** Retrieve a list of all configured telemetry export destinations, such as Splunk, S3, or Kafka endpoints. | GET /telemetry/exports | `weka telemetry exports list` |
 | **Create a telemetry export:** Create a new telemetry export destination to forward audit events to an external system. | PUT /telemetry/exports | `weka telemetry exports add` |
 | **View a specific telemetry export:** View the configuration details and status of a single, specific telemetry export destination by its unique ID | GET /telemetry/exports/{uid} | `weka telemetry exports show <export-name>` |
 | **Update a specific telemetry export:** Update the configuration of an existing telemetry export destination (for example, to change its endpoint URL or authentication token). | POST /telemetry/exports/{uid} | `weka telemetry exports update <export-name>` |
 | **Delete a specific telemetry export**: Delete a specific telemetry export destination from the configuration | DELETE /telemetry/exports/{uid} | `weka telemetry exports remove` |
 | **Attach a source to an export:** Attach a data source (such as "audit") to a specific telemetry export, instructing the system to send that type of data to the destination. | PUT /telemetry/exports/{uid}/sources | `weka telemetry exports attach --sources audit` |
 | **Detach a source from an export:** Detach a data source from a telemetry export, stopping the flow of that data type to the destination. | DELETE /telemetry/exports/{uid}/sources | `weka telemetry exports detach <export-name> --sources audit` |

## TLS

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- | ---------------------------- |
 | **View cluster TLS status:** Check the status and details of the cluster's TLS certificate. | GET ‚Äã/tls | `weka security tls status` |
 | **Configure Nginx with TLS:** Enable TLS for the UI and set or update the private key and certificate. | POST ‚Äã/tls | `weka security tls set` |
 | **Configure Nginx without TLS:** Disable TLS for the UI. | DELETE ‚Äã/tls | `weka security tls unset` |
 | **Download TLS certificate:** Download the cluster's TLS certificate. | GET ‚Äã/tls‚Äã/certificate | `weka security tls download` |

## Traces

Related information:

 | Task | REST API | CLI |
 | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------- | -------------------------------- |
 | **View traces configuration:** See the current configuration settings for trace collection. | GET ‚Äã/traces | `weka debug traces status` |
 | **Start trace collection:** Initiate the collection of trace data. | PUT ‚Äã/traces | `weka debug traces start` |
 | **Stop trace collection**: Stop the collection of trace data. | DELETE ‚Äã/traces | `weka debug traces stop` |
 | **View trace freeze period:** View the duration for which trace data is preserved for investigation. | GET ‚Äã/traces‚Äã/freeze | `weka debug traces freeze show` |
 | **Set trace freeze period:** Set the duration for which trace data is preserved for investigation. | PUT ‚Äã/traces‚Äã/freeze | `weka debug traces freeze set` |
 | **Clear frozen traces:** Remove all existing frozen traces and reset the freeze period to zero. | DELETE ‚Äã/traces‚Äã/freeze | `weka debug traces freeze reset` |
 | **Set trace verbosity level:** Modify the level of detail captured in trace logs. Low captures essential information for basic troubleshooting. High captures extensive details for in-depth analysis. | PUT ‚Äã/traces‚Äã/level | `weka debug traces level set` |

## User

Related information:

 | Task | REST API | CLI |
 | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | -------------------------------------------- |
 | **View local users:** Retrieve the list of all local users on the system. | GET ‚Äã/users | `weka user` |
 | **Create a local user:** Add a new local user account. | POST ‚Äã/users | `weka user add <username> <role> ` |
 | **Update a local user:** Modify the details of an existing local user. | PUT ‚Äã/users‚Äã/{uid} | `weka user update <username>` |
 | **Delete a local user:** Remove a local user account from the system. | DELETE ‚Äã/users‚Äã/{uid} | `weka user delete <username>` |
 | **Set a local user password:** Assign a password to a local user. | PUT ‚Äã/users‚Äã/password | `weka user passwd` |
 | **Update a local user password:** For any user, change your own password or the password of another user if you have the necessary permissions. For admins, change the password of any user within the organization. | PUT ‚Äã/users‚Äã/{uid}‚Äã/password | `weka user passwd <username>` |
 | **View the logged-in user:** Get information about the currently logged-in user. | GET ‚Äã/users‚Äã/whoami | `weka user whoami` |
 | **Invalidate user sessions:** Immediately terminate all active login sessions (GUI, CLI, API) associated with a specific internal user. This action prevents further access to the system using those tokens. | DELETE ‚Äã/users‚Äã/{uid}‚Äã/revoke | `weka user revoke-tokens` |

**Related information**

REST API Reference Guide

<!-- ============================================ -->
<!-- File 65/259: weka-system-overview.md -->
<!-- ============================================ -->

# WEKA System Overview

## Topics in this section

### Introduction

WEKA is a software solution that enables the implementation of a shareable, scalable, distributed filesystem storage.

### Cluster capacity and redundancy management

### Filesystems, object stores, and filesystem groups

This page describes the three entity types relevant to data storage in the WEKA system.

### WEKA networking

Explore network technologies in WEKA, including DPDK, SR-IOV, CPU-optimized networking, UDP mode, high availability, and RDMA/GPUDirect Storage, with configuration guidelines.

### Data lifecycle management

Explore the principles for data lifecycle management and how data storage is managed in SSD-only and tiered WEKA system configurations.

### WEKA client and mount modes

Understanding the WEKA system client and possible mount modes of operation in relation to the page cache.

### WEKA containers architecture overview

Overview of WEKA's container-based architecture, where interconnected processes within server-hosted containers provide scalable and resilient storage services in a cluster.

### Glossary

<!-- ============================================ -->
<!-- File 66/259: weka-system-overview_about.md -->
<!-- ============================================ -->

---
description:
---

# Introduction

The WEKA filesystem (WekaFS‚Ñ¢) redefines storage solutions with its software-only approach, compatible with standard AMD or Intel x86-based servers and NVMe SSDs. It eliminates the need for specialized hardware, allowing easy integration of technological advancements without disruptive upgrades. WekaFS addresses common storage challenges by removing performance bottlenecks, making it suitable for environments requiring low latency, high performance, and cloud scalability.

Use cases span various sectors, including AI/ML, Life Sciences, Financial Trading, Engineering DevOps, EDA, Media Rendering, HPC, and GPU pipeline acceleration. Combining existing technologies and engineering innovations, WekaFS delivers a powerful, unified solution that outperforms traditional storage systems, efficiently supporting various workloads.

WekaFS is a fully distributed parallel filesystem leveraging NVMe Flash for file services. Integrated tiering seamlessly expands the namespace to and from HDD object storage, simplifying data management. The intuitive GUI allows easy administration of exabytes of data without specialized storage training.

WekaFS stands out with its unique architecture, overcoming legacy systems‚Äô scaling and file-sharing limitations. Supporting POSIX, NFS, SMB, S3, and GPUDirect Storage, it offers a rich enterprise feature set, including snapshots, clones, tiering, cloud-bursting, and more.

Benefits include high performance across all IO profiles, scalable capacity, robust security, hybrid cloud support, private/public cloud backup, and cost-effective flash-disk combination. WekaFS ensures a cloud-like experience, seamlessly transitioning between on-premises and cloud environments.

WekaFS functionality running in its RTOS within the Linux container (LXC) is comprised of the following software components:

* **File services (frontend client access):** Manages multi-protocol connectivity.
* **File system computing and clustering (backend: compute):** Manages data distribution, data protection, file system metadata services, and tiering.
* **SSD drive agent (backend: drive):** Transforms the SSD into an efficient networked device.
* **Management process (not shown):** Manages events, CLI, statistics, and call-home capability.
* **Object connector** **(not shown)****:** Read and write to the object store.

By bypassing the kernel, WekaFS achieves faster, lower-latency performance, portable across bare-metal, VM, containerized, and cloud environments. Efficient resource consumption minimizes latency and optimizes CPU usage, offering flexibility in shared or dedicated environments.

WekaFS design departs from traditional NAS solutions, introducing multiple filesystems within a global namespace that share the same physical resources. Each filesystem has its unique identity, allowing customization of snapshot policies, tiering, role-based access control (RBAC), quota management, and more. Unlike other solutions, filesystem capacity adjustments are dynamic, enhancing scalability without disrupting I/O.

The WEKA system offers a robust, distributed, and highly scalable storage solution, allowing multiple application servers to access shared filesystems efficiently and with solid consistency and POSIX compliance.

**Related information**

WEKA Architectural Whitepaper

<!-- ============================================ -->
<!-- File 67/259: weka-system-overview_about_weka-system-functionality-features.md -->
<!-- ============================================ -->

# WEKA system functionality features

The WEKA system offers a range of powerful functionalities designed to enhance data protection, scalability, and efficiency, making it a versatile solution for various storage requirements.

## Protection

The WEKA system employs N+2 or N+4 protection, ensuring data protection even in the face of concurrent drive or backend failures. This complex protection scheme is determined during cluster formation and can vary, offering configurations starting from 3+2 up to 16+2 for larger clusters.

## Distributed network scheme

The WEKA system incorporates an any-to-any protection scheme that ensures the rapid recovery of data in the event of a backend failure. Unlike traditional storage architectures, where redundancy is often established across backend servers (backends), WEKA's approach leverages groups of datasets to protect one another within the entire cluster of backends.

Here's how it works:

* **Data recovery process:** If a backend within the cluster experiences a failure, the WEKA system initiates a rebuilding process using all the other operational backends. These healthy backends work collaboratively to recreate the data that originally resided on the failed backend. Importantly, all this occurs in parallel, with multiple backends simultaneously reading and writing data.
* **Speed of rebuild:** This approach results in a speedy rebuild process. In a traditional storage setup, only a small subset of backends or drives actively participate in rebuilding, often leading to slow recovery. In contrast, in the WEKA system, all but the failed backend are actively involved, ensuring swift recovery and minimal downtime.
* **Scalability benefits:** The advantages of this distributed network scheme become even more apparent as the cluster size grows. In larger clusters, the rebuild process is further accelerated, making the WEKA system an ideal choice for organizations that need to handle substantial data volumes without sacrificing data availability.

In summary, the WEKA system's distributed network scheme transforms data recovery by involving all available backends in the rebuild process, ensuring speedy and efficient recovery, and this efficiency scales with larger clusters, making it a robust and scalable solution for data storage and protection.

## **Efficient component replacement**

In the WEKA system, a hot spare is configured within the cluster to provide the additional capacity needed for a full recovery after a rebuild across the entire cluster. This differs from traditional approaches, where specific physical components are designated hot spares. For instance, in a 100-backend cluster, sufficient capacity is allocated to rebuild the data and restore full redundancy even after two failures. The system can withstand two additional failures depending on the protection policy and cluster size.

This strategy for replacing failed components does not compromise system reliability. In the event of a system failure, there's no immediate need to physically replace a failed component with a functional one to recreate the data. Instead, data is promptly regenerated, while replacing the failed component with a working one is a background process.

## **Enhanced fault tolerance with failure domains**

In the WEKA system, failure domains are groups of backends that could fail due to a single underlying issue. For instance, if all servers within a rack rely on a single power circuit or connect through a single ToR switch, that entire rack can be considered a failure domain. Imagine a scenario with ten racks, each containing five WEKA backends, resulting in a cluster of 50 backends.

To enhance fault tolerance, you can configure a protection scheme, such as 6+2 protection, during the cluster setup. This makes the WEKA system aware of these possible failure domains and creates a protection stripe across the racks. This means the 6+2 stripe is distributed across different racks, ensuring that the system remains operational even in case of a complete rack failure, preventing data loss.

It's important to note that the stripe width must be less than or equal to the count of failure domains. For instance, if there are ten racks, and one rack represents a single point of failure, having a 16+4 cluster protection is not feasible. Therefore, the level of protection and support for failure domains depends on the stripe width and the chosen protection scheme.

## Prioritized data rebuild process

In the event of a failure in the WEKA system, the data recovery process begins by reading all affected data stripes, reconstructing the lost data, and restoring full protection. If multiple failures occur, the affected stripes can be categorized as follows:

* Stripes unaffected by any of the failed components, requiring no action.
* Stripes affected by one of the failed components.
* Stripes affected by multiple failed components.

Typically, the number of stripes affected by multiple failed components is significantly smaller than those affected by a single failed component. However, if any of these multi-affected stripes remain unrecovered, additional failures could lead to data loss.

To mitigate this risk, the WEKA system employs a **prioritized rebuild process**. The system first restores stripes affected by the greatest number of failed components, depending on the data protection level, as these are fewer in number and can be recovered quickly. Once these high-risk stripes are rebuilt, the system proceeds with restoring stripes impacted by fewer failures. This structured approach ensures that the probability of data loss remains low while maintaining system performance and availability.

## **Seamless distribution, scaling, and enhanced performance**

In the WEKA system, every client installed on an application server directly connects to the relevant WEKA backends that store the required data. There's no intermediary backend that forwards access requests. Each WEKA client maintains a synchronized map, specifying which backend holds specific data types, creating a unified configuration shared by all clients and backends.

When a WEKA client attempts to access a particular file or offset in a file, a cryptographic hash function guides it to the appropriate backend containing the needed data. This unique mechanism enables the WEKA system to achieve linear performance growth. It synchronizes scaling size with scaling performance, providing remarkable efficiency.

For instance, when new backends are added to double the cluster's size, the system instantly redistributes part of the filesystem data between the backends, resulting in an immediate double performance increase. Complete data redistribution is unnecessary even in modest cluster growths, such as moving from 100 to 110 backends. Only a fraction (10% in this example) of the existing data is copied to the new backends, ensuring a balanced distribution and active participation of all backends in read operations.

The speed of these seamless operations depends on the capacity of the backends and network bandwidth. Importantly, ongoing operations remain unaffected, and the system's performance improves as data redistribution occurs. The finalization of the redistribution process optimizes both capacity and performance, making the WEKA system an ideal choice for scalable and high-performance storage solutions.

## **Efficient data reduction**

WEKA offers a cluster-wide data reduction feature that can be activated for individual filesystems. This capability employs block-variable differential compression and advanced de-duplication techniques across all filesystems to significantly reduce the storage capacity required for user data, resulting in substantial cost savings for customers.

The effectiveness of the compression ratio depends on the specific workload. It is particularly efficient when applied to text-based data, large-scale unstructured datasets, log analysis, databases, code repositories, and sensor data. For more information, see #data-reduction-in-weka-filesystems.

    Top-of-Rack switching (ToR) is a type of network infrastructure that uses network switches to connect servers and other devices in the same rack. This type of switching allows for faster data transfer between devices and improved performance.

    The number of protected data units.

<!-- ============================================ -->
<!-- File 68/259: weka-system-overview_about_converged-weka-system-deployment.md -->
<!-- ============================================ -->

# Converged WEKA system deployment

The WEKA system offers a converged deployment configuration as an alternative to the standard setup. In this configuration, hundreds of application servers running user applications are equipped with WEKA clients, allowing them to access the WEKA cluster.

Unlike the standard deployment that dedicates specific servers to WEKA backends, the converged setup involves installing a WEKA client on each application server. Additionally, one or more SSDs and backend processes (WekaFS) are integrated into the existing application servers.

The WEKA backend processes function collectively as a single, distributed, and scalable filesystem, leveraging the local SSDs. This filesystem is accessible to the application servers, much like in the standard WEKA system deployment. The critical distinction is that, in this configuration, WEKA backends share the same physical infrastructure as the application servers.

This blend of storage and computing capabilities enhances overall performance and resource usage. However, unlike the standard deployment, where an application server failure does not impact other backends, the converged setup is affected if an application server is rebooted or experiences a failure. The N+2 (or N+4) scheme still protects the cluster and can tolerate two concurrent failures. As a result, converged WEKA deployments require more careful integration and detailed coordination between computational and storage management practices.

In all other respects, this configuration mirrors the standard WEKA system, offering the same functionality features for protection, redundancy, failed component replacement, failure domains, prioritized data rebuilds, and seamless distribution, scale, and performance. Some servers may house a WEKA backend process and a local SSD, while others may have WEKA clients only. This allows for a cluster of application servers with a mix of WEKA software and WEKA clients, delivering a flexible solution.

<!-- ============================================ -->
<!-- File 69/259: weka-system-overview_weka-containers-architecture-overview.md -->
<!-- ============================================ -->

---
description:
---

# WEKA containers architecture overview

## Cluster architecture basics

In the WEKA system, servers operate as cluster members, each server hosting multiple containers. These containers run software instances, referred to as processes, that collaborate and communicate within the cluster to deliver robust and efficient storage services. This architecture ensures scalability and fault tolerance by distributing storage functionality across interconnected containers.

### Process types and core requirements

The WEKA system uses different types of processes, each dedicated to specific functions:

* **Drive processes**: A backend process that manages SSD drives and handle IO operations to drives. These processes are fundamental to storage operations and each requires a dedicated core to ensure optimal performance.
* **Compute processes:** A backend process that handles filesystems, cluster-level functions, and IO from clients. Each compute process requires a dedicated core to ensure consistent processing power for these critical operations.
* **Frontend processes**: A client process that manages POSIX client access and coordinates IO operations with compute and drive processes. Each frontend process requires a dedicated core to maintain responsive client interactions.
* **Management process**: A backend process that oversees overall cluster operations. It has lower resource demands and can share cores. The process uses kernel networking and unallocated OS-available cores.

## Multi-container backend (MCB) architecture

In the WEKA cluster, each server implements a multi-container backend architecture where containers are specialized by process type: drive, compute, or frontend.

## Benefits of MCB architecture

* **Non-Disruptive Upgrade (NDU) capabilities:**
  * Enables true non-disruptive upgrades where containers can run different software versions independently without system interruption
  * Supports individual container rollback without impacting cluster operations
  * Maintains continuous network control plane access throughout the upgrade process, ensuring uninterrupted client service
* **Optimized hardware utilization:**
  * Supports up to 64 WEKA cores per server
  * Multiple containers per process type
  * Flexible core allocation across containers
  * Up to 19 cores per container
* **Improved maintenance operations:**
  * Selective process management
  * Ability to maintain drive processes while stopping compute and frontend processes

## System limitations and specifications

### **Process limits**

* Total processes per cluster: 65,534 (includes all process types: management, drive, compute, and frontend).
* Maximum management processes: 32,767.
* Maximum drive processes: 62,244.

### **Server and container limits**

Each server has resource limits that affect how many containers it can run and how cores are allocated:

* Maximum WEKA cores per server: 64
* Maximum cores per container: 19
* Maximum containers of any type per server: 32
  *  Within this total, the maximum frontend containers per server is 7.

Note: When WEKA is deployed on Kubernetes as a multi-tenant solution, the limits above apply per tenant.

<!-- ============================================ -->
<!-- File 70/259: weka-system-overview_weka-client-and-mount-modes.md -->
<!-- ============================================ -->

---
description:
---

# WEKA client and mount modes

## The WEKA client

The WEKA client is a standard POSIX-compliant filesystem driver installed on application servers, facilitating file access to WEKA filesystems. Acting as a conventional filesystem driver, it intercepts and executes all filesystem operations, providing applications with local filesystem semantics and performance‚Äîdistinct from NFS mounts. This approach ensures centrally managed, shareable, and resilient storage for WEKA.

Tightly integrated with the Linux Page Cache, the WEKA client leverages this transparent caching mechanism to store portions of filesystem content in the client's RAM. The Linux operating system maintains a page cache in the unused RAM, allowing rapid access to cached pages and yielding overall performance enhancements.

The Linux Page Cache, implemented in the Linux kernel, operates transparently to applications. Utilizing unused RAM capacity, it incurs minimal performance penalties, often appearing as "free" or "available" memory.

The WEKA client retains control over the Linux Page Cache, enabling cache information management and invalidation when necessary. Consequently, WEKA leverages the Linux Page Cache for high-performance data access, ensuring data consistency across multiple servers.

A filesystem can be mounted in one of two modes with the Linux Page Cache:

* **Read cache mount mode****:** Only read operations use Linux Page Cache to sustain RAM-level performance for the frequently accessed data. WEKA ensures that the view of the data is coherent across various applications and clients.
* **Write cache mount mode (default)****:** Both read and write operations use the Linux Page Cache, maintaining data coherency across servers and providing optimal data performance.

Note: Symbolic links are consistently cached in all modes.

## **R**ead cache mount mode

In Read Cache mode, the Linux Page Cache operates in _write-through_ mode, meaning that write operations are acknowledged only after being securely stored on resilient storage. This applies to both data and metadata.

By default, data read or written by customer applications is stored in the local server's Linux Page Cache. The WEKA system monitors access to this data and invalidates the cache if another server attempts to read or write the same data. Cache invalidation occurs in the following scenarios:

* When one client writes to a file that another client is reading or writing.
* When one server writes to a file that another server is reading.

This approach ensures data coherence. The Linux Page Cache is fully used when a file is accessed by a single server or multiple servers in read-only mode. However, if multiple servers access a file and at least one server writes to it, the system bypasses the Linux Page Cache, and all I/O operations are handled by the backend servers.

Note: A server is considered to be "writing" to a file after the first write operation occurs, regardless of the read/write flags set by the open system call.
For workloads involving random reads of small blocks from large files, enabling the read cache and Linux prefetch mechanisms may not improve performance and could even be counterproductive. Assess whether enabling read-ahead aligns with your performance goals for truly random access patterns.

## Write cache mount mode (default)

In Write Cache mode, the Linux Page Cache operates in _write-back_ mode rather than _write-through_. When a write operation occurs, it is immediately acknowledged by the WEKA client and temporarily stored in the kernel memory cache. The data is then written to resilient storage in the background.

This mode improves performance by reducing write latency while maintaining data coherence. If the same file is accessed by another server, the local cache is invalidated, ensuring a consistent view of the data.

To ensure all changes in the write cache are committed to storage, particularly before taking a snapshot, you can use system calls like `sync`, `syncfs`, and `fsync`. These commands force the filesystem to flush the write cache and synchronize data to resilient storage.

## Multiple mounts on a single server

The WEKA client allows multiple mount points for the same filesystem on a single server, supporting different mount modes. This is useful in containerized environments where various server processes require distinct read/write access or caching schemes.

Each mount point on the same server is treated independently for cache consistency. For example, two mounts with write cache mode on the same server may have different data simultaneously, accommodating diverse requirements for applications or workflows on that server.

## Metadata management

Unlike file data, file metadata is managed in the Linux operating system through the directory entry (Dentry) cache. While maximizing efficiency in handling directory entries, the Dentry cache is not strongly consistent across WEKA clients. For applications prioritizing metadata consistency, it is possible to configure metadata for strong consistency by mounting without a Dentry cache.

**Related topic**

#mount-command-options

<!-- ============================================ -->
<!-- File 71/259: weka-system-overview_networking-in-wekaio.md -->
<!-- ============================================ -->

---
description:
---

# WEKA networking

## Overview

The WEKA system supports the following types of networking technologies:

* ‚ÄåInfiniBand (IB)
* Ethernet

‚ÄåThe networking infrastructure dictates the choice between the two. If a WEKA cluster is connected to both infrastructures, it is possible to connect WEKA clients from both networks to the same cluster.

The WEKA system networking can be configured as _performance-optimized_ or _CPU-optimized_. In performance-optimized networking, the CPU cores are dedicated to WEKA, and the networking uses DPDK. In CPU-optimized networking, the CPU cores are not dedicated to WEKA, and the networking uses DPDK (when supported by the NIC drivers) or in-kernel (UDP mode).

### Performance-optimized networking (DPDK)

For performance-optimized networking, the WEKA system does not use standard kernel-based TCP/IP services but a proprietary infrastructure based on the following:

* Use DPDK to map the network device in the user space and use it without any context switches and with zero-copy access. This bypassing of the kernel stack eliminates the consumption of kernel resources for networking operations. It applies to backends and clients and lets the WEKA system saturate network links (including, for example, 200 Gbps or 400 Gbps).
* Implementing a proprietary WEKA protocol over UDP, i.e., the underlying network, may involve routing between subnets or any other networking infrastructure that supports UDP.

The use of DPDK delivers operations with extremely low latency and high throughput. Low latency is achieved by bypassing the kernel and sending and receiving packages directly from the NIC. High throughput is achieved because multiple cores in the same server can work in parallel without a common bottleneck.

Before proceeding, it is important to understand several key terms used in this section, namely DPDK and SR-IOV.

#### DPDK

‚ÄåData Plane Development Kit (DPDK) is a set of libraries and network drivers for highly efficient, low-latency packet processing. This is achieved through several techniques, such as kernel TCP/IP bypass, NUMA locality, multi-core processing, and device access via polling to eliminate the performance overhead of interrupt processing. In addition, DPDK ensures transmission reliability, handles retransmission, and controls congestion.

DPDK implementations are available from several sources. OS vendors like Red Hat and Ubuntu provide DPDK implementations through distribution channels. Mellanox OpenFabrics Enterprise Distribution for Linux (Mellanox OFED), a suite of libraries, tools, and drivers supporting Mellanox NICs, offers its own DPDK implementation.

The WEKA system relies on the DPDK implementation provided by Mellanox OFED on servers equipped with Mellanox NICs. For servers equipped with Intel NICs, DPDK support is through the Intel driver for the card.‚Äå

#### SR-IOV

Single Root I/O Virtualization (SR-IOV) extends the PCI Express (PCIe) specification that enables PCIe virtualization. It allows a PCIe device, such as a network adapter, to appear as multiple PCIe devices or _functions_.

There are two function categories:

* Physical Function (PF): PF is a full-fledged PCIe function that can also be configured.
* Virtual Function (VF): VF is a virtualized instance of the same PCIe device created by sending appropriate commands to the device PF.

Typically, there are many VFs, but only one PF per physical PCIe device. Once a new VF is created, it can be mapped by an object such as a virtual machine, container, or, in the WEKA system, by a 'compute' process.

To take advantage of SR-IOV technology, the software and hardware must be supported. The Linux kernel provides SR-IOV software support. The computer BIOS and the network adapter provide hardware support (by default, SR-IOV is disabled and must be enabled before installing WEKA).

### CPU-optimized networking

For CPU-optimized networking, WEKA can yield CPU resources to other applications. That is useful when the extra CPU cores are needed for other purposes. However, the lack of CPU resources dedicated to the WEKA system comes with the expense of reduced overall performance.

#### DPDK without the core dedication

For CPU-optimized networking, when [mounting filesystems using stateless clients](../../weka-filesystems-and-object-stores/mounting-filesystems#mounting-filesystems-using-stateless-clients), it is possible to use DPDK networking without dedicating cores. This mode is recommended when available and supported by the NIC drivers. The DPDK networking uses RX interrupts instead of dedicating the cores in this mode.

Note: This mode is supported in most NIC drivers. Consult https://doc.dpdk.org/guides/nics/overview.html for compatibility.
AWS (ENA drivers) does not support this mode. Hence, in CPU-optimized networking in AWS, use the UDP mode.

#### UDP mode

WEKA can also use in-kernel processing and UDP as the transport protocol. This operation mode is commonly referred to as _UDP mode_.

UDP mode is compatible with older platforms that lack support for kernel offloading technologies (DPDK) or virtualization (SR-IOV) due to its use of in-kernel processing. This includes legacy hardware, such as the Mellanox CX3 family of NICs.

## Typical WEKA configuration

### Backend servers

In a typical WEKA system configuration, the WEKA backend servers access the network function in two different methods:

* Standard TCP/UDP network for management and control operations.
* High-performance network for data-path traffic.

Note: To run both functions on the same physical interface, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).

The high-performance network used to connect all the backend servers must be DPDK-based. This internal WEKA network also requires a separate IP address space. For details, see [Network planning](../../planning-and-installation/bare-metal/planning-a-weka-system-installation#network-planning) and [Configure the networking](../../planning-and-installation/bare-metal/setting-up-the-hosts#configure-the-networking).

The WEKA system maintains a separate ARP database for its IP addresses and virtual functions and does not use the kernel or operating system ARP services.

### Clients

While WEKA backend servers must include DPDK and SR-IOV, WEKA clients in application servers have the flexibility to use either DPDK or UDP modes. DPDK mode is the preferred choice for newer, high-performing platforms that support it. UDP mode is available for clients without SR-IOV or DPDK support or when there is no need for low-latency and high-throughput I/O.

### Configuration guidelines

* **DPDK backends and clients using NICs supporting shared networking (single IP):**
  * Require one IP address per client for both management and data plane.
  * SR-IOV enabled is not required.
* **DPDK backends and clients using NICs supporting dedicated networking:**
  * IP address for management: One per NIC (configured before WEKA installation).
  * IP address for data plane: One per [WEKA core](../../planning-and-installation/bare-metal/planning-a-weka-system-installation#cpu-resource-planning) in each server (applied during cluster initialization).
  * Virtual Functions (VFs):
    * Ensure the device supports a maximum number of VFs greater than the number of physical cores on the server.
    * Set the number of VFs to match the cores you intend to dedicate to WEKA.
    * Note that some BIOS configurations may be necessary.
  * SR-IOV: Enabled in BIOS.
* **UDP clients:**
  * Use a shared networking (single IP) for all purposes.

## Network **High Availability** <a href="#high-availability" id="high-availability"></a>

Network High Availability (HA) in a WEKA cluster is designed to eliminate single points of failure by leveraging redundancy across network components. This configuration ensures the system remains operational even in the event of hardware or connection failures.

**Network redundancy**

To achieve HA, the WEKA system requires multiple network switches with servers connected to at least two interfaces of the same type. Dual connectivity is provided either through two independent interfaces or through Link Aggregation Control Protocol (LACP) in Ethernet environments (mode 4).

**Interface configuration**

* **Non-LACP configuration**: Each server uses two network interfaces for redundancy and bandwidth enhancement. This approach doubles the number of IP addresses required on backend containers and IO processes.
*   **LACP configuration (Ethernet-only)**: LACP aggregates interfaces on a single Mellanox NIC for improved reliability and load balancing in Ethernet-only setups.

    Specifications and requirements:

    * LACP is not supported with Virtual Functions (VFs).
    * NIC must be set to `HW_LAG` (IEEE 802.3ad) with `queue_affinity` enabled and hashing disabled.
    * At least two WEKA processes must use DPDK.
    * Switch must support IEEE 802.3ad in active/active mode.

**Failover and load balancing**

Network HA ensures reliability and optimizes load balancing through failover and failback mechanisms. These mechanisms operate independently for InfiniBand and Ethernet networks. If an interface fails, another interface of the same type (InfiniBand or Ethernet) seamlessly takes over the workload.

Note: **Mixed-mode behavior:** In a cluster with servers equipped with both Ethernet and InfiniBand connections, the system remains operational even if a single server loses one of its connections. However, that server is excluded from participating in cluster-level operations. The cluster will continue I/O operations unless all servers lose connectivity on **either** the Ethernet or InfiniBand network; in that case, I/O operations will pause.

**Traffic optimization**

To optimize network traffic, the WEKA system can be configured to prioritize intra-switch communication over inter-switch links (ISL). This can be achieved by labeling connections using the `label` parameter in the `weka cluster container net add` command, which helps route data efficiently within the cluster.

## RDMA, RoCE, and GPUDirect Storage

RDMA, RoCE, and GPUDirect Storage (GDS) establish a direct data path between storage and memory (GPU memory in case of GDS) bypassing unnecessary data copies through the operating system. This approach allows Direct Memory Access (DMA) through the NIC to transfer data directly to or from application or GPU memory bypassing the operating system.

When RDMA and GDS are enabled, the WEKA system automatically uses the RDMA data path and GDS in supported environments. The system dynamically detects when RDMA is available‚Äîincluding in RoCE v1, RoCE v2, UDP, and DPDK modes‚Äîand applies it to workloads that can benefit from RDMA. Typically, RDMA is advantageous for I/O sizes of 32KB or larger for reads and 256KB or larger for writes.

By leveraging RDMA and GDS, you can achieve enhanced performance. A UDP client, which doesn't require dedicating a core to the WEKA system, can deliver significantly higher performance. Additionally, a DPDK client can experience an extra performance boost, or you can assign fewer cores to the WEKA system while maintaining the same level of performance in DPDK mode.

### Requirements and considerations for RDMA and GDS support

RDMA, including RoCE, is enabled by default. To support RDMA and GDS technologies, the following requirements and considerations must be met:

* **Cluster requirements**
  * **RDMA networking:** All servers within the cluster must be equipped with RDMA-capable networking interfaces.
* **Client requirements**
  * **GDS support:** The InfiniBand or Ethernet interfaces included in the GDS configuration must support RDMA networking.
  * **RDMA support:** All InfiniBand and Ethernet interfaces used by WEKA must support RDMA networking.

**Fallback to standard I/O**

* RDMA and GDS are not supported for encrypted filesystems.
* If any requirement for RDMA or GDS is not met, the system automatically reverts to standard I/O operations without RDMA or GDS acceleration.

Note: **Kernel bypass:** GDS bypasses the kernel and does not use the page cache. In contrast, standard RDMA clients continue to use the page cache.

#### Verification

To confirm RDMA usage, run the following command:

```
weka cluster processes
```

Example:

```bash
# weka cluster processes
PROCESS ID  HOSTNAME  CONTAINER   IPS         STATUS  ROLES       NETWORK      CPU  MEMORY   UPTIME
0           weka146   default     10.0.1.146  UP      MANAGEMENT  UDP                        16d 20:07:42h
1           weka146   default     10.0.1.146  UP      FRONTEND    DPDK / RDMA  1    1.47 GB  16d 23:29:00h
2           weka146   default     10.0.3.146  UP      COMPUTE     DPDK / RDMA  12   6.45 GB  16d 23:29:00h
3           weka146   default     10.0.1.146  UP      COMPUTE     DPDK / RDMA  2    6.45 GB  16d 23:29:00h
4           weka146   default     10.0.3.146  UP      COMPUTE     DPDK / RDMA  13   6.45 GB  16d 23:29:00h
5           weka146   default     10.0.1.146  UP      COMPUTE     DPDK / RDMA  3    6.45 GB  16d 22:28:58h
6           weka146   default     10.0.3.146  UP      COMPUTE     DPDK / RDMA  14   6.45 GB  16d 23:29:00h
7           weka146   default     10.0.3.146  UP      DRIVES      DPDK / RDMA  18   1.49 GB  16d 23:29:00h
8           weka146   default     10.0.1.146  UP      DRIVES      DPDK / RDMA  8    1.49 GB  16d 23:29:00h
9           weka146   default     10.0.3.146  UP      DRIVES      DPDK / RDMA  19   1.49 GB  16d 23:29:00h
10          weka146   default     10.0.1.146  UP      DRIVES      DPDK / RDMA  9    1.49 GB  16d 23:29:00h
11          weka146   default     10.0.3.146  UP      DRIVES      DPDK / RDMA  20   1.49 GB  16d 23:29:07h
12          weka147   default     10.0.1.147  UP      MANAGEMENT  UDP                        16d 22:29:02h
13          weka147   default     10.0.1.147  UP      FRONTEND    DPDK / RDMA  1    1.47 GB  16d 23:29:00h
14          weka147   default     10.0.3.147  UP      COMPUTE     DPDK / RDMA  12   6.45 GB  16d 23:29:00h
15          weka147   default     10.0.1.147  UP      COMPUTE     DPDK / RDMA  2    6.45 GB  16d 23:29:00h
16          weka147   default     10.0.3.147  UP      COMPUTE     DPDK / RDMA  13   6.45 GB  16d 23:29:00h
17          weka147   default     10.0.1.147  UP      COMPUTE     DPDK / RDMA  3    6.45 GB  16d 23:29:00h
18          weka147   default     10.0.3.147  UP      COMPUTE     DPDK / RDMA  14   6.45 GB  16d 23:29:00h
19          weka147   default     10.0.3.147  UP      DRIVES      DPDK / RDMA  18   1.49 GB  16d 23:29:00h
20          weka147   default     10.0.1.147  UP      DRIVES      DPDK / RDMA  8    1.49 GB  16d 23:29:00h
21          weka147   default     10.0.3.147  UP      DRIVES      DPDK / RDMA  19   1.49 GB  16d 23:29:07h
22          weka147   default     10.0.1.147  UP      DRIVES      DPDK / RDMA  9    1.49 GB  16d 23:29:00h
23          weka147   default     10.0.3.147  UP      DRIVES      DPDK / RDMA  20   1.49 GB  16d 23:29:07h
. . .
```

Note: GDS is automatically enabled and detected by the system. To enable or disable RDMA networking for the cluster or a specific client, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).

**Related topic**

#networking (in the **Prerequisites and compatibility** topic)

<!-- ============================================ -->
<!-- File 72/259: weka-system-overview_cluster-capacity-and-redundancy-management.md -->
<!-- ============================================ -->

# Cluster capacity and redundancy management

Effective cluster capacity and redundancy management are crucial for ensuring data protection, availability, and optimal performance in WEKA systems. This involves understanding key capacity metrics, redundancy configurations, and the system's mechanisms for handling failures.

## Key capacity terms

Understanding the terminology related to storage capacity is fundamental:

* **Raw capacity**: This represents the total physical storage capacity of all SSDs assigned to a WEKA cluster. For example, if a cluster has 10 SSDs, each with 1 terabyte of capacity, the raw capacity is 10 terabytes. This figure automatically adjusts when more servers or SSDs are integrated into the cluster.
* **Net (usable) capacity**: This is the actual space available on the SSDs for storing user data. The net capacity is derived from the raw capacity and is influenced by several factors:
  * The chosen stripe width and protection level, which dedicate some capacity to system protection.
  * The allocation for hot spares, reserved for redundancy and rebuilds.
  * The WEKA cluster reserved capacity, allocated for internal system operations.
* **Provisioned capacity**: This refers to the total capacity that has been assigned to filesystems within the WEKA cluster. It includes capacity from both SSDs and any configured object stores.
* **Available capacity**: This is the remaining net capacity that can be used to allocate additional capacity to existing filesystems and create new filesystems . It is calculated by subtracting the provisioned capacity from the net capacity.

## Redundancy and protection levels

WEKA employs a distributed RAID system that supports a range of redundancy configurations. These are based on a **D+P model**, where D represents the number of data blocks and P represents the number of parity blocks. Common configurations are denoted as N+2, N+3, or N+4, where N is the number of data blocks. A critical rule is that the number of data blocks must always be greater than the number of parity blocks (for example, a 3+3 configuration is not allowed).

The selection of an appropriate redundancy level is a balance between fault tolerance, usable storage capacity, and system performance:

* **N+2**: This is the recommended level for most environments, providing a standard degree of fault tolerance. A system with protection level 2 can survive up to 2 simultaneous drive or server failures.
* **N+3**: This level offers increased data protection and is suitable for environments with higher availability requirements. A system with protection level 3 can survive up to 3 simultaneous drive or 2 simultaneous server failures.
* **N+4**: Designed for very large-scale clusters (typically 100+ backend servers) or for scenarios involving critical data that demands maximum redundancy. Protection level 4 can withstand up to 4 simultaneous drive failures or 2 simultaneous server failures.

Higher protection levels inherently provide better data durability and availability. However, they also consume more raw storage space for parity blocks and can potentially impact system performance due to the additional processing.

The protection level for a WEKA cluster is determined at the time of its formation and **cannot be changed later**. If no specific protection level is configured, the system defaults to protection level 2.

## Stripe width

**Stripe width** refers to the total number of blocks, both data and parity, that constitute a common protection set. In a WEKA cluster, the stripe width can range from 5 to 20 blocks. This total is composed of 3 to 16 data blocks and 2 to 4 parity blocks. For instance, a stripe width of 18 could represent a configuration of 16 data blocks and 2 parity blocks (16+2).

WEKA uses a **distributed any-to-any protection** scheme. This means that instead of data and parity blocks being confined to fixed protection groups, for example, a specific set of drives, they are distributed across multiple servers in the cluster. For example, in a configuration with a stripe width of 8 (6 data blocks and 2 parity blocks), these 8 blocks are spread across various servers to enhance resilience.

Like the protection level, the stripe width is also determined during the initial cluster formation and **cannot be altered subsequently**. The stripe width has a direct impact on both system performance (especially write throughput) and the usable storage capacity. Larger stripe widths generally improve write throughput because they reduce the proportion of parity overhead in write operations. This is particularly beneficial for high-ingest workloads, such as initial data loading or applications where most of the work involves writing new data.

## Hot spare capacity

WEKA clusters proactively reserve a portion of the total storage space as **virtual hot spare capacity** to ensure that resources are immediately available for data rebuilds in the event of component failures. By default, WEKA allocates 1/N of the total space for this purpose. For example, a 3+2 redundancy configuration is effectively deployed as 3+2+1, meaning one-sixth of the cluster's capacity is reserved as hot spare.

The hot spare capacity represents the number of failure domains the system can afford to lose and still successfully perform a complete data rebuild while maintaining the system's net capacity. All failure domains in the cluster actively contribute to data storage, and this hot spare capacity is evenly distributed among them.

If not explicitly configured by the administrator, the hot spare value is automatically set to 1. While a higher hot spare count provides greater flexibility for IT maintenance and hardware replacements, it also necessitates additional hardware to achieve the same net usable capacity.

## WEKA cluster reserved capacity ratio

After accounting for the capacity dedicated to data protection (parity) and hot spares, an additional **10 percent** of the remaining capacity is reserved for WEKA cluster internal use.

## Failure domains

A **failure domain** is defined as a set of WEKA servers that are susceptible to simultaneous failure due to a single root cause. This could be, for example, a shared power circuit or a common network switch malfunction.

A WEKA cluster can be configured with either:

* **Explicit failure domains**: In this setup, blocks that provide mutual protection for each other (i.e., data and its corresponding parity) are deliberately distributed across distinct, administrator-defined failure domains.
* **Implicit failure domains**: In this configuration, data and parity blocks are distributed across multiple servers, and each server is implicitly considered a separate failure domain. Additional failure domains and servers can be integrated into existing or new failure domains.

Note: The documentation generally assumes a homogeneous WEKA system deployment, meaning an equal number of servers and identical SSD capacities per server in each failure domain. For guidance on heterogeneous configurations, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).

## SSD net storage capacity calculation

The formula for calculating the SSD net storage capacity is:

**Examples**:

**Scenario 1**: A homogeneous system of 10 servers, each with 1 terabyte of raw SSD capacity (total 10TB raw capacity). The system is configured with 1 hot spare and a protection scheme of 6+2 (6 data blocks, 2 parity blocks).

**Scenario 2**: A homogeneous system of 20 servers, each with 1 terabyte of raw SSD capacity (total 20TB raw capacity). The system is configured with 2 hot spares and a protection scheme of 16+2 (16 data blocks, 2 parity blocks).

## Performance and resilience during failures

### **Data rebuilds**

When a component like a drive or server fails, WEKA initiates a rebuild process to reconstruct the missing data. Rebuild operations are primarily **read-intensive**, as the system reads data from the remaining drives in the affected stripes to reconstruct the lost information.

While read performance may experience a slight degradation during this process, **write performance generally remains unaffected**, as the system can continue writing to the available backend servers.

A critical optimization in WEKA's rebuild process is its behavior when a failed component comes back online. If a failed drive or server returns to an operational state _after_ a rebuild has commenced, the rebuild process is **automatically aborted**.

This intelligent approach prevents unnecessary data movement and allows the system to quickly restore normal operations, especially in cases of transient failures (for example, servers returning from a brief maintenance window). This significantly differentiates WEKA from traditional systems that often continue lengthy rebuild processes even after the underlying fault has been resolved.

### **Resilience to serial failures**

Beyond the configured simultaneous failure protection (N+2, N+3, or N+4), a WEKA cluster also exhibits resilience to **serial failures** of additional failure domains. This means that as long as each data rebuild operation completes successfully and there is sufficient free NVMe capacity in the cluster, the system can tolerate subsequent individual server failures.

For example, consider a cluster of 20 servers with a stripe width of 18 (16+2). After successfully rebuilding from a simultaneous failure of 2 servers, the cluster is still resilient to two additional simultaneous server failures. If further serial server failures occur, the system attempts to rebuild its data stripes using the remaining healthy servers to maintain the required stripe width (for example, 18).

This process can continue, subject to sufficient NVMe space, until a lower limit of healthy servers is reached (for example, 9 servers in the 16+2 example). Failures beyond this critical threshold will result in the filesystem going offline.

In the event of serial server failures coupled with insufficient NVMe capacity to complete rebuilds, the cluster attempts to **tier data** that currently resides on NVMe out to its configured object stores. This is called **backpressure mode** where the tiering does not consider the age of the data (unlike normal, orderly tiering) but instead tiers data in an approximately random fashion. This process prioritizes data integrity by offloading data to an object store when NVMe available capacity is critically low.

### Minimum required healthy servers

The stripe data width and the protection level (number of data and parity blocks) determine the minimum number of healthy servers required for the cluster to remain operational. This can be represented by the following formula:

**Examples:**

 | Stripe data width + protection level | Minimum required healthy servers |
 | ------------------------------------ | -------------------------------- |
 | 5+2 | 4 |
 | 16+2 | 9 |
 | 5+4 | 3 |
 | 16+4 | 5 |

### Failure domain folding

In scenarios where hardware failures persist and components are not replaced promptly, WEKA employs a process called **failure domain folding** to maintain write availability. This process temporarily relaxes the standard requirement that each RAID stripe must span only distinct failure domains (for example, one block per backend storage server within a stripe).

By allowing a single failure domain to effectively appear multiple times within a newly allocated stripe, the system can continue to allocate new stripes and accept write operations even when in a degraded state.

Failure domain folding is automatically triggered when the number of active (healthy) failure domains becomes insufficient to satisfy the original stripe width requirement. This typically occurs due to server deactivation or the loss of multiple drives. This adaptive approach ensures that the system can remain operational and continue to accept writes during extended fault conditions, without necessitating immediate hardware replacement.

The process can be visualized in three stages:

1. **Stage A: Normal operation (all drives active)**: In the initial state, all backend storage servers are operational. Each server is treated as a distinct failure domain. RAID stripes, consisting of data (yellow blocks) and parity (purple blocks), span horizontally across all available failure domains. New stripe allocations proceed normally, utilizing free space across all failure domains. The system is configured with hot spare capacity (equivalent to two full servers) allocated across the system.
2. **Stage B: write blocking after a drive failure**: When a single drive fails, any new stripe that must span all failure domains can no longer be allocated if any one domain lacks sufficient space due to the failure. Even though only one drive has failed, this strict allocation requirement can effectively block new writes. This results in a disproportionate loss of writable capacity relative to the actual size of the failure, particularly noticeable in systems with fewer drives per server.
3. **Stage C: Write recovery through failure domain folding**: To mitigate the blocked write condition, the affected storage server (failure domain) can be manually deactivated. This allows WEKA to apply failure domain folding. The system relaxes the one-domain-per-stripe rule, permitting the reuse of the same failure domain within a newly allocated stripe. This mechanism restores write capability without requiring immediate hardware replacement, ensuring continued system operation under degraded conditions.

<!-- ============================================ -->
<!-- File 73/259: weka-system-overview_data-storage.md -->
<!-- ============================================ -->

---
description:
---

# Data lifecycle management

## Media options for data storage in the WEKA system

In the WEKA system, data can be stored on two forms of media:

* On locally-attached SSDs, which are an integral and required part of the WEKA system configuration.
* On optional object-store systems external to the WEKA system. Object stores are provided either as cloud services or as part of an on-premises installation using a number of third-party solutions.

The WEKA system can be configured as an SSD-only or data management system consisting of SSDs and object stores. By nature, SSDs provide high performance and low latency storage, while object stores compromise performance and latency but are the most cost-effective solution available for storage.

Consequently, users focused on high performance only must consider using an SSD-only WEKA system configuration, while users seeking to balance performance and cost must consider a tiered data management system, with the assurance that the WEKA system features control the allocation of hot data on SSDs and warm data on object stores, thereby optimizing the overall user experience and budget.

Note: In SSD-only configurations, the WEKA system sometimes uses an external object store for backup. For more details, see .

## Guidelines for data storage in tiered WEKA system configurations

In tiered WEKA system configurations, there are various locations for data storage as follows:

* Metadata is stored only on SSDs.
* Writing new files, adding data to existing files, or modifying the content of files is performed on the SSDs, irrespective of whether the file is stored on the SSD or tiered to an object store.
* When reading the content of a file, data can be accessed from either the SSD (if it is available on the SSD) or promoted from the object store (if it is not available on the SSD).

This data management approach to data storage on one of two possible media requires system planning to ensure that the most commonly used data (hot data) resides on the SSD to ensure high performance. In contrast, less-used data (warm data) is stored on the object store.

In the WEKA system, this determination of the data storage media is an entirely seamless, automatic, and transparent process, with users and applications unaware of the transfer of data from SSDs to object stores or from object stores to SSDs.

The data is always accessible through the same strongly-consistent POSIX filesystem API, irrespective of where it is stored. The actual storage media affects only latency, throughput, and IOPS.

Furthermore, the WEKA system tiers data into chunks rather than complete files. This enables the intelligent tiering of subsets of a file (and not only complete files) between SSDs and object stores.

The network resources allocated to the object store connections can be controlled. This enables cost control when using cloud-based object storage services since the cost of data stored in the cloud depends on the quantity stored and the number of requests for access made.

## States in the WEKA system data management storage process

Data management represents the media being used for the storage of data. In tiered WEKA system configurations, data can exist in one of three possible states:

* **SSD-only:** When data is created, it exists only on the SSDs.
* **SSD-cached:** A tiered copy of the data exists on both the SSD and the object store.
* **Object store only:** Data resides only on the object store.

Note: These states represent the lifecycle of data and not the lifecycle of a file. When a file is modified, each modification creates a separate data lifecycle for the modified data.

The data lifecycle flow diagram delineates the progression of data through various stages:

1. **Tiering**: This process involves data migration from the SSD to the object store, creating a duplicate copy. The criteria for this transition are governed by a user-specified, temporal policy known as the [Tiering Cue](../../weka-filesystems-and-object-stores/tiering/advanced-time-based-policies-for-data-storage-location#tiering-cue-policy).
2. **Releasing**: This stage entails removing data from the SSD and retaining only the copy in the object store. The need for additional SSD storage space typically triggers this action. The guidelines for this data release are dictated by a user-defined time-based policy referred to as the [Retention Period](../../weka-filesystems-and-object-stores/tiering/advanced-time-based-policies-for-data-storage-location#data-retention-period-policy).
3. **Promoting**: This final stage involves transferring data from the object store to the SSD to facilitate data access.

When accessing data which is solely on the object store, the data must first be promoted back to the SSD.

Within the WEKA system, file modifications are not executed as in-place writes. Instead, they are written to a new area on the SSD, and the corresponding metadata is updated accordingly. As a result, write operations are never linked with operations on the object store. This approach ensures data integrity and efficient use of storage resources.

## The role of SSDs in tiered configurations

All writing in the WEKA system is performed on SSDs. The data residing on SSDs is hot (meaning it is currently in use). In tiered WEKA configurations, SSDs have three primary roles in accelerating performance: metadata processing, a staging area for writing, and a cache for reading performance.

### Metadata processing

Since filesystem metadata is, by nature, a large number of update operations, each with a small number of bytes, the embedding of metadata on SSDs accelerates file operations in the WEKA system.

### SSD as a staging area

Direct writing to an object store involves high latency. To mitigate this, the WEKA system avoids direct writes to object stores. Instead, all data is initially written to SSDs, which offer low latency and high performance. In this setup, SSDs act as a staging area, temporarily holding data until it is later tiered to the object store. Once the writing process is complete, the WEKA system manages the tiering of data to the object store and subsequently frees up space on the SSD.

### SSD as a cache

Recently accessed or modified data is stored on SSDs, and most read operations are of such data and served from SSDs. This is based on a single, significant LRU clearing policy for the cache that ensures optimal read performance.

In a tiered filesystem, the total capacity refers to the maximum amount of data that can be stored. However, the way data is managed across different storage tiers (such as SSDs and object storage) can affect how this capacity is used.

For example, consider a filesystem with a total capacity of 100 TB, where 10 TB is allocated to SSD storage. In this scenario, it‚Äôs possible that all data resides in the object store, especially if SSD space is prioritized for metadata and caching. This situation could arise due to policies that manage data placement over time or based on SSD usage patterns. As a result, even though the SSD space isn't fully used for data storage, it remains reserved for essential functions like metadata management and caching. New data writes may be restricted until either some files are deleted or the filesystem‚Äôs total capacity is increased.

## Time-based policies for the control of data storage location

The WEKA system includes user-defined policies that serve as guidelines to control data storage management. They are derived from several factors:

* The rate at which data is written to the system and the quantity of data.
* The capacity of the SSDs configured to the WEKA system.
* The network speed between the WEKA system and the object store and its performance capabilities, e.g., how much the object store can contain.

Filesystem groups are used to define these policies, while a filesystem is placed in a filesystem group according to the desired policy if the filesystem is tiered.

For tiered filesystems, define the following parameters per filesystem:

* The size of the filesystem.
* The amount of filesystem data to be stored on the SSD.

Define the following parameters per filesystem group:

* The [Drive Retention Period Policy](../../weka-filesystems-and-object-stores/tiering/advanced-time-based-policies-for-data-storage-location#drive-retention-period-policy) is a time-based policy which is the target time for data to be stored on an SSD after creation, modification, or access, and before release from the SSD, even if it is already tiered to the object store, for metadata processing and SSD caching purposes (this is only a target; the actual release schedule depends on the amount of available space).
* The [Tiering Cue Policy](../../weka-filesystems-and-object-stores/tiering/advanced-time-based-policies-for-data-storage-location#tiering-cue-policy) is a time-based policy that determines the minimum time that data remains on an SSD before it is considered for release to the object store. As a rule of thumb, this must be configured to a third of the Retention Period, and in most cases, this works well. The Tiering Cue is important because it is pointless to tier a file about to be modified or deleted from the object store.

Note: **Example**
_When writing log files that are processed every month but retained forever,_ it is recommended to define a Retention Period of one month, a Tiering Cue of one day, and ensure sufficient SSD capacity to hold one month of log files.
_When storing genomic data, which is frequently accessed during the first three months after creation, requires a scratch space for six hours of processing, and requires output to be retained forever,_ it is recommended to define a Retention Period of three months and to allocate an SSD capacity that is sufficient for three months of output data and the scratch space. The Tiering Cue must be defined as one day to avoid a situation where the scratch space data is tiered to an object store and released from the SSD immediately afterward.

Note: Using the [Snap-To-Object](../weka-filesystems-and-object-stores/snap-to-obj) feature causes data to be tiered regardless of the tiering policies.

### Bypassing the time-based policies

Even when time-based policies are in place, you can override them using a unique mount option called `obs_direct`. When this option is used, any files created or written from the associated mount point are prioritized for release immediately without first considering other file retention policies.

For a more in-depth explanation, refer to [Advanced Data Lifecycle Management](../weka-filesystems-and-object-stores/tiering/advanced-time-based-policies-for-data-storage-location).

<!-- ============================================ -->
<!-- File 74/259: weka-system-overview_filesystems.md -->
<!-- ============================================ -->

---
description:
---

# Filesystems, object stores, and filesystem groups

## **Introduction to WEKA** filesystems

A WEKA filesystem operates much like a conventional on-disk filesystem but distributes the data across all servers in the cluster. Unlike traditional filesystems, it is not tied to any specific physical object within the WEKA system and serves as a root directory with space limitations.

The WEKA system supports up to 1024 filesystems, distributing them equally across all SSDs and CPU cores assigned to the cluster. This ensures that tasks like allocating new filesystems or resizing existing ones are immediate, without operational constraints.

Each filesystem is linked to a predefined filesystem group, each with a specified capacity limit. For those belonging to tiered filesystem groups, additional constraints, including a total capacity limit and an SSD capacity cap, apply.

The available SSD capacity of individual filesystems cannot exceed the total SSD net capacity allocated to all filesystems. This structured approach ensures effective resource management and optimal performance within the WEKA system.

### Thin provisioning **in WEKA filesystems**

Thin provisioning, a dynamic SSD capacity allocation method, addresses user needs on demand. In this approach, the filesystem's capacity is defined by a minimum guaranteed capacity and a maximum capacity, which can virtually exceed the available SSD capacity.

The system optimally allocates more capacity, up to the total available SSD capacity, for users who use their allocated minimum capacity. Conversely, as users free up space by deleting files or transferring data, the idle space undergoes reclamation, repurposing it for other workloads that require SSD capacity.

Thin provisioning proves beneficial in diverse scenarios:

* **Tiered filesystems:** On tiered filesystems, available SSD capacity is used for enhanced performance and can be released to the object store when needed by other filesystems.
* **Auto-scaling groups:** Thin provisioning facilitates automatic expansion and reduction (shrinking) of the filesystem's SSD capacity when using auto-scaling groups, ensuring optimal performance.
* **Filesystems separation per project:** Thin provisioning makes creating separate filesystems for each project efficient, especially when administrators don't anticipate full simultaneous usage of all filesystems. Each filesystem is allocated a minimum capacity but can consume more based on the actual available SSD capacity, offering flexibility and resource optimization.

### WEKA filesystem limits

* **Number of filesystems:** up to 1024
* **Number of files or directories:** Up to 6.4 trillion (6.4 * 10^12)
* **Number of files in a single directory:** Up to 6.4 billion (6.4 * 10^9)
* **Total capacity with object store:** Up to 14 EB
* **Total SSD capacity:** Up to 512 PB
* **File size:** Up to 4 PB

### Data reduction **in WEKA filesystems**

Data reduction is a cluster-wide feature that you can activate for individual filesystems. This feature uses block-variable differential compression and advanced deduplication techniques across all enabled filesystems. The result is a significant reduction in the storage capacity required for user data, which can lead to substantial cost savings. The capacity savings from a data reduction-enabled filesystem are returned to the cluster, not to the filesystem itself.

The effectiveness of the data reduction ratio depends on the workload. It is particularly effective for text-based data, large-scale unstructured datasets, log analysis, databases, code repositories, and sensor data.

Data reduction applies only to user data, not metadata, on a per-filesystem basis.

For example, the image below shows a cluster with a total physical SSD capacity of 979.2 TB. Thanks to data reduction, the cluster achieves a Data Reduction Ratio of 2.78:1, which results in 574.6 TB of saved capacity. This saving allows the cluster to have 1.6 PB of provisioned space, which is 159% of the actual physical capacity.

#### Prerequisites

To enable data reduction on a filesystem, the following conditions must be met:

* The filesystem is thin-provisioned.
* The filesystem is non-tiered.
* The filesystem is not encrypted.
* The cluster has a valid Data Efficiency Option (DEO) license.

#### How data reduction operates

Data reduction is a post-process, background activity with a lower priority than user I/O requests. When new data is written to a filesystem with data reduction enabled, it is initially stored uncompressed. The reduction process begins automatically as a background task once a sufficient amount of data accumulates.

The data reduction process during write involves the following tasks:

1. **Fingerprinting and ingestion:** As data is written, the system performs fingerprinting by calculating similarity hashes. A background ingest task then uses these hashes to find similar data blocks across all filesystems that have data reduction enabled. This technique, known as clusterization, operates at the 4K block level to identify similarities.
2. **Compression:** The system reads the similar and unique data blocks and compresses them. The newly compressed data is then written back to the filesystem.
3. **Defragmentation:** After data is successfully compressed, the original, uncompressed blocks are marked for deletion. A defragmentation process waits for a sufficient number of these blocks to be invalidated and then permanently deletes them, freeing up SSD capacity.

The data reduction process during read involves decompressio&#x6E;**.** When a client reads compressed data, the system performs decompression inline as part of the read operation. This decompression is handled by the drive containers.

#### Performance monitoring

You can monitor the performance and impact of data reduction using a dedicated Grafana dashboard. This dashboard provides insights into the resources being used and the efficiency of the reduction process.

Key monitoring panels include:

* **CPU Time % in background fibers:** Shows the percentage of CPU capacity used by background tasks, including data reduction.
* **Data Reduction Ingest Rate:** Tracks the rate (in blocks) at which data is being ingested for reduction.
* **Fingerprints - Performed FP Calcs:** Displays the rate of fingerprint calculations being performed per second.
* **Fingerprints - Skipped FP Calcs:** Shows the rate of fingerprint calculations that were skipped. An increase in skipped calculations can indicate a high system load.

### Encrypted filesystems in WEKA

WEKA ensures security by offering encryption for data at rest (residing on SSD and object store) and data in transit. This security feature is activated by enabling the filesystem encryption option. The decision on whether a filesystem should be encrypted is crucial during the filesystem creation process.

To create encrypted filesystems, deploying a Key Management System (KMS) is imperative, reinforcing the protection of sensitive data.

Note: Data encryption settings can only be configured during the initial creation of a filesystem, emphasizing the importance of making this decision from the beginning.

**Related topics**

### Metadata limitations **in WEKA filesystems**

In addition to the capacity constraints, each filesystem in WEKA has specific limitations on metadata. The overall system-wide metadata cap depends on the SSD capacity allocated to the WEKA system and the RAM resources allocated to the WEKA system processes.

WEKA carefully tracks metadata units in RAM. If the metadata units approach the RAM limit, they are intelligently paged to the SSD, triggering alerts. This proactive measure allows administrators sufficient time to increase system resources while sustaining IO operations with minimal performance impact.

By default, the metadata limit linked to a filesystem correlates with the filesystem's SSD size. However, users have the flexibility to override this default by defining a filesystem-specific `max-files` parameter. This logical limit empowers administrators to regulate filesystem usage, providing the flexibility to update it as needed.

The cumulative metadata memory requirements across all filesystems can surpass the server‚Äôs RAM capacity. In potential impact scenarios, the system optimizes by paging the least recently used units to disk, ensuring operational continuity with minimal disruption.

#### Metadata units calculation <a href="#metadata-calculations" id="metadata-calculations"></a>

Every metadata unit within the WEKA system demands 4 KB of SSD space (excluding tiered storage) and occupies 20 bytes of RAM.

Throughout this documentation, the restriction on metadata per filesystem is denoted as the `max-files` parameter. This parameter includes the files' count and respective sizes.

The following table outlines the requisite metadata units based on file size. These specifications apply to files stored on SSDs or tiered to object stores.

 | File size | Number of metadata units | Example |
 | --- | --- | --- |
 | < 0.5 MB | 1 | A filesystem containing 1 billion files, each sized at 64 KB, requires 1 billion metadata units. |
 | 0.5 MB - 1 MB | 2 | A filesystem containing 1 billion files, each sized at 750 KB, requires 2 billion metadata units. |
 | > 1 MB | 2 for the first 1 MB plus1 per MB for the rest MBs | A filesystem containing 1 million files, each sized at 129 MB, requires 130 million metadata units. This calculation includes 2 units for the first 1 MB and an additional unit per MB for the subsequent 128 MB.A filesystem containing 10 million files, each sized at 1.5 MB, requires 30 million metadata units.A filesystem containing 10 million files, each sized at 3 MB, requires 40 million metadata units. |

Note: Each directory requires two metadata units instead of one for a small file.

**Related topics**

#memory-resource-planning

### Filesystem Extended Attributes considerations

The maximum size for extended attributes (xattr) of a file or directory is 1024 bytes. This attribute space is used by Access Control Lists (ACLs) and Alternate Data Streams (ADS) within an SMB cluster and when configuring SELinux. When using Windows clients, named streams in smb-w are saved in the file‚Äôs xattr.

Given its finite capacity, exercise caution when using lengthy or complex ACLs and ADS on a WEKA filesystem.

When encountering a message indicating the file size exceeds the limit allowed and cannot be saved, carefully decide which data to retain. Strategic planning and selective use of ACLs and ADS contribute to optimizing performance and stability.

## **Introduction to** object stores

Within the WEKA system, object stores are an optional external storage medium strategically designed to store warm data. These object stores, employed in tiered WEKA system configurations, can be cloud-based, located in the same location as the WEKA cluster, or at a remote location.

WEKA extends support for object stores, leveraging their capabilities for tiering (both tiering and local snapshots) and backup (snapshots only). Both tiering and backup functionalities can be concurrently used for the same filesystem, enhancing flexibility.

The optimal usage of object store buckets comes into play when a cost-effective data storage tier is imperative and traditional server-based SSDs prove insufficient in meeting the required price point.

An object store bucket definition comprises crucial components: the object store DNS name, bucket identifier, and access credentials. The bucket must remain dedicated to the WEKA system, ensuring exclusivity and security by prohibiting access from other applications.

Moreover, the connectivity between filesystems and object store buckets extends beyond essential storage. This connection proves invaluable in data lifecycle management and facilitates the innovative Snap-to-Object features, offering a holistic approach to efficient data handling within the WEKA system.

**Related topics**

## **Introduction to f**ilesystem groups

Within the WEKA system, the organization of filesystems takes place through the creation of filesystem groups, with a maximum limit set at eight groups.

Each of these filesystem groups comes equipped with tiering control parameters. When filesystems are tiered and have associated object stores, the tiering policy remains consistent for all tiered filesystems residing within the same filesystem group. This unification ensures streamlined management and unified control over tiering strategies within the WEKA system.

**Related topics**

<!-- ============================================ -->
<!-- File 75/259: weka-system-overview_glossary.md -->
<!-- ============================================ -->

# Glossary

## A

## **Access Time (atime)**

Access time, often called "atime," is a file system metadata attribute that tracks the most recent instance when a file was accessed or read. This attribute is essential for monitoring and managing file usage, as it records when a file was last opened or viewed by a user or an application.

In the WEKA filesystem, the atime is updated locally on the container where the read operation took place, and this update is subsequently propagated to the cluster after the user closes the file. This update process doesn't occur immediately and may take up to 60 minutes to reflect the actual access time.

POSIX mount options that affect atime behavior, such as `relatime`, are supported. However, this updated atime still takes time to propagate, even if mounted with `strictatime`.

Directory atimes are currently not supported, therefore, listing a directory's contents does not update its atime.

### Agent

The WEKA agent is software installed on user application servers that need access to the WEKA file services. When using the Stateless Client feature, the agent ensures that the correct client software version is installed (depending on the cluster version) and that the client connects to the correct cluster.

## B

### Backend server

A backend server in the context of WEKA is a server equipped with SSD drives and running the WEKA software. These servers are dedicated to the WEKA system, offering services to clients. A storage cluster is formed by a group of such backend servers, collectively providing storage and processing capabilities within the WEKA infrastructure.

## C

### Client

The WEKA client is software installed on user application servers that need access to WEKA file services. The WEKA client implements a kernel-based filesystem driver and the logic and networking stack to connect to the WEKA backend servers and be part of a cluster. In general industry terms, "client" may also refer to an NFS, SMB, or S3 client that uses those protocols to access the WEKA filesystem. For NFS, SMB, and S3, the WEKA client is not required to be installed in conjunction with those protocols.

### Cluster

A collection of WEKA backend servers, together with WEKA clients installed on the application servers, forming one shareable, distributed, and scalable file storage system.

### Container

WEKA uses Linux containers (LXC) as the mechanism for holding one process or keeping multiple processes together. Containers can have different processes within them. They can have frontend processes and associated DPDK libraries within the container, compute processes, drive processes, a management process, and DPDK libraries, or NFS, SMB, or S3 services running within them. A server can have multiple containers running on it at any time.

### Converged deployment

A WEKA configuration in which WEKA backend containers run on the same server with applications.

## D

### Data Retention Period

The target period of time for tiered data to be retained on an SSD.

### Data Stripe Width

The number of data blocks in each logical data protection group.

### Dedicated Deployment

A WEKA configuration that dedicates complete servers and all of their allocated resources to WEKA backends, as opposed to a converged deployment.

## F

### Failure Domain

A collection of hardware components that can fail together due to a single root cause.

### Filesystem Group

A collection of filesystems that share a common tiering policy to object-store.

### Frontend

It is the collection of WEKA software that runs on a client and accesses storage services and IO from the WEKA storage cluster. The frontend consists of a process that delivers IO to the WEKA driver, a DPDK library, and the WEKA POSIX driver.

## H

### Host

The term "host" is deprecated. See #container.

### Hot Data

Frequently used data (as opposed to warm data), usually residing on SSDs.

## L

### Leader

In distributed systems, a leader is a process that assumes a special role, often responsible for coordination, synchronization, and making decisions on behalf of the cluster. The leader plays a crucial role in maintaining consistency and order among the distributed processes or nodes in the system. If the leader fails or is replaced, a new leader is typically elected to ensure the continued operation of the distributed system.

Within the context of WEKA, at the cluster's core resides the cluster leader, serving as the singular WEKA management process within the cluster. This unique role grants the cluster leader the exclusive capability to initiate and disseminate configuration changes throughout the entire cluster.

## M

### Machine

The term "machine" is deprecated. See #server.

## N

### Net Capacity

Amount of space available for user data on SSDs in a configured WEKA system.

### Node

The term "node" is deprecated. See #process.

## O

### **OBS**

Object Storage. WEKA uses object storage buckets to extend the WEKA filesystem and to store uploaded file system snapshots.

## P

### **POSIX**

POSIX (Portable Operating System Interface) is a set of standards established by the IEEE Computer Society to ensure compatibility across diverse operating systems. The WEKA client adheres to the POSIX specifications, ensuring that it interacts with the underlying operating system following the defined POSIX standard. This compliance ensures seamless interoperability and consistent behavior, making the WEKA client often referred to as the POSIX client or POSIX driver when discussing the broader storage system architecture.

### Process

A software instance that WEKA uses to run and manage the filesystem. Processes are dedicated to managing different functions such as (1) NVMe Drives and IO to the drives, (2) compute processes for filesystems and cluster-level functions and IO from clients, (3) frontend processes for POSIX client access and sending IO to the compute process and (4) management processes for managing the overall cluster.

### Provisioned Capacity

The total capacity that is assigned to filesystems. This includes both SSD and object store capacity.

### Prefetch

Prefetch in WEKA involves proactively promoting data from an object store to an SSD based on predictions of future data access. This process anticipates and preloads data onto faster storage, optimizing performance by ensuring that relevant information is readily available when needed.

### Promoting

Promoting refers to the action of moving data from a lower-tier storage, typically an object store, to a more accessible storage medium, such as an SSD, when the data is required for active use. This process aims to enhance performance by ensuring that frequently accessed or critical data is readily available on a faster storage tier.

## R

### Raw Capacity

Total SSD capacity owned by the user.

### Rehydrating

See #promoting.

### Retention Period

The designated time duration for data to be stored on SSDs before releasing from the SSDs to an object store.

### Releasing

Releasing, in the context of data tiering, refers to deleting the SSD copy of data that has been migrated to the object store.

## S

### Server

A physical or virtual server that has hardware resources allocated to it and software running on it that provides compute or storage services. WEKA uses backend servers in conjunction with clients to deliver storage services. In general industry terms, in a cluster of servers, sometimes the term node is used instead.

### SR-IOV

SR-IOV (Single Root I/O Virtualization) is a technology that enables a single physical resource to be leveraged as multiple virtual resources. In essence, SR-IOV facilitates the partitioning of a single hardware component into distinct virtual functions, each operating independently. Correspondingly, the term Virtual Function (VF) aligns with SR-IOV, referring to these individualized virtualized entities. This technology is particularly valuable in optimizing resource utilization and enhancing the efficiency of virtualized environments.

### Stem Mode

Stem Mode in WEKA refers to the installed and running software that has not yet been attached to a cluster.

### Snap-To-Object

Snap-To-Object is a WEKA feature facilitating the uploading of snapshots to object stores.

## T

### Tiered WEKA Configuration

A tiered WEKA configuration combines SSDs and object stores for data storage.

### Tiering

Tiering is the dynamic process of copying data from an SSD to an object store while retaining the original copy on the SSD. This optimization strategy balances performance and cost considerations by keeping frequently accessed data on the high-performance SSD and moving less accessed data to a more economical object store.

### Tiering Cue

Tiering Cue refers to the minimum duration that must elapse before considering data migration from an SSD to an object store. This time threshold is crucial in the context of data tiering strategies, where the decision to move data between different storage tiers is based on factors such as access frequency, performance requirements, and cost considerations. The Tiering Cue helps establish a timeframe for evaluating whether data should be transitioned from the faster but potentially more expensive SSD storage to the object store, which may offer more cost-effective, albeit slower, storage.

## U

### Unprovisioned Capacity

Unprovisioned capacity refers to the storage space that is currently unused and available for the creation of new filesystems or data storage allocations. This term indicates the portion of storage resources that have not been assigned or allocated to any specific purpose, making it ready and waiting to be provisioned for new file systems or data storage needs.

## V

### VF

Virtual Function (VF) in the context of WEKA typically denotes the creation of multiple virtual instances of a physical network adapter. This involves leveraging SR-IOV (Single Root I/O Virtualization) technology, where a single physical resource can be partitioned into distinct virtual functions, each capable of independent operation. In essence, both Virtual Function and SR-IOV are terms integral to WEKA's approach to optimizing resource allocation and enhancing the efficiency of virtualized network environments by enabling the creation of multiple independent virtual instances from a single physical network adapter.

## W

### Warm Data

Warm data is less frequently accessed or utilized data, unlike hot data, and is typically stored in an object store. This term is used to describe information that is accessed less regularly but remains relevant for specific use cases. Storing warm data on an object store allows for efficient management of data resources, providing a balance between accessibility and storage costs.

<!-- ============================================ -->
<!-- File 76/259: weka-filesystems-and-object-stores.md -->
<!-- ============================================ -->

# WEKA Filesystems & Object Stores

## Topics in this section

### Manage object stores

This page provides an overview about managing object stores.

### Manage filesystem groups

This page provides an overview about managing filesystem groups.

### Manage filesystems

Filesystem management is an integral part of the successful running and performance of the WEKA system and of overall data lifecycle management.

### Attach or detach object store buckets

This page describes how to attach or detach object stores buckets to or from filesystems.

### Advanced data lifecycle management

This page provides a detailed description of how data storage is managed in SSD-only and tiered WEKA system configurations.

### Mount filesystems

Discover the two modes for mounting a filesystem on a cluster server: persistent mount mode (stateful) and stateless mount mode. You can also use fstab or autofs for mounting.

### Snapshots

Snapshots enable the saving of a filesystem state to a directory and can be used for backup, archiving and testing purposes.

### Snap-To-Object

Explore the Snap-To-Object feature, a capability facilitating the seamless data transfer from a designated snapshot to an object store.

### Snapshot policies

Snapshot policies define rules and schedules for creating and managing point-in-time data copies, ensuring reliable recovery from deletion, corruption, or integrity issues.

### Quota management

Implement quota management to monitor and control usage of the WEKA filesystem effectively.

<!-- ============================================ -->
<!-- File 77/259: weka-filesystems-and-object-stores_managing-filesystems.md -->
<!-- ============================================ -->

---
description:
---

# Manage filesystems

Managing filesystems in the WEKA system involves overseeing entities that function like conventional on-disk filesystems but distribute data across all servers in the cluster. These filesystems are not tied to specific physical objects and act as root directories with space limitations.

The WEKA system can support up to 1024 filesystems. Effective management ensures optimal resource allocation and performance, including tasks such as creating new filesystems, resizing existing ones, and configuring features like thin provisioning and data reduction.

**Related topics**

 (detailed overview)

<!-- ============================================ -->
<!-- File 78/259: weka-filesystems-and-object-stores_managing-filesystems_managing-filesystems.md -->
<!-- ============================================ -->

---
description: This page describes how to view and manage filesystems using the GUI.
---

# Manage filesystems using the GUI

Using the GUI, you can perform the following actions:

* View filesystems
* Create a filesystem
* Edit a filesystem
* Delete a filesystem

## View filesystems

The filesystems are displayed on the **Filesystems** page. Each filesystem indicates the status, tiering, remote backup, encryption, SDD capacity, total capacity, filesystem group, and data reduction details.

**Before you begin**

Ensure a filesystem group is set with the required tiering policy. See #add-a-filesystem-group.

**Procedure**

1. From the menu, select **Manage > Filesystems**.

## Create a filesystem

When deploying a WEKA system on-premises, no filesystem is initially provided. You must create the filesystem and configure its properties, including capacity, group, tiering, thin provisioning, encryption, and required authentication during mounting.

When deploying a WEKA system on a cloud platform (AWS, Azure, or GCP) using Terraform or AWS CloudFormation, the WEKA system includes a default filesystem configured to maximum capacity. If your deployment necessitates additional filesystems with varied settings, reduce the provisioned capacity of the default filesystem and create a new filesystem with the desired properties to meet your specific requirements.

**Before you begin**

* Verify that the system has free capacity.
* Verify that a filesystem group is already set.
* If tiering is required, verify that an object store bucket is set.
* If audit logging required, verify that the Audit and Forwarding feature is enabled and configured.
* If encryption is required, verify that a KMS is configured.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. Select the **+Create** button.

3. In the **Create Filesystem** dialog, set the following:
   * **Name**: Enter a descriptive label for the filesystem, limited to 32 characters and excluding slash (`/`)  or backslash (`\`).
   * **Group**: Select the filesystem group that fits your filesystem.
   * **Capacity**: Enter the storage size to provision, or select **Use All** to provision all the free capacity.

4.  Optional: [**Tiering**](../../tiering/advanced-time-based-policies-for-data-storage-location#tiering-cue-policy).\
    If tiering is required, an object store bucket is already defined, and data reduction is not enabled, select the toggle button and set the details of the object store bucket:

    * **Object Store Bucket:** Select a predefined object store bucket from the list.
    * **Drive Capacity**: Enter the capacity to provision on the SSD, or select **Use All** to use all free capacity.
    * **Total Capacity**: Enter the total capacity of the object store bucket, including the drive capacity.

    When you set tiering, you can create the filesystem from an uploaded snapshot. See the related topics below.

5. Optional: **Thin Provision**.\
   If Thin Provision is required, select the toggle button, and set the minimum (guaranteed) and the maximum capacity for the thin provisioned filesystem.\
   The minimum capacity must be less or equal to the available SSD capacity.\
   You can set any maximum capacity, but the available capacity depends on the actual free space of the SSD capacity.\
   Thin provisioning is mandatory when enabling data reduction.

6. Optional: **Data Reduction**.\
   Data reduction can be enabled only on thin provision, non-tiered, and unencrypted filesystems on a cluster with a valid data reduction license (you can verify the data reduction license in the cluster settings). For more details, see the related topics below. \
   To enable the Data Reduction, select the toggle button.

7. Optional: If **Audit Logging** is required for this filesystem, select the toggle button. When on, the WEKA system Forwards this filesystem's audit logs to a configured events monitoring platform, provided that cluster-wide auditing is also enabled.

Note: To use the **Audit Logging** option, ensure the **Audit and Forwarding** feature is enabled and configured. For more information, see .

8. Optional: If **Encryption** is required and your WEKA system is deployed with a KMS, select the toggle button.
9. Optional: **Required Authentication**.\
   When ON, user authentication is required when mounting to the filesystem. This option is only relevant to a filesystem created in the root organization.\
   Enabling authentication is not allowed for a filesystem hosting NFS client permissions or SMB shares.\
   To authenticate during mount, the user must run the `weka user login` command or use the `auth_token_path` parameter.
10. Select **Save**.

**Related topics**

#data-reduction-in-weka-filesystems

#create-a-filesystem-from-an-uploaded-snapshot

## Edit a filesystem

You can modify the filesystem parameters according to your demand changes over time. The parameters you can modify include filesystem name, capacity, tiering, thin provisioning, and required authentication (but not encryption).

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. Select the three dots on the right of the filesystem you want to modify, and select **Edit**.

3. In the **Edit Filesystem** dialog, modify the parameters according to your requirements. (See the parameter descriptions in the Add a filesystem topic.)

4. Select **Save**.

## Delete a filesystem

You can delete a filesystem if its data is no longer required. Deleting a filesystem does not delete the data in the tiered object store bucket.

Note: If you must also delete the data in the tiered object store bucket, see the [Delete a filesystem](../managing-filesystems-1#delete-a-filesystem) topic in the CLI section.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. Select the three dots on the right of the filesystem you want to delete, and select **Remove**.
3. To confirm the filesystem deletion, enter the filesystem name and select **Confirm**.

<!-- ============================================ -->
<!-- File 79/259: weka-filesystems-and-object-stores_managing-filesystems_managing-filesystems-1.md -->
<!-- ============================================ -->

---
description: This page describes how to view and manage filesystems using the CLI.
---

# Manage filesystems using the CLI

Using the CLI, you can perform the following actions:

* View filesystems
* Add a filesystem
* Add a filesystem when thin-provisioning is used
* Edit a filesystem
* Remove a filesystem
* Rewrap the filesystem encryption key

Note: Several parameters in this topic relate to Key Management System (KMS) configuration, which supports both per-filesystem encryption keys and cluster encryption keys. For more information about how KMS integration works and setup guidance, see .

## View filesystems

**Command:** `weka fs`

Use this command to view information on the filesystems in the WEKA system.

## Add a filesystem

**Command:** `weka fs add`

Use the following command line to create a filesystem:

`weka fs add <name> <group-name> <total-capacity> [--obs-name <obs-name>] [--ssd-capacity <ssd-capacity>] [--thin-provision-min-ssd <thin-provision-min-ssd>] [--thin-provision-max-ssd <thin-provision-max-ssd>] [--audit] [--kms-key-identifier kms-key-identifier]  [--kms-namespace kms-namespace] [--kms-role-id kms-role-id] [--kms-secret-id kms-secret-id] [--auth-required auth-required] [--encrypted] [--data-reduction]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | name* | A descriptive label for the filesystem, limited to 32 characters and excluding slash (/) or backslash (\). | ‚Äã |
 | group-name* | Name of the filesystem group to which the new filesystem is to be connected. |  |
 | total-capacity* | Total capacity of the new filesystem.Minimum value: 1GiB. |  |
 | obs-name | Object store name for tiering.Mandatory for tiered filesystems. |  |
 | ssd-capacity | For tiered filesystems, this is the SSD capacity. If not specified, the filesystem is pinned to SSD.To set a thin provisioned filesystem, the thin-provision-min-ssd attribute must be used instead. | SSD capacity is set to total capacity |
 | thin-provision-min-ssd | For thin-provisioned filesystems, this is the minimum SSD capacity that is ensured to be always available to this filesystem.Must be set when defining a thin-provisioned filesystem.Minimum value: 1GiB. |  |
 | thin-provision-max-ssd | For thin-provisioned filesystem, this is the maximum SSD capacity the filesystem can consume.The value cannot exceed the total-capacity. |  |
 | audit | Forwards this filesystem's audit logs to a configured events monitoring platform, provided that cluster-wide auditing is also enabled. |  |
 | kms-key-identifier | Customize KMS key identifier for this filesystem (only for HashiCorp Vault). |  |
 | kms-namespace | Customize KMS namespace for this filesystem (only for HashiCorp Vault). |  |
 | kms-role-id | Customize KMS role-id for this filesystem (only for HashiCorp Vault). |  |
 | kms-secret-id | Customize KMS secret-id for this filesystem (only for HashiCorp Vault). |  |
 | auth-required | Require the mounting user to be authenticated for mounting this filesystem. This flag is only effective in the root organization, users in non-root organizations must be authenticated to perform a mount operation. Format: yes or no.See User management. | No |
 | encrypted | Encryption of filesystem. | No |
 | data-reduction | Enable data reduction.The filesystem must be non-tired and thin-provisioned. A license with data reduction is required. | No |

Note: To create an encrypted filesystem, you must define a KMS.
If a KMS is unavailable for a POC, contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team) for guidance.

## Remove a filesystem when thin-provisioning is used

To create a new filesystem, the SSD space for the filesystem must be free and unprovisioned. When using thin-provisioned filesystems, that might not be the case. SSD space can be occupied for the thin-provisioned portion of other filesystems. Even if those are tiered, and data can be released (to object-store) or deleted, the SSD space can still get filled when data keeps being written or promoted from the object-store.

To add a new filesystem, in this case, use the `weka fs reserve` CLI command. Once enough space is cleared from the SSD (either by releasing to object-store or explicitly deleting data), it is possible to add the new filesystem using the reserved space.

## Edit a filesystem

**Command:** `weka fs update`

Use the following command line to edit an existing filesystem:

`weka fs update <name> [--new-name new-name] [--total-capacity total-capacity] [--ssd-capacity ssd-capacity] [--thin-provision-min-ssd thin-provision-min-ssd] [--thin-provision-max-ssd thin-provision-max-ssd] [--audit] [--data-reduction data-reduction] [--auth-required auth-required] [--kms-key-identifier kms-key-identifier] [--kms-namespace kms-namespace] [--kms-role-id kms-role-id] [--kms-secret-id kms-secret-id] [--use-cluster-kms-key-identifier]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Name of the filesystem to edit. |
 | new-name | New name for the filesystem. |
 | total-capacity | Total capacity of the edited filesystem. |
 | ssd-capacity | SSD capacity of the edited filesystem.Minimum value: 1GiB. |
 | thin-provision-min-ssd | For thin-provisioned filesystems, this is the minimum SSD capacity that is ensured to be always available to this filesystem.Minimum value: 1GiB. |
 | thin-provision-max-ssd | For thin-proviosined filesystem, this is the maximum SSD capacity the filesystem can consume.The value must not exceed the total-capacity. |
 | audit | Forwards this filesystem's audit logs to a configured events monitoring platform, provided that cluster-wide auditing is also enabled. |
 | data-reduction | Enable data reduction.The filesystem must be non-tired and thin-provisioned. A license with data reduction is required. |
 | auth-required | Determines if mounting the filesystem requires being authenticated to Weka (weka user login).Possible values: yes or no. |
 | kms-key-identifier | Customize KMS key identifier for this filesystem (only for HashiCorp Vault). |
 | kms-namespace | Customize KMS namespace for this filesystem (only for HashiCorp Vault). |
 | kms-role-id | Customize KMS role-id for this filesystem (only for HashiCorp Vault). |
 | kms-secret-id | Customize KMS secret-id for this filesystem (only for HashiCorp Vault). |
 | use-cluster-kms-key-identifier | Enable cluster KMS configuration for this filesystem, which removes any custom KMS settings previously applied to it. |

## Remove a filesystem

**Command:** `weka fs remove`

Use the following command line to remove a filesystem:

`weka fs remove <name> [--purge-from-obs]`

**Parameters**

 | Name | Value | Default |
 | ---------------- | --------------------------------------------------------------------------------------------- | ------- |
 | `name`* | Name of the filesystem to delete. |  |
 | `purge-from-obs` | For a tiered filesystem, if set, all filesystem data is deleted from the object store bucket. | False |

Note: Using `purge-from-obs` removes all data from the object-store. This includes any backup data or snapshots created from this filesystem (if this filesystem has been downloaded from a snapshot of a different filesystem, it will leave the original snapshot data intact).
* If any of the removed snapshots have been (or are) downloaded and used by a different filesystem, that filesystem will stop functioning correctly, data might be unavailable and errors might occur when accessing the data.
It is possible to either un-tier or migrate such a filesystem to a different object store bucket before deleting the snapshots it has downloaded.

## Rewrap the filesystem encryption key

**Command:** `weka fs kms-rewrap`

Rewrap operations can be performed per filesystem, enabling each key to be re-encrypted with a new version if there are concerns about key compromise. Use the following command to run this operation:

`weka fs kms-rewrap <name>`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name* | Filesystem name |

<!-- ============================================ -->
<!-- File 80/259: weka-filesystems-and-object-stores_managing-filesystem-groups.md -->
<!-- ============================================ -->

---
description: This page provides an overview about managing filesystem groups.
---

# Manage filesystem groups

A filesystem group in the WEKA system is used specifically to manage tiering policies for filesystems. It defines key parameters, including the drive retention period and the tiering queue time, which determine how and when data is tiered.

When you add a filesystem, it must be associated with a filesystem group to apply these tiering behaviors. The WEKA system supports up to eight filesystem groups, allowing flexibility in managing tiering policies across different filesystems.

**Related topics**

<!-- ============================================ -->
<!-- File 81/259: weka-filesystems-and-object-stores_managing-filesystem-groups_managing-filesystem-groups.md -->
<!-- ============================================ -->

---
description: This page describes how to view and manage filesystem groups using the GUI.
---

# Manage filesystem groups using the GUI

Using the GUI, you can perform the following actions:

* View filesystem groups
* Add filesystem groups
* Edit filesystem groups
* Delete a filesystem group

## View filesystem groups

The filesystem groups are displayed on the **Filesystems** page. Each filesystem group indicates the number of filesystems that use it.

**Procedure**

1. From the menu, select **Manage > Filesystems**.

## Add a filesystem group

A filesystem group is required when adding a filesystem. You can create more filesystem groups if you want to apply a different tiering policy on specific filesystems.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. Select the + sign right to the Filesystem Groups title.
3. In the **Create Filesystem Group** dialog, set the following:
   * **Name:** Set a meaningful name for the filesystem group.
   * **Drive Retention Period**: Set the period for keeping data on the SSD after it is copied to the object store. After this period, the copy of the data is deleted from the SSD.
   * **Tiering Cue**: Set the time to wait after the last update before the data is copied from the SSD and sent to the object store.

4. Select **Create**.

**Related topics**

To learn more about the drive retention period and tiering cue, se&#x65;**:**

## Edit a filesystem group

You can edit the filesystem group policy according to your system requirements.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. Select the filesystem group you want to edit.
3. Select the pencil sign right to the filesystem group name.
4. In the **Edit Filesystem Group** dialog, update the settings as you need. (See the parameter descriptions in the Add a filesystem group topic.)

5. Select **Update**.

## Delete a filesystem group

You can delete a filesystem group no longer used by any filesystem.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. Select the filesystem group you want to delete.
3. Verify that the filesystem group is not used by any filesystems (indicates 0 filesystems).

5. Select the **Remove** icon. In the pop-up message, select **Yes** to delete the filesystem group.

<!-- ============================================ -->
<!-- File 82/259: weka-filesystems-and-object-stores_managing-filesystem-groups_manage-filesystem-groups-using-the-cli.md -->
<!-- ============================================ -->

---
description: This page describes how to view and manage filesystem groups using the CLI.
---

# Manage filesystem groups using the CLI

Using the CLI, you can perform the following actions:

* View filesystem groups
* Add a filesystem group
* Edit a filesystem group
* Remove a filesystem group

## **View filesystem groups**

**Command:** `weka fs group`

Use this command to view information on the filesystem groups in the WEKA system.

## Add a filesystem group

**Command:** `weka fs group add`

Use the following command to add a filesystem group:

`weka fs group add <name> [--target-ssd-retention=<target-ssd-retention>] [--start-demote=<start-demote>]`

**Parameters**

 | Name | Value | Default |
 | ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- |
 | `name`* | Set a meaningful name for the filesystem group. | ‚Äã |
 | `target-ssd-retention` | The time for keeping data on the SSD after it is copied to the object store. After this period, the copy of the data is deleted from the SSD. Format: 3s, 2h, 4m, 1d, 1d5h, 1w. | 1d |
 | `start-demote` | The time to wait after the last update before the data is copied from the SSD and sent to the object store. Format: 3s, 2h, 4m, 1d, 1d5h, 1w. | 10s |

## Edit a filesystem group

**Command:** `weka fs group update`

Use the following command to edit a filesystem group:

`weka fs group update <name> [--new-name=<new-name>] [--target-ssd-retention=<target-ssd-retention>] [--start-demote=<start-demote>]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Name of the filesystem group to edit.It must be a valid name. |
 | new-name | New name for the filesystem group. |
 | target-ssd-retention | The time for keeping data on the SSD after it is copied to the object store. After this period, the copy of the data is deleted from the SSD.Format: 3s, 2h, 4m, 1d, 1d5h, 1w. |
 | start-demote | The time to wait after the last update before the data is copied from the SSD and sent to the object store.Format: 3s, 2h, 4m, 1d, 1d5h, 1w. |

## Remove a filesystem group

**Command:** `weka fs group remove`

Use the following command line to delete a filesystem group:

`weka fs group remove <name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Name of the filesystem group to delete |

**Related topics**

To learn about the tiring policy, see:

<!-- ============================================ -->
<!-- File 83/259: weka-filesystems-and-object-stores_mounting-filesystems.md -->
<!-- ============================================ -->

---
description:
---

# Mount filesystems

## Overview

There are two modes available for mounting a filesystem in a cluster server:

* **Persistent mount mode (stateful):** This mode involves configuring a client to join the cluster before running the mount command.
* **Stateless mount mode:** This mode simplifies and improves client management by eliminating the need for the Adding Clients process.

If you need to mount filesystems from multiple clusters on a single client, refer to the relevant topic for detailed instructions.

In addition, you can mount a filesystem using **fstab** or **autofs**.

**Related topics**

#mount-a-filesystem-using-the-persistent-mount-mode

#mounting-filesystems-using-stateless-clients

#mount-a-filesystem-using-fstab

#mount-a-filesystem-using-autofs

***

## Mount a filesystem using the persistent mount mode

To mount a WEKA filesystem persistently, follow these steps:

1. **Install the WEKA client**: Ensure the WEKA client is installed, configured, and connected to your WEKA cluster. See .
2. **Identify the filesystem**: Determine the name of the filesystem you want to mount. For this example, we use a filesystem named `demo`.
3.  **Create a mount point**: SSH into one of your cluster servers and create a directory to serve as the mount point for the filesystem:

    ```bash
    mkdir -p /mnt/weka/demo
    ```
4.  **Mount the filesystem**: As the root user, run the following command to mount the filesystem:

    ```bash
    mount -t wekafs demo /mnt/weka/demo
    ```

**General command structure**: The general syntax for mounting a WEKA filesystem is:

```bash
mount -t wekafs [-o option[,option]...] <fs-name> <mount-point>
```

Replace `<fs-name>` with the name of your filesystem and `<mount-point>` with the directory you created for mounting.

**Read and write cache modes:** When mounting a filesystem, you can choose between two cache modes: read cache and write cache. Each mode offers distinct advantages depending on your use case. For detailed descriptions of these modes, refer to the following links:

* [Read cache mount mode](../../weka-system-overview/weka-client-and-mount-modes#read-cache-mount-mode-default)
* [Write cache mount mode](../../weka-system-overview/weka-client-and-mount-modes#write-cache-mount-mode)

***

## Mount a filesystem using the stateless mount mode <a href="#mounting-filesystems-using-stateless-clients" id="mounting-filesystems-using-stateless-clients"></a>

The stateless mount mode simplifies client management by deferring the joining of the cluster until the mount operation is performed. This approach is particularly beneficial in environments like AWS, where clients frequently join and leave the cluster.

**Key benefits**

* **Simplified client management**: Eliminates the need for tedious client management procedures.
* **Unified security**: Consolidates all security aspects within the mount command, removing the need to manage separate credentials for cluster join and mount.

**Prerequisites**

* The stateless clients must have a connection to all the backends, protocol gateway servers and stateful clients.
* Ensure the WEKA agent is installed on your client to utilize the stateless mount mode. See .

**Mount a filesystem**

Once the WEKA agent is installed, you can create and configure mounts using the mount command. To mount a filesystem:

* **Create and configure mounts**: Use the `mount` command to create and configure the mounts. See #mount-command-options.
* **Unmounting**: Remove existing mounts from the cluster using the `unmount` command.

**Authentication**

To restrict mounting to only WEKA authenticated users, set the `--auth-required` flag to `yes` for the filesystem. For more information, refer to .

### **Set a stateless client with restricted operations on an Isolated port**

To restrict a stateless client's operations to only the essential APIs for mounting and unmounting, connect to WEKA clusters through TCP base port + 3 (for example, 14003). This configuration enables operational segregation between client and backend control plane requests.

### **Mount with restricted options**

When mounting with the restricted option, the logged-in user's privileges are set to regular user privileges, regardless of the user's role.

### Install the WEKA agent

To install a WEKA agent on a client, run one of the following commands as `root` on the client:

* For a non-restricted client:

```sh
| curl -k https://hostname:14000/dist/v1/install | sh |
```

* For a restricted client:

```bash
| curl -k https://hostname:14003/dist/v1/install | sh |
```

Note: The `-k` flag instructs the `curl` command to bypass SSL certificate verification.

After running the appropriate command, the agent is installed on the client.

### Run the mount command

**Command:** `mount -t wekafs`

#### Command syntax

Use one of the following command lines to invoke the mount command. The delimiter between the server and filesystem can be either `:/` or `/`:

```

```bash
mount -t wekafs -o <options> <backend0>[,<backend1>,...,<backendN>]/<fs> <mount-point>

mount -t wekafs -o <options> <backend0>[,<backend1>,...,<backendN>]:/<fs> <mount-point>
```

```

### **Example: Mount for a restricted stateless client on an isolated port**

```

```bash
mount -t wekafs -o restricted -o <options> <backend0>[,<backend1>,...,<backendN>]/<fs> <mount-point>
```

```

This setup ensures that the stateless client operates with restricted privileges, maintaining a secure and controlled environment for mounting and unmounting operations on an isolated port.

**Parameters**

 | Name | Value |
 | --- | --- |
 | options | See Additional Mount Options below. |
 | backend | IP/hostname of a backend container.Mandatory. |
 | fs | Filesystem name.Mandatory. |
 | mount-point | Path to mount on the local server.Mandatory. |

***

## Mount command options

Each mount option can be passed by an individual `-o` flag to `mount.`

### For all client types

 | Option | Description | Default | Remount Supported |
 | --- | --- | --- | --- |
 | readcache | Enables read-only cache mode for mounts. When enabled, data is read from cache, and writecache is automatically disabled.Note: SMB share mounts always use readcache mode; set this option to Yes for SMB shares.Values: Yes, No | No | Yes |
 | writecache | Enables write-to-cache mode for mounts, allowing data to be written to the cache.Values: Yes, No | Yes | Yes |
 | forcedirect | Enables direct I/O mode, bypassing cache for both read and write operations. Automatically disables writecache and readcache when enabled.Note: This may impact performance. Use with caution. If unsure, contact the Customer Success Team. It is not supported for SMB shares.Values: Yes, No | No | Yes |
 | dentry_max_age_positive | Maximum time in milliseconds to cache positive directory entries before refreshing metadata. This ensures the WEKA client detects metadata changes made by other clients.Values: Time in milliseconds | 1000 | Yes |
 | dentry_max_age_negative | Time in milliseconds to cache "file not found" results. When a file lookup fails, the system remembers this failure for the specified duration. After this time expires, the system will check again, allowing detection of files created by other clients.Values: Time in milliseconds | 0 | Yes |
 | ro | Mounts the filesystem in read-only mode, preventing write operations.Values: Yes, No | No | Yes |
 | rw | Mounts the filesystem in read-write mode, allowing both read and write operations.Values: Yes, No | Yes | Yes |
 | inode_bits | Sets the inode size in bits. May be required for compatibility with 32-bit applications.Values: 32, 64, auto | Auto | No |
 | verbose | Enables debug logging output to the console.Values: Yes, No | No | Yes |
 | quiet | Disables all log output to the console.Values: Yes, No | No | Yes |
 | acl | Enables POSIX ACL support for the mount. When ACLs are defined, they can modify effective group permissions through mask permissions.If ACLs are configured but not present on the mount, effective group permissions are granted.Values: Yes, No | No | No |
 | obs_direct | Bypasses time-based file retention policies, prioritizing the immediate release of files to the object store regardless of other policies. Data is still written to the SSD first, but released with precedence.For more details, see #direct-object-store-mount-optionValues: Yes, No | No | Yes |
 | noatime | Disables updating of inode access times on file reads, improving performance by reducing metadata writes.Values: Yes, No | No | Yes |
 | strictatime | Always updates inode access times on file access, ensuring accurate access time tracking.Values: Yes, No | No | Yes |
 | relatime | Updates inode access times only when files are modified, changed, or when accessed after relatime_threshold has elapsed.Values: Yes, No | Yes | Yes |
 | relatime_threshold | Time in seconds to wait since the last inode access before updating the access time again. Set to 0 to never update access time on read-only access.Only applies when relatime is enabled.Values: Time in seconds | 0 (infinite) | Yes |
 | nosuid | Ignores setuid and setgid bits on files, preventing privilege escalation through these mechanisms.Values: Yes, No | No | Yes |
 | nodev | Prevents interpretation of character and block device files, disabling device access through the mount.Values: Yes, No | No | Yes |
 | noexec | Prevents direct execution of binaries on the mounted filesystem.Values: Yes, No | No | Yes |
 | file_create_mask | File creation mask. A numeric (octal) notation of POSIX permissions.Newly created file permissions are masked with the creation mask. For example, if a user creates a file with permissions=777 but the file_create_mask is 770, the file is created with 770 permissions. First, the umask is taken into account, followed by the file_create_mask and then the force_file_mode. | 0777 | Yes |
 | directory_create_mask | Directory creation mask in octal notation. Newly created directories have their permissions masked with this value. For example, creating a directory with 777 permissions and a mask of 770 results in 770 permissions.Permission precedence: umask -> directory_create_mask -> force_directory_modeValues: Octal permissions (for example, 755, 770) | 0777 | Yes |
 | force_file_mode | Forces file permissions using octal notation. Newly created files have their permissions logically OR'ed with this value. For example, creating a file with 770 permissions and force mode 775 results in 775 permissions.Permission precedence: umask -> file_create_mask -> force_file_modeValues: Octal permissions (for example, 644, 755) | 0 | Yes |
 | force_directory_mode | Forces directory permissions using octal notation. Newly created directories have their permissions logically OR'ed with this value. For example, creating a directory with 770 permissions and force mode 775 results in 775 permissions.Permission precedence: umask -> directory_create_mask -> force_directory_modeValues: Octal permissions (for example, 755, 775) | 0 | Yes |
 | sync_on_close | Ensures all file data is written to the server when files are closed, providing immediate data consistency. Simulates NFS open-to-close semantics with writecache mode and directory quotas. Required for applications that expect write errors at close() when quotas are exceeded.Values: Yes, No | No | Yes |
 | nosync_on_close | Disables sync_on_close behavior, allowing files to close without waiting for server confirmation that data is written to disk. Changes are buffered in memory and written asynchronously, improving performance but reducing immediate data consistency.Values: Yes, No | No | Yes |

### Remount of general options

You can remount using the mount options marked as `Remount Supported` in the above table (`mount -o remount)`.

When a mount option has been explicitly changed, you must set it again in the remount operation to ensure it retains its value. For example, if you mount with `ro`, a remount without it changes the mount option to the default `rw`. If you mount with `rw`, it is not required to re-specify the mount option because this is the default.

### **Additional mount options using the stateless clients feature**

 | Option | Description | Default | Remount Supported |
 | --- | --- | --- | --- |
 | memory_mb=<memory_mb> | The memory size in MiB the client can use for hugepages. | 1400 | Yes |
 | num_cores=<frontend-cores> | Specifies the number of processing cores allocated to handle client network operations.Valid values:1 to N (where N is the maximum available cores)0 (only valid with UDP networking mode)Notes:Cannot be used with core parameterWhen using NICs with Virtual Functions, num_cores must match the number of configured network devices (net=)Higher core counts may improve performance for multi-connection workloadsExample: core_num=4 # Allocates 4 cores for client processing | 1 | Yes |
 | core=<core-id> | Specifies which CPU cores to assign to the WEKA client.Multiple cores can be specified as a comma-separated list.Core 0 is reserved for system use and cannot be specified.Examples:-o core=1 # Single core -o core=1 -o core=3 -o core=5 # Multiple cores Restrictions:Core IDs must be unique and available on systemCannot be used with num_cores parameterCore 0 not allowed |  | Yes |
 | net=<netdev>[/<ip>/<bits>[/<gateway>]] | Specifies network devices for WEKA client connections. Required for on-premises installations.Format:Single device: -o net=eth1Multiple devices: -o net=eth1 -o net=eth2 -o net=eth3Important:For NICs with Virtual Functions (VFs), the number of network devices must equal num_coresSupports both physical NICs and virtual functionsMust specify at least one network deviceFor additional options, see #advanced-network-configuration-for-stateless-clients |  | Yes |
 | remove_after_secs=<secs> | The time in seconds without connectivity, after which the client is removed from the cluster. Minimum value: 60 seconds.3600 seconds = 1 hour. | 3600 | Yes |
 | traces_capacity_mb=<size-in-mb> | Traces capacity limit in MB.Minimum value: 512 MB. |  | No |
 | reserve_1g_hugepages=<true or false> | Controls the page allocation algorithm to reserve hugepages.Possible values:true: reserves 1 GBfalse: reserves 2 MB | true | Yes |
 | readahead_kb=<readahead> | The readahead size in KB per mount. A higher readahead is better for sequential reads of large files. | 32768 | Yes |
 | auth_token_path | The path to the mount authentication token (per mount). | ~/.weka/auth-token.json | No |
 | dedicated_mode | Controls CPU core allocation for DPDK networking.Set to full to dedicate an entire core to network processing, or none to operate without core dedication (requires NIC driver support).Only applies when DPDK networking is enabled (net=udp not set). See DPDK without the core dedication.Values: full, none | full | No |
 | dedicated_mode | Determine whether DPDK networking dedicates a core (full) or not (none). none can only be set when the NIC driver supports it. See DPDK without the core dedication. This option is relevant when using DPDK networking (net=udp is not set).Possible values: full or none | full | No |
 | qos_preferred_throughput_mbps | Specifies the preferred request rate for Quality of Service (QoS), in megabytes per second. This is a soft target used to guide bandwidth allocation. The system aims to maintain this rate under normal conditions but allows the frontend to exceed it, up to the maximum, when additional resources are available.The cluster admin can set the default value. See Set mount option default values. | 0 (unlimited) | Yes |
 | qos_max_throughput_mbps | Specifies the maximum request rate for Quality of Service (QoS), in megabytes per second. This is an average-based limit applied at the front end. The system allows short bursts above this value but aims to maintain the specified limit over time.The cluster admin can set the default value. See Set mount option default value. | 0 (unlimited) | Yes |
 | qos_max_ops | Maximum number of IO operations a client can perform per second.Set a limit to a client or clients to prevent starvation from the rest of the clients. (Do not set this option for mounting from a backend.) | 0 (unlimited) | Yes |
 | connect_timeout_secs | The timeout, in seconds, for establishing a connection to a single server. | 10 | Yes |
 | response_timeout_secs | The timeout, in seconds, waiting for the response from a single server. | 60 | Yes |
 | join_timeout_secs | The timeout, in seconds, for the client container to join the Weka cluster. | 360 | Yes |
 | dpdk_base_memory_mb | The base memory in MB to allocate for DPDK. Set this option when mounting to a WEKA cluster on GCP.Example: -o dpdk_base_memory_mb=16 | 0 | Yes |
 | weka_version | The WEKA client version to run. | The cluster version | No |
 | restricted | Restricts a stateless client‚Äôs operations to only the essential APIs for mounting and unmounting operations. |  | No |

Note: The additional mount options parameters above are only effective on the first mount command for each client, unless stated otherwise.

Note: By default, the command selects the optimal core allocation for WEKA. If necessary, multiple `core` parameters can be used to allocate specific cores to the WEKA client. For example, `mount -t wekafs -o core=2 -o core=4 -o net=ib0 backend-server-0/my_fs /mnt/weka`

Note: **Example: On-Premise Installations**
`mount -t wekafs -o num_cores=1 -o net=ib0 backend-server-0/my_fs /mnt/weka`
Running this command on a server installed with the Weka agent downloads the appropriate WEKA version from the `backend-server-0`and creates a WEKA container that allocates a single core and a named network interface (`ib0`). Then it joins the cluster that `backend-server-0` is part of and mounts the filesystem `my_fs` on `/mnt/weka.`
`mount -t wekafs -o num_cores=0 -o net=udp backend-server-0/my_fs /mnt/weka`
Running this command uses [UDP mode ](../../weka-system-overview/networking-in-wekaio#udp-mode)(usually selected when the use of DPDK is not available).

Note: **Example: AWS Installations**
`mount -t wekafs -o num_cores=2 backend1,backend2,backend3/my_fs /mnt/weka`
Running this command on an AWS EC2 instance allocates two cores (multiple-frontends), attaches and configures two ENIs on the new client. The client attempts to rejoin the cluster through all three backends specified in the command line.

For stateless clients, the first `mount` command serves a dual purpose:

1. It installs the WEKA client software.
2. It joins the WEKA cluster.

Subsequent `mount` commands can be simplified, requiring only the persistent or per-mount parameters as defined in the #mount-command-options. The full cluster configuration is not needed for these additional mounts.

WEKA filesystems can be accessed directly through the mount point. You can navigate to the filesystem using standard directory commands, such as `cd /mnt/weka/`.

When the final WEKA filesystem is unmounted using the `umount` command, two key actions occur:

* The client is automatically disconnected from the cluster.
* The WEKA client software is uninstalled by the agent.

As a result, initiating a new `mount` operation requires re-specifying the complete cluster configuration, including cluster details, cores, and networking parameters.

Note: When running in AWS, the instance IAM role must provide permissions to several AWS APIs (see the [IAM role created in template](../../planning-and-installation/aws/weka-installation-on-aws-using-the-cloud-formation/cloudformation#iam-role-created-in-the-template) section).

Note: Memory allocation for a client is predefined. To change the memory allocation, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).

### Remount options for stateless clients

Mount options explicitly marked as `Remount Supported` can be modified using the `mount -o remount` command. During a remount operation:

* Unspecified mount options retain their current configuration.
* To reset a specific option to its default value, use the `default` modifier.

Example of resetting an option to its default:

* `memory_mb=default` restores the default memory configuration.

This approach allows for flexible, granular adjustments to mount parameters without requiring a complete filesystem unmount and remount.

### Set mount option default values <a href="#set-mount-option-default-values" id="set-mount-option-default-values"></a>

#### Default throughput settings

* By default, `qos_max_throughput_mbps` and `qos_preferred_throughput_mbps` are unset, meaning no throughput limit is enforced.

#### Cluster administrator capabilities

* Set custom default values aligned with organizational requirements.
* Reset to initial unlimited configuration.
* View current default settings.

#### Key characteristics

* QoS settings apply to the frontend process, not individual mounts. All mounts on the same frontend share the same QoS limits.
* If a client connects to multiple WEKA clusters, each frontend enforces its QoS settings independently.
* Default value changes only affect new mounts. Existing mounts retain the QoS values they were created with.

#### Available commands

* Set defaults: `weka cluster mount-defaults set`
* Reset to initial values: `weka cluster mount-defaults reset`
* Display current defaults: `weka cluster mount-defaults show`

#### Command syntax

```

```
weka cluster mount-defaults set [--qos-max-throughput qos-max-throughput] [--qos-preferred-throughput qos-preferred-throughput]
```

```

**Parameters**

 | Option | Description |
 | --- | --- |
 | qos_max_throughput | Specifies the default maximum request rate for Quality of Service (QoS), in megabytes per second. This is an average-based limit applied at the frontend. The system allows short bursts above this value but aims to maintain the specified limit over time. |
 | qos_preferred_throughput | Specifies the default preferred request rate for Quality of Service (QoS), in megabytes per second. This is a soft target used to guide bandwidth allocation. The system aims to maintain this rate under normal conditions but allows the frontend to exceed it, up to the maximum, when additional resources are available. |

### Monitor active mounts per container

Tracking the number of active mounts per container is important for troubleshooting, validating mount configurations, and identifying potential issues in the WEKA cluster. It provides visibility into mount activity, helping users and automation tools detect anomalies and ensure expected behavior.

To view the active mount count for a specific container, read the following `/proc` interface:

```
/proc/wekafs/<container-name>/interface
```

***

## Advanced network configuration for stateless clients

Stateless clients allow for customizable network configurations to enhance performance and connectivity. The following parameters can be adjusted:

* Virtual Functions (VFs)
* IP addresses
* Gateway configuration (required if the client is on a different subnet)
* Physical network devices (for improved performance and high availability)
* UDP mode

To configure networking, use the `-o net=<netdev>` mount option with the appropriate modifiers.

#### **Identify `<netdev>`**

`<netdev>` can be specified using:

* Network interface name
* MAC address
* PCI address of the physical network device
* Bonded device for redundancy and load balancing

#### **Networking technology compatibility**

When using WEKA mounts (`wekafs`), ensure that clients and backends use the same network type. Supported options include InfiniBand (IB) or Ethernet.

#### **Key considerations**

* The `-o net=<netdev>` option provides detailed control over network interfaces.
* Selecting the appropriate configuration helps optimize performance and connectivity.
* Consistent networking technology is essential for system reliability.

### **Configure IP, subnet, gateway, and Virtual Functions (VFs)**

For improved performance, multiple frontend processes may be required. When using a Network Interface Card (NIC) other than Mellanox, or when deploying a DPDK client on a virtual machine (VM), **Single Root I/O Virtualization (SR-IOV)** must be used to expose a **Virtual Function (VF)** of the physical device to the client. Once exposed, the VF can be configured using the `mount` command.

#### **Assign VF IP addresses and routing**

To assign an IP address to a VF or to enable routing when the client is in a different subnet, use the following format:

```bash
net=<netdev>/[ip]/[bits]/[gateway]
```

* `ip`, `bits`, and `gateway` are optional parameters.
* If these parameters are not provided, the WEKA system assigns values based on the environment:
  * **Cloud environment**: The system automatically deduces the IP address, subnet mask, and gateway.
  * **On-premises environment**: The system assigns values based on the cluster‚Äôs default network configuration.
    * If the default network is not set, the WEKA cluster may fail to allocate an IP address for the client.

Note: **Important:** Ensure that the **WEKA cluster default data networking** is configured before executing the `mount` command. For configuration details, see #id-6.-configure-default-data-networking-optional.

#### **Example: Configuring VFs on a single physical network device**

The following command configures VFs for a specified network device and assigns each VF to a frontend process.

* The first frontend process is assigned **192.168.1.100**.
* The second frontend process is assigned **192.168.1.101**.
* Both IPs are configured with a **24-bit subnet mask** and a **default gateway of 192.168.1.254**.

```

```bash
mount -t wekafs -o num_cores=2 -o net=intel0/192.168.1.100+192.168.1.101/24/192.168.1.254 backend1/my_fs /mnt/weka
```

```

### Multiple physical network devices for performance and high availability

Utilizing multiple physical network interface cards (NICs) on a WEKA client can unlock significant gains in data throughput and enhance system resilience. By strategically distributing network traffic across several interfaces, you can overcome single-NIC bottlenecks for demanding applications and ensure continuous data access even if one network path fails.

This section delves into the various methods for configuring and managing multiple NICs with WEKA. It covers how to:

* Aggregate NICs for increased overall performance.
* Set up redundant configurations to achieve high availability.
* Implement advanced NUMA-aware setups for optimal efficiency on multi-socket servers.
* Use specific mount options, including detailed slot notation, to precisely control how client processes uses the available network interfaces.

The following subsections provide detailed explanations and practical examples for each of these configurations, enabling you to tailor your WEKA client's network setup to your specific performance and availability requirements.

<details>

<summary>Multiple physical network devices for better performance</summary>

Demanding workloads on WEKA can readily saturate the bandwidth of a single network interface. For higher throughput, you can leverage multiple network interface cards (NICs). By using the `-o net=<interface>` mount option for each desired NIC, you instruct the WEKA client driver to utilize these specific interfaces, potentially distributing the load and increasing overall bandwidth.

For example, the following command allocates two cores and two physical network devices for increased throughput:

```bash
mount -t wekafs \
-o num_cores=2 \
-o net=mlnx0 -o net=mlnx1 \
backend1/my_fs /mnt/weka
```

</details>

<details>

<summary>Multiple physical network devices for high availability configuration</summary>

Multiple NICs can also be configured to achieve redundancy and higher throughput for a complete, highly available solution. For that, use more than one physical device as previously described, and also, specify the client management IPs using `-o mgmt_ip=<ip1>+<ip2>` command-line option.

For example, the following command uses two network devices (`mlnx0` and `mlnx1`) for high availability and allocates both devices to four Frontend processes on the client(because `num_cores=4`). The modifier `ha` is used here, which stands for using the device on all processes.  Note that in this example, `10.0.0.1` is the IP address of `mlnx0` while `10.0.0.2` is the IP address of `mlnx1`.

```

```bash
mount -t wekafs \
-o num_cores=4 \
-o net:ha=mlnx0,net:ha=mlnx1 \
-o mgmt_ip=10.0.0.1+10.0.0.2 \
backend1/my_fs /mnt/weka
```

```

</details>

<details>

<summary>Advanced configuration: NUMA affinity with multiple physical network devices and sockets</summary>

For more complex systems, especially those with multiple CPU sockets and NUMA (Non-Uniform Memory Access) nodes, you can achieve higher performance and efficiency by pinning client processes and their network traffic to specific NUMA nodes. This involves assigning cores from a specific NUMA node to WekaFS client processes and then mapping these processes to a network interface card (NIC) physically located on the same NUMA node.

Consider a server with four NUMA nodes and four InfiniBand (IB) network interfaces, where each IB interface is assumed to reside on a different NUMA node. The NUMA configuration of the CPUs is as follows:

* NUMA node0 CPU(s): 0-63
* NUMA node1 CPU(s): 64-127
* NUMA node2 CPU(s): 128-191
* NUMA node3 CPU(s): 192-255

Let's assume you have four IB interfaces: `ib0` (on NUMA node0), `ib1` (on NUMA node1), `ib2` (on NUMA node2), and `ib3` (on NUMA node3). To configure WekaFS for optimal NUMA affinity, you would pin specific cores from each NUMA node to WekaFS frontend processes and then map these groups of processes to their corresponding NUMA-local IB interface. Management IPs must also be specified for high availability.

**Example:**

The following command configures 16 WekaFS client processes. Four processes are pinned to cores on each of the four NUMA nodes. Each group of four processes is then mapped to its local IB interface.

```
mount -t wekafs \
-o core=63 -o core=62 -o core=61 -o core=60 \
-o core=127 -o core=126 -o core=125 -o core=124 \
-o core=191 -o core=190 -o core=189 -o core=188 \
-o core=255 -o core=254 -o core=253 -o core=252 \
-o net:s1-4=ib0 \
-o net:s5-8=ib1 \
-o net:s9-12=ib2 \
-o net:s13-16=ib3 \
backend_servers/my_fs /mnt/weka
```

**Explanation of the options in this example:**

* **`-o core=...`**: Sixteen specific CPU cores are assigned to WekaFS client processes:
  * Cores 63, 62, 61, 60 are on NUMA node0.
  * Cores 127, 126, 125, 124 are on NUMA node1.
  * Cores 191, 190, 189, 188 are on NUMA node2.
  * Cores 255, 254, 253, 252 are on NUMA node3. This creates 16 frontend processes, with each group of four processes affinitized to a specific NUMA node.
* **`-o net:s1-4=ib0, net:s5-8=ib1, net:s9-12=ib2, net:s13-16=ib3`**: These options use the "multiple NIC slot notation" to map the WekaFS client processes (referred to by "slots") to the specified network interfaces (`ib0`, `ib1`, `ib2`, `ib3`). In this configuration with 16 frontend processes, the intended mapping is:
  * The first group of four processes (running on cores 63,62,61,60 on NUMA0) uses `ib0` (assumed to be on NUMA0).
  * The second group of four processes (running on cores 127,126,125,124 on NUMA1) uses `ib1` (assumed to be on NUMA1).
  * The third group of four processes (running on cores 191,190,189,188 on NUMA2) uses `ib2` (assumed to be on NUMA2).
  * The fourth group of four processes (running on cores 255,254,253,252 on NUMA3) uses `ib3` (assumed to be on NUMA3). This setup ensures that network traffic for processes on a given NUMA node utilizes the NIC local to that NUMA node, minimizing cross-NUMA data transfers and potentially improving performance.

- **`backend_servers/my_fs`**: Replace with your WekaFS backend server address(es) and filesystem name.
- **`/mnt/weka`**: Replace with your desired mount point.

This type of granular configuration is beneficial for maximizing throughput and minimizing latency in high-performance computing (HPC) and AI workloads that are sensitive to NUMA effects.

</details>

<details>

<summary>Advanced mounting options for multiple physical network devices</summary>

With multiple Frontend processes (as expressed by `-o num_cores=X`), it is possible to control what processes use what NICs. This can be accomplished through the use of special command line modifiers called _slots_. In WEKA, _slot_ is synonymous with a process number. Typically, the first WEKA Frontend process will occupy slot 1, then the second - slot 2 and so on.

Examples of slot notation include `s1`, `s2`, `s2+1`, `s1-2`, `slots1+3`, `slot1`, `slots1-4` , where `-` specifies a range of devices, while `+` specifies a list. For example, `s1-4` implies slots 1, 2, 3, and 4, while `s1+4` specifies slots 1 and 4.

For example, in the following command, `mlnx0` is bound to the second Frontend process while `mlnx1` to the first one for improved performance.

```

```bash
mount -t wekafs \
-o num_cores=2 -o net:s2=mlnx0,net:s1=mlnx1 \
backend1/my_fs /mnt/weka
```

```

For exampl&#x65;**,** in the following mounting command, two cores (two Frontend processes) and two physical network devices (`mlnx0`, `mlnx1`) are allocated. By explicitly specifying `s2+1`, `s1-2` modifiers for network devices, both devices will be used by both Frontend processes. Notation `s2+1` stands for the first and second processes, while `s1-2` stands for the range of 1 to 2, and are effectively the same.

```

```bash
mount -t wekafs \
-o num_cores=2 \
-o net:s2+1=mlnx0,net:s1-2=mlnx1 \
backend1/my_fs \
-o mgmt_ip=10.0.0.1+10.0.0.2 /mnt/weka
```

```

</details>

### Network label configuration for stateless clients

In environments with stateless clients and high-availability backend networks, configuring network labels is essential for optimizing data path locality and minimizing inter-switch traffic.

Stateless clients, which typically lack persistent state or configuration storage, often connect to a single top-of-rack switch. In contrast, backend servers are usually dual-connected across multiple switches to ensure high availability. In topologies where these switches are interconnected via inter-switch links (ISLs), traffic between nodes may traverse these ISLs unnecessarily if peer selection is left to default behavior. This can introduce additional latency and consume limited east-west bandwidth.

To influence peer selection and ensure efficient traffic routing, stateless clients can use **network labels**. These labels bind the client‚Äôs traffic to a specific network segment or switch, helping ensure that peering remains within the local switch when possible.

**Use case**

This configuration is especially beneficial in:

* Two-switch topologies with ISL connections.
* Deployments where backend nodes are dual-attached and clients are single-attached.
* Scenarios requiring controlled peering to reduce east-west traffic.

**Configuration**

To assign a network label, use the `-o net` mount option in the following format:

```
mount -t wekafs -o net=<device>/label@<label> <filesystem> <mountpoint>
```

**Parameters:**

* `<device>`: The name of the client‚Äôs network interface (for example, `eth0`).
* `<label>`: The label that corresponds to the client‚Äôs network attachment point.
* `<filesystem>`: The WEKA filesystem to mount.
* `<mountpoint`>: The local directory where the filesystem will be mounted.

**Example:**

```
mount -t wekafs -o net=eth0/label@datacenter-a  project-fs1/data
```

In this example:

* The client uses the `eth0` interface.
* The label `datacenter-a` indicates the switch or network zone the interface is connected to.
* The `project-fs1` WEKA filesystem is mounted at `/data`.

By using a label that reflects the client‚Äôs physical or logical network location, the system can make more informed decisions about peering and data path selection, reducing cross-switch communication and improving overall performance.

**Remount support**

The network label configuration using the `-o net` option is also supported during remount operations. This allows administrators to change the network label dynamically without needing to fully unmount and remount the filesystem. For example:

```
mount -o remount,net=eth0/label@datacenter-b /data
```

In this scenario, the client updates the network label to datacenter-b for the existing mount at `/data`. This flexibility is useful when network topology or client attachment changes, allowing adjustments to peering behavior with minimal disruption.

**Related topic**

#high-availability

### UDP mode

If DPDK cannot be used, you can use the WEKA filesystem UDP networking mode through the kernel. Use `net=udp` in the mount command to set the UDP networking mode, for example:

```bash
mount -t wekafs -o net=udp backend-server-0/my_fs /mnt/weka
```

Note: A client in UDP mode cannot be configured in high availability mode (`ha`). However, the client can still work with a highly available cluster.

Note: Providing multiple IPs in the \<mgmt-ip> in UDP mode uses their network interfaces for more bandwidth, which can be useful in RDMA environments rather than using only one NIC.

**Related topic**

#udp-mode (in the WEKA Networking topic)

***

## Mount a filesystem using fstab

Using the fstab (filesystem table) enables automatic remount after a reboot. This applies to stateless clients running on an OS that supports systemd, such as RHEL/CentOS 7.2 and up, Ubuntu 16.04 and up, and Amazon Linux 2 LTS.

#### Before you begin

* If the mount point you want to set in the fstab is already mounted, unmount it before setting the fstab file.

#### Procedure

1. **Create a mount point:** Run the following command to create a mount point:

```
mkdir -p /mnt/weka/my_fs
```

2. **Edit the `/etc/fstab` file:** Add the entry for the WEKA filesystem.

**fstab structure**

```

```php-template
<backend servers/my_fs> <mount point> <filesystem type> <mount options> <systemd mount options> 0 0
```

```

**Example**

```

```
backend-0,backend-1,backend-3/my_fs /mnt/weka/my_fs wekafs num_cores=1,net=eth1,x-systemd.after=weka-agent.service,x-systemd.mount-timeout=infinity,_netdev 0 0
```

```

**fstab configuration parameters**

 | Parameter | Description |
 | --- | --- |
 | Backend servers/my_fs | Comma-separated list of backend servers with the filesystem name. |
 | Mount point | If mounting multiple clusters, specify a unique name.For two client containers, set container_name=client1 and container_name=client2. |
 | Filesystem type | Must be wekafs. |
 | Systemd mount options | x-systemd.after=weka-agent.servicex-systemd.mount-timeout=infinity_netdevAdjust the mount-timeout to your preference, for example, 180 seconds. |
 | Mount options | See #additional-mount-options-using-the-stateless-clients-feature |

3. **Mount the filesystem:** Test the fstab setting by running:

```
mount /mnt/weka/my_fs
```

4. **Reboot the server:** Reboot the server to apply the fstab settings. The filesystem is automatically mounted after the reboot.

***

## Mount a filesystem using autofs

Autofs allows filesystems to be mounted dynamically when accessed and unmounted after a period of inactivity. This approach reduces system overhead and ensures efficient resource utilization. Follow these steps to configure autofs for mounting Weka filesystems.

#### Procedure

1. **Install autofs on the server:** Install the autofs package based on your operating system:
   *   **For Red Hat or CentOS**:

       ```
       yum install -y autofs
       ```
   *   **For Debian or Ubuntu**:

       ```
       apt-get install -y autofs
       ```
2. **Configure autofs for WEKA filesystems:** Set up the autofs configuration files according to the client type:
   *   **Stateless client**: Run the following commands, replacing `<backend-1>`, `<backend-2>`, and `<netdevice>` with appropriate values:

       ```

       ```
       echo "/mnt/weka /etc/auto.wekafs -fstype=wekafs,num_cores=1,net=<netdevice>" > /etc/auto.master.d/wekafs.autofs
       echo "* <backend-1>,<backend-2>/&" > /etc/auto.wekafs
       ```

```
   *   **Persistent client**: Run the following commands:

       ```

       ```
       echo "/mnt/weka /etc/auto.wekafs -fstype=wekafs" > /etc/auto.master.d/wekafs.autofs
       echo "* &" > /etc/auto.wekafs
       ```

```
3.  **Restart the autofs service:** Apply the changes by restarting the autofs service:

    ```
    service autofs restart
    ```
4.  **Ensure autofs starts automatically on reboot:** Verify that autofs is configured to start on reboot:

    ```bash
    systemctl is-enabled autofs
    ```

    * If the output is `enabled`, no further action is required.

    **For Amazon Linux**: Use `chkconfig` to confirm autofs is enabled for the current runlevel:

    ```
| chkconfig | grep autofs |
    ```

    Ensure the output indicates `on` for the active runlevel.\
    Example output:

    ```
    autofs 0:off 1:off 2:off 3:on 4:on 5:on 6:off
    ```
5.  **Access the WEKA filesystem:** Navigate to the mount point to access the WEKA filesystem. Replace `<fs-name>` with the desired filesystem name:

    ```
    cd /mnt/weka/<fs-name>
    ```

Note: * Adjust backend and network device configurations as needed for your deployment.
* Review distribution-specific documentation for additional configuration options.

<!-- ============================================ -->
<!-- File 84/259: weka-filesystems-and-object-stores_mounting-filesystems_mount-fs-from-scmc.md -->
<!-- ============================================ -->

---
description:
---

# Mount filesystems from Single Client to Multiple Clusters (SCMC)

## Overview

Mounting filesystems from a single WEKA client to multiple clusters provides several advantages:

* **Expanded cluster connectivity:** A single client can connect to up to seven clusters simultaneously, increasing storage capacity and computational capabilities.
* **Unified data access:** Provides a consolidated view of data across multiple clusters, simplifying access and management while improving data availability, flexibility, and resource efficiency.
* **Optimized workload distribution:** Enables efficient workload distribution across clusters, supporting scalable applications and enhancing overall performance.
* **Seamless integration:** WEKA‚Äôs SCMC feature ensures smooth and efficient integration for clients accessing multiple clusters.

### **Bandwidth division considerations in SCMC**

The bandwidth division in SCMC is a universal consideration based on the specific NIC's bandwidth. It applies across various NIC types, including those using DPDK or specific models like the X500-T1.

During SCMC mounts, each active connection can use the bandwidth available on its associated NIC port. This is true during peak usage and idle cases. In scenarios where NICs are dual-ported, each connection operates independently, leveraging its dedicated port.

When working with low-bandwidth NICs such as the X500-T1, a 10Gb/s NIC, consider bandwidth calculations. In the context of SCMC, each container (representing connectivity to a different cluster) uses half of the available bandwidth (5Gb/s) for a shared port. Note that a dual-port NIC has a dedicated port for each container, optimizing bandwidth distribution. Keep these factors in mind for an optimal SCMC setup.

## Prerequisites

Ensure the following requirements are met:

* All clusters that run in this configuration must be at least version 4.2.
* All client containers in the WEKA client must run the same minor version, at least version 4.2. The client version must be the same as the cluster or, at most, one version earlier.
* All client containers must be of the same type, persistent or stateless clients. Mixing different client types in a single WEKA client is not allowed.
* Each client container must run on its port. The default ports are 14000, 14101, 14201, 14301, 14401, 14501, and 14601. Ensure these ports allow egress on the client and ingress on the cluster.
* For DPDK, each client container must have 5 GB of free RAM, and it is recommended to have a dedicated CPU core to get optimal performance.

 Mounting a filesystem without these requirements may fail or overload the WEKA client.

Note: Mounting a persistent client using **autofs** is only supported on filesystems on a single cluster.

## Set the client target version in the clusters

When a stateless client mounts a filesystem in a cluster, it creates a client container with the same version as provided by the cluster. Because there may be situations where some of the clusters run a different WEKA version than the others, such as during an upgrade, it is required to set the same client target version on all clusters. The client target version is retained regardless of the cluster upgrade.

Note: The client target version must be consistent across all clusters. It can match the cluster version or be one major version earlier (regardless the minor), provided that version is available in the cluster for client download.
To upgrade the cluster to a version higher than the first major release above the client version, see .

#### Procedure:

1. Connect to each cluster and run the following command to set the client target version.

```bash
weka cluster client-target-version set <version>
```

Where: `<version>` is the designated client target version, which will be installed on the client container upon the mount command. Ensure this version is installed on the backend servers.

2. To display the existing client target version in the cluster, run the following command:

```bash
weka cluster client-target-version show
```

3. To reset the client target version to the cluster version, run the following command:

```bash
weka cluster client-target-version reset
```

## Mount a stateless client container on multiple clusters

Use the same commands as with a single client.

```

```bash
mount -t wekafs <backend-name> <fs-name> <mount-point> -o container_name=<container-name>
```

```

To mount a stateless client using UDP mode, add `-o net=udp -o core=<core-id>` to the command line. For example:

```

```bash
mount -t wekafs backend-server-0/my_fs /mnt/weka -o net=udp -o core=2 -o container_name=frontend0
```

```

## Mount persistent client containers on multiple clusters

For persistent client containers, the `client-target-version` parameter is not relevant. The version of the client container is determined when creating the container in the WEKA client using the `weka local setup container` command. Therefore, ensure that all client containers in the WEKA client have the same minor version as in the clusters.

To mount a persistent client container to a cluster, specify the container name for that mount.

```bash
mount -t wekafs <fs-name> <mount-point> -o container_name=<container-name>
```

## Run commands from a server with multiple client containers

When running WEKA CLI commands from a server hosting multiple client containers, each connected to a different WEKA cluster, it‚Äôs required to specify the client container port or the backend IP address/name of the cluster (linked to that client) in the command.

Consider a server with two client containers:

```bash
weka local ps
CONTAINER  STATE    DISABLED  UPTIME    MONITORING  PERSISTENT  PORT   PID    STATUS  VERSION LAST FAILURE
client1    Running  False     3:15:57h  True        False       14000  58318  Ready   4.3.0
client2    Running  False     3:14:35h  True        False       14101  59529  Ready   4.3.0
```

To run a WEKA CLI command on the second cluster (associated with `client2`), use either of the following methods:

*   By specifying the backend IP address or name linked to that client container (assuming the backend name is `DataSphere2-1`):

    ```
    weka status -H DataSphere2-1
    ```
*   By specifying the client container port:

    ```
    weka status -P 14101
    ```

This approach ensures that your WEKA CLI command targets the correct WEKA cluster associated with the specified client container.

#### Related topics

[](<> "mention")

<!-- ============================================ -->
<!-- File 85/259: weka-filesystems-and-object-stores_mounting-filesystems_manage-authentication-across-multiple-clusters-with-connection-profiles.md -->
<!-- ============================================ -->

---
description:
---

# Manage authentication across multiple clusters with connection profiles

## Overview

Managing authentication across multiple clusters in the WEKA CLI is streamlined with connection profiles. By default, when you run the `weka user login` command, it creates a profile stored as `.weka/auth-token.json`. This is sufficient for single-cluster environments. However, in a multi-cluster environment, use the `--profile` parameter to create and manage separate profiles for each cluster. This allows you to switch between clusters without needing to re-authenticate each time, enhancing efficiency and usability.

**Profile naming conventions**

When creating a connection profile, follow these guidelines:

* **Maximum length:** 50 characters
* **Allowed characters:**
  * Alphanumeric (A-Z, a-z, 0-9)
  * Underscores (_)
  * Hyphens (-)

Profile names dictate where authentication details are stored in the `.weka` directory:

* **Default profile:** `.weka/auth-token.json`
* **Named profiles:** `.weka/auth-token-<profile-name>.json`

## **Log in with profiles**

The `weka user login` command supports profiles, enabling you to specify which profile to use or create a new one.

**Command syntax:**

```bash
weka user login --profile <profile-name>
```

* **Default profile:** If no profile is specified, the system uses the default profile.
* **Profile-specific file:** Authentication information is saved in a file named after the profile.
*   **Success message:** After a successful login, the following message appears:

    ```bash
    Login completed successfully.
    <Default/profileN> profile updated.
    ```
* **Failure message:** If the profile is not found or the login fails, an error message displays the profile name and file path.

## **Log out of profiles**

The `weka user logout` command supports profiles, enabling you to remove the authentication details for a specific profile.

**Command syntax:**

```
weka user logout --profile <profile-name>
```

* The specified profile‚Äôs authentication file is deleted.
* If no profile is specified, the default profile is logged out.

## **Using profiles with WEKA CLI commands**

You can specify a profile when executing most WEKA CLI commands using the `--profile` option. If no profile is provided, the default profile is used.

**Command syntax:**

```
weka <command> --profile <profile-name>
```

Note: The `--profile` option is not supported with `weka diag` commands. The default profile is used for diagnostics.

**Related topic**

<!-- ============================================ -->
<!-- File 86/259: weka-filesystems-and-object-stores_managing-object-stores.md -->
<!-- ============================================ -->

---
description: This page provides an overview about managing object stores.
---

# Manage object stores

Object stores in WEKA are optional external storage media, complementing SSD storage with a more cost-effective solution. This allows for the strategic allocation of resources, with object stores accommodating warm data (infrequently accessed) and SSDs handling hot data (frequently accessed).

In WEKA, object store buckets can be distributed across different physical object stores. However, to ensure optimal Quality of Service (QoS), a crucial mapping between the bucket and the physical object store is required.

WEKA treats object stores as physical entities, either on-premises or in the cloud, grouping multiple object store buckets. These buckets can be categorized as either local (used for tiering and snapshots) or remote (exclusively for snapshots). An object-store bucket must be added to an object store with the same type and remain inaccessible to other applications.

While a single object store bucket can potentially serve different filesystems and multiple WEKA systems, it is advisable to dedicate each bucket to a specific filesystem. For instance, if managing three-tiered file systems, assigning a dedicated local object storage bucket to each file system is recommended.

For each filesystem, users can attach up to three object store buckets:

* A local object store bucket for tiering and snapshots.
* A second local object store bucket for additional tiering and snapshots. Note that adding a second local bucket renders the first local bucket read-only.
* A remote object store bucket exclusively for snapshots.

Note: Remote object store buckets employ a write-once-delete-never approach to snapshot uploads. This means the bucket will only grow in size over time, even if the snapshots uploaded to it are deleted from the filesystem.

Multiple object store buckets offer flexibility for various use cases, including:

* Migrating to different local object stores when detaching a read-only bucket from a filesystem tiered to two local object store buckets.
* Scaling object store capacity.
* Increasing total tiering capacity for filesystems.
* Backing up data in a remote site.

In cloud environments, users can employ cloud lifecycle policies to transition storage tiers or classes. For example, in AWS, users can move objects from the S3 standard storage class to the S3 intelligent tiering storage class for long-term retention using the AWS lifecycle policy.

<!-- ============================================ -->
<!-- File 87/259: weka-filesystems-and-object-stores_managing-object-stores_managing-object-stores.md -->
<!-- ============================================ -->

---
description: This page describes how to view and manage object stores using the GUI.
---

# Manage object stores using the GUI

Using the GUI, you can perform the following actions:

* Edit the default object stores
* Add an object store bucket
* View object store buckets
* Edit an object store bucket
* Show recent operations of an object store bucket
* Delete an object store bucket

## Edit the default object stores <a href="#edit-the-default-object-stores" id="edit-the-default-object-stores"></a>

Object store buckets can reside in different physical object stores. To achieve good QoS between the buckets, WEKA requires mapping the buckets to the physical object store.

You can edit the default local and remote object stores to meet your connection demands. When you add an object store bucket, you apply the relevant object store to it.

Editing the default object store provides you with the following additional advantages:

* Set restrictions on downloads from a remote object store.\
  For on-premises systems where the remote bucket is in the cloud, to reduce the cost, you set a very low bandwidth for downloading from a remote bucket.
* Ease of adding new buckets.\
  You can set the connection parameters on the object store level and, if not specified differently, automatically use the default settings for the buckets you add.

**Procedure**

1. From the menu, select **Manage > Object Stores**.
2. On the left, select the pencil icon near the default object store you want to edit.
3. On the **Edit Object Store** dialog, select the type of object store, and update the relevant parameters. Select one of the following tabs according to the object store type you choose.\
   For details, see the parameter descriptions in the Add an object store bucket topic.

It is not mandatory to set the Access Key and Secret Key in the **Edit Object Store** dialog in AWS. The AWS object store type is accessed from the WEKA EC2 instances to the object store and granted by the IAM roles assigned to the instances.

If you select **Enable AssumeRole API**, set also the **Role ARN** and **Role Session Name**. For details, see the Add an object store bucket topic.

It is not mandatory to set the Access Key and Secret Key in the **Edit Object Store** dialog in GCP. Google Cloud Storage is accessed using a service account attached to each Compute Engine Instance that is running WEKA software, provided that the service account has the required permissions granted by the IAM role (`storage.admin` for creating buckets.  `storage.objectAdmin` for using an existing bucket ).

## Add an object store bucket <a href="#add-an-object-store-bucket" id="add-an-object-store-bucket"></a>

Add object store buckets to be used for tiering or snapshots.

**Procedure**

1. From the menu, select **Manage > Object Stores**.
2. Select the **+Create** button.

3. In the **Create Object Store Bucket** dialog, set the following:
   * **Name**: Enter a meaningful name for the bucket.
   * **Object Store**: Select the location of the object store. For tiering and snapshots, select the local object store. For snapshots only, select the remote object store.
   * **Type**: Select the type of object store: AWS, AZURE, or OTHER (for GCP and others).
   * **Buckets Default Parameters**: Select one of the following tabs according to the object store type you choose.

WEKA supports the following options for creating AWS S3 buckets:

* AWS S3 bucket creation for WEKA cluster on EC2.
* AWS S3 bucket creation for WEKA cluster not on EC2 using STS.

Set the following:

1. **Protocol and Port:** Select the protocol to use when connecting to the bucket.
2. **Bucket:** Set the name of the bucket to store and access data.
3. **Region:** Set the region assigned to work with.
4. **For AWS S3 bucket creation for WEKA cluster on EC2:**\
   If the WEKA EC2 instances have the required permissions granted by the IAM role, then it is not required to provide the Access Key and Secret Key. Otherwise, set the Access Key and Secret Key of the user granted with read/write access to the bucket.
5. **For AWS S3 bucket creation for WEKA cluster not on EC2 using STS:**
   * Select **Enable AssumeRole API**.
   * **Role ARN:** Set the Amazon Resource Name (ARN) to assume. The ARN must have the equivalent permissions defined in the IAM role for S3 access. See [IAM role created in the template](../../../planning-and-installation/aws/weka-installation-on-aws-using-the-cloud-formation/cloudformation#iam-role-created-in-the-template).
   * **Role Session Name:** Set a unique identifier for the assumed role session.
   *   **Session Duration:** Set the duration of the temporary security credentials in seconds.

       Possible values: 900 - 43200 (default 3600).
   * **Access Key and Secret Key:** Set the keys of the user granted with the AssumeRole permissions.
6. **Advanced settings:**
   * **Download Bandwidth**: Object store download bandwidth limitation per core (Mbps).
   * **Upload Bandwidth**: Object store upload bandwidth limitation per core (Mbps).
   * **Max concurrent Downloads**: Maximum number of downloads concurrently performed on this object store in a single IO node.
   * **Max concurrent Uploads**: Maximum number of uploads concurrently performed on this object store in a single IO node.
   * **Max concurrent Removals**: Maximum number of removals concurrently performed on this object store in a single IO node.
   * **Enable Upload Tags**: Enable tagging of uploaded objects. For details, see [object-tagging](../../tiering/data-management-in-tiered-filesystems#object-tagging).
   * **Data Storage Class**: Configurable Amazon S3 storage classes, allowing users to optimize storage based on cost and access needs. Supports STANDARD, REDUCED_REDUNDANCY, STANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, OUTPOSTS, GLACIER_IR, and EXPRESS_ONEZONE. For details, For details, see the documentation for Amazon S3 Storage Classes.
   * **Metadata Storage Class:** Configurable Amazon S3 storage classes for metadata. Supports STANDARD, REDUCED_REDUNDANCY, STANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, OUTPOSTS, GLACIER_IR, and EXPRESS_ONEZONE.

Set the following:

* **Protocol and Port:** Select the protocol and port to use when connecting to the bucket.
* **Hostname:** Set the DNS name (or IP address) of the bucket entry point.
* **Bucket:** Set the name of the bucket to store and access data.
* **Auth Method:** Select the authentication method to connect to the bucket.
* **Region:** Set the region assigned to work with (usually you can leave it empty).
* **Access Key and Secret Key:** If the service account has the required permissions granted by the IAM role, then it is not required to provide the Access Key and Secret Key. If the WEKA cluster is not running on GCP instances then the Access Key and Secret Key are required.
* **Advanced settings:**
  * **Download Bandwidth**: Object store download bandwidth limitation per core (Mbps).
  * **Upload Bandwidth**: Object store upload bandwidth limitation per core (Mbps).
  * **Max concurrent Downloads**: Maximum number of downloads concurrently performed on this object store in a single IO node.
  * **Max concurrent Uploads**: Maximum number of uploads concurrently performed on this object store in a single IO node.
  * **Max concurrent Removals**: Maximum number of removals concurrently performed on this object store in a single IO node.
  * **Enable Upload Tags**: Enable tagging of uploaded objects. For details, see [object-tagging](../../tiering/data-management-in-tiered-filesystems#object-tagging).

Set the following:

1. **Protocol and Port:** Select the protocol and port to use when connecting to the bucket.
2. **Hostname:** Set the DNS name (or IP address) of the bucket entry point.
3. **Bucket:** Set the name of the bucket to store and access data.
4. **Auth Method:** Select the authentication method to connect to the bucket.
5. **Access Key and Secret Key:** Set the the Access Key and Secret Key of the user granted with read/write access to the bucket.
6. **Advanced settings:**
   * **Download Bandwidth**: Object store download bandwidth limitation per core (Mbps).
   * **Upload Bandwidth**: Object store upload bandwidth limitation per core (Mbps).
   * **Max concurrent Downloads**: Maximum number of downloads concurrently performed on this object store in a single IO node.
   * **Max concurrent Uploads**: Maximum number of uploads concurrently performed on this object store in a single IO node.
   * **Max concurrent Removals**: Maximum number of removals concurrently performed on this object store in a single IO node.
   * **Enable Upload Tags**: Enable tagging of uploaded objects. For details, see [object-tagging](../../tiering/data-management-in-tiered-filesystems#object-tagging).
   * **Data Storage Class:** Configurable Azure access storage tier, allowing users to optimize storage based on cost and access needs. Supports HOT, COOL, and COLD. For details, see the documentation for Azure Access tiers for blob data.
   * **Metadata Storage Class:** Configurable Azure access storage tier for metadata. Supports HOT, COOL, and COLD.

4. To validate the connection to the object store bucket, select **Validate**.
5. Select **Create**.

Note: If an error message about the object store bucket configuration appears, to save the configuration, select **Create Anyway**.

## View object store buckets <a href="#view-object-store-buckets" id="view-object-store-buckets"></a>

The object store buckets are displayed on the **Object Stores** page. Each object store indicates the status, bucket name, protocol (HTTP/HTTPS), port, region, object store location (local or remote), authentication method, and error information (if it exists).

**Procedure**

1. From the menu, select **Manage > Object Stores**.

The following example shows two object store buckets.

## Edit an object store bucket <a href="#edit-an-object-store-bucket" id="edit-an-object-store-bucket"></a>

You can modify the object store bucket parameters according to your demand changes.

**Procedure**

1. From the menu, select **Manage > Object Stores**.
2. Select the three dots on the right of the object store you want to modify and select **Edit**.

3. In the Edit Object Store Bucket dialog, modify the details, and select **Update**.

## Show recent operations of an object store bucket

For active object store buckets connected to filesystems, the system tracks this activity and provides details about each activity on the Bucket Operations page.

The details include the operation type (download or upload), start time, execution time, previous attempts results, cURL errors, and more.

**Procedure**

1. From the menu, select **Manage > Object Stores**.
2. Select the three dots on the right of the object store bucket you want to show its recent operation, and select **Show Recent Operations**.

The recent operations page for the selected object store bucket appears. To focus on specific operations, you can sort the columns and use the filters that appear on the top of the columns.

## Delete an object store bucket

You can delete an object store bucket if it is no longer required. The data in the object store remains intact.

**Procedure**

1. From the menu, select **Manage > Object Stores**.
2. Select the three dots on the right of the object store bucket you want to delete, and select **Remove**.
3. To confirm the object store bucket deletion, select **Yes**.

<!-- ============================================ -->
<!-- File 88/259: weka-filesystems-and-object-stores_managing-object-stores_managing-object-stores-1.md -->
<!-- ============================================ -->

---
description: This page describes how to view and manage object stores using the CLI.
---

# Manage object stores using the CLI

Using the CLI, you can perform the following actions:

* View object stores
* Edit an object store
* Add an object store
* View object store buckets
* Add an object store bucket
* Edit an object store bucket
* List recent operations of an object store bucket
* Delete an object store bucket

## View object stores

**Command:** `weka fs tier obs`

Use this command to view information on all the object stores configured to the WEKA system.

Note: Using the GUI, only object store buckets are present. Adding an object store bucket only adds to the present `local` or `remote` object store. If more than one is present (such as during the time recovering from a remote snapshot), use the CLI.

## Edit an object store

**Command:** `weka fs tier obs update`

Use the following command line to edit an object store:

`weka fs tier obs update <name> [--new-name new-name] [--site site] [--hostname=<hostname>] [--port=<port>] [--auth-method=<auth-method>] [--region=<region>] [--access-key-id=<access-key-id>] [--secret-key=<secret-key>] [--protocol=<protocol>] [--bandwidth=<bandwidth>] [--download-bandwidth=<download-bandwidth>] [--upload-bandwidth=<upload-bandwidth>] [--remove-bandwidth=<remove-bandwidth>] [--max-concurrent-downloads=<max-concurrent-downloads>] [--max-concurrent-uploads=<max-concurrent-uploads>] [--max-concurrent-removals=<max-concurrent-removals>] [--enable-upload-tags=<enable-upload-tags>]`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name * | Name of the object store to create. |
 | new-name | New name for the object store. |
 | site | Site location of the object store.Possible values:local - for tiering+snapshotsremote - for snapshots only |
 | hostname | Object store host identifier (hostname or IP address) to use as a default for added buckets. |
 | port | Object store port, to be used as a default for added buckets. |
 | auth-method | Authentication method to use as a default for added buckets.Possible values: None,AWSSignature2,AWSSignature4 |
 | region | Region name to use as a default for added buckets. |
 | access-key-id | Object store access key ID to use as a default for added buckets. |
 | secret-key | Object store secret key to use as a default for added buckets. |
 | protocol | Protocol type to use as a default for added buckets.Possible values: HTTP,HTTPS,HTTPS_UNVERIFIED |
 | bandwidth | Bandwidth limitation per core (Mbps). |
 | download-bandwidth | Object store download bandwidth limitation per core (Mbps). |
 | upload-bandwidth | Object store upload bandwidth limitation per core (Mbps). |
 | remove-bandwidth | A bandwidth (Mbps) to limit the throughput of delete requests sent to the object store.Setting a bandwidth equal to or lower than the object store deletion throughput prevents an increase in the object store deletions queue. |
 | max-concurrent-downloads | Maximum number of downloads concurrently performed on this object store in a single IO node.Possible values: 1-64 |
 | max-concurrent-uploads | Maximum number of uploads concurrently performed on this object store in a single IO node.Possible values: 1-64 |
 | max-concurrent-removals | Maximum number of removals concurrently performed on this object store in a single IO node.Possible values: 1-64 |
 | enable-upload-tags | Determines whether to enable object-tagging or not. To use as a default for added buckets.Possible values: true,false |

## View object store buckets

**Command:** `weka fs tier s3`

Use this command to view information on all the object store buckets configured to the WEKA system.

## Add an object store bucket

**Command:** `weka fs tier s3 add`

Use the following command line to add an S3 object store:

`weka fs tier s3 add <name> [--site site] [--obs-name obs-name] [--hostname=<hostname>] [--port=<port> [--bucket=<bucket>] [--auth-method=<auth-method>] [--region=<region>] [--access-key-id=<access-key-id>] [--secret-key=<secret-key>] [--protocol=<protocol>] [--bandwidth=<bandwidth>] [--download-bandwidth=<download-bandwidth>] [--remove-bandwidth=<remove-bandwidth>] [--upload-bandwidth=<upload-bandwidth>] [--errors-timeout=<errors-timeout>] [--prefetch-mib=<prefetch-mib>] [--enable-upload-tags=<enable-upload-tags>] [--max-concurrent-downloads=<max-concurrent-downloads>] [--max-concurrent-uploads=<max-concurrent-uploads>] [--max-concurrent-removals=<max-concurrent-removals>] [--max-extents-in-data-blob=<max-extents-in-data-blob>] [--max-data-blob-size=<max-data-blob-size>] [--enable-upload-tags enable-upload-tags] [--data-storage-class data-storage-class] [--metadata-storage-class metadata-storage-class][--sts-operation-type=<sts-operation-type>] [--sts-role-arn=<sts-role-arn>] [--sts-role-session-name=<sts-role-session-name>] [--sts-session-duration=<sts-session-duration>]`

**Parameters**

 | Name | Description | Default |
 | --- | --- | --- |
 | name* | Name of the object store to edit. | ‚Äã |
 | site | local - for tiering+snapshots, remote - for snapshots only.It must be the same as the object store site it is added to (obs-name). | local |
 | obs-name | Name of the existing object store to add this object store bucket to. | If there is only one object store of type mentioned in site it is chosen automatically |
 | hostname * | Object store host identifier or IP.Mandatory, if not specified at the object store level. | The hostname specified in obs-name if present |
 | port | A valid object store port. | The port specified in obs-name if present, otherwise 80 |
 | bucket | A valid object store bucket name. |  |
 | auth-method * | Authentication method.Possible values: None, AWSSignature2, AWSSignature4.Mandatory, if not specified in the object store level . | The auth-method specified in obs-name if present |
 | region * | Region name.Mandatory, if not specified in the object store level . | The region specified in obs-name if present |
 | access-key-id * | Object store bucket access key ID.Mandatory, if not specified in the object store level (can be left empty when using IAM role in AWS or GCP). | The access-key-id specified in obs-name if present |
 | secret-key * | Object store bucket secret key.Mandatory, if not specified in the object store level (can be left empty when using IAM role in AWS or GCP). | The secret-key specified in obs-name if present |
 | protocol | Protocol type to be used.Possible values: HTTP, HTTPS or HTTPS_UNVERIFIED. | The protocol specified in obs-name if present, otherwiseHTTP |
 | bandwidth | Bucket bandwidth limitation per core (Mbps). |  |
 | download-bandwidth | Bucket download bandwidth limitation per core (Mbps) |  |
 | upload-bandwidth | Bucket upload bandwidth limitation per core (Mbps) |  |
 | remove-bandwidth | A bandwidth (Mbps) to limit the throughput of delete requests sent to the object store.Setting a bandwidth equal to or lower than the object store deletion throughput prevents an increase in the object store deletions queue. |  |
 | errors-timeout | If the object store link is down longer than this timeout period, all IOs that need data return an error.Possible values: 1m-15m, or 60s-900s.For example, 300s. | 300s |
 | prefetch-mib | The data size (MiB) to prefetch when reading a whole MiB on the object store. | 128 |
 | enable-upload-tags | Whether to enable object-tagging or not.Possible values: true or false | false |
 | max-concurrent-downloads | Maximum number of downloads we concurrently perform on this object store in a single IO node.Possible values: 1-64 |  |
 | max-concurrent-uploads | Maximum number of uploads we concurrently perform on this object store in a single IO node.Possible values: 1-64 |  |
 | max-concurrent-removals | Maximum number of removals we concurrently perform on this object store in a single IO node.Possible values: 1-64 |  |
 | max-extents-in-data-blob | Maximum number of extents' data to upload to an object store data blob. |  |
 | max-data-blob-size | Maximum size to upload to an object store data blob.Format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB. |  |
 | enable-upload-tags | Enable tagging of uploaded objects. For details, see object-tagging. |  |
 | data-storage-class | AWSConfigurable Amazon S3 storage classes, allowing users to optimize storage based on cost and access needs. Supports STANDARD, REDUCED_REDUNDANCY, STANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, OUTPOSTS, GLACIER_IR, and EXPRESS_ONEZONE. For details, see the documentation for Amazon S3 Storage Classes.AzureConfigurable Azure access storage tier, allowing users to optimize storage based on cost and access needs. Supports HOT, COOL, and COLD. For details, see the documentation for Azure Access tiers for blob data. |  |
 | metadata-storage-class | AWSConfigurable storage classes for the metadata on AWS. AzureConfigurable Azure access storage tier for metadata. Supports HOT, COOL, and COLD. |  |
 | sts-operation-type | AWS STS operation type to use.Possible values: assume_role or none | none |
 | sts-role-arn | The Amazon Resource Name (ARN) of the role to assume. Mandatory when setting sts-operation to assume_role. |  |
 | sts-role-session | A unique identifier for the assumed role session.The length must be between 2 and 64 characters. Allowed characters include alphanumeric characters (upper and lower case), underscore (_), equal sign (=), comma (,), period (.), at symbol (@), and hyphen (-). Space is not allowed. |  |
 | sts-session-duration | The duration of the temporary security credentials in seconds.Possible values: 900 - 43200. | 3600 |

Note: When using the CLI, by default a misconfigured object store are not created. To create an object store even when it is misconfigured, use the `--skip-verification` option.

Note: The `max-concurrent` settings are applied per WEKA compute process and the minimum setting of all object stores is applied.

Note: When you create the object store bucket in AWS, to use the storage classes: S3 Intelligent-Tiering, S3 Standard-IA, S3 One Zone-IA, and S3 Glacier Instant Retrieval, do the following:
1. Create the bucket in S3 Standard.
2. Create an AWS lifecycle policy to transition objects to these storage classes.
3. Make the relevant changes and click **Update** to update the object store bucket.

## Edit an object store bucket

**Command:** `weka fs tier s3 update`

Use the following command line to edit an object store bucket:

`weka fs tier s3 update <name> [--new-name=<new-name>] [--new-obs-name new-obs-name] [--hostname=<hostname>] [--port=<port> [--bucket=<bucket>] [--auth-method=<auth-method>] [--region=<region>] [--access-key-id=<access-key-id>] [--secret-key=<secret-key>] [--protocol=<protocol>] [--bandwidth=<bandwidth>] [--download-bandwidth=<download-bandwidth>] [--upload-bandwidth=<upload-bandwidth>] [--remove-bandwidth=<remove-bandwidth>] [--errors-timeout=<errors-timeout>] [--prefetch-mib=<prefetch-mib>] [--enable-upload-tags=<enable-upload-tags>] [--max-concurrent-downloads=<max-concurrent-downloads>] [--max-concurrent-uploads=<max-concurrent-uploads>] [--max-concurrent-removals=<max-concurrent-removals>] [--max-extents-in-data-blob=<max-extents-in-data-blob>] [--max-data-blob-size=<max-data-blob-size>] [--sts-operation-type=<sts-operation-type>] [--sts-role-arn=<sts-role-arn>] [--sts-role-session-name=<sts-role-session-name>] [--sts-session-duration=<sts-session-duration>]`

**Parameters**

 | Name | Value |
 | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `name`* | A valid name of the object store bucket to edit. |
 | `new-name` | New name for the object store bucket |
 | `new-obs-name` | A new object store name to add this object store bucket to. It must be an existing object store with the same `site` value. |
 | `hostname` | Object store host identifier or IP. |
 | `port` | A valid object store port |
 | `bucket` | A valid object store bucket name |
 | `auth-method` | Authentication method. Possible values: None , AWSSignature2 or AWSSignature4 |
 | `region` | Region name |
 | `access-key-id` | Object store bucket access key ID |
 | `secret-key` | Object store bucket secret key |
 | `protocol` | Protocol type to be used. Possible values: HTTP , HTTPS or HTTPS_UNVERIFIED |
 | `bandwidth` | Bandwidth limitation per core (Mbps) |
 | `download-bandwidth` | Bucket download bandwidth limitation per core (Mbps) |
 | `upload-bandwidth` | Bucket upload bandwidth limitation per core (Mbps) |
 | `remove-bandwidth` | A bandwidth (Mbps) to limit the throughput of delete requests sent to the object store. Setting a bandwidth equal to or lower than the object store deletion throughput prevents an increase in the object store deletions queue. |
 | `errors-timeout` | If the object store link is down longer than this timeout period, all IOs that need data return an error. Possible values: 1m - 15m , or 60s - 900s . For example, 300s . |
 | `prefetch-mib` | The data size in MiB to prefetch when reading a whole MiB on the object store |
 | `enable-upload-tags` | Whether to enable <a href="../../tiering/data-management-in-tiered-filesystems#object-tagging">object-tagging</a> or not. Possible values: true , false |
 | `max-concurrent-downloads` | Maximum number of downloads we concurrently perform on this object store in a single IO node. Possible values: 1 - 64 |
 | `max-concurrent-uploads` | Maximum number of uploads we concurrently perform on this object store in a single IO node. Possible values: 1 - 64 |
 | `max-concurrent-removals` | Maximum number of removals we concurrently perform on this object store in a single IO node. Possible values: 1 - 64 |
 | `max-extents-in-data-blob` | Maximum number of extents' data to upload to an object store data blob. |
 | `max-data-blob-size` | Maximum size to upload to an object store data blob. Format: capacity in decimal or binary units: 1B, 1KB, 1MB, 1GB, 1TB, 1PB, 1EB, 1KiB, 1MiB, 1GiB, 1TiB, 1PiB, 1EiB. |
 | `sts-operation-type` | AWS <a data-footnote-ref href="#user-content-fn-1">STS</a> operation type to use. Possible values: assume_role or none |
 | `sts-role-arn` | The Amazon Resource Name (ARN) of the role to assume. Mandatory when setting `sts-operation` to `assume_role`. |
 | `sts-role-session` | A unique identifier for the assumed role session. The length must be between 2 and 64 characters. Allowed characters include alphanumeric characters (upper and lower case), underscore (_), equal sign (=), comma (,), period (.), at symbol (@), and hyphen (-). Space is not allowed. |
 | `sts-session-duration` | The duration of the temporary security credentials in seconds. Possible values: 900 - 43200 . |

## List recent operations of an object store bucket

**Command:** `weka fs tier ops`

Use the following command line to list the recent operations running on an object store:

`weka fs tier ops <name> [--format format] [--output output]...[--sort sort]...[--filter filter]...[--raw-units] [--UTC] [--no-header] [--verbose]`

**Parameters**

 | Name | Value | Default |
 | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- |
 | `name`* | A valid object store bucket name to show its recent operations. | ‚Äã |
 | `format` | Specify the output format. Possible values: view , csv , markdown , json , or oldview | `view` |
 | `output` | Specify the columns in the output. Possible values: node , obsBucket , key , type , execution , phase , previous , start , size , results , errors , lastHTTP , concurrency , inode | All columns |
 | `sort` | Specify the column(s) to consider when sorting the output. For the sorting order, ascending or descending, add - or + signs respectively before the column name. |  |
 | `filter` | Specify the values to filter by in a specific column. Usage: `column1=val1[,column2=val2[,..]]` |  |
 | `raw-units` | Print values in a readable format of raw units such as bytes and seconds. Possible value examples: 1KiB 234MiB 2GiB . |  |
 | `no-header` | Don't show column headers in the output, |  |
 | `verbose` | Show all columns in the output. |  |

## Delete an object store bucket

**Command:** `weka fs tier s3 delete`

Use the following command line to delete an object store bucket:

`weka fs tier s3 delete <name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | A valid name of the object store bucket to delete. |

<!-- ============================================ -->
<!-- File 89/259: weka-filesystems-and-object-stores_attaching-detaching-object-stores-to-from-filesystems.md -->
<!-- ============================================ -->

---
description:
---

# Attach or detach object store buckets

## Attachment of a local object store bucket to a filesystem

Two local object store buckets can be attached to a filesystem, but only one of the buckets is writable. A local object store bucket is used for both tiering and snapshots. When attaching a new local object store bucket to an already tiered filesystem, the existing local object store bucket becomes read-only, and the new object store bucket is read/write. Multiple local object stores allow a range of use cases, including migration to different object stores, scaling of object store capacity, and increasing the total tiering capacity of filesystems.

When attaching a local object store bucket to a non-tiered filesystem, the filesystem becomes tiered.

## Detachment of a local object store bucket from a filesystem

Detaching a local object store bucket from a filesystem migrates the filesystem data residing in the object store bucket to the writable object store bucket (if one exists) or to the SSD.

When detaching, the background task of detaching the object store bucket begins. Detaching can be a long process, depending on the amount of data and the load on the object stores.

Note: Detaching an object store bucket is irreversible. Attaching the same bucket again is considered as re-attaching a new bucket regardless of the data stored in the bucket.

* **Migration to a different object store:**  When detaching from a filesystem tiered to two local object store buckets, only the read-only object store bucket can be detached. In such cases, the background task copies the relevant data to the writable object store. In addition, the allocated SSD capacity only requires enough SSD capacity for the metadata.
* **Un-tiering a filesystem:** Detaching from a filesystem tiered to one object store bucket un-tiers the filesystem and copies the data back to the SSD. The allocated SSD capacity must be at least the total capacity the filesystem uses.

On completion of detaching, the object store bucket does not appear under the filesystem when using the `weka fs` command. However, it still appears under the object store and can be removed if any other filesystem does not use it. The data in the read-only object store bucket remains in the object store bucket for backup purposes. If this is unnecessary or the reclamation of object store space is required, it is possible to delete the object store bucket.

Note: Before deleting an object store bucket, remember to consider data from another filesystem or data not relevant to the WEKA system on the object store bucket.

Note: Once the migration process is completed, while relevant data is migrated, old snapshots (and old locators) reside on the old object store bucket. To recreate snapshot locators on the new object store bucket, snapshots should be re-uploaded to the (new) bucket.

## Migration considerations

When migrating data (using the detach operation), copy only the necessary data (to reduce migration time and capacity). However, you may want to keep snapshots in the old object store bucket.

**Migration workflow**

The order of the following steps is important.

1. Attach a new object store bucket (the old object store bucket becomes read-only).
2. Delete any snapshot that does not need to be migrated. This action keeps the snapshot on the old bucket but does not migrate its data to the new bucket.
3. Detach the old object store bucket.

Note: If you perform the workflow steps in a different order, the snapshots can be completely deleted from any of the object store buckets. It is also possible that the snapshots are already in a migration process and cannot be deleted until the migration is completed.

### Attach a remote object store bucket

One remote object store bucket can be attached to a filesystem. A remote object store bucket is used for backup. Only snapshots are uploaded using **Snap-To-Object**. The snapshot uploads are incremental to the previous one.

### Detach a remote object store bucket

Detaching a remote object store bucket from a filesystem keeps the backup data within the bucket intact. It is still possible to use these snapshots for recovery.

**Related topics**

<!-- ============================================ -->
<!-- File 90/259: weka-filesystems-and-object-stores_attaching-detaching-object-stores-to-from-filesystems_attaching-detaching-object-stores-to-from-filesystems.md -->
<!-- ============================================ -->

---
description:
---

# Attach or detach object store bucket using the GUI

Using the GUI, you can:

* Attach object store bucket to a filesystem
* Detach object store bucket from a filesystem

## Attach object store bucket to a filesystem

**Before you begin**

Verify that an object store bucket is available.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. On the **Filesystem** page, select the three dots on the right of the filesystem that you want to attach to the object store bucket. Then, from the menu, select **Attach Object Store Bucket**.
3. On the Attach Object Store Bucket dialog, select the relevant object store bucket.

## Detach object store bucket from a filesystem

Detaching a local object store bucket from a filesystem migrates the filesystem data residing in the object store bucket either to the writable object store bucket (if one exists) or to the SSD.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. On the **Filesystem** page, select the filesystem from which you want to detach the object store bucket.
3. From the **Detach Object Store Bucket** dialog, select **Detach.**\
   If the filesystem is attached to two object store buckets (one is read-only, and the other is writable), you can detach only the read-only one. The data of the detached object store bucket is migrated to the writable object store bucket.
4. In the message that appears, to confirm the detachment, select **Yes**.

5. If the filesystem is tiered and only one object store is attached, detaching the object store bucket opens the following message:

6. Object store buckets usually expand the filesystem capacity. Un-tiering of a filesystem requires adjustment of its total capacity. Select one of the following options:
   * Increase the SSD capacity to match the current total capacity.
   * Reduce the total filesystem capacity to match the SSD or used capacity (the decrease option depends on the used capacity).
   * Configure a different capacity.

Note: Used capacity must be taken into account. Un-tiering takes time to propagate the data from the object store to the SSD. When un-tiering an active filesystem, to accommodate the additional writes during the detaching process, it is recommended to adjust to a higher value than the used capacity.

7. Select the option that best meets your needs, and select **Continue**.
8. In the message that appears, select **Detach** to confirm the action.

<!-- ============================================ -->
<!-- File 91/259: weka-filesystems-and-object-stores_attaching-detaching-object-stores-to-from-filesystems_attaching-detaching-object-stores-to-from-filesystems-1.md -->
<!-- ============================================ -->

---
description:
---

# Attach or detach object store buckets using the CLI

Using the CLI, you can:

* Attach an object store bucket to a filesystem
* Detach an object store bucket from a filesystem

## **Attach an object store bucket** to a filesystem

**Command:** `weka fs tier s3 attach`

To attach an object store to a filesystem, use the following command:

`weka fs tier s3 attach <fs-name> <obs-name> [--mode mode]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | fs-name* | Name of the filesystem to attach with the object store. | ‚Äã |
 | obs-name* | Name of the object store to attach. |  |
 | mode | The operational mode for the object store bucket.The possible values are:writable: Local access for read/write operations.remote: Read-only access for remote object stores. | writable |

## **Detach an object store bucket** from a filesystem

**Command:** `weka fs tier s3 detach`

To detach an object store from a filesystem, use the following command:

`weka fs tier s3 detach <fs-name> <obs-name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | fs-name* | Name of the filesystem to be detached from the object store |
 | obs-name* | Name of the object store to be detached |

Note: To [recover from a snapshot](../../snap-to-obj#creating-a-filesystem-from-a-snapshot-using-the-cli) uploaded when two `local` object stores have been attached, use the `--additional-obs` parameter in the `weka fs download` command. The primary object store should be the one where the locator has been uploaded to

<!-- ============================================ -->
<!-- File 92/259: weka-filesystems-and-object-stores_tiering.md -->
<!-- ============================================ -->

---
description:
---

# Advanced data lifecycle management

This section explains how data lifecycle is maintained when working with a tiered WEKA system configuration, together with the options for control. The following subjects are covered:

* Advanced explanation of [time-based policies for data storage location](tiering/advanced-time-based-policies-for-data-storage-location).
* System behavior when [tiering, accessing or deleting data in tiered filesystems](tiering/data-management-in-tiered-filesystems).
* [Transition between SSD-only and tiered filesystems.](tiering/transition-between-tiered-and-ssd-only-filesystems)
* How to [manually pre-fetch tiered data from an object-store and release of data from SSD to the object-store](tiering/pre-fetching-from-object-store).

<!-- ============================================ -->
<!-- File 93/259: weka-filesystems-and-object-stores_tiering_data-management-in-tiered-filesystems.md -->
<!-- ============================================ -->

---
description:
---

# Data management in tiered filesystems

## Overview

In tiered filesystems, the WEKA system optimizes storage efficiency and manages storage resources effectively by:

* Tiering only infrequently accessed portions of files (warm data), keeping hot data on SSDs.
* Efficiently bundling subsets of different files (to 64 MB objects) and tiering them to object stores, resulting in significant performance enhancements.
* Retrieving only the necessary data from the object store when accessing it, regardless of the entire object it was originally tiered with.
* Reclaiming logically freed data occurs when data is modified or deleted and is not used by any snapshots. Reclamation is a process of freeing up storage space that was previously allocated to data that is no longer needed.

Note: Only data that is not logically freed is considered for licensing purposes.

## SSD space reclamation in tiered filesystems

For logically freed data that resides on the SSD, the WEKA system immediately deletes the data from the SSD, leaving the physical space reclamation for the SSD erasure technique.

## Object store space reclamation in tiered filesystems

Object store space reclamation is an important process that efficiently manages data stored on object storage.

Note: In the WEKA system, object store space reclamation is only relevant for object store buckets used for tiering (`local`) and not for buckets used for backup only (`remote`).

WEKA organizes sections of files into objects for tiering, with the default maximum object size capped at 64 MB. Each object can contain data from multiple files. Files smaller than 1 MB are consolidated into a single object, while larger files are distributed across multiple objects. When a file is deleted (or updated and is not used by any snapshots), the space within one or more objects is marked as available for reclamation. However, these objects are only deleted under specific conditions.

Deleting related objects happens when all associated files are deleted, allowing for complete space reclamation within the object or during the reclamation process. Reclamation entails reading an eligible object from object storage and packing the active portions (representing data from undeleted files) with sections from other files that must be written to the object store. The resulting object is then written back to the object store, freeing up reclaimed space.

WEKA automates the reclamation process by monitoring the filesystems. When the reclaimable space within a filesystem exceeds 13%, the reclamation process begins. It continues until the total reclaimable space drops below 7%. This mechanism prevents write amplifications, allows time for higher portions of eligible 64 MB objects to become logically free, and prevents unnecessary object storage workload for small space reclamation. It's important to note that reclamation is only executed for objects with reclaimable space exceeding 5% within that object.

Note: To calculate the amount of space that can be reclaimed, consider the following examples:
1. If we write 1 TB of data, and 15% of that space can be reclaimed, we have 150 GB of reclaimable space.
2. If we write 10 TB of data, and 5% of that space can be reclaimed, we have 500 GB of reclaimable space.
The starting point for the reclamation process differs in each example. In example 1, reclamation begins at 130 GB (13%), while in example 2, it doesn't start. This is important to note because even though there is more total reclaimable space in example 2, the process starts later.

For regular filesystems where files are frequently deleted or updated, this behavior can result in the consumption of 7% to 13% more object store space than initially expected based on the total size of all files written to that filesystem. When planning object storage capacity or configuring usage alerts, it's essential to account for this additional space. Remember that this percentage may increase during periods of high object store usage or when data/snapshots are frequently deleted. Over time, it will return to the normal threshold as the load/burst is reduced.

Note: If the filesystem was created from a snapshot, only the data uploaded to the object store after the new filesystem was created can be reclaimed. Pre-existing data from the original snapshot is unreclaimable. To ensure all data is reclaimable, migrate the restored filesystem to a new bucket. For details, see .

Note: If tuning of the system interaction with the object store is required, such as object size, reclamation threshold numbers, or the object store space reclamation is not fast enough for the workload, contact the Customer Success Team.

### View object store bucket capacity details

Run the `weka fs tier capacity` command to retrieve a comprehensive listing of data capacities associated with object store buckets per filesystem.

Note: If the filesystem was created from an uploaded snapshot, data from the original filesystem is not accounted for in the displayed capacity.

Example:

```
$ weka fs tier capacity
FILESYSTEM  BUCKET               TOTAL CONSUMED CAPACITY   USED CAPACITY   RECLAIMABLE%   RECLAIMABLE THRESHOLD%
bmrb        wekalow-bmrb         0 B                       0 B             0.00           10.00
cam_archive wekalow-archive      20.39 TB                  18.80 TB        7.79           10.00
nmr_backup  wekalow-nmrbackup    519.07 GB                 518.05 GB       0.19           10.00

```

Where:

* **TOTAL CONSUMED CAPACITY**: The total storage allocated or provisioned for the filesystem, including space for data, metadata, and system overhead.
* **USED CAPACITY**: The storage actively used by data written to the filesystem.
* **RECLAIMABLE%**: Represents the percentage of storage in the tier that can potentially be reclaimed, typically consisting of data that is no longer actively used or referenced but has not yet been deleted or overwritten.
* **RECLAIMABLE THRESHOLD%**: The predefined percentage at which the system begins prioritizing the reclamation of reclaimable storage to free up capacity.

To list the data capacities of a specific filesystem, add the option `--filesystem <filesystem name>`.

Example:

```
$ weka fs tier capacity --filesystem cam_archive
FILESYSTEM  BUCKET               TOTAL CONSUMED CAPACITY   USED CAPACITY   RECLAIMABLE%   RECLAIMABLE THRESHOLD%
cam_archive wekalow-archive      20.39 TB                  18.80 TB        7.79           10.00

```

## Object tagging

When WEKA uploads objects to the object store, it assigns tags to categorize them. These tags are crucial because they enable the customer to implement specific lifecycle management rules in the object store based on the assigned tags.

For example, you can transfer objects of a specific filesystem when interacting with the S3 Glacier Deep Archive.

To enable upload tags, set it when adding or updating the object store bucket. For details, see the following:

* Using the GUI:
  * #add-an-object-store-bucket, or
  * #edit-an-object-store-bucket by selecting **Enable Upload Tags** in the Advanced section.
* Using the CLI:
  * #add-an-object-store-bucket, or
  * #edit-an-object-store-bucket by setting the `enable-upload-tags` parameter in `weka fs tier s3 add/update` commands.

The following table indicates the additional tags WEKA adds to the object when using object tagging:

 | Tag | Description |
 | --- | --- |
 | wekaBlobType | The WEKA-internal type representation of the object.Possible values: DATA, METADATA, METAMETADATA, LOCATOR, RELOCATIONS |
 | wekaFsId | A unique filesystem ID (a combination of the filesystem ID and the cluster GUID). |
 | wekaGuid | The cluster GUID. |
 | wekaFsName | The filesystem name that uploaded this object. |

The object store must support S3 object-tagging and might require additional permissions to use object tagging.

For example, the following extra permissions are required in AWS S3:

* `s3:PutObjectTagging`
* `s3:DeleteObjectTagging`

Note: Additional charges may apply by your cloud service provider.

<!-- ============================================ -->
<!-- File 94/259: weka-filesystems-and-object-stores_tiering_transition-between-tiered-and-ssd-only-filesystems.md -->
<!-- ============================================ -->

---
description:
---

# Transition between tiered and SSD-only filesystems

### The transition from SSD-only filesystem to tiered filesystem

An SSD-only filesystem can be reconfigured as a tiered filesystem by attaching an object store. In such a situation, the default is to maintain the filesystem size. In order to increase the filesystem size, the total capacity field can be modified, while the existing SSD capacity remains the same.

Note: Once an SSD-only filesystem has been reconfigured as a tiered filesystem, all existing data will be considered to belong to interval 0 and will be managed according to the 7-interval process. This means that the release process of the data created before the reconfiguration of the filesystem is performed in an arbitrary order and does not depend on the timestamps.

### The transition from tiered filesystem to SSD-only filesystem

A tiered filesystem can be un-tiered (and only use SSDs) by detaching its object stores. This will copy the data back to the SSD.

Note: The SSD must have sufficient capacity, i.e., the allocated SSD capacity should be at least the total capacity used by the filesystem.

For more information, refer to [Attaching/Detaching Object Stores Overview](../../attaching-detaching-object-stores-to-from-filesystems#overview).

<!-- ============================================ -->
<!-- File 95/259: weka-filesystems-and-object-stores_tiering_pre-fetching-from-object-store.md -->
<!-- ============================================ -->

---
description:
---

# Manual fetch and release of data

## Pre-fetching API for data lifecycle management

### Fetch files from an object store

Tiered files are always accessible and are generally treated like regular files. Moreover, while files may be tiered, their metadata is always maintained on the SSDs. This allows traversing files and directories without worrying about how such operations may affect performance.

Sometimes, it's necessary to access previously-tiered files quickly. In such situations, you can request the WEKA system to fetch the files back to the SSD without accessing them directly.

**Command:** `weka fs tier fetch`

Use the following command to fetch files:

`weka fs tier fetch <path> [-v]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | path* | A comma-separated list of file paths. | ‚Äã |
 | -v, --verbose | Showing fetch requests as they are submitted. | Off |

### Fetch a directory containing many files

To fetch a directory that contains a large number of files, it is recommended to use the `xargs` command in a similar manner as follows:

```bash
| find -L <directory path> -type f | xargs -r -n512 -P64 weka fs tier fetch -v |
```

Note: The pre-fetching of files does not guarantee that they will reside on the SSD until they are accessed.

To ensure effective fetch, adhere to the following:

* **Free SSD capacity**: The SSD has sufficient free capacity to retain the fetched filesystems.
* **Tiering policy**: The tiering policy may release some of the files back to the object store after they have been fetched, or during the fetch if it takes longer than expected. The tiering policy must be long enough to allow for the fetch to complete and the data to be accessed before it is released again.

## Release API for data lifecycle management

### Release files from SSD to an object store

Using the manual release command, it is possible to clear SSD space in advance (e.g., for shrinking one filesystem SSD capacity for a different filesystem without releasing important data, or for a job that needs more SSDs space from different files). The metadata will still remain on SSD for fast traversal over files and directories but the data will be marked for release and will be released to the object store as soon as possible, and before any other files are scheduled to release due to other lifecycle policies.

**Command:** `weka fs tier release [-v]`

Use the following command to release files:

`weka fs tier release <path>`

**Parameters**

 | Name | Value | Default |
 | --------------- | ---------------------------------------------- | ------- |
 | `path`* | A comma-separated list of file paths. | ‚Äã |
 | `-v, --verbose` | Showing release requests as they are submitted | Off |

### Release a directory containing many files

To release a directory that contains a large number of files, it is recommended to use the `xargs` command in a similar manner, as follows:

```bash
# directory
| find -L <directory path> -type f | xargs -r -n512 -P64 weka fs tier release |

# similarly, a file containing a list of paths can be used
| cat file-list | xargs -P32 -n200 weka fs tier release |
```

## Find the location of tiered files

Depending on the retention period in the tiering policy, files can be found on the object store or the SSD or both locations as follows:

* Before the file is tiered to the object store, it is found in the SSD.
* During data tiering, the tiered data is on the SSD (read cache) and the object store.
* Once the entire file data is tiered and the retention period has past, the complete file is found in the object store only.

Use this command to find the file location during the data lifecycle operations.

**Command:** `weka fs tier location`

Use the following command to find files:

`weka fs tier location <path>`

For multiple paths, use the following command:

`weka fs tier location <paths>`

To find all files in a single directory, use the following command:

`weka fs tier location *`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | path* | A path to get information about. | ‚Äã |
 | paths | Space-separated list of paths to get information about. |  |

#### Examples of a tiered file location during the data lifecycle management

1. Before the file named `image` is tiered to the object store, it is found in the SSD (WRITE-CACHE).

```
[root@kenny-0 weka] 2023-07-13 14:57:11 $ weka fs tier location image
PATH   FILE TYPE  FILE SIZE  CAPACITY IN SSD (WRITE-CACHE)  CAPACITY IN SSD (READ-CACHE)  CAPACITY IN OBJECT STORAGE  CAPACITY IN REMOTE STORAGE
image  regular    102.39 MB  102.39 MB                      0 B                           0 B                         0 B

```

2. The file is tiered and the retention period has not past yet, so the file is found in the SSD (READ-CACHE) and the object store.

```
[root@kenny-0 weka] 2023-07-13 14:58:14 $ weka fs tier location image
PATH   FILE TYPE  FILE SIZE  CAPACITY IN SSD (WRITE-CACHE)  CAPACITY IN SSD (READ-CACHE)  CAPACITY IN OBJECT STORAGE  CAPACITY IN REMOTE STORAGE
image  regular    102.39 MB  0 B                            102.39 MB                     102.39 MB                   0 B

```

3. The file is tiered and the retention period past, so the file is found in the object store only.

```
[root@kenny-0 weka] 2023-07-13 14:59:14 $ weka fs tier location image
PATH   FILE TYPE  FILE SIZE  CAPACITY IN SSD (WRITE-CACHE)  CAPACITY IN SSD (READ-CACHE)  CAPACITY IN OBJECT STORAGE  CAPACITY IN REMOTE STORAGE
image  regular    102.39 MB  0 B                            0 B                     102.39 MB                   0 B

```

<!-- ============================================ -->
<!-- File 96/259: weka-filesystems-and-object-stores_tiering_advanced-time-based-policies-for-data-storage-location.md -->
<!-- ============================================ -->

---
description:
---

# Advanced time-based policies for data storage location

This page provides an in-depth explanation for the [Data Lifecycle Management](../../../weka-system-overview/data-storage#time-based-policies-for-the-control-of-data-storage-location) overview section.

## Drive retention period policy

The Drive Retention Period policy refers to the amount of time you want to keep a copy of the data on SSD that you previously offloaded/copied to the object storage via the Tiering Cue Policy described further below.

Consider a scenario of a 100 TB filesystem (total capacity), with 100 TB of SSD space (as explained in[ ](../../../weka-system-overview/data-storage#the-role-of-ssds-in-tiered-weka-configurations)[The role of SSDs in tiered configurations](../../../weka-system-overview/data-storage#the-role-of-ssds-in-tiered-configurations) section). If the data Drive Retention Period policy is defined as 1 month and only 10 TB of data are written per month, it will probably be possible to maintain data from the last 10 months on the SSDs. On the other hand, if 200 TB of data is written per month, it will only be possible to maintain data from half of the month on the SSDs. Additionally, there is no guarantee that the data on the SSDs is the data written in the last 2 weeks of the month, which also depends on the Tiering Cue.

To further help describe this section, let us use an example where the Tiering Cue Policy described below is set to 1 day, and the Drive Retention Period is set to 3 days. After one day, the WEKA system offloads period 0‚Äôs data to the object store. Setting the Drive Retention Period to 3 days means leaving a copy of that data in WEKA Cache for three days, and after three days, it is removed from the WEKA Cache. The data is not gone, it is on the object store, and if an application or a user accesses that data, it is pulled back from the object store and placed back on the WEKA SSD tier where it is tagged again with a new Tiering Cue Policy Period.

Consequently, the drive Retention Period policy determines the resolution of the WEKA system release decisions. If it is set to 1 month and the SSD capacity is sufficient for 10 months of writing, then the first month will be kept on the SSDs.

Note: If the WEKA system cannot comply with the defined Retention Period, for example, the SSD is full, and data is not released to the object store, a Break In Policy event occurs. In such a situation, an event is received in the WEKA system event log, advising that the system has not complied with the policy and that data has been automatically released from the SSD to the object store before the completion of the defined Retention Period. No data will be lost (since the data has been transferred to the object store), but slower performance may be experienced.

Note: If the data writing rate is always high and the WEKA system fails to successfully release the data to the object store, an Object Store Bottleneck will occur. If the bottleneck continues, this will also result in a Policy Violation event.

## Tiering cue policy

The Tiering Cue policy defines the period of time to wait before the data is copied from the SSD and sent to the object store. It is typically used when it is expected that some of the data being written will be rewritten/modified/deleted in the short term.

The WEKA system integrates a rolling progress control with three rotating periods of 0, 1, and 2.

1. Period 0: All data written is tagged as written in the current period.
2. Period 1: The switch from 0 to 1 is according to the Tiering Cue policy.
3. Period 2: Starts after the period of time defined in the Tiering Cue, triggering the transfer of data written in period 0 from the SSD to the object store.

Note: Not all data is transferred to the object store in the order that it was written. If, for example, the Tiering Cue is set to 1 month, there is no priority or order in which the data from the whole month is released to the object store; data written at the end of the month may be released to the object store before data written at the beginning of the month.

Note: **Example:**
If the Tiering Cue Policy is set to 1 day, all data written within the first day is tagged for Period 0. After one day, and for the next day, the next set of data is tagged for Period 1, and the data written the next day is tagged for Period 2.
As Period 0 rolls around to be next, the data marked for Period 0 is offloaded to the object store, and new data is then tagged for Period 0. When Period 1 rolls around to be next, it is time to offload the data tagged for Period 1 to the object store and so on.

One important caveat to mention is that in the above example, if none of the data is touched or modified during the time set for the Tiering Cue Policy, then all the data as described will offload to the object store as planned. But let‚Äôs say there is some data in Period 0 that was updated/modified, that data is pulled out of Period 0 and is then tagged with the current Period of data being written at the moment, let‚Äôs say that is Period 2. So now, that newly modified data will not get offloaded to the object store until it is Period 2‚Äôs time. This is true for any data modified residing in one of the 3 Period cycles. It will be removed from its original Period and placed into the current Period marking the active writes.

## Management of drive retention policies <a href="#management-of-data-retention-policies" id="management-of-data-retention-policies"></a>

Since the WEKA system is a highly scalable data storage system, data storage policies in tiered WEKA configurations cannot be based on cluster-wide FIFO methodology, because clusters can contain billions of files. Instead, drive retention is managed by time-stamping every piece of data, where the timestamp is based on a resolution of intervals that may extend from minutes to weeks. The WEKA system maintains the interval in which each piece of data was created, accessed, or last modified.

Users only specify the Drive Retention Period and based on this, each interval is one-quarter of the Drive Retention Period. Data written, modified, or accessed prior to the last interval is always released, even if SSD space is available.

Note: The timestamp is maintained per piece of data in chunks of up to 1 MB, and not per file. Consequently, different parts of big files may have different tiering states.

Note: **Example:**
In a WEKA system configured with a Drive Retention Period of 20 days, data is split into 7 interval groups, each spanning 5 days in this scenario (5 is 25% of 20, the Drive Retention Period).
If the system starts operating on January 1, data written, accessed, or modified between January 1-5 are classified as belonging to interval 0, data written, accessed, or modified between January 6-10 belongs to interval 1, and so on. In such a case, the 7 intervals will be timestamped and divided as follows:

In the above scenario, there are seven data intervals on the SSDs (the last one is accumulating new/modified data). In addition, another interval is currently being released to the object-store. Yes, the retention period is almost twice as long as the user specifies, as long as there is sufficient space on the SSD. Why? If possible, it provides better performance and reduces unnecessary release/promotion of data to/from the object-store if data is modified.

## Data release process from SSD to object store <a href="#data-release-process-from-ssd-to-object-store" id="data-release-process-from-ssd-to-object-store"></a>

At any given moment, the WEKA system releases the filesystem data of a single interval, transferring it from the SSD to the object-store. _The release process is based on data aging characteristics_ (as implemented through the intervals system and revolving tags). Consequently, if there is sufficient SSD capacity, only data modified or written before seven intervals will be released. The release process also considers the amount of available SSD capacity through the mechanism of _**Backpressure**_. Backpressure works against two watermarks - 90% and 95%. It kicks in when SSD utilization per file system crosses above 95% and stops when it crosses below 90%. It's also important to understand that _Backpressure_ works in parallel and **independently** of the _Tiering Policy_. If the SSD utilization crosses the 95% watermark, then data will be released from SSD and sent to the object-store sooner than was configured.

Note: **Example:**
If 3 TB of data is produced every day, i.e., 15 TB of data in each interval, the division of data will be as follows:

Now consider a situation where the total capacity of the SSD is 100 TB. The situation in the example above will be as follows:

Since the resolution in the WEKA system is the interval, in the example above the SSD capacity of 100 TB is insufficient for all data written over the defined 35-day Retention Period. Consequently, the oldest, most non-accessed, or modified data, has to be released to the object store. In this example, this release operation will have to be performed in the middle of interval 6 and will involve the release of data from interval 0.

This counting of the age of the data in resolutions of 5 days is performed according to 8 different categories. A constantly rolling calculation, the following will occur in the example above:

* Data from days 1-30 (January 1-30) will all be on the SSD. Some of it may be tiered to the object store, depending on the defined Tiering Cue.
* Data from more than 35 days will be released to the object store.
* Data from days 31-35 (January 31-February 4) will be partially on the SSD and partially tiered to the object store. However, there is no control over the order in which data from days 31-35 is released to the object store.

Note: **Example:** If no data has been accessed or modified since creation, then the data from interval 0 will be released and the data from intervals 1-6 will remain on the SSDs. If, on the other hand, 8 TB of data is written every day, meaning that 40 TB of data is written in each interval (as shown below), then the last two intervals, i.e., data written, accessed, or modified in a total of 10 days will be kept on the SSD, while other data will be released to the object-store.

Now consider the following filesystem scenario, where the whole SSD storage capacity of 100 TB is utilized in the first 3 intervals:

When much more data is written and there is insufficient SSD capacity for storage, the data from interval 0 will be released when the 100 TB capacity is reached. This represents a violation of the Retention Period. In such a situation, it is also possible to either increase the SSD capacity or reduce the Retention Period.

## Tiering cue <a href="#tiering-cue" id="tiering-cue"></a>

The tiering process (the tiering of data from the SSDs to the object stores) is based on when data is created or modified. It is managed similar to the Drive Retention Period, with the data timestamped in intervals. The length of each interval is the size of the user-defined Tiering Cue. The WEKA system maintains 3 such intervals at any given time, and always tiers the data in the third interval. Refer to the example provided in the "Tiering Cue Policy" section above for further clarity.

Note: While the data release process is based on timestamps of access, creation, or modification, the tiering process is based only on the timestamps of the creation or modification.

Note: These timestamps are per 1 MB chunk and not the file timestamp.

Note: **Example:** If the Tiering Cue is 1 day, then the data will be classified according to the following timeline for a system that starts working on January 1:

Since the tiering process applies to data in the first interval in this example, the data written or modified on January 1 will be tiered to the object store on January 3. Consequently, data will never be tiered before it is at least 1 day old (which is the user-defined Tiering Cue), with the worst case being the tiering of data written at the end of January 1 at the beginning of January 3.

Note: The Tiering Cue default is 10 seconds and cannot exceed 1/3 of the Drive Retention period.

## Breaks in retention period or tiering cue policies <a href="#breaks-in-retention-period-or-tiering-cue-policies" id="breaks-in-retention-period-or-tiering-cue-policies"></a>

If it is impossible to maintain the defined Retention Period or Tiering Cue policies, a TieredFilesystemBreakingPolicy event will occur, and old data will be released to free space on the SSDs. Users are alerted to such a situation through an ObjectStoragePossibleBottleneck event, enabling them to consider either raising the bandwidth or upgrading the object store performance.

## Direct object store mount option

The [`obs_direct`](../../mounting-filesystems#mount-command-options) mount option allows you to bypass time-based file retention policies. When files are created or written to a mount point with this option enabled, they are prioritized for release to the object store as soon as possible, regardless of other policies. Although data is first written to the SSD, it is released to the object store with precedence.

Additionally, any file read through this mount point retrieves data from the object store. While the data temporarily passes through the SSD, it is immediately released and not retained.

Note: In AWS, use this mode only for data import. Avoid using it for general filesystem access, as any data read through this mount point is immediately released from the SSD tier, potentially resulting in excessive S3 charges.

<!-- ============================================ -->
<!-- File 97/259: weka-filesystems-and-object-stores_snapshots.md -->
<!-- ============================================ -->

---
description:
---

# Snapshots

Snapshots allow the saving of a filesystem state to a hidden `.snapshots` directory under the root filesystem. They can be used for:

* **Physical backup:** The snapshots directory can be copied into a different storage system, possibly on another site, using either the WEKA system Snap-To-Object feature or third-party software.
* **Logical backup:** Periodic snapshots enable filesystem restoration to a previous state if logical data corruption occurs.
* **Archive:** Periodic snapshots enable accessing a previous filesystem state for compliance or other needs.
* **DevOps environments:** Writable snapshots enable the execution of software tests on copies of the data.

Snapshots do not impact system performance and can be taken for each filesystem while applications run. They consume minimal space, according to the differences between the filesystem and the snapshots, or between the snapshots, in 4K granularity.

By default, snapshots are read-only, and any attempt to change the content of a read-only snapshot returns an error message.

It is possible to create a writable snapshot. A writable snapshot cannot be changed to a read-only snapshot.

The WEKA system supports the following snapshot operations:

* View snapshots.
* Create a snapshot of an existing filesystem.
* Delete a snapshot.
* Access a snapshot under a dedicated directory name.
* Restore a filesystem from a snapshot.
* Create a snapshot of a snapshot (relevant for writable snapshots or read-only snapshots before being made writable).
* List the snapshots and obtain their metadata.
* Schedule automatic snapshots. For details, see .

To access the hidden `.snapshot` directory, see #access-the-.snapshots-directory.

## Working with snapshots considerations

* **Do not move a file within a snapshot directory or between snapshots:** \
  Moving a file within a snapshot directory or between snapshots is implemented as a copy operation by the kernel, similar to moving between different filesystems. However, such operations for directories will fail.
* **Working with symlinks (symbolic links):**\
  When accessing symlinks through the `.snapshots` directory, symlinks with absolute paths can lead to the current filesystem. Depending on your needs, consider either not following symlinks or using relative paths.
* **Snapshot estimated reclaimable space:**\
  The estimate is an upper limit of capacity that can be freed by deleting the snapshot. It is the size of all data accessible from a snapshot but not accessible from newer snapshots or the active file system. In snapshot chains without writable snapshots, deleting multiple consecutive snapshots (only the oldest N snapshots) release the sum of Estimated Reclaimable Space amounts. Snapshots created before an upgrade to 4.4 or downloaded from OBS may not have an Estimated Reclaimable Space available.

## Maximum supported snapshots

The maximum number of snapshots in a system depends on whether they are read-only or writeable.

* If all snapshots are read-only, the maximum is 24K (24,576).
* If all snapshots are writable, the maximum is 14K (14,336).

A system can have a mix of read-only and writable snapshots, given that a writable snapshot consumes about twice the internal resources of a read-only snapshot.

Some examples of mixing maximum read-only and writable snapshots that a system can have:

* 20K read-only and 4K writable snapshots.
* 12K read-only and 8K writable snapshots.

Note: A live filesystem is counted as part of the maximum writable snapshots.

**Related topics**

<!-- ============================================ -->
<!-- File 98/259: weka-filesystems-and-object-stores_snapshots_snapshots.md -->
<!-- ============================================ -->

---
description: This page describes how to manage snapshots using the GUI.
---

# Manage snapshots using the GUI

Using the GUI, you can:

* View snapshots
* Create a snapshot
* Duplicate a snapshot
* Delete a snapshot
* Restore a snapshot to a filesystem or another snapshot
* Update a snapshot

## View snapshots

**Procedure**

1. To display all snapshots, select **Manage > Snapshots** from the menu.\
   The Snapshots page opens.

2\. To display a snapshot of a selected filesystem, do one of the following:

* Select the Filesystem filter. Then, select the filesystem from the list.
* From the menu, select **Manage > Filesystems**.\
  From the filesystem, select the three dots, and from the menu, select **Go To Snapshot**.

## Create a snapshot

You can create a snapshot from the **Snapshots page** or directly from the **Filesystems** page.

**Before you begin**

Create a directory for filesystem-level snapshots that serves as the access point for snapshots.

**Procedure:**

1. Do one of the following:
   * From the menu, select **Manage > Snapshots**. From the Snapshots page, select **+Create**.\
     The Create Snapshot dialog opens.
   * From the menu, select **Manage > Filesystems**. From the Filesystems page, select the three dots, and from the menu, select **Create Snapshot** (the source filesystem is automatically set).

3. On the Create Snapshot dialog set the following properties:
   * **Name**: A unique name for the filesystem snapshot.
   * **Access Point**: A name of the newly-created directory for filesystem-level snapshots that serves as the snapshot's access point. If you do not specify the access point, the system sets it automatically (in GMT format).
   * **Writable**: Determines whether to set the snapshot to be writable.
   * **Source Filesystem**: The source filesystem from which to create the snapshot.
   * **Upload to local object store**: Determines whether to upload the snapshot to a local object store. You can also upload the snapshot later (see [Snap-To-Object](../snap-to-obj)).
   * **Upload to remote object store**: Determines whether to upload the snapshot to a remote object store. You can also upload the snapshot later.
4. Select **Create**.

## Duplicate a snapshot

You can duplicate a snapshot (clone), which enables creating a writable snapshot from a read-only snapshot.

**Procedure**

1. From the menu, select **Manage > Snapshots**.
2. From the Snapshots page, select the three dots of the snapshot you want to duplicate, and from the menu, select **Duplicate Snapshot**.

3. In the Duplicate Snapshot dialog, set the properties like you create a snapshot.\
   The source filesystem and source snapshot are already set.
4. Select **Duplicate**.

## Delete a snapshot

When deleting a snapshot, consider the following guidelines:

* Deleting a snapshot parallel to a snapshot upload to the same filesystem is impossible. Uploading a snapshot to a remote object store can take time. Therefore, it is advisable to delete the desired snapshot before uploading it to the remote object store.
* When uploading snapshots to both local and remote object stores. While the local and remote uploads can progress in parallel, consider the case of a remote upload in progress. A snapshot is deleted, and later a snapshot is uploaded to the local object store. In this scenario, the local snapshot upload waits for the pending deletion of the snapshot (which happens only once the remote snapshot upload is done).

**Procedure**

1. From the menu, select **Manage > Snapshots**.
2. From the Snapshots page, select the three dots of the snapshot you want to delete, and from the menu, select **Remove**.
3. In the Deletion Of Snapshot message, select **Yes** to delete the snapshot.

## Restore a snapshot to a filesystem or another snapshot <a href="#restore-a-snapshot" id="restore-a-snapshot"></a>

Restoring a snapshot to a filesystem or another snapshot (target) modifies the data and metadata of the target.

**Before you begin**

If you restore the snapshot to a filesystem, make sure to stop the IO services of the filesystem during the restore operation.

**Procedure**

1. From the menu, select **Manage > Snapshots**.
2. From the Snapshots page, select the three dots of the snapshot you want to restore, and from the menu, select **Restore To**.
3. In the Restore To dialog, select the destination: **Filesystem** or **Snapshot**.
4. Select **Save**.

## Update a snapshot

You can update the snapshot name and access point properties.

**Procedure**

1. From the menu, select **Manage > Snapshots**.
2. From the Snapshots page, select the three dots of the snapshot you want to update, and from the menu, select **Edit**.
3. Modify the **Name** and **Access Point** properties as required.
4. Select **Save**.

<!-- ============================================ -->
<!-- File 99/259: weka-filesystems-and-object-stores_snapshots_snapshots-1.md -->
<!-- ============================================ -->

---
description: This page describes how to manage snapshots using the CLI.
---

# Manage snapshots using the CLI

Using the CLI, you can:

* #add-a-snapshot
* #remove-a-snapshot
* #restore-a-snapshot-to-a-filesystem-or-another-snapshot
* #update-a-snapshot
* #access-the-.snapshots-directory
* #retrieve-snapshot-details

## Add a snapshot

**Command:** `weka fs snapshot add`

Use the following command line to create a snapshot:

`weka fs snapshot add <file-system> <name> [--access-point access-point] [--source-snap=<source-snap>] [--is-writable]`

Note: The newly created snapshot is saved in the `.snapshots` directory. \
See #access-the-.snapshots-directory.

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | file-system* | A valid filesystem identifier. | ‚Äã |
 | name* | Unique name for filesystem snapshot. |  |
 | access-point | Name of the newly-created directory for filesystem-level snapshots, which serves as the access point for the snapshots. | Controlled by weka fs snapshot access-point-naming-convention update <date/name>. By default, it is <date> format: @GMT_%Y.%m.%d-%H.%M.%S, which is compatible with Windows' previous versions' format for SMB. |
 | source-snap | Must be an existing snapshot. | The snapshot name of the specified filesystem. |
 | is-writable | Sets the created snapshot to be writable. | False |

## Remove a snapshot

**Command:** `weka fs snapshot remove`

Use the following command line to remove a snapshot:

`weka fs snapshot remove <file-system> <name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | file-system* | A valid filesystem identifier |
 | name* | Unique name for filesystem snapshot |

Note: A snapshot deletion cannot happen parallel to a snapshot upload to the same filesystem. Since uploading a snapshot to a remote object store might take a while, it is advisable to delete the desired snapshots before uploading to the remote object store.
This becomes more important when uploading snapshots to local and remote object stores. While local and remote uploads can progress in parallel, consider the case of a remote upload in progress, then a snapshot is deleted, and later a snapshot is uploaded to the local object store. In this scenario, the local snapshot upload waits for the pending deletion of the snapshot (which happens only once the remote snapshot upload is done).

## Restore a snapshot to a filesystem or another snapshot

**Commands:** `weka fs restore` or `weka fs snapshot copy`

Use the following command line to restore a filesystem from a snapshot:

`weka fs restore <file-system> <source-name> [--preserved-overwritten-snapshot-name=preserved-overwritten-snapshot-name] [--preserved-overwritten-snapshot-access-point=preserved-overwritten-snapshot-access-point]`

Use the following command line to restore a snapshot to another snapshot:

`weka fs snapshot copy <file-system> <source-name> <destination-name> [--preserved-overwritten-snapshot-name=preserved-overwritten-snapshot-name] [--preserved-overwritten-snapshot-access-point=preserved-overwritten-snapshot-access-point]`

**Parameters**

 | Name | Value | Default |
 | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `file-system`* | A valid filesystem identifier | ‚Äã |
 | `source-name`* | Unique name for the source of the snapshot |  |
 | `destination-`name`* | The destination name to which the existing snapshot should be copied. |  |
 | `preserved-overwritten-snapshot-name` | A new name for the overwritten snapshot to preserve, thus allowing the IO operations continuity to the filesystem. If not specified, the original snapshot or active filesystem is overwritten, and IO operations to an existing filesystem might fail. |  |
 | `preserved-overwritten-snapshot-access-point` | A directory that serves as the access point for the preserved overwritten snapshot. | If the `preserved-overwritten-snapshot-name` parameter is specified, but the `preserved-overwritten-snapshot-access-point`parameter is not, it is created automatically based on the snapshot name. |

Note: When restoring a filesystem from a snapshot (or copying over an existing snapshot), the filesystem data and metadata are changed. If you do not specify the `preserved-overwritten-snapshot-name` parameter, ensure IOs to the filesystem are stopped during this time.

## Update a snapshot

**Command:** `weka fs snapshot update`

This command changes the snapshot attributes. Use the following command line to update an existing snapshot:

`weka fs snapshot update <file-system> <name> [--new-name=<new-name>] [--access-point=<access-point>]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | file-system* | A valid filesystem identifier |
 | name* | Unique name for the updated snapshot |
 | new-name | New name for the updated snapshot |
 | access-point | Name of a directory for the snapshot that serves as the access point for the snapshot |

## Access the `.snapshots` directory

The `.snapshots` directory is located in the root directory of each mounted filesystem. It is not displayed with the `ls -la` command. You can access this directory using the `cd .snapshots` command from the root directory.

#### Example

The following example shows a filesystem named `default` mounted to `/mnt/weka`.

To confirm you are in the root directory of the mounted filesystem, change into the `.snapshots` directory, and then display any snapshots in that directory:

```
[root@ip-172-31-23-177 weka]# pwd
/mnt/weka
[root@ip-172-31-23-177 weka]# ls -la
total 0
drwxrwxr-x 1 root root   0 Sep 19 04:56 .
drwxr-xr-x 4 root root  33 Sep 20 06:48 ..
drwx------ 1 user1 user1 0 Sep 20 09:26 user1
[root@ip-172-31-23-177 weka]# cd .snapshots
[root@ip-172-31-23-177 .snapshots]# ls -l
total 0
drwxrwxr-x 1 root root 0 Sep 21 02:44 @GMT-2023.09.21-02.44.38
[root@ip-172-31-23-177 .snapshots]#
```

## Retrieve snapshot details

**Command:** `weka fs snapshot`

Use the following command to retrieve snapshot details, such as its UID, local object locator, estimated reclaimable space, and metadata size:

```

```sh
weka fs snapshot [--file-system file-system] [--name name] [--output output]...
```

```

 | Parameter | Description |
 | --- | --- |
 | --file-system | Filesystem name |
 | --name | Snapshot name |
 | -o, --output... | Specify which columns to output. May include any of the following: uid, id, filesystem, name, access, writeable, created, local_upload_size, remote_upload_size, local_object_status, local_object_progress, local_object_locator, remote_object_status, remote_object_progress, remote_object_locator, removing, prefetched, est_reclaimable_size, metadata_size (may be repeated or comma-separated) |

<!-- ============================================ -->
<!-- File 100/259: weka-filesystems-and-object-stores_snapshot-policies.md -->
<!-- ============================================ -->

---
description:
---

# Snapshot policies

## Overview

Snapshot policies establish the rules and schedules for creating, managing, and retaining point-in-time copies of data, known as snapshots. These snapshots provide a reliable mechanism for data recovery or rollback in scenarios such as accidental deletion, corruption, or other data integrity issues.

By automating the creation of snapshots based on specified criteria, such as time intervals or frequency, snapshot policies enhance data protection, streamline disaster recovery, and ensure business continuity. They enable organizations to restore their data to a consistent state quickly without requiring full backups, optimizing storage usage while minimizing the risk of data loss.

WEKA provides a default system policy that can serve as a foundation for customizing snapshot policies. Each policy defines the following key parameters:

* **Schedule:** Specifies when snapshots are created, including hourly, daily, weekly, monthly, or at periodic intervals.
* **Retention:** Determines the number of snapshots to retain, defining a rotation policy.
* **Destination:** Determines whether snapshots are also uploaded to a local and remote object store.

Background tasks handle operations related to snapshots, including creation and uploads to local or remote object stores. This system operates in the background to improve efficiency and ensure uninterrupted performance during snapshot management processes.

Administrators of the root organization only can configure policies to create hourly, daily, weekly, monthly, and periodic snapshots. These policies can be assigned to filesystems already connected to a local or remote object store.

The example below demonstrates how to configure a policy using the GUI. The policy schedules a snapshot every Saturday and uploads it to a local object store. Alternatively, you can use CLI commands to achieve the same result.

## General guidelines and considerations

* **Plan and structure policies:**
  * Decide whether to store snapshots on the local server or upload them to a local or remote object store.
  * If uploading snapshots to an object store, configure the filesystem to use the appropriate local or remote object store before attaching the policy.
  * Attach a snapshot policy to each filesystem.
  * Design policies based on workload priorities, recovery objectives, and storage capacity requirements.
* **Adhere to system limits:**
  * You can define up to **1,024 snapshot policies** across the cluster, in addition to the default policy.
  * A single snapshot policy can be attached to maximum **1,024 filesystems**.
  * A single filesystem can have maximum **10 snapshot policies** assigned.
* **Optimize policy assignments:**
  * Consolidate policies wherever possible to reduce complexity and duplication.
* **Monitor object store connectivity:**
  * Ensure that each filesystem is properly connected to a local or remote object store to enable seamless assignment to a snapshot policy.
*   **Multiple schedules overlap:**

    * When multiple schedules overlap on the same filesystem, only one snapshot is taken, following this priority:
      1. **Monthly** (highest priority)
      2. **Weekly**
      3. **Daily**
      4. **Hourly**
      5. **Periodic** (lowest priority)

    This means that if multiple schedules overlap, the snapshot with the highest priority (for example, Monthly) are taken, and the lower-priority ones (for example, Weekly, Daily, Hourly, Periodic) are skipped. This hierarchy prevents redundant snapshots. Plan schedules accordingly.
* **Snapshot name format:**
  * \<policy name>-\<schedule type>.\<time-stamp format: YYMMDDHHMM>
  * Example: `policy1-weekly.2412301152`
* **Restricted manual uploads of policy-based snapshots:**
  * Snapshots created by policies cannot be manually uploaded to an object store. Ensure all uploads align with the configured policy.
* **Retaining snapshots outside rotation:**
  * To prevent a snapshot from being deleted during the rotation process, rename the snapshot to exclude it from automated deletion.
* **Manually upload snapshots to object store:**
  * To manually upload a policy-created snapshot that isn't configured for automatic object store upload, rename the snapshot. This change enables you to manually upload the snapshot and prevents automatic deletion.
* **Snapshot deletion with disabled policy:**
  * Snapshots of attached filesystems may still be deleted even when the policy is disabled. If the retention period is reduced, snapshots are deleted according to the updated retention settings, regardless of the policy's status.
*   **Snapshot behavior during DST transitions**:

    During Daylight Saving Time (DST) transitions, the following behavior applies to snapshot schedules (for example, every 15 minutes):

    * **DST start (clocks move forward):** Snapshots in the skipped hour (for example, 1:15 AM to 2:00 AM) are not created. For example, after a snapshot at 1:00 AM, the next one is at 2:15 AM.
    * **DST end (clocks move back):** Snapshots in the repeated hour (for example, 1:15 AM to 2:00 AM) are not duplicated, as they are already created during the first pass.

Note: The Snapshot Policies feature replaces the external SnapTool, which will be deprecated in a future release.

**Related topics**

<!-- ============================================ -->
<!-- File 101/259: weka-filesystems-and-object-stores_snapshot-policies_manage-snapshot-policies-using-the-cli.md -->
<!-- ============================================ -->

---
description:
---

# Manage snapshot policies using the CLI

## Overview

Creating policies using the CLI involves leveraging policy templates for efficient and consistent policy management that align with organizational requirements.

**Process overview:**

1. **Export an existing policy to a template**:\
   The first step in creating a policy template is exporting an existing policy. If this is your first time, you can use the `sys-default` policy (json file), a predefined system policy that serves as a baseline. The `sys-default` policy is not editable, so it is ideal for use as an initial template.
2. **Edit the exported policy template**:\
   After exporting the `sys-default` policy, you can modify the exported json file to suit your specific requirements. This customization allows you to create tailored templates for different groups of policies, streamlining policy creation for various scenarios.
3. **Create a policy from a policy template**:\
   Create a new policy from the desired policy template and customize it further as needed to address specific use cases. This approach provides flexibility while ensuring consistency across policies derived from the same template.
4. **Attach filesystems to a snapshot policy**:\
   Attach the relevant filesystems to the snapshot policy to ensure that the policy governs the creation, management, and retention of snapshots for these specific filesystems. This step links the policy to the filesystems, enabling consistent enforcement of snapshot rules and schedules.

After understanding the workflow for creating policies using the CLI, you can use the following commands to manage snapshot policies:

* List snapshot policies
* Show snapshot policy details
* Export snapshot policy
* Create snapshot policy
* Attach filesystems to a snapshot policy
* Detach filesystems from a snapshot policy
* Update snapshot policy
* Delete snapshot policy

## List snapshot policies

**Command:** `weka fs protection snapshot-policy list`

This command displays a list of all existing snapshot policies in the system. The output includes details such as the policy ID, name, enabled status, description, and any filesystems the policy is attached to.

```sh
weka fs protection snapshot-policy list
```

<details>

<summary>Example: List snapshot policies</summary>

```bash
$ weka fs protection snapshot-policy list
SNAPSHOT POLICY ID  NAME         IS ENABLED  DESCRIPTION                                                                                         ATTACHED FILESYSTEMS
0                   sys-default  True        This snapshot policy is a fixed example configuration, it can be used as-is but cannot be modified
1                   weekly       True        Create a snapshot weekly on Saturdays                                                               fs1
2                   Policy1      True        Schedule daily snapshots                                                                            fs1, default
```

</details>

## Show snapshot policy details

**Command:** `weka fs protection snapshot-policy show`

This command displays the configuration of a snapshot policy in JSON format. It provides a detailed representation of the policy, including schedules (hourly, daily, weekly, monthly, and periodic), retention settings, associated filesystems, and whether specific features are enabled.

**JSON overview**

* **Schedules**: Defines hourly, daily, weekly, monthly, and periodic snapshot schedules, including time, days, and upload settings.
* **Retention**: Specifies the number of snapshots to retain for each schedule type.
* **Filesystems**: Lists the filesystems attached to the policy.
* **General settings**: Includes the policy name, description, and enable/disable status.

```sh
weka fs protection snapshot-policy show <name>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name* | Policy name |

<details>

<summary>Example: Show snapshot policy details</summary>

```
$ weka fs protection snapshot-policy show Policy1
{
    "daily": {
        "days": "monday, wednesday, friday",
        "enable": true,
        "retention": 7,
        "time": "22:05",
        "upload": "local"
    },
    "description": "Policy description",
    "enabled": true,
    "filesystems": [
        "fs1",
        "default"
    ],
    "hourly": {
        "days": "monday, tuesday, wednesday, thursday, friday",
        "enable": false,
        "hours": "09, 10, 11, 12, 13, 14, 15, 16, 17, 18",
        "minuteOffset": 10,
        "retention": 10,
        "upload": "none"
    },
    "monthly": {
        "days": "07",
        "enable": false,
        "months": "all",
        "retention": 12,
        "time": "00:05",
        "upload": "local"
    },
    "name": "Policy1",
    "periodic": {
        "days": "monday, tuesday, wednesday, thursday, friday",
        "enable": false,
        "end_time": "18:00",
        "interval": 30,
        "retention": 4,
        "start_time": "09:00",
        "upload": "none"
    },
    "weekly": {
        "days": "saturday",
        "enable": false,
        "retention": 4,
        "time": "23:05",
        "upload": "local"
    }
}
```

</details>

## Export snapshot policy

**Command:** `weka fs protection snapshot-policy export`

This command exports the configuration of an existing snapshot policy to a template file. Use the `sys-default` policy to export the cluster's default configuration as a baseline for creating customized policy templates.

```sh
weka fs protection snapshot-policy export <name> <path>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name* | The snapshot policy to export. |
 | path* | The path to the directory to save the export policy file. |

<details>

<summary>Example: Export snapshot policy</summary>

```
$ weka fs protection snapshot-policy export sys-default /tmp/policy_template
Exported snapshot policy to /tmp/policy_template
```

</details>

### Customize the policy template

To customize a policy template, follow these steps:

1. **Open the exported template**:\
   Use a text editor, such as `vi`, to open the policy template file that you exported from the `sys-default` template or an existing snapshot policy.
2. **Modify configuration details**:\
   Edit the template to customize the policy's configuration, such as schedules, retention rules, or other relevant settings, to meet your specific requirements.
3. **Reuse the customized template**:\
   Save your changes. The modified template can now be used to create new policies tailored to your needs.

<details>

<summary>Example: Customize the policy template</summary>

In this example, the daily schedule is set for Monday, Wednesday, and Friday. The remaining schedules are disabled (`"enable"=false,`).

```
$ vi /tmp/policy_template

{
    "daily": {
        "days": "monday, wednesday, friday",
        "enable": true,
        "retention": 7,
        "time": "12:10",
        "upload": "local"
    },
    "hourly": {
        "days": "monday, tuesday, wednesday, thursday, friday",
        "enable": false,
        "hours": "09, 10, 11, 12, 13, 14, 15, 16, 17, 18",
        "minuteOffset": 5,
        "retention": 10,
        "upload": "none"
    },
    "monthly": {
        "days": "07",
        "enable": false,
        "months": "all",
        "retention": 12,
        "time": "00:05",
        "upload": "local"
    },
    "periodic": {
        "days": "monday, tuesday, wednesday, thursday, friday",
        "enable": false,
        "end_time": "18:00",
        "interval": 30,
        "retention": 4,
        "start_time": "09:00",
        "upload": "none"
    },
    "weekly": {
        "days": "saturday",
        "enable": false,
        "retention": 4,
        "time": "23:05",
        "upload": "local"
    }
}
-- INSERT --
```

</details>

## Create snapshot policy

**Command:** `weka fs protection snapshot-policy add`

This command creates a new snapshot policy based on a specified template file. Provide the policy name, template file path, and optional parameters such as a description or enabled status.

```

```sh
weka fs protection snapshot-policy add <name> <path> [--description description] [--enabled enabled]
```

```

**Parameters**

 | Parameter | Description | Default |
 | --- | --- | --- |
 | name* | The snapshot policy name. Up to 12 alphanumeric characters, hyphens (-), underscores (_), and periods (.) |  |
 | path* | The path to the snapshot policy file. It must be in JSON format. |  |
 | description | Policy description. Up to 128 characters. |  |
 | enabled | Set snapshot policy status.Possible values: true or false | true |

<details>

<summary>Example: Create a snapshot policy from a policy template</summary>

In this example, a new snapshot policy named `policy2` is created using the template file located at `/tmp/policy_template`. The system returns the newly created policy's ID.

```
$ weka fs protection snapshot-policy create policy2 /tmp/policy_template
SnapPolicyId: 3
```

</details>

## Attach filesystems to a snapshot policy

**Command:** **weka fs protection snapshot-policy attach**

This command attaches existing filesystems to a snapshot policy. Before proceeding, ensure each  filesystem is attached to an object store.

```sh
weka fs protection snapshot-policy attach <name> [<filesystems>]...
```

**Parameters**

 | Parameter | Description |
 | ------------------- | ------------------------------------------------------- |
 | `name`* | The snapshot policy name. |
 | `filesystems`... * | A list of filesystems you want to attach to the policy. |

<details>

<summary>Example: </summary>

This command attaches the snapshot policy `policy1` to the filesystems `fs1` and `default`.

```
$ weka fs protection snapshot-policy attach policy1 fs1 default
$ weka fs protection snapshot-policy list
SNAPSHOT POLICY ID  NAME         IS ENABLED  DESCRIPTION                     ATTACHED FILESYSTEMS
0                   sys-default  True        Cluster default configuration
1                   policy1      False       Schedule daily snapshots        fs1, default
```

</details>

## Detach filesystems from a snapshot policy

**Command:** `weka fs protection snapshot-policy detach`

This command detaches the specified filesystems from the snapshot policy. To remove waiting tasks associated with the filesystems, add the `--remove-waiting-tasks`  option.

<pre data-overflow="wrap"><code><strong>weka fs protection snapshot-policy detach <name> [--remove-waiting-tasks] [<filesystems>]...
</strong>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name* | The snapshot policy name. |
 | filesystems... * | A list of filesystems you want to detach from the policy. |
 | remove-waiting-tasks | Allow to delete all waiting tasks corresponding to the filesystems. |

<details>

<summary>Example: Detach a snapshot policy from filesystems</summary>

```
$ weka fs protection snapshot-policy detach pol1 fs1

Warning: You are about to detach filesystems. This action detach existing filesystem from the snapshot policy and cannot be undone.
Are you sure you want to continue (yes/no)? yes
Filesystems detached successfully
```

</details>

## Update a snapshot policy

**Command:** `weka fs protection snapshot-policy update`

This command updates an existing snapshot policy. You can modify its name, description, policy parameters or enabled status.

```

```sh
weka fs protection snapshot-policy update <name> [--new-name new-name] [--description description] [--path path] [--enabled enabled]
```

```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name* | Existing snapshot policy name. |
 | new-name | New policy name. Up to 12 alphanumeric characters, hyphens (-), underscores (_), and periods (.). |
 | description | New policy description. Up to 128 characters. |
 | path | The path to the new or modified snapshot policy file. It must be in JSON format. |
 | enabled | Set snapshot policy status.Possible values: true or false |

## Delete a snapshot policy

**Command:** `weka fs protection snapshot-policy delete <name>`

This command deletes the specified snapshot policy from the system. Ensure that no filesystems are attached to the policy before proceeding with the deletion.

```
weka fs protection snapshot-policy delete <name>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name* | Existing snapshot policy name. |

<details>

<summary>Example: Delete a snapshot policy</summary>

```
$ weka fs protection snapshot-policy delete policy2
Warning: You are about to delete a snapshot policy. This action deletes the snapshot policy and cannot be undone.

Are you sure you want to continue (yes/no)? yes
```

</details>

<!-- ============================================ -->
<!-- File 102/259: weka-filesystems-and-object-stores_snapshot-policies_manage-snapshot-policies-using-the-gui.md -->
<!-- ============================================ -->

---
description: Manage snapshot policies using the GUI, ensuring efficient data protection.
---

# Manage snapshot policies using the GUI

Using the GUI, you can:

* Explore the snapshot policies
* Create a snapshot policy
* Attach filesystems to a snapshot policy
* Detach a filesystem from a snapshot policy
* Modify an existing snapshot policy
* Delete a snapshot policy

## Explore the snapshot policies

The **Snapshot Policies** page provides a centralized interface for managing and reviewing snapshot policies. This page allows administrators to search for specific policies, view a comprehensive list of configured policies, and examine detailed information about individual policies. Additionally, you can search for filesystems attached to a policy and view a list of all associated filesystems.

The following is a screenshot of the Snapshot Policies page with callouts highlighting its key features:

* **Search a policy:** Filter and identify specific snapshot policies by entering keywords in the search bar.
* **View the list of policies:** Browse all configured snapshot policies in a clear list format.
* **Details of a selected policy:** Access detailed configuration and status information for a highlighted snapshot policy.
* **Search for an attached filesystem:** Filter and identify specific filesystems assigned to the selected snapshot policy by entering keywords in the search bar.
* **View a list of attached filesystems:** See all filesystems assigned with the selected snapshot policy.

<div align="left"></div>

**Procedure**

1. From the **Manage** menu, select **Snapshot Policies**.

The next sections describe how to perform common tasks on this page, leveraging the features highlighted above.

## Create a snapshot policy

This procedure guides you through creating a snapshot policy, which includes defining the policy name, description, schedule, retention settings, and optional upload configuration. Follow these steps to configure a policy tailored to your data protection requirements.

<div align="left"></div>

**Procedure**

1. From the **Manage** menu, select **Snapshot Policies**.
2. On the top-right of the **Snapshot Policies** page, select **+Create Policy**.
3. Configure the following settings:
   * **Policy Name:** Provide a descriptive name for the snapshot policy, up to 12 characters.
   * **Description:** Enter a brief description of the policy's purpose, up to 128 characters.
   * **Schedule:** Select the desired scheduling option:
     * **Hourly:** Creates one snapshot in specific hours or per hour with a customizable start time (offset).
     * **Daily:** Creates one snapshot at specific times and days.
     * **Weekly:** Creates one snapshot on specified days and times each week.
     * **Monthly:** Supports up to four snapshots on specified days, either monthly or in selected months.
     * **Periodic:** Creates snapshots at custom intervals within a defined time window.
   * **Retention:** Define the number of snapshots to retain, allowing for automatic rotation. Alternatively, use the default retention settings for the selected schedule.
   * **Upload to OBS (Object Store):** Specify whether to upload snapshots to a local, remote, or both object stores.
   * **Enable or disable the schedule:**
     * **ON:** Enable the schedule.
     * **OFF:** Disable the schedule.
4. Select **Save** to finalize the policy configuration.

The newly created snapshot policy appears in the list on the **Snapshot Policies** page.

## Attach filesystems to a snapshot policy

Attaching filesystems to a snapshot policy ensures that the policy governs the creation, management, and retention of snapshots for these specific filesystems. This association helps maintain consistent data protection and recovery practices across selected filesystems.

**Procedure**

1. Select the snapshot policy to which you want to attach a filesystem from the **Snapshot Policies** list.
2. In the **Assigned Filesystems** pane on the right, click the **Attach Filesystems** icon (represented by a link symbol) to open the attachment dialog.
3. Select the required filesystems from the available list.
4. Select **Attach** to complete the process.

The filesystem is associated with the selected snapshot policy, and the policy's configurations apply to snapshots for the attached filesystem.

## Detach filesystems from a snapshot policy

Detaching filesystems from a snapshot policy can be necessary when you no longer need to associate the filesystems with the policy, either due to changes in backup strategies or system configurations. This procedure ensures that the filesystems are removed from the policy without affecting its data or storage.

**Procedure**

1. Navigate to the list of snapshot policies and choose the one from which you want to detach filesystems.
2. In the **Assigned Filesystems** pane (on the right), locate the filesystems you want to detach.
3. Move your mouse over the **Detach** icon (represented by an unlink symbol).
4. In the Detach dialog, choose **ON** if you also want to remove any waiting tasks associated with the filesystems.
5. Select **Detach** to complete the process.

## Modify an existing snapshot policy

Updating a snapshot policy is necessary when modifications to schedules, retention settings, or other parameters are required to align with evolving data protection needs. Regularly reviewing and updating policies ensures that they remain effective and consistent with organizational objectives.

**Procedure**

1. Select the snapshot policy you want to update from the **Snapshot Policies** list.
2. Modify the policy configuration as needed:\
   Update the policy name, description, schedule, retention, object store upload, or status settings.
3. Select **Save** to apply the changes.

The updated snapshot policy immediately reflects the new configuration and continue managing snapshots based on the revised settings.

## Set policy status

You can enable or disable a policy directly from the policies list pane, for example, to temporarily disable a policy while adjusting configurations.

**Procedure**

1. In the policies list pane, locate the desired policy.
2.  Click on the current status of the policy (Enabled or Disabled).\

    <div align="left"></div>
3. In the confirmation message that appears, select **Yes** to confirm the status change.\

## Delete a snapshot policy

Snapshot policies may need to be deleted when they are no longer required, are incorrectly configured, or are replaced by updated policies. Removing unnecessary policies helps maintain a clean and manageable environment, ensuring that only relevant configurations are active.

<div align="left"></div>

**Procedure**

1. Select the snapshot policy you wish to delete from the **Snapshot Policies** list.
2. Move your mouse over the policy and click the **trash icon**.
3. In the **Remove Snapshot Policy** confirmation message, select **Yes** to confirm the deletion.

The selected snapshot policy is permanently removed and is no longer appear in the policy list.

<!-- ============================================ -->
<!-- File 103/259: weka-filesystems-and-object-stores_snap-to-obj.md -->
<!-- ============================================ -->

---
description:
---

# Snap-To-Object

The Snap-To-Object feature enables the consolidation of all data from a specific snapshot, including filesystem metadata, every file, and all associated data, into an object store. The complete snapshot data can be used to restore the data on the WEKA cluster or another cluster running the same or a higher WEKA version.

## Snap-To-Object feature use cases

The Snap-To-Object feature is helpful for a range of use cases, as follows:

* **On-premises and cloud use cases**
  * External backup of data
  * Archiving data
  * Data replication
* **Cloud-only use cases**
  * Cloud pause/restart
  * Data protection against cloud availability zone failures
  * Migration of filesystems to another region
* **Hybrid cloud use case**
  * Cloud bursting

### External backup of data

Suppose it is required to recover data stored on a WEKA filesystem due to a complete or partial loss of the data within it. You can use a data snapshot saved to an object store to recreate the same data in the snapshot on the same or another WEKA cluster.

This use case supports backup in any of the following WEKA system deployment modes:

* **Local object store:** The WEKA cluster and object store are close to each other and will be highly performant during data recovery operations. The WEKA cluster can recover a filesystem from any snapshot on the object store for which it has a reference locator.
* **Remote object store:** The WEKA cluster and object store are located in different geographic locations, typically with longer latencies between them. In such a deployment, you can send snapshots to local and remote object stores.

Note: This deployment type requires supporting the latency of hundreds of milliseconds. For performance issues on Snap-To-Object tiering cross-interactions/resonance, contact the [Customer Success Team](../support/getting-support-for-your-weka-system).

* **Local object store replicating to a remote object store:** A local object store in one data center replicates data to another object store using the object store system features, such as AWS S3 cross-region replication.\
  This deployment provides both integrated tiering and Snap-To-Object local high performance between the WEKA object store and the additional object store. The object store manages the data replication, enabling data survival in multiple regions.

Note: This deployment requires ensuring that the object store system perfectly replicates all objects on time to ensure consistency across regions.

### Archiving data

The periodic creation and uploading of snapshots to an object store generate an archive, allowing access to past copies of data.

When any compliance or application requirement occurs, it is possible to make the relevant snapshot available on a WEKA cluster and view the content of past versions of data.

### Data replication

Combining a local cluster with a replicated object store in another data center allows for the following use cases:

* **Disaster recovery:** where you can take the replicated data and make it available to applications in the destination location.
* **Backup:** where you can take multiple snapshots and create point-in-time images of the data that can be mounted, and specific files may be restored.

### Cloud pause/restart

In a public cloud, with a WEKA cluster running on compute instances with local SSDs, sometimes the data needs to be retained, even though ongoing access to the WEKA cluster is unnecessary. In such cases, using Snap-To-Object can save the costs of compute instances running the WEKA system.

To pause a cluster, you need to take a snapshot of the data and then use Snap-To-Object to upload the snapshot to an S3-compliant object store. When the upload process is complete, the WEKA cluster instances can be stopped, and the data is safe on the object store.

To re-enable access to the data, you need to form a new cluster or use an existing one and download the snapshot from the object store.

### Data protection against cloud availability zone failures

This use case ensures data protection against cloud availability zone failures in the various clouds: AWS Availability Zones, Google Cloud Platform (GCP) Zones, and Oracle Cloud Infrastructure (OCI) Availability Domains.

In AWS, for example, the WEKA cluster can run on a single availability zone, providing the best performance and no cross-AZ bandwidth charges. Using Snap-To-Object, you can take and upload snapshots of the cluster to S3 (which is a cross-AZ service). If an AZ failure occurs, a new WEKA cluster can be created on another AZ, and the last snapshot uploaded to S3 can be downloaded to this new cluster.

### Migration of filesystems to another region

Using WEKA snapshots uploaded to S3 combined with S3 cross-region replication enables the migration of a filesystem from one region to another.

### Cloud bursting

On-premises WEKA deployments can often benefit from cloud elasticity to consume large quantities of computation power for short periods.

Cloud bursting requires the following steps:

1. Take a snapshot of an on-premises WEKA filesystem.
2. Upload the data snapshot to S3 at AWS using Snap-To-Object.
3. Create a WEKA cluster in AWS and make the data uploaded to S3 available to the newly formed cluster at AWS.
4. Process the data in-cloud using cloud compute resources.

Optionally, you may also promote data back to on-premises by doing the following:

1. Take a snapshot of the WEKA filesystem in the cloud on completion of cloud processing.
2. Upload the cloud snapshot to the on-premises WEKA cluster.

## Snapshots management considerations

* **Simultaneous snapshot uploads**: WEKA supports concurrent uploads of multiple snapshots from different filesystems to both remote and local object stores.
* **Writeable snapshots cannot be uploaded**: A writeable snapshot is a clone of the live filesystem or other snapshots at a specific point in time. Because its data continues to change, the snapshot cannot be uploaded to the object store as a read-only snapshot and is tiered according to existing policies.
* **Snapshot upload order**: Uploading snapshots to a remote object store benefits from a chronological approach. When uploading a monthly snapshot, it may be more efficient to first upload the preceding daily snapshots.
* **Snapshot deletion and upload constraints**: Parallel deletion during snapshot upload requires careful handling. The local snapshot upload pauses and waits for any pending snapshot deletion, which only proceeds after the remote snapshot upload is complete.
* **Pausing or aborting snapshot uploads**: Users can pause or abort snapshot uploads using commands detailed in the background tasks section.
* **New filesystem creation from snapshots**: When creating a new filesystem from a snap-to-object operation, the original filesystem quotas are not preserved in the new filesystem.

Note: The system does not support restoring or downloading a filesystem from snapshots stored in object storage when the object store is located within the same cluster.

## Synchronous snapshots

Synchronous snapshots are point-in-time backups for filesystems. When taken, they consist only of the changes since the last snapshot (incremental snapshots). When you download and restore a snapshot to a live filesystem, the system reconstructs the filesystem on the fly with the changes since the previous snapshot.

This capability for filesystem snapshots makes them more cost-effective because you do not have to update the entire filesystem with each snapshot. You only update the changes since the last snapshot.

It is recommended that the synchronous snapshots be applied in chronological order.

## Delete snapshots residing on an object store

Deleting a snapshot uploaded from a filesystem removes all its data from the local object store bucket. It does not remove any data from a remote object store bucket.

Note: If the snapshot has been (or is) downloaded and used by a different filesystem, that filesystem stops functioning correctly, data can be unavailable, and errors can occur when accessing the data.
Before deleting the downloaded snapshot, it is recommended to either un-tier or migrate the filesystem to a different object store bucket.

## Snap-To-Object and tiering

Snap-To-Object and tiering use SSDs and object stores for data storage. The WEKA system uses the same paradigm for holding SSD and object store data for both Snap-To-Object and tiering to save storage and performance resources.

You can implement this paradigm for each filesystem using one of the following use cases:

* **Data resides on the SSDs only, and the object store is used only for the various Snap-To-Object use cases, such as backup, archiving, and bursting:**\
  The allocated SSD capacity must be identical to the filesystem size (total capacity) for each filesystem. The drive retention period must be defined as the longest time possible (which is 60 months).\
  The Tiering Cue must be defined using the same considerations based on IO patterns. In this case, the applications always work with a high-performance SSD storage system and use the object store only as a backup device.
* **Snap-To-Object on filesystems is used with active tiering between the SSDs and the object store:**\
  Objects in the object store are used to tier all data and back up using Snap-To-Object. If possible, the WEKA system uses the same object for both purposes, eliminating the unnecessary need to acquire additional storage and copy data.

Note: When using Snap-To-Object to promote data from an object store, some metadata may still be in the object store until it is accessed for the first time.

**Related topics**

<!-- ============================================ -->
<!-- File 104/259: weka-filesystems-and-object-stores_snap-to-obj_snap-to-obj.md -->
<!-- ============================================ -->

---
description:
---

# Manage Snap-To-Object using the GUI

Using the GUI, you can:

* Upload a snapshot
* Create a filesystem from an uploaded snapshot
* Sync a filesystem from a snapshot

**Related topics**

To learn about how to view, create, update, delete, and restore snapshots, see [Manage snapshots using the GUI](../snapshots/snapshots).

## Upload a snapshot

You can upload a snapshot to a local, remote, or both object store buckets.

**Procedure**

1. From the menu, select **Manage > Snapshots**.
2. Select the three dots on the right of the required snapshot. From the menu, select **Upload To Object Store**.

3. A relevant message appears if a local or remote object store bucket is not attached to the filesystem. It enables opening a dialog to select an object store bucket and attach it to the filesystem. To add an object store, select **Yes**.
4. In the Attach Object Store to Filesystem dialog, select the object store bucket to attach the snapshot.

5. Select **Save**.\
   The snapshot is uploaded to the target object store bucket.
6. **Copy the snapshot locator:**
   * Select the three dots on the right of the required snapshot, and select **Copy Locator to Clipboard**.
   * Save the locator in a dedicated file so later you can use it for creating a filesystem from the uploaded snapshot.

***

**Related topics**

#pause-resume-abort-a-background-task

## Create a filesystem from an uploaded snapshot

You can create (or recreate) a filesystem from an uploaded snapshot, for example, when you need to migrate the filesystem data from one cluster to another.

When recreating a filesystem from a snapshot, adhere to the following guidelines:

* **Pay attention to upload and download costs**: Due to the bandwidth characteristics and potential costs when interacting with remote object stores, it is not allowed to download a filesystem from a remote object store bucket. If a snapshot on a local object store bucket exists, it is advisable to use that one. Otherwise, follow the procedure in the [Recover from a remote snapshot](../snap-to-obj-1#recover-from-a-remote-snapshot) topic using the CLI.
* **Use the same KMS master key**: For an encrypted filesystem, to decrypt the snapshot data, use the same KMS master key as used in the encrypted filesystem. See the [KMS Management Overview](../../../security/kms-management#overview) topic.

**Before you begin**

* Verify that the locator of the required snapshot (from the source cluster) is available (see the last step in the Upload a snapshot procedure).
* Ensure the object store is attached to the destination cluster.

**Procedure**

1. Connect to the destination cluster where you want to create the filesystem.
2. From the menu, select **Manage > Filesystems**, and select **+Create**.
3. In the Create Filesystem, do the following:
   * Set the filesystem name, group, and tiering properties.
   * Select **Create From Uploaded Snapshot** (it only appears when you select **Tiering**).\
     Paste the copied snapshot locator in the Object Store Bucket Locator (from the source cluster).\
     In the Snapshot Name, set a meaningful snapshot name to override the default (uploaded snapshot name).\
     In the Access Point, set a meaningful access point name to override the default (uploaded access point name) for the directory that serves as the snapshot's access point.
4. Select **Save**.

## Sync a filesystem from a snapshot <a href="#sync-a-filesystem-from-a-snapshot" id="sync-a-filesystem-from-a-snapshot"></a>

You can synchronize a filesystem from a snapshot using the Synchronous Snap feature (incremental snapshot). Synchronous Snap only downloads changes since the last snapshot from the object store bucket.

Note: Only snapshots uploaded from version 4.3 or later can be downloaded using Synchronous Snap.

**Before you begin**

Copy the locator of the snapshot you want to sync with the filesystem.

**Procedure**

1. From the menu, select **Manage > Filesystems**.
2. From the Filesystems page, select the three dots of the filesystem you want to sync, and from the menu, select **Synchronous Snap**.

3. Paste the snapshot object locator in the Run Synchronous Snap to Existing Filesystem dialog.
4. Select **Start**.\
   The filesystem starts syncing with the snapshot.

5. Once the sync is completed, restore the snapshot to update the production filesystem.

**Related topics**

#add-a-filesystem

<!-- ============================================ -->
<!-- File 105/259: weka-filesystems-and-object-stores_snap-to-obj_snap-to-obj-1.md -->
<!-- ============================================ -->

---
description:
---

# Manage Snap-To-Object using the CLI

Using the CLI, you can:

* Upload a  snapshot
* Create a filesystem from an uploaded snapshot
* Manage synchronous snapshots
* Recover from a remote snapshot

## Upload a  snapshot

**Command:** `weka fs snapshot upload`

Use the following command line to upload an existing snapshot:

`weka fs snapshot upload <file-system> <snapshot> [--site site]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | file-system* | Name of the filesystem |  |
 | snapshot* | Name of the snapshot of the <file-system> filesystem to upload. |  |
 | site* | Location for the snapshot upload.Mandatory only if both local and remote buckets are attached.Possible values: local or remote | Auto-selected if only one bucket for upload is attached. |

## Create a filesystem from an uploaded snapshot

**Command:** `weka fs download`

Use the following command to create or recreate a filesystem from an existing snapshot. If the snapshot originates from an encrypted source, be sure to include the required KMS-related parameters:

`weka fs download <name> <group-name> <total-capacity> <ssd-capacity> <obs-bucket> <locator>` \[--auth-required auth-required] `[--additional-obs additional-obs] [--snapshot-name snapshot-name] [--access-point access-point] [--kms-key-identifier kms-key-identifier] [--kms-namespace kms-namespace] [--kms-role-id kms-role-id] [--kms-secret-id kms-secret-id] [--skip-resource-validation]`

When creating a filesystem from a snapshot, a background cluster task automatically prefetches its metadata, providing better latency for metadata queries.

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | name* | Name of the filesystem to create. |  |
 | group-name* | Name of the filesystem group in which the new filesystem is placed. |  |
 | total-capacity* | The total capacity of the downloaded filesystem. |  |
 | ssd-capacity* | SSD capacity of the downloaded filesystem. |  |
 | obs-bucket* | Object store name for tiering. |  |
 | locator* | Object store locator obtained from a previously successful snapshot upload. |  |
 | auth-required | Require authentication for the mounting user when mounting this filesystem. This setting is only applicable in the root organization; users in non-root organizations must always be authenticated to perform a mount operation. Format: yes or no. | no |
 | additional-obs | An additional object-store name.If the data to recover reside in two object stores (a second object store attached to the filesystem, and the filesystem has not undergone full migration), this object store is attached in a read-only mode.The snapshot locator must be in the primary object store specified in the obs parameter. |  |
 | snapshot-name | The downloaded snapshot name. | The uploaded snapshot name. |
 | access-point | The downloaded snapshot access point. | The uploaded access point. |
 | kms-key-identifier | Customize KMS key name for this filesystem (applicable only for HashiCorp Vault). |  |
 | kms-namespace | Customize the KMS role ID for this filesystem (applicable only for HashiCorp Vault). |  |
 | kms-role-id | Customize the KMS role ID for this filesystem (applicable only for HashiCorp Vault). |  |
 | kms-secret-id | Customize the KMS secret ID for this filesystem (applicable only for HashiCorp Vault). |  |
 | skip-resource-validation | Skip verifying RAM and SSD resource allocation for the downloaded filesystem on the cluster. |  |

Note: For encrypted filesystems, when downloading, you must use the same KMS cluster-wide key or, if configured, the per-filesystem encryption parameters to decrypt the snapshot data. For more information, see .

The `locator` can be a previously saved locator for disaster scenarios, or you can obtain the `locator` using the `weka fs snapshot` command on a system with a live filesystem with snapshots.

If you need to pause and resume the download process, use the command: `weka cluster task pause / resume`. To abort the download process, delete the downloaded filesystem directly. For details, see .

Note: Due to the bandwidth characteristics and potential costs when interacting with remote object stores it is not allowed to download a filesystem from a remote object-store bucket. If a snapshot on a local object-store bucket exists, it is advisable to use that one. Otherwise, follow the procedure in#recover-from-a-remote-snapshot.

## Manage synchronous snapshots

The workflow to manage the synchronous snapshots includes:

1. Upload snapshots using, for example, the snapshots scheduler. See .
2. Download the synchronous snapshot (described below).
3. Restore a specific snapshot to a filesystem. See #restore-a-snapshot-to-a-filesystem-or-another-snapshot.

### Download a synchronous snapshot

**Command:** `weka fs snapshot download`

Use the following command line to download a synchronous snapshot. This command is only relevant for snapshots uploaded from a system of version 4.3 and later:

 `weka fs snapshot download <file-system> <locator>`

Note: Make sure to download synchronous snapshots in chronological order. Non-chronological snapshots are inefficient and are not synchronous.
If you need to download a snapshot earlier than the latest downloaded one, for example, when you need one of the daily synchronous snapshots after the weekly synchronous snapshot was downloaded, add the `--allow-non-chronological` flag to download it anyway.

**Parameters**

 | Name | Value |
 | --- | --- |
 | file-system* | Name of the filesystem. |
 | locator* | Object store locator obtained from a previously successful snapshot upload. |

If you need to pause and resume the download process, use the command: `weka cluster task pause / resume`. To abort the download process, delete the downloaded snapshot directly. For details, see .

**Related topics**

#synchronous-snapshots

## Recover from a remote snapshot

When recovering a snapshot residing on a remote object store, it is required to define the object store bucket containing the snapshot as a local bucket.

A remote object store has restrictions over the download, and we want to use a different local object store due to the QoS reasons explained in [Manage object stores](../../managing-object-stores#overview).

To recover a snapshot residing on a remote object store, create a new filesystem from this snapshot as follows:

1. Add a new local object-store, using `weka fs tier obs add` CLI command.
2. Add a local object-store bucket, referring to the bucket containing the snapshot to recover, using `weka fs tier s3 add.`
3. Download the filesystem, using `weka fs download.`
4. If the recovered filesystem should also be tiered, add a local object store bucket for tiering using `weka fs tier s3 add.`
5. Detach the initial object store bucket from the filesystem.
6. Assuming you want a remote backup to this filesystem, attach a remote bucket to the filesystem.
7. Remove the local object store bucket and local object store created for this procedure.

<!-- ============================================ -->
<!-- File 106/259: weka-filesystems-and-object-stores_quota-management.md -->
<!-- ============================================ -->

---
description:
---

# Quota management

## Overview

The WEKA system offers multiple layers where you can limit capacity usage:

* **Organization level**: You can monitor an organization‚Äôs usage (SSD and total) and restrict usage with quotas per organization. This feature can be used for charge-backs based on the capacity used or allocated by SSD or object store data. For more details, see .
* **Filesystem level**: Allocate a unique filesystem for each department or project.
* **Directory level**: Assign a unique quota for each project directory (beneficial when users are involved in multiple projects) or for each user‚Äôs home directory.

In the context of directory quotas, the organization admin can set a quota on a directory. This action initiates calculating the current directory usage in a background task. Once this calculation is complete, the quota is considered.

Note: To set a quota on a directory, a native POSIX mount to the relevant filesystem is necessary. The quota set command must not be interrupted until the quota accounting process is finished.

The organization admin‚Äôs role in setting quotas is to inform and restrict users from overusing the filesystem capacity. In this regard, only data that the user controls is considered. Therefore, the quota does not include the overhead of protection bits and snapshots. However, it accounts for the data and metadata of files in the directory, irrespective of whether they are tiered.

## Guidelines for quota management

When managing quotas, adhere to the following guidelines:

* **Setting quotas**: To establish a quota, ensure the relevant filesystem is mounted on the server where the quota command is executed.
* **Quota coloring:** During the procedure of setting or unsetting a directory quota, a background task referred to as `QUOTA_COLORING` is launched. This process scans the entire directory tree and assigns the quota ID to each file and directory within the tree. Set at least one Data Services container to run this task in the background to optimize system performance. For details, see .
* **Nested quotas**: Quotas can be established within nested directories (supporting up to four levels of nested quotas) and over-provisioned under the same directory quota tree. For instance, a `/home` directory can have a 1TiB quota with 200 users, each having a user directory under it with a 10GiB quota. This scenario illustrates over-provisioning, where parent quotas are enforced on all subdirectories, irrespective of any remaining capacity in the child quotas.
* **File movement**: The `rename()` operation, when implemented using `link()` and `unlink()`, behaves similarly to an atomic file move between different filesystems when moving files into or out of quota-enforced directories. As a result, it returns `EXDEV`. Consequently, file movement operations require a fallback mechanism: the file must be copied to the new location before deleting the original. Tools like `mv` in Linux handle this automatically, gracefully reverting to a non-atomic copy-and-delete approach when necessary.
* **Quotas and hard links**: Once a directory has a quota, only newly created hard links within the quota limits are part of quota calculations.
* **Restoring filesystems**: Restoring a filesystem from a snapshot reverts the quotas to the configuration at the time of the snapshot.
* **Creating new filesystems**: Creating a new filesystem from a snap-to-object does not retain the original quotas.
* **Enforcing quotas**: When enforcing quotas in conjunction with a `writecache` mount-mode, exceeding the quota might not sync all the cache writes to the backend servers, similar to other POSIX solutions. Use `sync`, `syncfs`, or `fsync` to commit the cached changes to the system (or fail due to exceeding the quota).

## Integration with the `df` utility

By default, when a hard quota is set on a directory, the `df` utility interprets it as the directory's total capacity, displaying the usage percentage (`use%`) relative to the quota. This helps users understand their usage and proximity to the quota limit.

Note: The integrated behavior of the `df` utility with quotas is a global setting in the WEKA system. To modify this global behavior to instead use soft quotas or ignore quotas, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).

<!-- ============================================ -->
<!-- File 107/259: weka-filesystems-and-object-stores_quota-management_quota-management.md -->
<!-- ============================================ -->

---
description: This page describes how to manage quotas using the CLI.
---

# Manage quotas using the CLI

Using the CLI, you can:

* Set default quota
* Set directory quota
* List directory quotas/default quotas
* Unset default quota
* Reset directory quota

## Set default quota

**Command**: `weka fs quota set-default`

Before using this command, ensure the following requirements are met:

* A mount point to the relevant filesystem is set.
* Deploy at least one Data Services container before setting any quotas. If not running, quota operations defaults to single-process mode, potentially causing CLI to hang for extended periods. See .

Default quotas apply to newly created subdirectories, not the directory or existing children.

Use the following command to set a default quota of a directory:

`weka fs quota set-default <path>  [--soft soft] [--hard hard] [--grace grace] [--owner owner]`

####  **Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | path* | Path to the directory to set the quota.The relevant filesystem must be mounted when setting the quota. | ‚Äã |
 | soft | Soft quota limit.Exceeding this number is displayed as an exceeded quota, but it is not enforced until the grace period is over.The capacity can be in decimal or binary units.Format: 1GB, 1TB, 1GiB, 1TiB, unlimited | unlimited |
 | hard | Hard quota limit.Exceeding this number does not allow more writes before clearing some space in the directory.The capacity can be in decimal or binary units.Format: 1GB, 1TB, 1GiB, 1TiB, unlimited | unlimited |
 | grace | Specify the grace period before the soft limit is treated as a hard limit.Format: 1d, 1w, unlimited | unlimited |
 | owner | A unique string identifying the directory owner (can be a name, email, slack ID, etc.) This owner will be shown in the quota report and can be notified upon exceeding the quota.Supports up to 48 characters. |  |

Note: * To set advisory only quotas, use a `soft` quota limit without setting a `grace` period.
* When `hard` and `soft` quotas exist, setting the value of one of them to `0` clears this quota.

## Set directory quota

**Command**: `weka fs quota set`

Before using the commands, verify that at least one Data Services container is set to enable the command to run the `QUOTA_COLORING` task in the background.\
For details, see .

Use the following command to set a directory quota:

`weka fs quota set <path> [--soft soft] [--hard hard] [--grace grace] [--owner owner] [--file-system file-system] [--snap-name snap-name] [--color color]`

#### **Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | path* | Path to the directory to set the quota.The relevant filesystem must be mounted when setting the quota. | ‚Äã |
 | soft | Soft quota limit.Exceeding this number is displayed as an exceeded quota, but it is not enforced until the grace period is over.The capacity can be in decimal or binary units.Format: 1GB, 1TB, 1GiB, 1TiB, unlimited | unlimited |
 | hard | Hard quota limit.Exceeding this number does not allow more writes before clearing some space in the directory.The capacity can be in decimal or binary units.Format: 1GB, 1TB, 1GiB, 1TiB, unlimited | unlimited |
 | grace | Specify the grace period before the soft limit is treated as a hard limit.Format: 1d, 1w, unlimited | unlimited |
 | owner | A unique string identifying the directory owner (can be a name, email, slack ID, etc.) This owner will be shown in the quota report and can be notified upon exceeding the quota.Supports up to 48 characters. |  |
 | file-system | Filesystem name. Use this parameter to set a quota outside the mount point. |  |
 | snap-name | Name of the writable snapshot. Use this parameter to set a quota outside the mount point. |  |

## List directory quotas/default quotas

**Command**: `weka fs quota list` / `weka fs quota list-default`

Use the following command to list the directory quotas (by default, only exceeding quotas are listed) :

`weka fs quota list [fs-name] [--snap-name snap-name] [--path path] [--under under] [--over over] [--quick] [--all]`

#### **Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | fs-name* | Filesystem name. Use this parameter to display a quota report only on the specified filesystem. | All filesystems |
 | snap-name | Displays the quota report from the time of the snapshot.It must be a valid snapshot name and be given along with the corresponding fs-name. |  |
 | path | Path to a directory. Shows quota report only on the specified directory.The relevant filesystem must be mounted in the server running the query. |  |
 | under | A path to a directory under a wekafs mount.The relevant filesystem must be mounted in the server running the query. |  |
 | over | Shows only quotas over this percentage of usage.Possible values: 0-100 |  |
 | quick | Do not resolve inode to a path. Provides quicker results if the report contains many entries. | False |
 | all | Shows all the quotas, not just the exceeding ones. | False |

Use the following command to list the directory default quotas:

`weka fs quota list-default [fs-name] [--snap-name snap-name] [--path path]`

#### **Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | fs-name | Filesystem name. Use this parameter to display the default quotas only on the specified filesystem. | All filesystems |
 | snap-name | Displays the default quotas from the time of the snapshot.It must be a valid snapshot name and be given along with the corresponding fs-name. |  |
 | path | Path to a directory. Shows the default quotas report only on the specified directory.The relevant filesystem must be mounted in the server running the query. |  |

## Unset default quota

**Command**: `weka fs quota unset-default`

Use the following command to unset a default quota of a directory:

`weka fs quota unset-default <path>`

#### **Parameters**

 | Name | Value |
 | --- | --- |
 | path* | Path to the directory to set the quota.The relevant filesystem must be mounted when setting the quota. |

## Reset directory quota

**Command**: `weka fs quota reset`

Use the following command to reset a directory quota:

`weka fs quota reset <path>`

#### **Parameters**

 | Name | Value |
 | --- | --- |
 | path* | Path to the directory to unset the quota.The relevant filesystem must be mounted when setting the quota. |

<!-- ============================================ -->
<!-- File 108/259: weka-filesystems-and-object-stores_quota-management_manage-quotas-using-the-gui.md -->
<!-- ============================================ -->

---
description: This page describes how to manage quotas using the GUI.
---

# Manage quotas using the GUI

Directory quotas monitor a directory's filesystem capacity usage and allow restricting the amount of space used by the directory.

Using the GUI, you can:

* Set directory quota
* View directory quotas and default quota
* Update a directory quota or default quota
* Remove a directory quota
* Remove the default quota for new directories

Note: To set a default quota, use the CLI. See #set-default-quota.
Default quotas apply to newly created subdirectories, not the directory or existing children.

## Set directory quota

The organization admin can set a quota on a directory. This action initiates calculating the current directory usage in a background task. Once this calculation is complete, the quota is considered.

**Before you begin**

* To set a quota on a directory, a mount point to the relevant filesystem is necessary. The quota set command mustn‚Äôt be interrupted until the quota accounting process is finished.
* Deploy at least one Data Services container before setting any quotas. If not running, quota operations defaults to single-process mode, potentially causing CLI to hang for extended periods. See .\

**Procedure**

1. From the menu, select **Manage > Directory Quotas**.
2. Select **Directory Quotas**.
3. Select the filesystem in which you want to set the directory quotas.
4. In the Create Quota dialog, set the following:
   * **Directory Path:** The full path to the directory quota to be set on.
   * **Hard Quota Limit:** The hard quota limit defines the maximum used capacity above the soft quota limit, which prevents writing to the directory.
   * **Soft Quota Limit:** The soft quota limit defines the maximum used capacity that triggers a grace period timer. Data can be written to the directory until the grace period ends or the hard quota limit is reached.
   * **Owner:** The directory‚Äôs owner, such as user name, email, or slack ID (up to 48 characters).
   * **Grace Period:** When the soft quota limit is reached, a grace period starts. After this period, data cannot be written to the directory.\
     The system sets the directory quota in the background.
5. To monitor the directory quota setting background task, select **Monitor > Background Tasks.**

## View directory quotas and default quota

You can view existing directory quotas and the default quota that are already set.

**Procedure**

1. From the menu, select **Manage > Directory Quotas**.
2. Select the relevant tab: **Directory Quotas** or **Default Directories Quota**.
3. Select the filesystem in which the directory quotas are already set.
4. To view all quotas or only the exceeding quotas, select the **Exceeding quotas/All quotas** switch.

## Update a directory quota or default quota

You can update an existing directory quota or the default quota for directories. Updating the default quota only applies to new directories.

**Procedure**

1. From the menu, select **Manage > Directory Quotas**.
2. Select the relevant tab: **Directory Quotas** or **Default Directories Quota**.
3. Select the filesystem in which the directory quotas are set (through the CLI).
4. Select the three dots on the right of the required directory. From the menu, select **Update**.

5. In the Quota Settings Update dialog, modify the following settings according to your needs:
   * **Hard Quota Limit:** The hard quota limit defines the maximum used capacity above the soft quota limit, which prevents writing to the directory.
   * **Soft Quota Limit:** The soft quota limit defines the maximum used capacity that triggers a grace period timer. Data can be written to the directory until the grace period ends or the hard quota limit is reached.
   * **Owner:** The directory‚Äôs owner, such as user name, email, or slack ID (up to 48 characters).
   * **Grace Period:** When the soft quota limit is reached, a grace period starts. After this period, data cannot be written to the directory.
6. Click **Save**.

## Remove a directory quota

You can remove (unset) a directory quota if it is no longer required.

**Procedure**

1. From the menu, select **Manage > Directory Quotas**.
2. Select the **Directory Quotas** tab.
3. Select the filesystem in which the directory quota is set.
4. Select the three dots on the right of the required default quota. From the menu, select **Remove**.
5. In the Quota Deletion message, select **Yes**.

## Remove the default quota for new directories

You can remove (unset) the default quota settings for new directories created in a specific filesystem. The quota of existing directories is not affected.

**Procedure**

1. From the menu, select **Manage > Directory Quotas**.
2. Select the **Default Directories Quota** tab.
3. Select the filesystem in which the default quotas are already set (through the CLI).
4. Select the three dots on the right of the required default quota. From the menu, select **Remove**.
5. In the Default Quota Deletion message, select **Yes**.

<!-- ============================================ -->
<!-- File 109/259: additional-protocols.md -->
<!-- ============================================ -->

# Additional Protocols

## Topics in this section

### Additional protocol containers

### Manage the NFS protocol

The WEKA system enables file access through the NFS protocol instead of the WEKA client.

### Manage the S3 protocol

The WEKA configuration of the S3 protocol.

### Manage the SMB protocol

Configure and control WEKA's SMB-W implementation for seamless cross-platform file access across Windows, Linux, and macOS clients with concurrent multi-protocol support.

<!-- ============================================ -->
<!-- File 110/259: additional-protocols_additional-protocols-overview.md -->
<!-- ============================================ -->

# Additional protocol containers

In a WEKA cluster, the frontend container provides the default POSIX protocol, serving as the primary access point for the distributed filesystem. You can also define protocol containers for NFS, SMB, and S3 clients.

To configure protocol containers, you have two options for creating a cluster for the specified protocol:

1. Set up protocol services on existing backend servers.
2. Prepare additional dedicated servers for the protocol containers.

Note: In cloud environments, setting up protocol services on existing backend servers (option 1) is not supported. Instead, use option 2 and prepare additional dedicated servers (protocol gateways) when creating the `main.tf` file.
For more details, refer to the relevant deployment section:
*
*
*

### Dedicated filesystem requirement for cluster-wide persistent protocol configurations

A dedicated filesystem is required to maintain persistent protocol configurations across a cluster. This filesystem is pivotal in orchestrating coherent, synchronized access to files from multiple servers. It is recommended that this configuration filesystem be named with a significant name, for instance, `.config_fs`. The total capacity must be **100 GB** while refraining from employing additional features such as tiering and thin-provisioning.

When establishing a Data Services container for background tasks, it is recommended to increase the `.config_fs` size to **122 GB** (an additional 22 GB on top of the initial 100 GB). For further details, see .

<details>

<summary>.config_fs setting example</summary>

**Related topics**

#add-a-filesystem-group (a prerequisite for creating a filesystem using the GUI)

#create-a-filesystem (using the GUI)

#create-a-filesystem (using the CLI)

</details>

## **Set up protocol containers** on existing backend servers

With this option, you configure the existing cluster to provide the required protocol containers. The following topics guide you through the configuration for each protocol:

*
*
*

## **Prepare dedicated protocol servers**

Dedicated protocol servers enhance the cluster's capabilities and address diverse use cases. Each dedicated protocol server in the cluster can host one of these additional protocol containers alongside the existing frontend container.

These dedicated protocol servers function as complete and permanent members of the WEKA cluster. They run essential processes to access WEKA filesystems and incorporate switches supporting the protocols.

#### Benefits

Dedicated protocol servers offer the following advantages:

* **Optimized performance:** Leverage dedicated CPU resources for tailored and efficient performance, optimizing overall resource usage.
* **Independent protocol scaling:** Scale specific protocols independently, mitigating resource contention and ensuring consistent performance across the cluster.

**Procedure**

1. **Install the WEKA software on the dedicated protocol servers:** Do one of the following:
   * Follow the default method as specified in .
   *   Use the WEKA agent to install from a working backend. The following commands  demonstrate this method:

       ```bash
| curl http://<EXISTING-BACKEND-IP>:14000/dist/v1/install | sudo sh # Install the agent |
       sudo weka version get 4.2.7.64                                      # Get the full software
       sudo weka version set 4.2.7.64                                      # Set a default version
       ```
2. **Configure the WEKA container for running protocols:**

* To configure the protocol containers with **DPDK** networking for optimal performance, run the following command:

```

```bash
sudo weka local setup container --name frontend0 --only-frontend-cores --cores 1 --join-ips <EXISTING-BACKEND-IP> --allow-protocols true --net=<NETWORK_INTERFACE>/<IP_ADDRESS>/<SUBNET_MASK>
```

```

Note: Replace the network configuration parameters with values appropriate for your environment. For example: `--net=eth1/192.168.114.50/24`.

* To configure the protocol containers with **UDP** networking, run the following command:

```

```
sudo weka local setup container --name frontend0 --only-frontend-cores --cores 1 --join-ips <EXISTING-BACKEND-IP> --allow-protocols true --net=UDP
```

```

3. **Verify protocol server configuration:** Confirm the dedicated protocol servers have joined the cluster:

```
weka cluster containers
```

Expected response example:

```bash
CONTAINER ID  HOSTNAME        CONTAINER  IPS              STATUS  RELEASE  FAILURE DOMAIN  CORES  MEMORY   LAST FAILURE  UPTIME
42            protocol-node1  frontend0  192.168.114.31   UP      4.2.7.64 AUTO            1      1.47 GB                0:09:54h
43            protocol-node2  frontend0  192.168.114.115  UP      4.2.7.64 AUTO            1      1.47 GB                0:09:08h
44            protocol-node3  frontend0  192.168.114.13   UP      4.2.7.64 AUTO            1      1.47 GB                0:04:46h
```

With dedicated protocol servers in place, proceed to manage individual protocols.

**Related topics**

*
*
*

<!-- ============================================ -->
<!-- File 111/259: additional-protocols_nfs-support.md -->
<!-- ============================================ -->

---
description:
---

# Manage the NFS protocol

NFS (Network File System) is a protocol that enables clients to access the WEKA filesystem without requiring WEKA's client software. This leverages the standard NFS implementation of the client's operating system.

WEKA supports an advanced NFS implementation, NFS-W, designed to overcome inherent limitations in the NFS protocol. NFS-W is compatible with NFSv3 or NFSv4 protocols and offers enhanced capabilities, including support for more than 16 user security groups and NFS file-locking.

Note: The legacy NFS stack is no longer supported.

## NFS service deployment guidelines and requirements

Adhere to the following guidelines and requirements when deploying the NFS service.

### **Configuration filesystem**

A persistent cluster-wide configuration filesystem is required for the protocol's internal operations using NFSv4 or Kerberos integration. See #dedicated-filesystem-for-persistent-protocol-configurations-requirement.

### **Interface groups**

An interface group is a configuration framework designed to optimize resiliency among NFS servers. It enables the seamless migration of IP addresses, known as floating IPs, from an unhealthy server to a healthy one, ensuring continuous and uninterrupted service availability.

An interface group consists of the following:

* A collection of WEKA servers with a network port for each server, where all the ports must be associated with the same subnets. For resiliency, a minimum of two NFS servers are required.
* A collection of floating IPs to support the NFS protocol on specified servers and NICs. All IP addresses are required to be within the same subnet, and the servers must already have static IP addresses on those NICs within that subnet.
* A routing configuration for the IPs. The IP addresses must comply with the IP network configuration.

Note: Floating IPs are supported on AWS but not on Azure, GCP, and OCI cloud environments.

An interface group can have only a single port. Therefore, two interface groups are required to support High Availability (HA) in NFS. When assigning the other server ports to these interface groups, consider the network topology to ensure no single point of failure exists in the switch.

You can define up to 10 different Interface groups. Use multiple interface groups if the cluster connects to multiple subnets. You can set up to 50 servers in each interface group.

The WEKA system automatically distributes the IP addresses evenly on each server and port. If a server fails, the WEKA system redistributes the IP addresses associated with the failed server to other servers.

Note: The WEKA system automatically configures the floating IP addresses used by the NFS service on the appropriate server. Refrain from manually configuring or using the floating IP.

### Round-robin DNS server configuration

To ensure load balancing between the NFS clients on the different WEKA servers serving NFS, it is recommended that a round-robin DNS entry be resolved to the list of floating IPs.

Note: Set the TTL (Time to Live) for all records assigned to the NFS servers to 0 (Zero). This action ensures that the client or the DNS server does not cache the IP.

**Related information**

Round-robin DNS

### NFS client mount

The NFS client mount is configured using the standard NFS stack operating system. The NFS server IP address must point to the round-robin DNS name.

### Access Control List (ACL) in NFS

Access Control List (ACL) in NFS (Network File System) provide fine-grained control over file permissions, offering more flexibility than traditional POSIX permissions. NFS supports multiple ACL flavors, each serving different use cases and interoperability needs.

Note: To enable ACL functionality, you must configure LDAP to manage user and group information.

**ACL types in NFS**

NFS supports the following ACL types:

* **None**: No ACL enforcement or updates occur, even if POSIX ACLs exist on a file or directory. This flavor is used when ACL management is unnecessary.
* **POSIX**: NFS enforces POSIX ACLs, ensuring compatibility with other protocols. However, the finer granularity of NFSv4 ACLs is lost when mapped to POSIX ACLs. This option is suitable for environments requiring basic ACL management across multiple protocols.
* **NFSv4**: NFSv4 ACLs are enforced directly, without mapping to POSIX ACLs. This flavor preserves the full granularity of NFSv4 ACLs but does not support interoperability with other protocols. ACLs are stored as extended attributes and mapped to user and group IDs (UID/GID). Use NFSv4 when full NFSv4 ACL granularity is required, and interoperability with other protocols is not a concern.
* **Hybrid**: This flavor combines both POSIX and NFSv4 ACLs to support interoperability. NFS ensures consistency between the two ACL types, and if any inconsistency arises, POSIX ACL is used for enforcement. Hybrid is ideal for environments requiring both interoperability and full NFSv4 ACL functionality.

Note: **NFSv3 and ACLs:** The NFSv3 implementation does not support ACLs. Access control for NFSv3 clients is enforced by the underlying filesystem using standard POSIX file permissions.

**Managing ACLs in NFS**

ACL configuration and management in NFS can be done through various interfaces:

* **CLI**: The `weka nfs permission` and `weka nfs global-config` commands allow users to configure ACLs at the permission and cluster level. NFS permissions exporting files from the same backend file system must share the same ACL flavor.
* **GUI**: The NFS settings in the user interface include options to enable ACLs and configure default ACL flavors (None, POSIX, NFSv4, Hybrid). Changes to ACL settings may require restarting the NFS containers.
* **Configuration filesystem**:\
  ACL flavors and related configurations are tracked in the global configuration filesystem, ensuring consistent management of permissions across the system.

**Upgrading and ACLs**

When upgrading, the default ACL flavor for all permissions sets to **POSIX**. ACLs are enabled by default. To ensure proper ACL functionality, both `.config_fs` and LDAP must be configured.

### NFS integration with Kerberos service

WEKA facilitates the seamless integration of NFS with an existing Kerberos service. This integration enables clients' authentication, data integrity, and data privacy over the wire when interacting with the NFS server, ensuring robust security even across untrusted networks.

The Kerberos security levels are:

* **krb5:** Implements basic Kerberos authentication.
* **krb5i:** Incorporates Kerberos authentication with data integrity assurance.
* **krb5p:** Integrates Kerberos authentication with data integrity and privacy measures.

Note: NFS exports created before configuring Kerberos are not updated automatically when using Kerberos. The Authenticator Type must be modified to one of the Kerberos types to leverage the Kerberos advantages.

#### Kerberos LDAP configurations

WEKA supports Kerberos authentication for NFS using AD and Kerberos MIT:

* **Active Directory (AD):** NFS integrates with Active Directory (AD), which includes built-in Kerberos services. WEKA interacts with the AD using the Kerberos protocol to authenticate service requests among trusted devices.
* **Kerberos MIT:** NFS integrates with Kerberos MIT, implementing the Kerberos protocol, which uses secret-key cryptography for authentication across insecure networks. This protocol is widely standardized and utilized.

#### Kerberos service interactions basic outline

The following Kerberos service interactions ensure secure communication between the client and the WEKA NFS server:

1. **Client authentication and ticket request:** The client sends a request, including encrypted credentials, to the Authentication Server for a Ticket Granting Ticket (TGT).
2. **Ticket generation and delivery:** The Authentication Server verifies the client‚Äôs identity, generates a session key, forms a TGT, and sends these to the client.
3. **Ticket extraction and service request:** The client decrypts the received message, extracts the session key and the TGT, and sends a service request to the Ticket Granting Server.
4. **Service session key generation & ticket formation:** The Ticket Granting Server verifies the TGT, generates a Service Session Key, and forms a Service Ticket.
5. **Service ticket delivery & extraction:** The Ticket Granting Server sends the Service Ticket and the Service Session Key to the client, who then decrypts the response and extracts these for later use.
6. **Service access & verification:** The client generates an authenticator for the network service and sends it along with the Service Ticket to the network service, which then verifies the Service Ticket and the authenticator.

Note: This diagram illustrates the Kerberos service interactions in a simplified manner. It highlights how secure communication is established over insecure networks. Note that this is a broad representation, and actual implementations may differ.

### Scalability, load balancing, and resiliency

For performance scalability, add as many servers as possible to the interface group.

The cluster supports a maximum of 200 floating IPs to facilitate load balancing by distributing them evenly across all interface group servers and ports. In systems with more NFS interfaces than this limit, not every interface will have a dedicated floating IP.

When different clients resolve the DNS name into an IP service, each receives a different IP address, ensuring that other clients access different servers. This design allows the WEKA system to scale and service thousands of clients.

To ensure service resilience, if a server fails, the system reassigns all IP addresses associated with the failed server to other servers using GARP network messages. The clients then reconnect to the new servers without any reconfiguration or service interruption.

### NFS file-locking support

WEKA supports NFS byte-range advisory locking for NFS versions 3, 4, and 4.1. This mechanism ensures synchronized access to files in a networked environment by allowing multiple processes to coordinate access to shared files. It helps maintain data integrity and consistency by preventing concurrent modifications that could lead to data corruption. WEKA‚Äôs implementation is interoperable with POSIX byte-range advisory locks, enabling compatibility and coordination between NFS clients and WEKA‚Äôs filesystem.

#### NFS file-locking prerequisites for NFSv3

* **Port prerequisites:** Ports used by the `nlockmgr` and `status` services must be open on the clients and WEKA servers. Use **one** of the following methods to meet this requirement:
  *   Disable and stop `firewalld` using the commands:

      ```
      systemctl stop firewalld.service
      systemctl disable firewalld.service
      ```
  *   Define the ports in `/etc/services` and restart the `rpc.statd` (ensure the port numbers are open). For example:

      ```
      status\t\t46999/tcp\t\t# rpc status
      status\t\t46999/udp\t\t# rpc status
      nlockmgr \t47000/tcp\t\t# nlockmgr
      nlockmgr \t47000/udp\t\t# nlockmgr
      ```
* **NFS client prerequisite:** To use NFSv3 with locking on an NFS client, ensure the `rpc.statd` service runs in the NFS client. This enables clients to mount NFSv3 shares.

#### View file locks

To inspect the active locks on a specific file, use the following command:

```bash
weka debug flock list <inode-id>
                      [--snap-view-id snap-view-id]
                      [--verbose]
```

* `<inode-id>`: The unique identifier of the file‚Äôs inode.
* `--snap-view-id snap-view-id`: (Optional) Specifies the snapshot view ID for listing locks on a file within a particular snapshot.
* `--verbose`: (Optional) Provides detailed lock information, including the lock owner and type.

This command outputs a list of all current locks on the specified file, enabling administrators to monitor and manage file access effectively.

## NFS service deployment high-level workflow

For detailed procedures, see the related topics.

**Related topics**

    A file locking mechanism where processes voluntarily check for and honor locks set by others. It requires all cooperating processes to follow the locking protocol, as the operating system does not enforce it automatically.

<!-- ============================================ -->
<!-- File 112/259: additional-protocols_nfs-support_nfs-support.md -->
<!-- ============================================ -->

---
description: This page describes how to configure the NFS networking using the GUI.
---

# Manage NFS networking using the GUI

Using the GUI, you can:

* Configure the NFS global settings
* Configure the NFS cluster level
* Integrate the NFS and Kerberos service
* Configure the NFS export level (permissions)

## Configure the NFS global settings

NFS global settings consist of parameters that enable you to customize various aspects of the NFS service, including the support of the NFS protocol versions, the types of Kerberos authentication to use, and the port for mount requests and NFS locking.

By tailoring these settings, you can ensure that the NFS service meets your needs and requirements, such as supporting NFS V3 and V4 for compatibility with different client systems.

Note: The possible Kerberos authentication types are available only after configuring the Kerberos integration.

**Before you begin**

To support NFS file-locking, ensure the system meets the prerequisites outlined in #nfs-file-locking-support.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. On the left pane, select **NFS**.
3. Select the **Settings** tab.

4. In the Global Settings section, select **Update**, and do the following:
   * **Config FS**: Select the cluster-wide configuration filesystem that maintains the NFS and Kerberos configurations.
   * **Supported Versions**: Select the NFS versions you want to support based on your needs. Options include V3, V4, or both.
   * **ACL**: Sets the ACL to ON (default) or OFF.
   * **ACL Type**: Defines the default access control method for the share. Options are:
     * **None:** No ACL enforcement or updates, regardless of existing POSIX ACLs.
     * **POSIX** (default): Enforces POSIX ACLs, compatible across protocols, but loses NFSv4's finer granularity.
     * **NFSv4**: Enforces NFSv4 ACLs directly, retaining full granularity, but lacks interoperability with other protocols.
     * **Hybrid**: Combines both POSIX and NFSv4 ACLs to support interoperability. NFS ensures consistency between the two ACL types, and if any inconsistency arises, POSIX ACL is used for enforcement.
   *   **Authentication Type**: Enable the authentication types that can be used when setting the NFS client permissions. \
       Possible values:

       * NONE: No authentication.
       * SYS: System authentication.
       * KRB5: Basic Kerberos authentication.
       * KRB5i: Kerberos authentication with data integrity.
       * KRB5p: Kerberos authentication with data integrity and privacy.

       The Kerberos authentication types are visible only if Kerberos is configured.\
       Example: KRB5  KRB5i  KRB5p.\
       The default values depend on Kerberos configuration:

       * If not configured: NONE SYS
       * If configured: KRB5
   * **Mount Port:** Set the port that the mountd service binds to.
   * **Lock Manager Port**: Set the port for the network lock manager‚Äôs registration.
   * **Status Monitor Port:** Set the port for the network status monitor‚Äôs registration.
   * **Notification Port:** Set the port for the notification‚Äôs registration.

Note: These ports are only relevant for NFSv3. The default value of 0 indicates using the default published ports.

5. Select **Save** to apply the settings.

## **Configure the NFS cluster level** <a href="#create-interface-groups" id="create-interface-groups"></a>

Configuring the NFS cluster level involves creating an interface group and assigning at least one server with its corresponding port.

### Create an interface group <a href="#create-interface-groups" id="create-interface-groups"></a>

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. On the left pane, select **NFS**.
3. In the Configuration tab, select the **+** sign near the Interface Groups title.

4. In the Create Interface Group dialog, set the following properties:
   * **Name**: A unique interface group name (maximum 11 characters).
   * **Gateway**: A valid IP address of the gateway.
   * **Subnet mask**: The subnet mask in CIDR (Classless Inter-Domain Routing) format. For example, a value of 16 equals 255.255.0.0.
5. Select **Save**.

### Set interface group ports

After creating an interface group, set the ports for this group to establish the NFS cluster. You can only set these ports on frontend containers. To ensure system resiliency, have at least two NFS servers in place.

Repeat this port setting process for each server participating in the NFS cluster.

**Procedure**

1. In the Configuration tab, select the interface group.
2. In the Group Ports table, select **+Create**.
3.  In the Add Port dialog, set the following properties:

    * **Hostname**: Select the server on which the port resides.
    * **Port:** Select the port from the list.

    Select **Save**.

#### Example

### Remove an interface group port

You might need to remove an interface group due to a change in network configuration, for efficiency, for troubleshooting, during network reorganization, or to replace it with a more suitable group. Always check that the group isn‚Äôt in use before you remove it to avoid disruptions.

**Procedure**

1. In the Configuration tab, select the interface group.
2. In the Group Ports table, select the three dots, and from the menu, select **Remove**.

### **Set interface group IPs**

Note: Floating IPs are not supported in WEKA installations on Azure and GCP.

**Procedure**

1. In the Configuration tab, select the interface group.
2. In the Group IPs table, select **+Create**.
3. In the Add Range IP dialog, set the relevant IP range.
4. Select **Save**.

### Remove an interface group IP range

**Procedure**

1. In the Configuration tab, select the interface group.
2. In the Group IPs table, select the three dots, and from the menu, select **Remove**.

## Integrate the NFS and Kerberos service

Integrating the NFS and Kerberos service is critical to setting up a secure network communication process. This procedure involves defining the Key Distribution Center (KDC) details, administrative credentials, and other parameters to ensure a robust and secure authentication process.

**Before you begin**

* Ensure a configuration filesystem is set. See #configure-nfs-global-settings.
* Ensure the NFS cluster is configured and running. See #create-interface-groups.
* For Active Directory (AD) integration, obtain the required information from the AD administrator. (WEKA handles the generation of the keytab file.)
* For MIT integration, obtain the required information from the MIT KDC and OpenLDAP administrators, and a pre-generated keytab file stored in an accessible location is required.

Note: In all KDC and LDAP parameters, use the FQDN format. The hostname part of the FQDN is restricted to a maximum of 20 characters.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. On the left pane, select **NFS**.
3. Select the **Settings** tab.
4. In the Kerberos Authentication section, select **Configure**.

Note: Configuring the NFS-Kerberos service integration automatically restarts the NFS containers, leading to a temporary disruption in the IO service for connected NFS clients.

5. Choose the tab that matches your authentication method and follow its instructions.

1) From the Kerberos Authentication Type, select Active Directory (AD).
2) Set the following parameters to configure the Kerberos with AD KDC servers:
   * **KDC Realm Name**: Specifies the realm (domain) used by Kerberos.
   * **KDC Primary Server**: Identifies the server hosting the primary Key Distribution Center service.
   * **KDC Secondary Server**: Identifies the server hosting the secondary Key Distribution Center service.
   * **KDC Admin Server**: Identifies the server hosting the administrative Key Distribution Center service.
3) Set the following parameters to register the Kerberos service:
   * **NFS Service Name**: This refers to the complete domain name for a specific NFS server.
   * **KDC Realm Admin Name**: The username of an administrator who has access to the LDAP directory. This user manages the KDC within a realm.
   * **KDC Realm Admin Password**: The password of the administrative user who manages the KDC within a realm.
4) Select **Save** to apply the changes.

1. In From the Kerberos Authentication Type, select MIT.
2. Set the following parameters to the MIT KDC servers:
   1. **KDC Realm Name**: Specifies the realm (domain) used by Kerberos.
   2. **KDC Primary Server**: Identifies the server hosting the primary Key Distribution Center service.
   3. **KDC Secondary Server**: Identifies the server hosting the secondary Key Distribution Center service.
   4. **KDC Admin Server**: Identifies the server hosting the administrative Key Distribution Center service.
3. Set the following parameters to register the Kerberos with LDAP service and uploaded keytab file:
   * **NFS Service Name**: This refers to the complete domain name for a specific NFS server.
   * **Upload keytab file**: Use the **Browse** option to upload the pre-generated keytab file. This file contains the keys for the NFS service‚Äôs unique identity, known as a principal, in Kerberos.
   * **LDAP Server**: Specifies the server hosting the Lightweight Directory Access Protocol service.
   * **LDAP Domain**: Defines the domain that the Lightweight Directory Access Protocol service will access.
   * **LDAP Reader User Name**: The username of an administrative user, used to generate the keytab file.
   * **LDAP Reader User Password**: The password of the administrative user.
   * **LDAP Base DN**: The base Distinguished Name (DN) for the Lightweight Directory Access Protocol directory tree.
   * **LDAP Port**: The port number on which the Lightweight Directory Access Protocol server listens.
4. Select **Save** to apply the changes.

Note: After completing the kerberos integration settings, the enabled authentication type is **KRB5**. If you want to modify the enabled authentication types, in the Configure NFS Global Settings, select **Update**, and set the authentication types. See #configure-the-nfs-global-settings.

### **Reset the** Kerberos configuration <a href="#create-interface-groups" id="create-interface-groups"></a>

Resetting the Kerberos configuration is necessary when you need to completely remove the Kerberos service configuration data. Once the data is removed, you can set up a new Kerberos service integration.

Upon resetting the Kerberos configuration, it triggers the following two actions:

* The NFS containers are restarted, leading to a temporary disruption in the I/O service for connected NFS clients.
* The authentication types in the NFS Global Settings are reset to their default values.

Note: These actions may impact your system‚Äôs performance and functionality. Proceed with caution.

## Configure the NFS export level (permissions)

### Create client access groups <a href="#define-client-access-groups" id="define-client-access-groups"></a>

Creating additional client groups helps in better organization, customization of settings, and enhanced security by segregating access levels.

**Procedure**

1. In the Permissions tab, select the **+** sign near the Client Groups title.

2. In the Create Client Group dialog, set the client group name.
3. Select **Save**.

### Assign a DNS and IP to a client group

Assigning a DNS and IP to a client group facilitates network communication and resource access. This step is crucial for the group‚Äôs operational functionality.

**Procedure**

1. In the NFS configuration, select the **Permissions** tab.
2. In the Permissions tab, select **Add DNS** for the relevant Client Group.

3. In the Create Client Group DNS Rule dialog, set the DNS server name. Then, select **Save**.

4. In the Permissions tab, select **Add IP** for the relevant Client Group.
5. In the Create Client Group IP Rule dialog, set the IP address and bitmask. Then, select **Save**.

### Remove the DNS or IP of a client group

**Procedure**

1. In the Permissions tab, select the **trash** symbol displayed next to the DNS or IP for the relevant Client Group.

### Create NFS client permission <a href="#create-nfs-client-permission" id="create-nfs-client-permission"></a>

Creating NFS permissions for a client group enhances access control and efficiency. It allows system administrators to manage access to files, protecting sensitive data and simplifying permission management.

NFS permissions also provide flexibility and foster collaboration. They can be adjusted as needed, especially when a team needs to work on the same files. However, they work best in trusted environments.

**Before you begin**

If you create an NFS v4 client permission, verify that a global configuration filesystem is already set in the system. See #configure-the-nfs-global-settings.

**Procedure**

1. In the Permissions table, select **+Create**.

2. In the Create NFS Permission dialog, set the following properties:
   * **Client Group**: The client group to which the permissions are applied.
   * **Filesystem**: The filesystem to which the permissions are applied. A filesystem with Required Authentication set to ON cannot be used for NFS client permissions.
   * **Path**: The shared directory path (root share).
   * **Type**: The access type: RO (read-only) or RW (read/write).
   * **Priority:** Permissions are processed in ascending priority order during access evaluation, beginning with the lowest number. If a client matches multiple permission entries, the entry with the highest priority number determines the effective permission. Using a numbering system in tens (10, 20, 100) is advisable to facilitate the addition of priorities between existing ones.
   * **Supported Versions:** The supported NFS versions (V3, V4, or both).
   * **User Squash**: The system enforces squash mode with the client's permission.
   * **Authentication Types:** The method of authentication. The enabled authentication types in the NFS global settings determine the possible options and the default.\
     Examples:
     * Enabled types: NONE, SYS, KRB5, KRB5i, KRB5p. Default: KRB5.
     * Enabled types: NONE, SYS. Default: NONE, SYS.
     * Enabled types: NONE, SYS, KRB5i, KRB5p. Default: KRB5i.
   * **Anon. UID**: Anonymous user ID. Only relevant for Root and All user squashing.
   * **Anon. GID:** Anonymous group ID. Only relevant for Root and All user squashing.
3. Select **Save**.

### Edit NFS client permission <a href="#edit-nfs-client-permission" id="edit-nfs-client-permission"></a>

You can edit the existing NFS permission settings for a client group.  You can also move the priority to the top or bottom priority (related to other client group priorities). If the client group permission setting is no longer required, you can remove it.

**Procedure**

1. In the Permissions table, select the three dots of the client group to edit, and select **Edit**.

2. Set the relevant properties: Type, Priority, Supported Versions, Squash Root, Authentication Type, Anon. UID, and Anon. GID. Then, select **Save**.
3. To move the priority of a client group setting to the top or bottom priority, select **Move to top priority** or **Move to bottom priority**.
4. To remove the client group permission setting, select **Remove**.

**Related topics**

<!-- ============================================ -->
<!-- File 113/259: additional-protocols_nfs-support_nfs-support-1.md -->
<!-- ============================================ -->

---
description: This page describes how to configure the NFS networking using the CLI.
---

# Manage NFS networking using the CLI

Using the CLI, you can:

* Configure the NFS global settings
* Configure the NFS cluster level
* Integrate the NFS and Kerberos service
* Configure the NFS export level (permissions)

## Configure the NFS global settings

NFSv4 and Kerberos require a persistent cluster-wide configuration filesystem for the protocol's internal operations.

Use the following command line to set the NFS configuration on the configuration filesystem:

`weka nfs global-config set [--mountd-port mountd-port] [--config-fs config-fs] [--lockmgr-port lockmgr-port] [--statmon-port statmon-port] [--notify-port notify-port] [--acl acl] [--extended-stats extended-stats] [--default-acl-type default-acl-type] [--default-supported-versions default-supported-versions]... [--enable-auth-types enable-auth-types]... [--no-restart]`

Note: * To support NFS file-locking, ensure the system meets the prerequisites outlined in #nfs-file-locking-support.
* For the default published ports, see the #required-ports.

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | mountd-port | Set the alternate port if the existing mountd service is not operating on the default published port.0 means use the default published port. | 0 |
 | config-fs* | The predefined filesystem name for maintaining the persisting cluster-wide protocols' configurations.Verify that the filesystem is already created. If not, create it. For details, see #dedicated-filesystem-requirement-for-persistent-protocol-configurations |  |
 | lockmgr-port | Set the alternate port for the NFS lock manager used in NFSv3.0 means use the default published port. | 0 |
 | statmon-port | Set the alternate port for the NFS status monitor used in NFSv3.0 means use the default published port. | 0 |
 | notify-port | Set the alternate port for notification used in NFSv3.0 means use the default published port. | 0 |
 | acl | Enables or disables NFSv4 ACL. Options are: on or off. | on |
 | default-acl-type | Specifies the default ACL type. Options are none, posix, nfsv4, or hybrid. For details, see #access-control-list-acl-in-nfs. | posix |
 | default-supported-versions | Determines the default NFS version.Possible values: v3v4v3,v4 | v3 |
 | extended-stats | Enable or disable NFS statistics collection for each client and permission.Possible values: on or off | on |
 | enable-auth-types | A comma-separated list of authentication types that can be used when setting the NFS client permissions.Possible values: none,sys,krb5,krb5i,krb5pExample:krb5,krb5i,krb5p | Depends on Kerberos configuration:If not configured: none,sysIf configured: krb5 |
 | no-restart | Prevents the restart of NFS-W containers when applying changes. | False |

#### Show NFS global configuration

**Command:** `weka nfs global-config show`

**Example**

<pre><code>$ weka nfs global-config show
NFS Global Configuration
   mountd port: 0
     Config FS: .config_fs
   acl: on
   default acl type: posix
   Default Supported Versions: V3
<strong>   Enabled Auth Types: KRB5, KRB5i, KRB5p
</strong>   Default Auth Types: KRB5
   Supported Auth Types: NONE, SYS, KRB5, KRB5i, KRB5p

```

Note: The parameters `Default Auth Types` and `Supported Auth Types` are determined internally.

## **Configure the NFS cluster level**

### Create interface groups

**Command:** `weka nfs interface-group add`

Use the following command line to add an interface group:

`weka nfs interface-group add <name> <type> [--subnet subnet] [--gateway gateway]`

**Example**

`weka nfs interface-group add nfsw NFS  --subnet 255.255.255.0 --gateway 10.0.1.254`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | name* | Unique interface group name. |  |
 | type* | Group type.Can only be NFS. |  |
 | subnet | The valid subnet mask in the 255.255.0.0 format. | 255.255.255.255 |
 | gateway | Gateway valid IP. | 255.255.255.255 |

### Set interface group ports

**Commands:**

`weka nfs interface-group port add`

`weka nfs interface-group port remove`

Use the following command lines to add or remove an interface group port:

`weka nfs interface-group port add <name> <container-id> <port>`

`weka nfs interface-group port remove <name> <container-id> <port>`

**Example**

The following command line adds the interface `enp2s0` on the Frontend container-id `3` to the interface group named `nfsw`.

`weka nfs interface-group port add nfsw 3 enp2s0`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Interface group name. |
 | container-id* | Valid frontend container ID on which the port resides. You can obtain the container ID by running the weka cluster container command. |
 | port* | Valid port's device. Maximum 14 characters.Example: eth1. |

### Set interface group IPs

**Commands:**

`weka nfs interface-group ip-range add`

`weka nfs interface-group ip-range remove`

Use the following command lines to add or remove an interface group IP:

`weka nfs interface-group ip-range add <name> <ips>`

`weka nfs interface-group ip-range remove <name> <ips>`

**Example**

The following command line adds IPs in the range `10.0.1.101` to `10.0.1.118` to the interface group named `nfsw`.

`weka nfs interface-group ip-range add nfsw 10.0.1.101-118`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Interface group name |
 | ips* | Valid IP range |

### Configure the service mountd port

The mountd service receives requests from clients to mount to the NFS server. It is possible to set it explicitly rather than have it randomly selected on each server startup. This allows an easier setup of the firewalls to allow that port.

Use the following command lines to set and view the mountd configuration:

`weka nfs global-config set --mountd-port <mountd-port>`

`weka nfs global-config show`

### Configure user group resolution

NFS-W can authenticate more than 16 user groups, but it requires the external resolution of the user's groups, which means associating users with their respective group-IDs outside of the NFS protocol.

**Procedure**

1. **Configure interface groups:**
   * See Create interface groups.
2. **Configure NFS client permissions:**
   * See Set the NFS client permissions.
3. **Set up servers for group-IDs retrieval:**
   * Configure relevant servers to retrieve user group-IDs information.\
     This task is specific to NFS-W and does not involve WEKA management. See the following procedure.

<details>

<summary>Set up the servers to retrieve user's group-IDs information</summary>

For the servers that are part of the interface group, set the servers to retrieve the user's group-IDs information in any method that is part of the environment.

You can also set the group resolution by joining the AD and Kerberos domains or using LDAP with a read-only user.

Configure the `sssd` on the server to serve as a group IDs provider. For example, you can configure the `sssd` directly using LDAP or as a proxy to a different `nss` group IDs provider.

**Example: set `sssd` directly for `nss` services using LDAP with a read-only user**

```
[sssd]
services = nss
config_file_version = 2
domains = LDAP

[domain/LDAP]
id_provider = ldap
ldap_uri = ldap://ldap.example.com
ldap_search_base = dc=example,dc=com

# The DN used to search the ldap directory with.
ldap_default_bind_dn = cn=ro_admin,ou=groups,dc=example,dc=com

# The password of the bind DN.
ldap_default_authtok = password

```

If you use another method than the `sssd` but with a different provider, configure an `sssd proxy` on each relevant server. The proxy is used for the WEKA container to resolve the groups by any method defined on the server.

To configure `sssd proxy` on a server, use the following:

```
# install sssd
yum install sssd

# set up a proxy for WEKA in /etc/sssd/sssd.conf
[sssd]
services = nss
config_file_version = 2
domains = proxy_for_weka

[nss]
[domain/proxy_for_weka]
id_provider = proxy
auth_provider = none

# the name of the nss lib to be proxied, e.g., ldap, nis, winbind, vas4, etc.
proxy_lib_name = ldap
```

All users must be present and resolved in the method used in the `sssd` for the group's resolution. In the above example, using an LDAP-only provider, local users (such as a local root) absent in LDAP do not receive their groups resolved and are denied. For such users or applications, add the LDAP user.

</details>

## Integrate the NFS and Kerberos service

Integrating the NFS and Kerberos service is critical to setting up a secure network communication process. This procedure involves defining the Key Distribution Center (KDC) details, administrative credentials, and other parameters to ensure a robust and secure authentication process.

**Before you begin**

* Ensure a configuration filesystem is set. See #configure-the-nfs-global-settings.
* Ensure the NFS cluster is configured and running. see #configure-the-nfs-cluster-level.
* For Active Directory (AD) integration, obtain the required information from the AD administrator. (WEKA handles the generation of the keytab file.)
* For MIT integration, ensure the following:
  * Obtain the required information from the MIT Key Distribution Center (KDC) and OpenLDAP administrators.
  * A pre-generated keytab file in base64 format stored in an accessible location is required.

Note: In all KDC and LDAP parameters, use the FQDN format. The hostname part of the FQDN is restricted to a maximum of 20 characters.

### Set the Kerberos service

**Command:** `weka nfs kerberos service setup`

Use the following command to set up NFS Kerberos Service information:

`weka nfs kerberos service setup <kdc-realm-name> <kdc-primary-server> <kdc-admin-server> [--kdc-secondary-server kdc-secondary-server][--force] [--restart]`

**Example**

```

```
weka nfs kerberos service setup WEKA-REALM kdc.primary.weka.io kdc.admin.weka.io --kdc-secondary-server kdc.secondary.weka.io
```

```

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | kdc-realm-name* | Specifies the realm (domain) used by Kerberos. |  |
 | kdc-primary-server* | Identifies the server hosting the primary Key Distribution Center service. |  |
 | kdc-admin-server* | Identifies the server hosting the administrative Key Distribution Center service. |  |
 | kdc-secondary-server | Identifies the server hosting the secondary Key Distribution Center service. |  |
 | force | When used, it forces the action to proceed without further confirmation. Typically used when the service is configured or registered. | Not used |
 | restart | When used, the command restarts the NFS-W containers after the changes are applied. | Not used |

#### Show NFS Kerberos service setup information

**Command:** `weka nfs kerberos service show`

**Example**

```

```bash
$ weka nfs kerberos service show
REALM NAME          PRIMARY SERVER           SECONDARY SERVER   ADMIN SERVER           GENERATION ID     SERVICE STATUS
TEST.WEKALAB.IO     Zeus.test.wekalab.io                        Zeus.test.wekalab.io   1                 CONFIGURED
```

```

### Integrate Kerberos with AD

Integrating Kerberos with AD involves the following:

1. Register Kerberos with AD
2. Set up Kerberos to use AD LDAP

#### Register Kerberos with AD

**Command:** `weka nfs kerberos registration setup-ad`

Use the following command to register the Kerberos with Microsoft Active Directory:

`weka nfs kerberos registration setup-ad <nfs-service-name> <realm-admin-name> [realm-admin-passwd] [--force] [--restart]`

**Example**

```

```
weka nfs kerberos registration setup-ad myservicename.test.example.com myrealmadmin
```

```

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | nfs-service-name* | This refers to the complete domain name for a specific NFS server. |  |
 | realm-admin-name* | The username of an administrator who has access to the LDAP directory. This user manages the KDC within a realm. |  |
 | realm-admin-passwd | This parameter is for the password of the administrative user who manages the KDC within a realm.It‚Äôs not stored in the configuration for security reasons. If it‚Äôs not provided during setup, the system asks for it. The entered password isn‚Äôt shown on the screen to protect privacy and security. |  |
 | force | When used, it forces the action to proceed without further confirmation. Typically used when the service is configured or registered. | Not used |
 | restart | When used, the command restarts the NFS-W containers after the changes are applied. | Not used |

#### Set up Kerberos to use AD LDAP

**Command:** `weka nfs ldap setup-ad`

Use the following command to set up NFS configuration to use AD LDAP:

`weka nfs ldap setup-ad [--force] [--no-restart]`

**Example**

```

```
weka nfs ldap setup-ad
```

```

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | force | When used, it forces the action to proceed without further confirmation. Typically used when the service is configured or registered. | Not used |
 | no-restart | When used, it prevents NFS-W containers from restarting to apply changes. | Not used |

Note: In a successful operation, the system automatically restarts the NFS containers, leading to a temporary disruption in the IO service for connected NFS clients. However, if you want to avoid restarting the NFS-W containers, add the `--no-restart` option to the command line.

### Integrate Kerberos with MIT

Integrating Kerberos with MIT involves the following:

1. Register Kerberos with MIT
2. Set up Kerberos to use OpenLDAP

#### Register Kerberos with MIT

**Command:** `weka nfs kerberos registration setup-mit`

Use the following command to register the Kerberos with MIT KDC:

`weka nfs kerberos registration setup-mit <nfs-service-name> <keytab-file> [--force] [--restart]`

Note: To register the Kerberos service with MIT, a pre-generated  keytab file , stored in an accessible location, is required.

**Example**

```

```
weka nfs kerberos registration setup-mit myservicename.test.example.com myservicename.keytab
```

```

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | nfs-service-name* | Fully Qualified Domain Name (FQDN) for the NFS Service. This refers to the complete domain name for a specific NFS server. The hostname part of the FQDN is restricted to a maximum of 20 characters. |  |
 | keytab-file* | The path to the pre-generated keytab file containing the keys for the NFS service‚Äôs unique identity in base64 format. |  |
 | force | When used, it forces the action to proceed without further confirmation. Typically used when the service is configured. | Not used |
 | restart | When used, the command restarts the NFS-W containers after the changes are applied. | Not used |

#### Set up Kerberos to use OpenLDAP

**Command:** `weka nfs ldap setup-openldap`

Use the following command to set up Kerberos to use OpenLDAP:

`weka nfs ldap setup-openldap  <server-name> <ldap-domain> <reader-user-name>[reader-user-password] [--base-dn base-dn] [--ldap-port-number ldap-port-number][--force] [--no-restart]`

**Example**

```

```
weka nfs ldap setup-openldap myldapserver.test.example.com, myldapdomain.example.com, cn=readonly-user,dc=test,dc=example,dc=com
```

```

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | server-name* | Specifies the server hosting the Lightweight Directory Access Protocol service. |  |
 | ldap-domain* | Defines the domain the Lightweight Directory Access Protocol service will access. |  |
 | reader-user-name* | The username of an administrative user used to generate the keytab file. |  |
 | reader-user-password | The administrative user's password.(It is maintained in a configuration file.) |  |
 | base-dn | The base Distinguished Name (DN) for the Lightweight Directory Access Protocol directory tree. |  |
 | ldap-port-number | The port number on which the Lightweight Directory Access Protocol server listens. | 389 |
 | force | When used, it forces the action to proceed without further confirmation. Typically used when the service is configured or registered. | Not used |
 | no-restart | When used, it prevents NFS-W containers from restarting to apply changes. | Not used |

Note: In a successful operation, the system automatically restarts the NFS containers, leading to a temporary disruption in the IO service for connected NFS clients. However, if you want to avoid restarting the NFS-W containers, add the `--no-restart` option to the command line.

### Show Kerberos LDAP setup information

**Command:** `weka nfs ldap show`

**Example**

```

```bash
$ weka nfs ldap show
SERVER TYPE      LDAP DOMAIN      SERVER NAME  SERVER PORT  BASE DN  READER NAME  READER PASSWORD  GENERATION ID  SETUP STATUS
ActiveDirectory  test.wekalab.io               0                                                   1              CONFIGURED
```

```

### Clear the Kerberos LDAP configuration

**Command:** `weka nfs ldap reset`

Use the following command to clear the NFS LDAP configuration:

`weka nfs ldap reset [--force] [--no-restart]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | force | When used, it forces the action to proceed without further confirmation. Typically used when the service is configured. | Not used |
 | no-restart | When used, it prevents NFS-W containers from restarting to apply changes. | Not used |

### Show Kerberos registration information

**Command:** `weka nfs kerberos registration show`

**Example**

```bash
$ weka nfs kerberos registration show
NFS SERVICE NAME          NFS KDC TYPE        GENERATION ID      REGISTRATION STATUS
nfs.test.wekalab.io       ActiveDirectory     1                  REGISTERED
```

### Clear Kerberos configuration

**Command:** `weka nfs kerberos reset`

Use the following command to clear the NFS Kerberos service configuration:

`weka nfs kerberos reset [--force] [--no-restart]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | force | When used, it forces the action to proceed without further confirmation. Typically used when the service is configured or registered.Use this flag only to clear the configuration created by a previous call to weka nfs kerberos service setup succeeded. | False |
 | no-restart | Prevents NFS-W containers from restarting when applying changes. | False |

Note: In a successful operation, the system automatically restarts the NFS containers, leading to a temporary disruption in the IO service for connected NFS clients. However, if you want to avoid restarting the NFS-W containers, add the `--no-restart` option to the command line.

### Update Kerberos configuration during maintenance mode

Once the Kerberos integration with NFS is configured, there might be instances where the Kerberos setup is modified.

Note: Changes to the Kerberos configuration in a production environment are rare. We recommend making any necessary updates during periods of low load from NFS clients, such as when the system are in maintenance mode. This approach helps to minimize potential disruptions to your operations.

Select the relevant tab to learn what to do for each scenario:

Use this procedure if you want to add or remove a secondary KDC server:

```
kdc-secondary-server
```

**Procedure**

1. Run the command: `weka nfs kerberos reset --no-restart --force`
2. Run the command: `weka nfs kerberos service setup <options>`
3. Run one of the following commands:
   * **For AD implementation:** `weka nfs kerberos registration setup-ad <options> --restart`
   * **For MIT implementation:** `weka nfs kerberos registration setup-mit <options> --restart`

Use this procedure if one of the following is changed:

```
realm-admin-name
realm-admin-passwd
```

**Procedure**

Run the command: \
`weka nfs kerberos registration setup-ad --restart --force`

Use this procedure if one of the following is changed:

```
keytab-file
```

**Procedure**

Run the command:\
`weka nfs kerberos registration setup-mit <options> --restart --force`

Use this procedure if one of the following is changed:

```
reader-user-name
reader-user-password
```

**Procedure**

* For AD implementation, run the following:
  1. `weka nfs ldap reset --no-restart --force`
  2. `weka nfs ldap <setup-ldap> <options/params>`
* For MIT implementation, run the following:
  1. `weka nfs ldap reset --no-restart --force`
  2. `weka nfs ldap <setup-openldap> <options/params>`

### Configure NFS for LDAP with ACLs (without Kerberos)

**Command:** `weka nfs ldap setup-ad-nokrb`

Use the following command to configure NFS to use LDAP for ACLs only when Kerberos is not in use:

`weka nfs ldap setup-ad-nokrb <server-name> <ldap-domain> <nfs-service-name> <admin-user-name> [admin-user-password] [--force] [--no-restart]`

**Parameters**

 | Parameter | Description | Default |
 | --- | --- | --- |
 | server-name* | AD server name. |  |
 | ldap-domain* | AD domain. |  |
 | nfs-service-name* | NFS FQDN service name.Maximum 20 characters for hostname in FQDN. |  |
 | admin-user-name* | AD Admin Name |  |
 | admin-user-password | AD Admin password |  |
 | force | Force this action when Active Directory LDAP client is already setup. | False |
 | no-restart | Don't restart the NFS-W containers to apply changes. | False |

## **Manage the NFS export level (permissions)**

### Define client access groups <a href="#uploading-a-snapshot-using-the-ui" id="uploading-a-snapshot-using-the-ui"></a>

**Command:** `weka nfs client-group`

Use the following command lines to add or remove a client access group:

`weka nfs client-group add <name>`

`weka nfs client-group remove <name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Valid group name. |

### Manage client access groups' rules

Clients are part of groups when their IP address or DNS hostname match the rules of that group. Similar to IP routing rules, clients are matched to client groups according to the most specific matching rule.

**Command:** `weka nfs rules`

#### **Add DNS-based client group rules**

Use the following command lines to add a rule that causes a client to be part of a client group based on its DNS hostname:

`weka nfs rules add dns <name> <dns>`

**Example**

 `weka nfs rules add dns client-group1 hostname.example.com`

#### **Remove DNS-based client group rules**

Use the following command lines to remove a rule that causes a client to be part of a client group based on its DNS hostname:

`weka nfs rules remove dns <name> <dns>`

**Example**

`weka nfs rules remove dns client-group1 hostname.example.com`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Valid client group name. |
 | dns* | DNS rule with *?[] wildcard rules. |

#### **Add IP-based client group rules**

**Command:** `weka nfs rules`

Use the following command lines to add or remove a rule which causes a client to be part of a client group based on its IP and subnet mask (both CIDR and standard subnet mask formats are supported for enhanced flexibility):

`weka nfs rules add ip <name> <ip>`

**Examples**

 `weka nfs rules add ip client-group1 192.168.114.0/8`\
 `weka nfs rules add ip client-group2 172.16.0.0/255.255.0.0`

#### **Remove IP-based client group rules**

`weka nfs rules remove ip <name> <ip>`

**Examples**

 `weka nfs rules remove ip client-group1 192.168.114.0/255.255.255.0`\
 `weka nfs rules remove ip client-group2 172.16.0.0/16`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | Valid client group name. |
 | ip* | Valid IP address with subnet mask.Both CIDR and standard subnet mask formats are supported for enhanced flexibility.CIDR format: 1.1.1.1/16 Standard format: 1.1.1.1/255.255.0.0 |

### **Manage NFS client permissions**

**Command:** `weka nfs permission`

Use the following command lines to add NFS permissions:

`weka nfs permission add <filesystem> <group> [--path path] [--permission-type permission-type] [--root-squashing root-squashing] [--squash squash] [--anon-uid anon-uid] [--anon-gid anon-gid] [--obs-direct obs-direct] [--manage-gids manage-gids] [--privileged-port privileged-port] [--acl-type acl-type] [--force-acl-type force-acl-type] [--supported-versions supported-versions]... [--enable-auth-types enable-auth-types]... [--no-restart]`

Use the following command lines to update NFS permissions:

`weka nfs permission update <filesystem> <group> [--path path] [--permission-type permission-type] [--squash squash] [--anon-uid anon-uid] [--anon-gid anon-gid] [--obs-direct obs-direct] [--manage-gids manage-gids] [--privileged-port privileged-port] [--acl-type acl-type] [--force-acl-type force-acl-type] [--supported-versions supported-versions]... [--enable-auth-types enable-auth-types]... [--no-restart]`

Use the following command lines to remove NFS permissions:

`weka nfs permission remove <filesystem> <group> [--path path]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | filesystem* | Existing filesystem name.A filesystem with Required Authentication set to ON cannot be used for NFS client permissions. |  |
 | group* | Existing client group name. |  |
 | path | The root of the valid share path. | / |
 | root-squashing | Toot squashing is a security feature in NFS that prevents root users on client machines from having root privileges on the NFS server. When enabled, it typically maps the root user to an anonymous UID/GID.Possible values: on, off | off |
 | permission-type | Permission type.Possible values: ro (read-only), rw (read-write) | rw |
 | squash | Permission squashing. Possible values: none, root, allThe option 'all' can be used only on interface groups with --allow-manage-gids=on | none |
 | anon-uid* | Anonymous user ID.Relevant only for root squashing.Possible values: 1 to 65535. | 65534 |
 | anon-gid* | Anonymous user group ID.Relevant only for root squashing.Possible values: 1 to 65535. | 65534 |
 | obs-direct | See Object-store Direct Mount.Possible values: on, off. | on |
 | manage-gids | Sets external group IDs resolution.The list of group IDs received from the client is replaced by a list determined by an appropriate lookup on the server.Possible values: on, off. | off |
 | privileged-port | Sets the share only to be mounted via privileged ports (1-1024), usually allowed by the root user.Possible values: on, off. | off |
 | acl-type | Specifies the ACL type. Possible values: none, posix, nfsv4, and hybrid. For details, see #access-control-list-acl-in-nfs. | Default is determined by the NFS global configuration. |
 | force-acl-type | Forces a change to the ACL type for existing permissions on the same filesystem. Possible values: on or off. | off |
 | supported-versions | A comma-separated list of supported NFS versions.Possible values: v3, v4. | The default-supported-versions setting in NFS global settings determines the default NFS version. |
 | enable-auth-types | A comma-separated list of NFS authentication types.Possible values are determined by the enable-auth-types in NFS global settings. | The default-auth-types in NFS global settings determine the default. |
 | no-restart | Prevents NFS-W containers from restarting when applying changes. | False |

### View connected NFS clients

**Command:** `weka nfs clients show`

Use the following command line to view insights of NFS clients connected to the NFS-W cluster in JSON output format.

`weka nfs clients show [--interface-group interface-group] [--container-id container-id] [--fip floating-ip] [--fsnames fsnames]...`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | interface-group | Interface group name.A filter to show only the clients connected to the containers in the specified group. | The output shows all clients connected to any container in the NFS-W cluster regardless of the assigned interface group. |
 | container-id | NFS-W container ID.A filter to show only the clients connected to the specified container ID. | The output shows all clients connected to any container in the NFS-W cluster. |
 | fip | Destination floating IP address. | The output shows all clients connected to all floating IP addresses. |
 | fsnames | A comma-separated list of filesystems. | All NFS exported filesystems are selected. |

<!-- ============================================ -->
<!-- File 114/259: additional-protocols_nfs-support_supported-nfs-client-mount-options.md -->
<!-- ============================================ -->

---
description:
---

# Supported NFS client mount parameters

To ensure optimal performance and reliability when using NFS clients with WEKA, it is essential to configure specific mandatory and recommended mount parameters. These parameters have been tested and validated in various real-world scenarios.

## **Mandatory parameters**

The following parameters **must** be included alongside the client's default mount options:

* **NFSv3** and **NFSv4**: `proto=tcp`

## Recommended parameters

For enhanced performance and stability, include the following parameters in addition to the mandatory ones:

* **NFSv3**:
  * `hard`
  * `vers=3`
* **NFSv4**:
  * `hard`
  * `vers=4`

## Additional information

* **Specifying NFS Client Version:** Always explicitly define the NFS client version (`vers=3` or `vers=4`) to prevent unexpected protocol negotiation during server configuration changes.
* **Resiliency to network interruptions:** Use the `hard` option to ensure the client retries operations during temporary network interruptions, maintaining data integrity and operation continuity.
* **Improving NFS performance:** For the latest WEKA versions, consider setting the `nconnect` parameter to a value greater than `1` to optimize NFS performance by enabling multiple TCP connections.
* **Default NFS client options:** Beyond the parameters listed above, the default options negotiated by the NFS client at mount time are suitable for most use cases. For advanced configurations or additional NFS client options, refer to the documentation provided by your operating system.

**Related topic**

<!-- ============================================ -->
<!-- File 115/259: additional-protocols_smb-support.md -->
<!-- ============================================ -->

---
description:
---

# Manage the SMB protocol

SMB (Server Message Block) is a network file-sharing protocol that facilitates connections to shared file and print services from remote systems. WEKA implements a modern SMB stack, referred to as SMB-W, which supports SMB versions 2 and 3.

WEKA SMB-W is an implementation of an SMB protocol enabling Windows, Linux CIFS, and macOS clients to access WEKA storage services with support for multi-protocol concurrent file access through SMB stack.

## Key features of SMB in WEKA

The SMB-W stack is designed for scalability, resilience, and distributed performance.

* **Scalability**: WEKA supports an SMB-W cluster consisting of 3 to 8 servers, each running the SMB gateway service. The backend can be any WEKA filesystem, with no limitations on size or performance.
* **Resilience**: SMB-W provides clustered file access with transparent failover. If a server fails, another server in the cluster automatically takes over operations, maintaining high availability.
* **Distribution**: All servers in the SMB-W cluster manage SMB filesystems concurrently. Performance scales with added hardware. SMB-W supports SMB Multichannel and SMB Direct, delivering advanced throughput and reliability.

### Advanced capabilities of SMB-W

* **SMB multichannel:** Enhances performance by leveraging multiple network connections simultaneously. Supported for properly configured SMB clients.
* **High availability and failover:** If an SMB-W container becomes isolated from the cluster, it stops automatically. Other servers take over its operations. (To manually restart a stopped container, run: `weka local restart smbw`).
*   **SMB Direct:** Enables SMB over RDMA (Remote Direct Memory Access) for reduced latency and improved performance.

    To enable SMB Direct, ensure the following prerequisites are met:

    * SMB-W servers are RDMA-enabled in both hardware and OS.
    * For Windows clients, configure the SMB client as multichannel.
    * When configuring a CIFS client to work with RDMA, perform the mounting on the host IP (not the floating IP).

## **SMB usage considerations**

* When managing SMB-W clusters through the GUI, note that any CLI limitations also apply to the GUI.
* Use ASCII format for fields such as domain names and share names.

#### Public cloud requirements

Before deploying SMB-W in public cloud environments:

* Ensure Active Directory and DNS services are properly configured.
* For AWS, WEKA has been validated with:
  * AWS Managed Microsoft AD
  * Amazon Route 53 Resolver

Follow AWS guidance to configure these services if they are not already deployed.

Note: High availability (HA) for SMB-W is not supported in public cloud environments.

## SMB-W user mapping in WEKA

Authentication in the WEKA SMB-W cluster is integrated with a single Active Directory (AD) that can include multiple trusted domains. For SMB-W access, the Active Directory must support POSIX user (UID) and group (GID) resolution.

### **ID mapping from Active Directory**

WEKA supports two types of ID mapping from Active Directory:

* **RFC2307 mapping:** Requires `uidNumber` and `gidNumber` attributes to be defined for users and groups in AD. This method provides explicit control over UID/GID values.
*   **RID mapping:** Automatically maps AD users and groups without requiring additional attribute configuration. User IDs are derived from the AD security identifier (SID), simplifying deployment.

    (Changes to RID range configuration may affect UID/GID resolution and result in mismatches.)

### **Relevant Active Directory attributes**

For **RFC2307** mapping, ensure the following attributes are met:

* Users:
  * `uidNumber`: 0-4290000000
  * `gidNumber`: 0-4290000000; must correlate with a real group
* Groups:
  * `gidNumber:` 0-4290000000

### **ID range configuration**

The WEKA system allows custom configuration of AD ID ranges to prevent UID/GID overlap across domains:

* Each trusted domain must have a distinct ID range.
* The primary domain uses a default configurable range.
* A fallback range is available for users not assigned to any domain.

To prevent ID collisions, configure non-overlapping ranges for all domains.

For authoritative reference on Active Directory schema attributes, consult Microsoft documentation.

## Workflow overview: configure SMB support

This workflow outlines the key steps to configure SMB-W support in the WEKA system. For detailed CLI and GUI procedures, refer to the related How-To sections.

**Before you begin**

Ensure that a dedicated filesystem exists for storing persistent protocol configurations. If not, create one. For guidance, #dedicated-filesystem-requirement-for-persistent-protocol-configurations.

**Workflow**

1. **Configure SMB-W cluster**:  Define the WEKA system servers that will participate in the SMB-W cluster and specify the Active Directory (AD) domain name.\
   In on-premises deployments, you can configure a pool of public IP addresses distributed across the SMB-W cluster. If a server fails, its IP addresses are reassigned to other servers in the cluster to maintain availability.
2. **Join the SMB-W cluster to the Active Directory domain:** Connect the WEKA system to the target AD domain. This includes required pre-configuration in AD and post-configuration in both the DNS Manager and Active Directory Users and Computers.
3. **Create SMB shares and set permissions:** Create the required shares and directories. By default, filesystem permissions are `root:root` with 755 access and must initially be set using a WEKA filesystem or NFS mount.

After the initial configuration, administrators can connect through Windows to manage and refine share-level permissions.

## **Round-robin DNS configuration for SMB-W load balancing**

To achieve effective load balancing across multiple WEKA servers running SMB-W, configure a round-robin DNS entry that resolves to the list of floating IPs assigned to the SMB-W cluster.

#### Configuration steps

1. **Create round-robin DNS entry**:
   * Define a DNS A record that maps to all floating IPs associated with the SMB-W servers.
   * The DNS entry must use the SMB-W cluster name, which must not exceed 15 characters.
2. **Set TTL to zero**:
   * Configure the TTL (Time to Live) value for each DNS record to 0.
   * This prevents client-side and recursive DNS caching, ensuring real-time IP resolution and balanced connection distribution.

#### Related Information

Refer to your DNS provider‚Äôs documentation for configuring round-robin DNS entries and TTL settings.

## SMB-W share creation

After configuring the SMB-W cluster, you can create SMB shares. Each share must include a unique name and a path within the target filesystem, either the root or a specific subdirectory.

* If no subdirectory is specified, the share maps to the root of the filesystem. Creating a separate root folder is not required in this case.
* To create subdirectories, mount the filesystem locally or through shell access. Create the required directories and set appropriate ownership and permissions before assigning them to SMB shares.

## Filesystem permissions and access rights for SMB-W

When integrating SMB-W clusters with Active Directory (AD), administrators can manage permissions and access rights for SMB-W filesystems using POSIX standards, Windows Access-Control Lists (ACLs), or a combination of both. This ensures secure, consistent access control across mixed environments.

#### **Grant root access**

To assign root-level access to an AD user:

* Set both the `uidNumber` and `gidNumber` attributes of the user to 0.

This setup grants full administrative control over all SMB-W shares.

#### **Access Control Lists (ACLs)**

ACLs in SMB-W allow fine-grained permission management for shares. Administrators can configure the following:

* **ACLs enabled:** Enables or disables Windows ACL support for the share. When enabled, the selected access control model is applied.
* **Access control model:**
  * **POSIX**: Enforces POSIX permission rules only.
  * **Windows**: Applies Windows-style security descriptors.
  * **Hybrid** _(default: POSIX)_: Supports both models, with the most recent change taking precedence.

This flexibility enables tailored access control strategies aligned with enterprise security policies and operational requirements.

## Integrate WEKA filesystem snapshots with Windows previous versions

WEKA supports integration with the Windows **Previous Version**s feature by creating filesystem snapshots using a specific access point format. When snapshots are labeled using the `@GMT_%Y.%m.%d-%H.%M.%S` syntax, they become visible to Windows clients through the Previous Versions tab.

#### Access snapshots from Windows

To view available snapshots:

1. Right-click any file or folder within a WEKA SMB-W share on a Windows client.
2. Select **Properties -> Previous Versions**.

Available snapshots are listed by timestamp, corresponding to the snapshot access point labels.

#### Example: Create a snapshot using the CLI

Use the following command to create a timestamped snapshot accessible from Windows:

```

```
$ weka fs snapshot create fs_name snapshot_name --access-point `TZ=GMT date +@GMT-%Y.%m.%d-%H.%M.%S`
```

```

Ensure the access point is generated in GMT to align with Windows expectations.

**Related topics**

<!-- ============================================ -->
<!-- File 116/259: additional-protocols_smb-support_smb-management-using-the-cli.md -->
<!-- ============================================ -->

---
description:
---

# Manage SMB using the CLI

Using the CLI, you can manage the SMB-W:

* Show the SMB cluster
* Show the SMB domain configuration
* Add an SMB cluster
* Update the SMB cluster
* Check the status of SMB cluster readiness
* Join an SMB cluster in Active Directory
* Delete an SMB cluster
* Add or remove SMB cluster containers
* Configure trusted domains
* List SMB shares
* Add an SMB share
* Update SMB shares
* Control SMB share user-lists
* Remove SMB shares
* Control SMB access based on hosts' IP/name

Note: The CLI refers to the feature as SMB, but it applies to SMB-W only. Support for the legacy SMB implementation has been removed.

## Show the SMB cluster <a href="#show-the-smb-cluster" id="show-the-smb-cluster"></a>

**Command:** `weka smb cluster`

Use this command to view information about the SMB cluster managed by the WEKA system.

## Show the SMB domain configuration <a href="#show-smb-domain-cfg" id="show-smb-domain-cfg"></a>

**Command:** `weka smb domain`

Use this command to view information about the SMB domain configuration.

## Add an SMB cluster <a href="#create-smb-cluster" id="create-smb-cluster"></a>

**Command:** `weka smb cluster add`

Use the following command line to create a new SMB cluster to be managed by the WEKA system:

`weka smb cluster add <netbios-name> <domain> <config-fs-name> [--domain-netbios-name domain-netbios-name] [--idmap-backend idmap-backend] [--default-domain-mapping-from-id default-domain-mapping-from-id] [--default-domain-mapping-to-id default-domain-mapping-to-id] [--joined-domain-mapping-from-id joined-domain-mapping-from-id] [--joined-domain-mapping-to-id joined-domain-mapping-to-id] [--encryption encryption] [--smb-conf-extra smb-conf-extra] [--container-ids container-ids]... [--smb-ips-pool smb-ips-pool]... [--smb-ips-range smb-ips-range]...[--symlink symlink]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | netbios-name* | NetBIOS name for the SMB cluster must be 1-15 characters long, using only alphanumeric characters (A-Z, 0-9) and hyphens (-). Names are case-insensitive, cannot start with a hyphen, and must be unique within the network. Spaces and special characters are not allowed.This will be the name of the Active Directory computer object and the hostname part of the FQDN. |  |
 | domain* | The Active Directory domain to which the SMB cluster will be joined. | ‚Äã |
 | config-fs-name* | The predefined filesystem for storing persistent cluster-wide protocol configurations. Ensure the filesystem exists; if not, create it.For details, see #dedicated-filesystem-requirement-for-persistent-protocol-configurations |  |
 | domain-netbios-name | Domain NetBIOS name. | The first part of the domain parameter |
 | idmap-backend | The ID mapping method to use.Possible values: rfc2307 or rid | rfc2307 |
 | default-domain-mapping-from-id | The first ID of the range for the default AD ID mapping (for trusted domains that have no defined range). | 4290000001 |
 | default-domain-mapping-to-id | The last ID of the range for the default AD ID mapping (for trusted domains that have no defined range). | 4291000000 |
 | joined-domain-mapping-from-id | The first ID of the range for the main AD ID mapping. | 0 |
 | joined-domain-mapping-to-id | The last ID of the range for the main AD ID mapping. | 4290000000 |
 | encryption | The global encryption policy to use:enabled - enables encryption negotiation but doesn't turn it on automatically for supported sessions and share connections.desired - enables encryption negotiation and turns on data encryption on supported sessions and share connections.required - enforces data encryption on sessions and share connections. Clients that do not support encryption will be denied access to the server. | enabled |
 | smb-conf-extra | Additional SMB configuration options. |  |
 | container-ids | The container IDs of the containers with a frontend process to serve the SMB service.Minimum of 3 containers. |  |
 | smb-ips-pool | A pool of virtual IPs, used as floating IPs for the SMB cluster to provide HA to clients.These IPs must be unique; do not assign these IPs to any host on the network.Format: comma-separated IP addresses. |  |
 | smb-ips-range | A range of virtual IPs, used as floating IPs for the SMB cluster to provide HA to clients.These IPs must be unique; do not assign these IPs to any host on the network.Format: A.B.C.D-EExample: 10.10.0.1-100 |  |
 | symlink | Determines if symbolic links are allowed in the SMB cluster.on: Enables symbolic links. Use with caution, as it can introduce security risks by exposing data across shares.off: Disables symbolic links, enhancing security by preventing link-based vulnerabilities.Important: If a symbolic link in one share points to a file system in another share, users in the first share can access the data in the second share. Ensure you understand the security implications before enabling this option.Only applicable for SMB-W clusters. | Off |

### Guidelines for configuring an SMB cluster

* **Enable High Availability (HA):**
  * Ensure all floating IPs reside on the same subnet to enable IP takeover for HA.
* **Floating IP requirements:**
  * Floating IPs must not be used by any other applications, servers, or WEKA components, including:
    * WEKA system management nodes
    * WEKA system IO nodes
    * WEKA system NFS floating IPs
  * In all-cloud installations, where listing SMB floating IPs is restricted by cloud provider network limitations, access the SMB service via the primary addresses of the cluster nodes.
* **Configure SMB floating IPs:**
  * Use the `--smb-ips` parameter to specify the virtual IPs exposed by the SMB cluster.
  * Clients must connect through one of these virtual IPs to ensure automatic reconnection if an SMB container fails.
* **Customizing SMB library options:**
  * If global options for the SMB library need adjustment, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system).

**Example command:**\
In this example, an SMB cluster named `wekaSMB` is created using containers 0-4, within the domain `mydomain`. The cluster is configured with virtual IPs ranging from 1.1.1.1 to 1.1.1.5.

```

```bash
weka smb cluster create wekaSMB mydomain --container-ids 0,1,2,3,4 --smb-ips-pool 1.1.1.1,1.1.1.2 --smb-ips-range 1.1.1.3-5
```

```

## Update the SMB cluster <a href="#update-smb-cluster" id="update-smb-cluster"></a>

**Command:** `weka smb cluster update`

Use the following command line to update an existing SMB cluster:

`weka smb cluster update [--encryption encryption] [--smb-ips-pool smb-ips-pool]... [--smb-ips-range smb-ips-range]...[--symlink symlink]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | encryption | The global encryption policy to use:enabled: enables encryption negotiation but doesn't turn it on automatically for supported sessions and share connections.desired: enables encryption negotiation and turns on data encryption on supported sessions and share connections.required: enforces data encryption on sessions and share connections. Clients that do not support encryption are denied access to the server. |
 | smb-ips-pool | A pool of virtual IPs, used as floating IPs for the SMB cluster to provide HA to clients.These IPs must be unique; do not assign these IPs to any host on the network.Format: comma-separated IP addresses. |
 | smb-ips-range | A range of public IPs is used as floating IPs to provide high availability for the SMB cluster to serve the SMB clients.These IPs must be unique; do not assign these IPs to any host on the network.Format: A.B.C.D-EExample: 10.10.0.1-100 |
 | symlink | Controls whether symbolic links are supported within the SMB cluster.Possible values:on: Enables the creation and use of symbolic links within the SMB cluster.off: Disables symbolic links, enhancing security by preventing potential link-based attacks. |

## Check the status of SMB cluster readiness <a href="#check-status-smb-host-readiness" id="check-status-smb-host-readiness"></a>

**Command:** `weka smb cluster status`

The SMB cluster is comprised of three to eight SMB containers. Use this command to check the status of the SMB containers that are part of the SMB cluster. Once all the SMB containers are prepared and ready, it is possible to join an SMB cluster to an Active Directory domain.

## Join an SMB cluster in Active Directory <a href="#join-smb-cluster-in-ad" id="join-smb-cluster-in-ad"></a>

**Command:** `weka smb domain join`

Use the following command line to join the SMB cluster to an Active Directory domain:

`weka smb domain join <username> <password> [--server server] [--create-computer create-computer]`

Note: Ensure the AD servers are resolvable to all WEKA servers. This resolution enables the WEKA servers to join the AD domain.

**Parameters**

 | Name | Value |
 | ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | `username`* | Name of an AD user with permission to add a server to the domain. |
 | `password`* | The password of the AD user. This password is not retained or cached. |
 | `server` | Specifies the remote domain controller for SMB-W domain join commands. WEKA automatically identifies an AD Domain Controller server (from /etc/resolv.conf ) based on the AD domain name. You do not need to set the server name. In some cases, specify the AD server if required. See <a data-mention href="../smb-management-using-the-gui#resolve-the-a-d-domain-controllers">#resolve-the-a-d-domain-controllers</a>. |
 | `create-computer` | Creates an SMB cluster computer account in AD under a specified OU. The default is the "Computers" container in AD. |

To join an existing SMB cluster to another Active Directory domain, leave the current Active Directory using the following command line:

`weka smb domain leave <username> <password>`

On completion of this operation, it is possible to join the SMB cluster to another Active Directory domain.

## Remove an SMB cluster <a href="#delete-an-smb-cluster" id="delete-an-smb-cluster"></a>

**Command:** `weka smb cluster remove`

Use this command to remove an SMB cluster managed by the WEKA system.

Removing an existing SMB cluster managed by the WEKA system does not delete the backend WEKA filesystems but removes the SMB share exposures of these filesystems.

## Add or remove SMB cluster containers <a href="#add-or-remove-smb-cluster-hosts" id="add-or-remove-smb-cluster-hosts"></a>

**Command:** `weka smb cluster container add`

**Command:** `weka smb cluster container remove`

Use these commands to add or remove containers from the SMB cluster.

`weka smb cluster container add [--containers-id containers-id]...`

`weka smb cluster container remove [--containers-id containers-id]...`

Note: This operation might take some time to complete. During that time, SMB IOs are stalled.

**Parameters**

 | Name | Value |
 | --- | --- |
 | containers-id* | Container IDs of containers with a frontend process to serve the SMB service.Specify a comma-separated list with a minimum of 3 containers. |

## Configure trusted domains <a href="#configure-trusted-domains" id="configure-trusted-domains"></a>

### List trusted domains

**Command:** `weka smb cluster trusted-domains`

Use this command to list all the configured trusted domains and their ID ranges.

### Add trusted domains

**Command:** `weka smb cluster trusted-domains add`

Use the following command line to add an SMB trusted domain:

`weka smb cluster trusted-domains add <domain-name> <from-id> <to-id>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | domain-name* | The name of the domain to add. |
 | from-id* | The first ID of the range for the domain ID mapping.The range cannot overlap with other domains. |
 | to-id* | The last ID of the range for the domain ID mapping.The range cannot overlap with other domains |

### Remove trusted domains

**Command:** `weka smb cluster trusted-domains remove`

Use the following command line to remove an SMB-trusted domain:

`weka smb cluster trusted-domains remove <domain-id>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | domain-id* | The internal ID of the domain to remove |

Note: **SMB-W cluster restart and verification**
The commands  `weka smb cluster trusted-domains add` and `weka smb cluster trusted-domains remove` (and the related APIs) trigger a background restart of the SMB-W cluster. This restart is necessary for the changes to take effect.
To confirm that the cluster has resumed normal operation following the restart, run the command: `weka smb cluster status`
This command provides the current status of the SMB-W cluster and ensures that it is operational.

## List SMB shares <a href="#list-smb-shares" id="list-smb-shares"></a>

**Command:** `weka smb share`

Use this command to list all existing SMB shares.

## Add an SMB share <a href="#add-an-smb-share" id="add-an-smb-share"></a>

**Command:** `weka smb share add`

Use the following command line to add a new share to be exposed by SMB. \
Ensure the SMB cluster is joined to the Active Directory. For details, see #join-smb-cluster-in-a-d.

```

```
weka smb share add <share-name> <fs-name> [--description description] [--internal-path internal-path] [--file-create-mask file-create-mask]  [--directory-create-mask directory-create-mask] [--acl acl] [--map-acls map-acls] [--case-sensitivity case-sensitivity] [--obs-direct obs-direct] [--encryption encryption] [--read-only read-only] [--user-list-type user-list-type] [--allow-guest-access allow-guest-access]
[--enable-ADS enable-ADS] [--hidden hidden] [--vfs-zerocopy-read vfs-zerocopy-read] [--users users]...
```

```

Note: The mount mode for the SMB share is `readcache` and cannot be modified.

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | share-name* | A unique name of the share to add to the filesystem. The share name must adhere to the following rules:Alphanumeric characters: A-Z, a-z, 0-9.Maximum length: 80 characters.Allowed special characters: hyphens (-) and underscores (_).Prohibited special characters: space ( ), backslash (\), slash (/), colon (:), semicolon (;).Prohibited control characters: 0x00 through 0x1F.No reserved names: Avoid using reserved names such as CON, PRN, AUX, NUL, COM1, LPT1. They may cause conflicts.SMB-W: Do not create the same share name with different case insensitivity. | ‚Äã |
 | fs-name* | Valid name of the filesystem to share.A filesystem with Required Authentication set to ON cannot be used for SMB share. | ‚Äã |
 | description | The description of the share received in remote views. | ‚Äã |
 | internal-path | The internal valid path within the filesystem (relative to its root) which will be exposed. | . |
 | file-create-mask | POSIX permissions for the file created through the SMB share.Numeric (octal) notation. | 0744 |
 | directory-create-mask | POSIX permissions for directories created through the SMB share.Numeric (octal) notation.SMB-W: the specified string must be greater or equal to 0600. | 0755 |
 | acl | Enable Windows ACLs on the share (translated to POSIX).Supports up to 16 ACLs per file, depending on the available space in the Extended Attribute (xattr). For details, see #filesystem-extended-attributes-considerationsPossible values: on, offFor a MAC client, if acl is off, set enable-ADS to off. | off |
 | map-acl | Specifies the type of access control to use for the share. Options include POSIX, Windows, or Hybrid. Hybrid ACL allows seamless interoperability between POSIX and Windows systems by exchanging permissions based on timestamps. Regardless of the system it originated from, the most recent permission takes precedence. | POSIX |
 | case-sensitivity | Enables or disables case sensitivity for the specified SMB share. When enabled, the share distinguishes between files with the same name but different capitalization. | on |
 | obs-direct | A special mount option to bypass the time-based policies. For details, see #object-store-direct-mount-optionPossible values: on, off | off |
 | encryption | The share encryption policy.cluster_default: The share encryption policy follows the global SMB cluster setting.desired: If negotiation is enabled globally, it turns on data encryption for this share for clients that support encryption.required: Enforces encryption for the shares. Clients that do not support encryption are denied when accessing the share. | cluster_default |
 | read-only | Sets the share as read-only. Users cannot create or modify files in this share.Possible values: on, off | off |
 | user-list-type | The type of initial permissions list for users.Possible values:read_only: List of users who have been denied write access to the share, regardless of the read-only setting.read_write: List of users given write access to the share, regardless of the read-only setting.valid : List of users that are allowed to log in to this share (empty list = all users are allowed)invalid: List of users that are not allowed to log in to this share |  |
 | allow-guest-access | Allows connecting to the SMB service without a password. Permissions are as the nobody user account permissions.Possible values: on, off | off |
 | enable-ADS | Enables using Alternate Data Streams (ADS) on a specified SMB share.Possible values: yes, nomacOS clients:If ACLs are disabled (acl=off), set enable-ADS to off.Windows clients:When enabled, ADS data is stored in the file‚Äôs extended attributes (XAttr), which consumes XAttr space. | on |
 | hidden | Sets the share as non-browsable. It will be accessible for mounting and IOs but not discoverable by SMB clients.Possible values: on, off | off |
 | vfs-zerocopy-read | If supported, enable zero-copy reads. This allows data to transfer directly from disk to application memory without intermediate copying, reducing CPU usage and latency and enhancing throughput and efficiency for large file access.Possible values: on, off. | on |
 | users | A list of users to use with the user-list-type list.Format: Domain short name followed by group name, for example WEKAAD\internalShareUsersPossible values: Up to 8 users/groups for all lists combined per share. | Empty list |

### Guidelines for adding an SMB share

* **Adding SMB shares:**
  *   Example commands:

      ```bash
      weka smb share add rootShare default
      weka smb share add internalShare default --internal-path some/dir --description "Exposed share"
      ```

      The first command creates a root SMB share for the `default` filesystem.

      The second command creates an internal SMB share for the `default` filesystem with a specified subdirectory and description.
* **Custom SMB library options:** For configuring SMB shares with specific library options, contact the Customer Success Team.
* **Setting share permissions:** After adding an SMB share, configure POSIX permissions to grant SMB users access.\
  **Examples:**
  *   Grant full access:

      ```bash
      mount -t wekafs smbw-fs /mnt/smbw
      chmod 777 /mnt/smbw
      umount /mnt/smbw
      ```
  *   Assign group ownership:

      ```bash
      mount -t wekafs smbw-fs /mnt/smbw
      chown :smb-group /mnt/smbw
      umount /mnt/smbw
      ```

For more details, see #filesystem-permissions-and-access-rights-configuration.

## Update SMB shares <a href="#update-smb-shares" id="update-smb-shares"></a>

**Command:** `weka smb share update`

Use the following command line to update an existing share:

`weka smb share update <share-id> [--encryption encryption] [--read-only read-only] [--allow-guest-access allow-guest-access] [--hidden hidden]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | A valid share ID to update. |
 | encryption | The share encryption policy.cluster_default: The share encryption policy follows the global SMB cluster setting.desired: If negotiation is enabled globally, it turns on data encryption for this share for clients that support encryption.required: Enforces encryption for the shares. Clients that do not support encryption are denied when accessing the share. If the global option is disabled, access is restricted to these shares for all clients. |
 | read-only | Sets the share as read-only. Users cannot create or modify files in this share.Possible values: on, off |
 | allow-guest-access | Allows connecting to the SMB service without a password. Permissions are as the nobody user account permissions.Possible values: on, off |
 | hidden | Sets the share as non-browsable. It will be accessible for mounting and IOs but not discoverable by SMB clients.Possible values: on, off |

## **Control SMB share user-lists** <a href="#control-smb-share-user-lists" id="control-smb-share-user-lists"></a>

**Command:** `weka smb share list show`

Use this command to view the various user-list settings.

**Command:** `weka smb share list add`

Use the following command line to add users to a share user-list:

`weka smb share list add <share-id> <user-list-type> <--users users>...`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | The ID of the share to update. |
 | user-list-type* | The type of permissions list for users:read_only: list of users that do not get write access to the SMB share, regardless of the read-only setting.read_write: list of users get write access to the SMB share, regardless of the read-only setting.valid: list of users allowed to log in to this SMB share service (an empty list means all users are allowed).invalid: list of users that are not allowed to log in to this share SMB service. |
 | users* | A comma-separated list of users to add to the user-list-type list.Can use the @ notation to allow groups of users. For example, root, Jack, @domain\admins.You can set up to 8 users/groups for all lists combined per share. |

***

**Command:** `weka smb share list remove`

Use the following command line to remove users from a share user-list:

`weka smb share list remove <share-id> <user-list-type> <--users users>...`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | The ID of the share to be updated. |
 | user-list-type* | The type of permissions list for users:read_only: list of users that do not get write access to the SMB share, regardless of the read-only setting.read_write: list of users get write access to the SMB share, regardless of the read-only setting.valid: list of users allowed to log in to this SMB share service (an empty list means all users are allowed).invalid: list of users not allowed to log in to this SMB share service. |
 | users* | A comma-separated list of users to remove from the user-list-type list. Can use the @ notation to allow groups of users, e.g. root, Jack, @domain\admins.You can set up to 8 users/groups for all lists combined per share. |

***

**Command:** `weka smb share list reset`

Use the following command line to remove all users from a share user-list:

`weka smb share list reset <share-id> <user-list-type>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | The ID of the share to be updated |
 | user-list-type* | The type of permissions list to reset:read_only: list of users that do not get write access to the SMB share, regardless of the read-only setting.read_write: list of users get write access to the SMB share, regardless of the read-only setting.valid: list of users allowed to log in to this SMB share service (an empty list means all users are allowed).invalid: list of users not allowed to log in to this SMB share service. |

## Remove SMB shares <a href="#remove-smb-shares" id="remove-smb-shares"></a>

**Command:** `weka smb share remove`

Use the following command line to remove a share exposed to SMB:

`weka smb share remove <share-id>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | The ID of the share to remove. |

Note: **Example:** The following is an example of removing an SMB share defined as ID 1:
`weka smb share remove 1`

## Control SMB access based on hosts' IP/name <a href="#control-smb-access-based-on-hosts" id="control-smb-access-based-on-hosts"></a>

You can control which hosts are permitted to access the SMB share. The maximum number of share host access definitions across all shares is 1024.

Note: SMB-W supports access based on the host IP addresses (but not host names).

**Command:** `weka smb share host-access list`

Use this command to view the various host access settings.

**Command:** `weka smb share host-access add`

Use the following command line to add a host to the allow/deny list:

`weka smb share host-access add <share-id> <mode> <--ips ips> <--hosts hosts>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | The ID of the share to update.Mandatory for the share-level command. |
 | mode* | The access mode of the host.Possible values: allow, deny |
 | ips | A comma-separated list of host IP addresses to allow or deny.Must provide at least one of the IP addresses.Format example for multiple IPs: 192.192.168.192.168.1192.168.1.1/24192.168.1.2, 192.168.1.2 |
 | hosts | Host names to allow/deny.You must provide at least one of the hostnamesSeparate host names with spaces.In SMB-W, use the ips parameter instead of hosts. |

**Command:** `weka smb share host-access remove`

Use the following command line to remove hosts from the allow or deny list.

`weka smb share host-access remove <share-id> <hosts>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | The ID of the share to update.Mandatory for the share-level command. |
 | hosts* | A list of hostnames you want to remove from access.Separate host names with spaces.Use the IP addresses displayed under the HOST column when running the corresponding list command. |

**Command:** `weka smb share host-access reset`

Use the following command line to remove all hosts from the allow or deny list:

`weka smb share host-access reset <share-id> <mode>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | share-id* | The ID of the share to update.Mandatory for the share-level command. |
 | mode* | The specified access mode will remove all associated hosts from the list.Possible values: allow, deny. |

<!-- ============================================ -->
<!-- File 117/259: additional-protocols_smb-support_smb-management-using-the-gui.md -->
<!-- ============================================ -->

---
description:
---

# Manage SMB using the GUI

Using the GUI, you can:

* Configure the SMB cluster
* Edit the SMB cluster
* Join the SMB cluster to Active Directory
* Add servers to the SMB cluster
* Remove servers from the SMB cluster
* Delete the SMB cluster
* Display the SMB shares list
* Add an SMB share
* Edit an SMB share
* Remove an SMB share

Note: The GUI refers to the feature as SMB, but it applies to SMB-W only. Support for the legacy SMB implementation has been removed.

## **Configure the SMB cluster** <a href="#configure-the-smb-cluster" id="configure-the-smb-cluster"></a>

An SMB cluster comprises at least three WEKA servers running the SMB-W stack.

**Before you begin**

Verify that the dedicated filesystem for persistent protocol configurations is created. If not, create it. For details, see #dedicated-filesystem-requirement-for-persistent-protocol-configurations

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **SMB**.
3. On the Configuration tab, select **Configure**.
4. In the SMB Cluster Configuration dialog, set the following properties:
   * **Name**: NetBIOS name for the SMB cluster must be 1-15 characters long, using only alphanumeric characters (A-Z, 0-9) and hyphens (-). Names are case-insensitive, cannot start with a hyphen, and must be unique within the network. Spaces and special characters are not allowed.\
     This will be the name of the Active Directory computer object and the hostname part of the FQDN.
   * **Domain**: The Active Directory domain to join the SMB cluster.
   * **Domain NetBIOS Name**: (Optional) The domain NetBIOS name.
   * **Encryption:** Select the in-transit encryption mode to use in the SMB cluster:
     * **enabled:** Enables encryption negotiation but doesn't turn it on automatically for supported sessions and shared connections.
     * **desired**: Enables encryption negotiation and turns on data encryption for supported sessions and shared connections.
     * **required**: Enforces data encryption on sessions and shared connections. Clients that do not support encryption will be denied access to the server.
   * **Servers**: List 3-8 WEKA system servers to participate in the SMB cluster based on the server IDs in WEKA.
   * **IPs**: (Optional) List of virtual IPs (comma-separated) used as floating IPs for the SMB cluster to provide HA to clients. These IPs must be unique; do not assign these IPs to any host on the network.\
     For an IP range, use the following format: **a.b.c.x-y**.
   * **Config Filesystem:** select the filesystem used for persisting cluster-wide protocol configurations.
   * **Symbolic Link:** Determines if symbolic links are allowed in the SMB cluster:
     * **ON:** Enables symbolic links. Use with caution, as it can introduce security risks by exposing data across shares.
     * **OFF:** Disables symbolic links, enhancing security by preventing link-based vulnerabilities.

Note: **Important**: If a symbolic link in one share points to a file system in another share, users in the first share can access the data in the second share. Ensure you understand the security implications before enabling this option.

Note: Due to cloud provider network limitations, setting a list of SMB floating IPs in all cloud installations is impossible. In this case, the SMB service must be accessed using the cluster nodes' primary addresses.

5. Select **Save**.

Once the system completes configuration, the server statuses change from not ready (‚ùå) to ready (‚úÖ).

## Edit the SMB cluster <a href="#edit-the-smb-cluster" id="edit-the-smb-cluster"></a>

You can modify the encryption and IP settings according to your needs.

**Procedure**

1. In the SMB Cluster Configuration, select the **pencil** icon.

2. In the Edit SMB Configuration dialog, do the following:
   * **Encryption:** Select one of the in-transit encryption enforcements: enabled, desired, or required.
   *  **IPs:** List of virtual IPs (comma-separated) used as floating IPs for the SMB cluster. (Floating IPs are not supported for cloud installations.)

3\. Select **Save**.

## Join the SMB cluster to Active Directory <a href="#join-the-smb-cluster-in-the-active-directory" id="join-the-smb-cluster-in-the-active-directory"></a>

To enable the SMB cluster to use Active Directory to resolve the access of users and user groups, join the SMB cluster to Active Directory (AD).

**Before you begin**

<details>

<summary>Resolve the AD domain controllers</summary>

Add the AD DNS configuration to every SMB protocol backend.

Follow these steps:

1. Access the CLI.
2. Edit the `/etc/resolv.conf` file to include the DNS settings specific to your domain.

For example, your configuration might look like this:

```bash
nameserver 8.8.8.8
nameserver 8.8.4.4
search example.com
```

Replace `8.8.8.8` and `8.8.4.4` with the appropriate nameserver IP addresses for your domain and `example.com` with your actual domain name.

</details>

**Procedure**

1. In the SMB Cluster Configuration, select **Join**.

2. In the Join to Active Directory dialog, set the following properties:
   * **Username** and **Password**: A username and password of an account that has join privileges to the Active Directory domain. WEKA does not save these credentials. Instead, the SMB cluster creates a computer account for use.
   * **Server**: (Optional) WEKA automatically identifies an AD Domain Controller server (from `/etc/resolv.conf`) based on the AD domain name. You do not need to set the server name. In some cases, specify the AD server if required.
   * **Computers Org. Unit**: The default AD organizational unit (OU) for the computer account is the Computers directory. You can define any OU to create the computer account that the joining account has permission to, such as SMB servers or corporate computers.

Once the SMB cluster joins the Active Directory domain, the join status next to the domain changes to **Joined**.

Note: To join an existing SMB cluster to a different Active Directory domain, select **Leave**. To confirm the action, enter the username and password used to join the Active Directory domain.

### Post-configuration in the DNS Manager and Active Directory

Note: The following procedures are provided for reference purposes. For specific steps related to your environment, contact your IT administrator.

<details>

<summary>Add an <strong>A</strong> record for SMB protocol backends</summary>

1. **Open DNS Manager:** Navigate to **Start > Programs > Administrative Tools > DNS**.
2. **Access DNS zones:** In the DNS Manager console, double-click the DNS server name to display the list of zones.
3. Open **Forward Lookup Zones.**
4. **Create a new A record:** Right-click on the relevant domain and select **New Record**.
5. **Enter record details:**
   * Specify the name (for example, TAZ) and the IP address of the backend server.
   * Select the record type as **A**.
6. **Configure record options:**
   * Select the **Create Associated PTR record** option.
   * Select the **Allow any authenticated user to update DNS record with the same owner name** option.
7. **Finalize the Record:** Select **OK** to add the new A record.

</details>

<details>

<summary>Set UID and GID for SMB protocol backends</summary>

Repeat the following steps for every backend participating in the SMB protocol.

1. Navigate to **Start > Programs > Administrative Tools > Active Directory Users and Computers**.
2. From the **View** menu, select **Advanced Features**.
3. In the **Computers** section, right-click on an SMB protocol backend and select **Properties**.
4. Select the **Attribute Editor** tab and modify the following:
   * Locate the **gidNumber** attribute and set its value to **0**.
   * Locate the **uidNumber** attribute and set its value to **0**.
5. Select **OK** to save the changes.

</details>

<details>

<summary>Set UID and GID for SMB users</summary>

Repeat the following steps for every user consuming WEKA services over the SMB protocol.

1. Navigate to **Start > Programs > Administrative Tools > Active Directory Users and Computers**.
2. From the **View** menu, select **Advanced Features**.
3. In the **Users** section, right-click on a user consuming WEKA services over the SMB protocol, and select **Properties**.
4. Select the **Attribute Editor** tab and modify the following:
   * Locate the **gidNumber** attribute and set its value to an appropriate number or, if unknown, any numeric value between 0 and 4290000000.
   * Locate the **uidNumber** attribute and set its value to an appropriate number or, if unknown, any numeric value between 0 and 4290000000.

5) Select **OK** to save the changes.

</details>

## Add servers to the SMB cluster <a href="#add-or-remove-smb-cluster-hosts" id="add-or-remove-smb-cluster-hosts"></a>

Adding servers to the SMB cluster can provide several benefits and address various requirements, such as scalability, load balancing, high availability, and improved fault tolerance.

**Before you begin**

* Ensure the SMB cluster is joined to an Active Directory domain.\
  See #join-the-smb-cluster-in-the-active-directory.

#### Procedure

1. On the Servers pane, select **Add**.
2. In the Add SMB Cluster Servers dialog, select one or more available servers (a maximum of eight servers) from the list.
3. Select **Save**.

## Remove servers from the SMB cluster <a href="#delete-the-smb-cluster" id="delete-the-smb-cluster"></a>

 If the SMB cluster has more servers than you need, you can remove the server.

The minimum required number of servers in an SMB cluster is three.

#### Procedure

1. To remove one server, select the three dots next to the server to remove and select **Remove**.

2. To remove more than one server, select the servers to remove from the Remove SMB Cluster Servers dialog (click the **X**), and select **Save**.

## Delete the SMB cluster <a href="#delete-the-smb-cluster" id="delete-the-smb-cluster"></a>

Deleting the SMB cluster resets its configuration data. Deleting an SMB cluster only applies to SMB-W.

#### **Procedure**

1. In the SMB Cluster Configuration, select the **trash** icon.

2. In the SMB Configuration Reset message, select **Reset**.

## **Display the SMB shares list** <a href="#display-the-smb-shares-list" id="display-the-smb-shares-list"></a>

The Shares tab displays the SMB shares created in the system. You can also customize the table columns of the SMB shares.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **SMB**.
3. Select the **Shares** tab.\
   You can filter the list using any column in the table.

## Add an SMB share <a href="#add-an-smb-share" id="add-an-smb-share"></a>

Once the SMB cluster is created, you can create SMB shares (maximum of 1024). Each share must have a name and a shared path to the filesystem, which can be the root of the filesystem or a subdirectory

**Before you begin**

* Ensure the SMB cluster is joined to the Active Directory. For details, see #join-the-smb-cluster-in-the-active-directory.
* Ensure the filesystem is already mounted and the directory you want to share is created in the filesystem. For details, see ;

**Procedure**

1. In the Shares tab, select **+Create**.
2.  In the Add SMB Share dialog, set the following properties:

    * **Name**: A meaningful name for the SMB share.
    * **Filesystem**: The filesystem name that includes the directory to share. Select one from the list. A filesystem with Required Authentication set to ON cannot be used for SMB share.
    * **Description**: A description or purpose of the SMB share.
    * **Path**: A valid internal path, relative to the root, within the filesystem to expose the SMB share.

    If required, select **Advanced** and set the following:

    * **ACLs Enabled**: Enables or disables Windows Access-Control Lists (ACLs) for the share. When enabled, WEKA applies the selected Access Control Model. Only applicable for SMB-W.
    * **Access Control Model:** Specifies the type of access control to use for the share. Options include POSIX, Windows, or Hybrid (default: POSIX). Hybrid ACL allows seamless interoperability between POSIX and Windows systems by exchanging permissions based on timestamps. The most recent permission, regardless of the system it originated from, takes precedence. Only applicable for SMB-W.
    * **File Default Permission:** The new default file permissions for the POSIX mode mask in a numeric (octal) format created through the share. Default 0744.
    * **Directory Default Permission:** The new default directory permissions for the POSIX mode mask in a numeric (octal) format created through the share. Default 0755.
    * **Access Permissions:** Define the share access permissions. If you select ON, select the access type and the users or groups allowed to access the share (comma-separated users and groups list, add '@' as a group prefix).
| * **Users List:** Users and groups list (add '@' as a group prefix). Don‚Äôt use the following characters: / \[ ] : ; | = + * ? < > ". |
    * **Types:** Select the users or groups allowed to access the share and the access type.
    * **Read Only:** Select to set the share as read-only.
    * **Hidden:** Select if you want to hide the share so it is not visible when viewing the list of system shares.
    * **Allow Guest Access:** Select if you want guests to access without authentication.
    * **Case Sensitivity**: Enables or disables case sensitivity for the specified SMB share (default: ON). When enabled, the share distinguishes between files with the same name but different capitalization. This option applies exclusively to the SMB-W cluster.
    * **ADS:** Enables using Alternate Data Streams (ADS) on a specified SMB share.\
      Possible values: ON, OFF (default: ON).  For **macOS clients**, if ACLs are disabled (`acl=off`), set `enable-ADS` to `off`. For **Windows clients**, when enabled, ADS data is stored in the file‚Äôs extended attributes (XAttr), which consumes XAttr space.
    * **Encryption:** Select in-transit encryption enforcement of the share. The global cluster encryption settings can affect the actual encryption.
    * **Direct Object Store Sync:** Enables immediate synchronization of files to the object store, bypassing time-based file retention policies. When enabled, newly created or modified files in the share are prioritized for release without delay.
3. Select **Save**.

<details>

<summary>Access the share from Windows</summary>

1. Right-click on **This PC**.
2. Select **Map network drive**.
3. In the **Folder** field, enter the path to the share, for example, `\\smbshare\mynewshare`.
4. If prompted, enter the required credentials.

</details>

## Edit an SMB share <a href="#edit-an-smb-share" id="edit-an-smb-share"></a>

You can update some of the SMB share settings. These include encryption, hiding the share, allowing guest access, and setting the share as read-only.

**Procedure**

1. In the Shares tab, select the three dots of the share and select **Edit**.

2. In the Update Share Settings dialog, update the relevant properties and select **Save**.

## Remove an SMB share <a href="#remove-an-smb-share" id="remove-an-smb-share"></a>

**Procedure**

1. In the Shares tab, select the three dots of the share and select **Remove**.

2. In the confirmation message that appears, select **Confirm**.\
   The removed share no longer appears in the SMB Shares list.

<!-- ============================================ -->
<!-- File 118/259: additional-protocols_s3.md -->
<!-- ============================================ -->

---
description: The WEKA configuration of the S3 protocol.
---

# Manage the S3 protocol

The S3 protocol is integral to numerous cloud-native applications, and within the WEKA system, it offers a range of technical capabilities:

* **Data management:**
  * Ingest data using the S3 protocol.
  * Access ingested data using S3 or other supported protocols.
* **Data exposure to S3:**
  * Enable seamless migration of applications within the WEKA data platform by making existing data accessible through the S3 protocol. This integration ensures a smooth transition without the need for data relocation.
* **Cloud integration:**
  * Enable cloud bursting to use new applications without relocating data.
* **Multi-protocol access with WEKA:**
  * Leverage WEKA's scale, performance, and resiliency advantages.
  * Gradually transition applications to S3, maintaining data access through multiple protocols: POSIX, S3, SMB, NFS, and GPUDirect Storage.

The WEKA S3 service is designed for scalability and resilience. Implementation involves specifying WEKA servers running the S3 protocol and creating a logical S3 cluster to expose the service. Scalability is achieved through multiple servers, and load balancing or round-robin DNS integration facilitates access by numerous clients.

The WEKA S3 service builds on the WEKA filesystem service, mapping buckets to top-level directories and objects to files, allowing data exposure through various WEKA-supported protocols.

## Guidelines for managing S3 access and security

### S3 access

Users can access S3 APIs with either authenticated or anonymous methods.

* **Authenticated S3 access:** To gain authenticated S3 access, follow these guidelines:
  * Create a local WEKA user with an assigned S3 user role.
  * Attach an IAM policy to the S3 user, specifying S3 operations and resource permissions.
  * S3 users can generate temporary security tokens (STS AssumeRole) or employ service accounts with restricted permissions.
* **Anonymous access:** You can use the following options:
  * Bucket policies.
  * Pre-signed URLs.

### S3 security

* **Encryption of data at rest:**
  * Data written through the S3 protocol can be encrypted at rest by configuring an encrypted filesystem.
* **Transport Layer Security (TLS):**
  * Clients access the service securely through HTTPS, using the same certificates as WEKA's other API access points.

### S3 audit

* **Auditing S3 API calls:**
  * S3 API calls are auditable using an HTTP webhook service, integrating into applications such as Splunk.
* **Setting audit targets:**
  * Set an audit target using the `weka s3 cluster audit-webhook enable` CLI command.

## Workflow: Managing S3 resources in WEKA

This workflow guides you through the process of setting up and managing S3 resources within the WEKA system. It covers essential procedures, from creating an S3 cluster and bucket to uploading and downloading objects from S3 buckets. Each step demonstrates the seamless integration of WEKA's capabilities in handling data through the S3 protocol.

Select each tab in the specified order to explore the demonstrations.

Create an S3 cluster within the WEKA environment, specifying the servers to run the S3 protocol and establishing a logical cluster for exposing the S3 service.

A predefined configuration filesystem (`.config_fs` in this demo) must exist to maintain the persisting cluster-wide protocol configurations.

**Alternative CLI command**

```
weka s3 cluster create default .config_fs --all-servers
```

**Related topics**

#create-an-s3-cluster (using the GUI)

#create-an-s3-cluster (using the CLI)

Create of an S3 bucket, a fundamental container for storing and organizing objects within the WEKA S3 service.

**Alternative CLI command**

```
weka s3 bucket create my_bucket
```

**Related topics**

#create-a-bucket (using the GUI)

#create-a-bucket (using the CLI)

Create a dedicated local user with S3 role and assign it with an S3 policy to facilitate secure access to S3 resources.

The S3 local user name and password serve as the S3 access key and secret key, respectively for uploading and downloading objects from S3 buckets.

**Alternative CLI commands**

```
weka user add S3_user S3
weka s3 policy attach readwrite S3_user
```

**Related topics**

#create-a-local-user (using the GUI)

#create-a-local-user (using the CLI)

You can implement either the self-signed certificate or a custom certificate. The cluster TLS certificate is enabled using an auto-generated self-signed certificate, providing access to the GUI, CLI, and API through HTTPS. If a custom TLS certificate is preferred, you can set in place of the auto-generated self-signed certificate.

To apply your chosen certificate, download it to your client and place it in the designated folder for seamless integration.

**Alternative CLI command**

```
weka security tls download my_path
```

**Related topics**



Once we have created an S3 cluster that can be accessed through a certain port (default: 9000),  a bucket, and an S3 user assigned with a policy, let's see how the client can upload and download objects from the S3 bucket.

Users can use any client application that can access the bucket through the WEKA cluster URL and port.

This example demonstrates using Boto3, the official Python client of AWS.

In the code snippet named `s3.py`, we set the following parameters in the `S3 = boto3 resource` section:

* `endpoint_url and port`: The URL and port of the WEKA S3 cluster.
* `aws_access_key_id`: The S3 local user name.
* `aws_secret_access_key`:  The S3 local user password.

```

```python
#!/usr/bin/env/python
import boto3
import logging
from botocore.exceptions import ClientError
from botocore.client import Config

config = Config(
   signature_version = 's3v4'
)

s3 = boto3.resource('s3',
                    endpoint_url='https://ari:9000',
                    aws_access_key_id='S3_user1',
                    aws_secret_access_key='S3_user1',
                    config=config)

try:
  # upload a file from the local filesystem 'myfile' to bucket 'mybucket1' with 'my_uploaded_object' as the object name.
  s3.Bucket('mybucket1').upload_file('myfile','my_uploaded_object')

  # download the object 'myfile' from the bucket 'mybucket1' and save it to the local filesystem as my_downloaded_object.
  s3.Bucket('mybucket1').download_file('my_uploaded_object', 'my_downloaded_object')

except ClientError as e:
        logging.error(e)

print("Downloaded 'my_downloaded_object' as 'my_uploaded_object'. a")
```

```

**Upload and download**

Test the upload and download using the Python script.

```python
‚ùØ python3 s3.py
Downloaded 'my_downloaded_object' as 'my_uploaded_object'. a
‚ùØ cat my_downloaded_object
Hello World!
```

**Related topic**

As a quick workflow guide, here is a summary of the CLI commands for configuring your S3 cluster and implementing the desired settings:

```
weka s3 cluster create default .config_fs --all-servers
weka s3 bucket create mybucket
weka user add S3_user S3
weka s3 policy attach readwrite S3user
weka security tls download mypath
python3 s3.py
```

    For details, see Boto3 documentation.

<!-- ============================================ -->
<!-- File 119/259: additional-protocols_s3_configure-and-use-aws-cli-with-weka-s3-storage.md -->
<!-- ============================================ -->

---
description: Learn how to configure and use the AWS CLI with WEKA S3 storage.
---

# Configure and use AWS CLI with WEKA S3 storage

To use the AWS CLI to access S3-compatible storage on WEKA, configure the CLI with the appropriate endpoint and credentials.

Note: * If you are using a self-signed SSL certificate or a WEKA backend IP address as your S3 endpoint, append `--no-verify-ssl` to any AWS CLI commands listed below.
* These examples assume you have a properly configured WEKA S3 protocol servers. For guidance on configuring the S3 protocol on your WEKA cluster, see [](<> "mention").

## **Install and configure the AWS CLI**

1. **Verify AWS CLI is installed:**
   1. Verify that the AWS CLI is installed on your system. If required, see **Install the AWS CLI****.**
2. **Configure AWS CLI with WEKA credentials:**
   1.  Use the following command to start configuration:

       ```bash
       aws configure
       ```
   2. Enter the following information when prompted:
      * **AWS Access Key ID**: Your WEKA S3 user access key.
      * **AWS Secret Access Key**: Your WEKA S3 user secret key.
      * **Default region name**: You can leave this blank.
      * **Default output format**: You can leave this blank.
3. **Enable AWS Signature Version 4 for WEKA server:**
   1.  WEKA requires AWS Signature Version 4 for authentication. Set it using:

       ```bash
       aws configure set default.s3.signature_version s3v4
       ```

## **AWS CLI usage**

When using AWS CLI commands with WEKA, specify the custom endpoint URL. The following are some common operations:

### **List buckets**

```bash
aws --endpoint-url https://your-weka-server:9000 s3 ls
```

Replace `https://your-weka-server:9000` with your WEKA server's actual address.

### **Create a bucket**

```bash
aws --endpoint-url https://your-weka-server:9000 s3 mb s3://mybucket
```

This command creates a new bucket named `mybucket`.

### **Upload a file**

```bash
aws --endpoint-url https://your-weka-server:9000 s3 cp local-file.txt s3://mybucket/
```

This command uploads `local-file.txt` to the `mybucket` bucket.

### **List bucket contents**

```bash
aws --endpoint-url https://your-weka-server:9000 s3 ls s3://mybucket
```

This command lists the contents of the `mybucket` bucket.

### **Download a file**

```bash
aws --endpoint-url https://your-weka-server:9000 s3 cp s3://mybucket/remote-file.txt ./
```

This command downloads `remote-file.txt` from the `mybucket` bucket to the current directory.

### **Delete a file**

```bash
aws --endpoint-url https://your-weka-server:9000 s3 rm s3://mybucket/file-to-delete.txt
```

This command deletes `file-to-delete.txt` from the `mybucket` bucket.

### **Remove a bucket**

If you attempt to remove a bucket that is not empty, you receive an error. You must either empty all object versions from the bucket or add `--force` to the remove bucket command. `--force` deletes the bucket and all object versions within it.

```bash
aws --endpoint-url https://your-weka-server:9000 s3 rb s3://mybucket
```

This command removes the `mybucket` bucket.

<!-- ============================================ -->
<!-- File 120/259: additional-protocols_s3_s3-examples-using-boto3.md -->
<!-- ============================================ -->

---
description: This page provides some examples of using the S3 API.
---

# S3 examples using boto3

## Boto3

Boto3, the official AWS SDK for Python, is used to create, configure, and manage AWS services.

The following are examples of defining a resource/client in boto3 for the WEKA S3 service, managing credentials and pre-signed URLs, generating secure temporary tokens, and using those to run S3 API calls.

### Installation

`pip install boto3`

## Credentials

There are many ways to set credentials in boto3, as described on the boto3 credentials page. Specifically, look into the Assume Role Provider method, which uses the access/secret keys to automatically generate and use the temporary security token.

## Resource

Resources represent an object-oriented interface to Amazon Web Services (AWS). They provide a higher-level abstraction than service clients' raw, low-level calls. To use resources, invoke the resource() method of a Session and pass in a service name.

```python
s3 = boto3.resource('s3',
                    endpoint_url='https://weka:9000',
                    aws_access_key_id='s3_key',
                    aws_secret_access_key='s3_secret')

```

## Client

Clients provide a low-level interface to AWS, whose methods map close to 1:1 with service APIs. All service operations are supported by clients (in our case, `s3` and `sts`).

```python
s3_client = boto3.client('sts',
                         endpoint_url='https://weka:9000',
                         aws_access_key_id='s3_key',
                         aws_secret_access_key='s3_secret',
                         region_name='us-east-1'))
```

## Assume role example

Example code of using an access/secret key to obtain a temporary security token for the S3 service:

```python
#!/usr/bin/env/python
import boto3
import logging
from botocore.exceptions import ClientError
from botocore.client import Config

config = Config(
   signature_version = 's3v4'
)

s3_client = boto3.client('sts',
        endpoint_url='https://weka:9000',
        aws_access_key_id='s3_key',
        aws_secret_access_key='s3_secret',
        config=config,
        region_name='us-east-1')

try:

  response = s3_client.assume_role(
      RoleArn='arn:x:ignored:by:weka-s3:',
      RoleSessionName='ignored-by-weka-s3',
      DurationSeconds=900
  )

except ClientError as e:
    logging.error(e)

print 'AccessKeyId:' + response['Credentials']['AccessKeyId']
print 'SecretAccessKey:' + response['Credentials']['SecretAccessKey']
print 'SessionToken:' + response['Credentials']['SessionToken']
```

## Pre-signed URL example

Example of signing on a GET request for `myobject`  within `mybucket` for anonymous access:

```python
#!/usr/bin/env/python
import boto3
import logging
from botocore.exceptions import ClientError
from botocore.client import Config

config = Config(
   signature_version = 's3v4'
)

s3_client = boto3.client('s3',
                         endpoint_url='https://weka:9000',
                         aws_access_key_id='s3_key',
                         aws_secret_access_key='s3_secret',
                         config=config,
                         region_name='us-east-1')

try:
    response = s3_client.generate_presigned_url('get_object',
                                                Params={'Bucket': 'mybucket',
                                                        'Key': 'myobject'},
                                                ExpiresIn=3600)
except ClientError as e:
    logging.error(e)

# The response contains the pre-signed URL
print response
```

Use the response to access the object without providing any credentials:

```python
$ curl "http://weka:9000/mybucket/myobject?AWSAccessKeyId=s3_key&Expires=1624801707&Signature=4QBcfEUsUdR7Jaffg6gLRVpNTY0%3D"
myobject content
```

## Pre-signed URL with assume role example

Combine the above two examples by providing a pre-signed URL from a temporary security token:

```python
#!/usr/bin/env/python
import boto3
import logging
from botocore.exceptions import ClientError
from botocore.client import Config

config = Config(
   signature_version = 's3v4'
)

s3_client = boto3.client('s3',
                         endpoint_url='https://weka:9000',
                         aws_access_key_id='access_key',
                         aws_secret_access_key='secret_key',
\t                       aws_session_token='session_token',
\t                       config=config,
                         region_name='us-east-1')
try:
    response = s3_client.generate_presigned_url('get_object',
                                                Params={'Bucket': 'mybucket',
                                                        'Key': 'myobject'},
                                                ExpiresIn=3600)
except ClientError as e:
    logging.error(e)

# The response contains the pre-signed URL
print response

```

## Upload/Download example

An example of using the boto3 resource to upload and download an object:

```python
#!/usr/bin/env/python
import boto3
import logging
from botocore.exceptions import ClientError
from botocore.client import Config

config = Config(
   signature_version = 's3v4'
)

s3 = boto3.resource('s3',
                    endpoint_url='https://weka:9000',
                    aws_access_key_id='s3_key',
                    aws_secret_access_key='s3_secret',
                    config=config)

try:
  # upload a file from local file system 'myfile' to bucket 'mybucket' with 'my_uploaded_object' as the object name.
  s3.Bucket('mybucket').upload_file('myfile','my_uploaded_object')

  # download the object 'myfile' from the bucket 'mybucket' and save it to local FS as /tmp/classical.mp3
  s3.Bucket('mybucket').download_file('my_uploaded_object', 'my_downloaded_object')

except ClientError as e:
        logging.error(e)

print ("Downloaded 'my_downloaded_object' as 'my_uploaded_object'. a")

```

## Create bucket example

An example of creating a bucket `newbucket` with a boto3 client:

```python
#!/usr/bin/env/python
import boto3
import logging
from botocore.exceptions import ClientError
from botocore.client import Config

config = Config(
   signature_version = 's3v4'
)

s3_client = boto3.client('s3',
                         endpoint_url='https://weka:9000',
                         aws_access_key_id='s3_key',
                         aws_secret_access_key='s3_secret',
                         config=config)

try:
  s3_client.create_bucket(Bucket='newbucket')
except ClientError as e:
        logging.error(e)

```

<!-- ============================================ -->
<!-- File 121/259: additional-protocols_s3_s3-limitations.md -->
<!-- ============================================ -->

---
description:
---

# S3 supported APIs and limitations

## Supported URL styles for API requests to S3 buckets

WEKA supports two URL styles for API requests to S3 buckets: _path-style_ and _virtual-hosted-style_.

 | Style | URL format |
 | --- | --- |
 | Path-style | https://s3.domain-name.com/bucket-name/object-name |
 | Virtual-hosted-style | https://bucket-name.s3.domain-name.com/object-name |

The difference between the styles is subtle but significant. When using a URL to reference an object, the DNS resolution maps the subdomain name to an IP address. With the path style, the subdomain is always `s3.domain-name.com`. With the virtual-hosted-style, the subdomain is specific to the bucket.

The addressing style used to construct the request is determined by the S3 client sending the request.

## Supported S3 APIs

The following standard S3 APIs are supported:

* **Bucket APIs:**
  * HEAD Bucket
  * GET Bucket
  * CREATE Bucket
  * DELETE Bucket
  * LIST Objects
  * LIST Objects V2
  * LIST Buckets
* **Bucket Lifecycle APIs:**
  * GET Bucket Lifecycle
  * PUT Bucket Lifecycle
  * DELETE Bucket Lifecycle
* **Bucket Policy APIs:**
  * GET Bucket Policy
  * PUT Bucket Policy
  * DELETE Bucket Policy
* **Bucket Tagging APIs:**
  * GET Bucket Tagging
  * PUT Bucket Tagging
  * DELETE Bucket Tagging
* **Object APIs:**
  * GET Object
  * PUT Object
  * DELETE Object
  * DELETE Objects
  * COPY Object
  * HEAD Object
* **Object Tagging APIs:**
  * GET Object Tagging
  * PUT Object Tagging
  * DELETE Object Tagging
* **Object Multipart APIs:**
  * POST Create Multipart Upload
  * POST Complete Multipart Upload
  * GET Object Parts
  * PUT Part
  * DELETE Multipart Upload

## General limits

 | Item | Limits |
 | --- | --- |
 | Maximum number of buckets | 10000 |
 | Maximum object size | 5 TiB |
 | Maximum number of parts per upload | 10000 |
 | Part numbers | 1 to 10000 (inclusive) |
 | Part size | 5 MiB to 5 GiB. The last part can be < 5 MiB |
 | Maximum number of parts returned for a list parts request | 1000 |
 | Maximum number of multipart uploads returned in a list multipart uploads request | 1000 |
 | User-defined metadata per object | 2 KB |
 | Maximum length of an S3 IAM user policy | 2048 |
 | Maximum number of S3 IAM user policies | 1024 |
 | Maximum number of S3 regular users | 1024 |
 | Maximum number of S3 service accounts | 5000 |
 | Maximum number of S3 STS credentials | 5000 |

## Naming limitations

### Buckets

* Bucket names must be between 3 and 63 characters long.
* Bucket names can consist only of lowercase letters, numbers, dots ("`.`"), and hyphens ("`-`").
* Bucket names must begin and end with a letter or number.
* Bucket names must not be formatted as IP addresses (for example, `192.168.5.4`).
* Bucket names must be unique across the cluster.

### Objects

* Object key names may be up to 1024 characters long.
* An object prefix cannot begin with a forward slash ("`/`").
* Adding a forward slash ("`/`") in the object's prefix after the first character is interpreted as a directory. Such directory segments are limited to 255 characters.

Note: - For naming convention details, see Creating object key names (AWS portal).
- Ensure the object key name is also compatible with protocols other than S3. Specifically, avoid special characters that might be unsupported in the other protocols.

## Policy limitations

### Supported S3 policy actions

The S3 protocol implementation supports the following policy actions:

* `s3:*`\
  This wildcard is supported for IAM policies but not for bucket policies.
* `s3:AbortMultipartUpload`
* `s3:CreateBucket`
* `s3:DeleteBucket`
* `s3:DeleteBucketPolicy`
* `s3:DeleteObject`
* `s3:GetBucketLocation`
* `s3:GetLifecycleConfiguration`
* `s3:PutLifecycleConfiguration`
* `s3:ListBucketMultipartUploads`
* `s3:ListMultipartUploadParts`
* `s3:GetBucketPolicy`
* `s3:GetObject`
* `s3:ListAllMyBuckets`
* `s3:ListBucket`
* `s3:PutBucketPolicy`
* `s3:PutObject`
* `s3:GetBucketTagging`
* `s3:PutBucketTagging`

## Supported checksum

Only MD5 checksum algorithm is supported.

## Lifecycle configuration

WEKA supports the Amazon S3 Lifecycle Configuration elements and definitions, with the limitation of only supporting the lifecycle `Expiration` action.

<!-- ============================================ -->
<!-- File 122/259: additional-protocols_s3_s3-cluster-management.md -->
<!-- ============================================ -->

---
description: This page describes how to set up, update, monitor, and delete an S3 cluster.
---

# S3 cluster management

## Considerations

* **Performance scale:** The S3 service can be exposed from the cluster container&#x73;**.** The service performance scales linearly as the S3 cluster scales. Depending on the workload, you may need several Frontend cores to gain maximum performance.
* **Redundancy:** A minimum of two containers is required for the S3 cluster to ensure redundancy and fault tolerance. However, creating a single-container S3 cluster is possible, so there will be no redundancy.
* **Cluster-wide configuration filesystem:** Verify that the dedicated filesystem for persistent protocol configurations is created. If not, create it. For details, see #dedicated-filesystem-requirement-for-persistent-protocol-configurations.
* **Interfaces:** The S3 protocol can be accessed using the assigned port (default: 9000) on all configured interfaces on each WEKA server where the protocol is enabled. It does not use dedicated or floating IPs.

## Round-robin DNS or load balancer **configuration**

To distribute S3 client traffic across WEKA servers with the S3 protocol enabled, it is recommended to set up a round-robin DNS entry that resolves to the IP addresses of the servers. If the WEKA servers have multiple network interfaces, ensure that the DNS entry uses the IPs corresponding to the network(s) intended for S3 traffic.

For added resilience, consider using a DNS server that supports health checks to detect unresponsive servers. Keep in mind that even robust DNS servers or load balancers may become overwhelmed under extreme load conditions.

Alternatively, a client-side load balancer can be used, allowing each client to check the health of S3 containers in the cluster. Configure the load balancer to probe the following endpoint: `/wekas3api/health/ready`.

An example of a suitable load balancer is the open-source **Sidekick Load Balancer**.

**Related information**

Round-robin DNS

Sidekick Load Balancer

**Related topics**

<!-- ============================================ -->
<!-- File 123/259: additional-protocols_s3_s3-cluster-management_s3-cluster-management.md -->
<!-- ============================================ -->

---
description:
---

# Manage the S3 service using the GUI

Using the GUI, you can:

* Create an S3 cluster
* Update an S3 cluster configuration
* Delete an S3 cluster configuration

## Create an S3 cluster

An S3 cluster configuration includes a filesystem, port, and list of servers.

**Before you begin**

Verify that a predefined filesystem for maintaining the persisting cluster-wide protocols' configurations is created. If not, create it. For details, see #dedicated-filesystem-requirement-for-persistent-protocol-configurations.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. On the Configuration tab, select **Configure**.
4. In the S3 Cluster Configuration dialog, set the following properties:
   * **Filesystem**: The filesystem to use for the S3 service. When adding a bucket, it is created in this filesystem by default.
   * **Port**: Default 9000. If required, modify the port through which the cluster exposes the S3 service. Do not set port 9001.
   * **Anonymous Posix UID:** If required, modify the POSIX UID assigned to anonymous users.
   * **Anonymous Posix GID:** If required, modify the POSIX GID assigned to anonymous users.
   * **All servers**: To use all available servers for the S3 configuration, switch on **All servers**. If new servers are deployed later, they do not automatically participate in the S3 cluster.\
     To use specific servers, switch off **All servers**, and select one or more of the available servers from the list to participate in the S3 cluster.
   * **Virtual-hosted-style Domains:** Using the HTTP host header, virtual-hosted-style domains enable addressing the S3 bucket in a REST API request. The bucket name is part of the domain name in the URL. For the domain name, use DNS-compatible values.\
     A domain name can contain only letters, numbers, hyphens, and dots (a maximum of 64 characters for a domain).\
     You can add a list of domains with a total of 1024 characters. Press Enter after setting each domain in the list, or use a comma separator between the domains (no space allowed).
5. In the **Config Filesystem**, select the filesystem used for persisting S3 cluster-wide configuration.
6. Select **Save**.

Once the system completes configuration, the servers' statuses change from not ready (red X icon) to ready (green V icon).

## Update an S3 cluster configuration

You can update the port and the servers to participate in the S3 cluster.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. On the **Configuratio**n tab, select the pencil icon next to the S3 cluster configuration.

4. Update the properties as required. Do not set port 9001.

Note: Modifying the **Virtual-hosted-style Domains** parameter automatically triggers a restart of all S3 containers, resulting in I/O disruption.

5. Select **Save**.

## Delete an S3 cluster configuration

Deleting an existing S3 cluster managed by the WEKA system does not delete the backend WEKA filesystem but removes the S3 bucket exposures of these filesystems.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. On the Configuration tab, select the trash icon next to the S3 cluster configuration.

4. In the S3 Configuration Reset message, select **Reset**.

<!-- ============================================ -->
<!-- File 124/259: additional-protocols_s3_s3-cluster-management_s3-cluster-management-1.md -->
<!-- ============================================ -->

---
description:
---

# Manage the S3 service using the CLI

Using the CLI, you can:

* Add an S3 cluster
* Check the status of the S3 cluster readiness
* List the S3 cluster containers
* Update an S3 cluster configuration
* Add containers to the S3 cluster
* Remove containers from the S3 cluster
* Remove an S3 cluster

## Add an S3 cluster

**Command:** `weka s3 cluster add`

Use the following command line to add an S3 cluster:

`weka s3 cluster add <default-fs-name> <config-fs-name> [--port port] [--key key] [--secret secret] [--max-buckets-limit max-buckets-limit] [--anonymous-posix-uid anonymous-posix-uid] [--anonymous-posix-gid anonymous-posix-gid] [--domain domain] [--container container]... [--all-servers]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | default-fs-name* | The filesystem name to be used for the S3 service. |  |
 | config-fs-name* | The predefined filesystem name for maintaining the persisting cluster-wide protocols' configurations.Verify that the filesystem is already created. If not, create it. For details, see #dedicated-filesystem-requirement-for-persistent-protocol-configurations |  |
 | port | The port where the S3 service is exposed.Do not set port 9001. | 9000 |
 | key | The object store bucket access key ID. | As set when adding an object store bucket. |
 | secret | The object store bucket secret key. | As set when adding an object store bucket. |
 | max-buckets-limit | The maximum number of buckets that can be created.Maximum value: 10000. |  |
 | anonymous-posix-uid | POSIX UID for objects (when accessed via POSIX) created with anonymous access (for buckets with an IAM policy allowing that). | 65534 |
 | anonymous-posix-gid | POSIX GID for objects (when accessed via POSIX) created with anonymous access (for buckets with an IAM policy allowing that). | 65534 |
 | domain | Virtual hosted-style comma-separated domains.Maximum characters for a domain: 64.Maximum characters for a list: 1024.Example: --domain sub1.domain-name.com,sub2.domain-name.com.To remove the existing domain, set "".Example: --domain "" |  |
 | container* | Container IDs with a frontend process to serve the S3 service.To ensure redundancy and fault tolerance, a minimum of two containers is required for the S3 cluster. However, it is possible to create a single-container S3 cluster, which means there will be no redundancy.If you add all-servers to the command, do not specify the list of containers in the container parameter. |  |
 | all-servers* | Use all backend servers to serve S3 commands.If you add all-servers to the command, do not specify the list of containers in the container parameter. | None |

## Check the status of the S3 cluster readiness

**Command:** `weka s3 cluster` or `weka s3 cluster status`

The S3 cluster is comprised of a few S3 containers. Use this command to check the status of the S3 containers that are part of the S3 cluster. Once all the S3 containers are prepared and ready, it is possible to use the S3 service.

## List the S3 cluster containers <a href="#list-the-s3-cluster-containers" id="list-the-s3-cluster-containers"></a>

**Command:** `weka s3 cluster container list`

Use this command to list the containers that serve the S3 cluster.

## Update an S3 cluster configuration <a href="#update-an-s3-cluster-configuration" id="update-an-s3-cluster-configuration"></a>

**Command:** `weka s3 cluster update`

Use the following command line to update an S3 cluster configuration:

`weka s3 cluster update [--key key] [--secret secret] [--port port] [--anonymous-posix-uid anonymous-posix-uid] [--anonymous-posix-gid anonymous-posix-gid] [--domain domain] [--container container]... [--all-servers]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | key | The object store bucket access key ID. | As set when adding an object store bucket. |
 | secret | The object store bucket secret key. | As set when adding an object store bucket. |
 | port | The port where the S3 service is exposed.Do not set port 9001. |  |
 | anonymous-posix-uid | POSIX UID for objects (when accessed via POSIX) created with anonymous access (for buckets with an IAM policy allowing that). | 65534 |
 | anonymous-posix-gid | POSIX GID for objects (when accessed via POSIX) created with anonymous access (for buckets with an IAM policy allowing that). | 65534 |
 | domain | Virtual-hosted-style comma-separated domains.Maximum number of characters: 1024.Example: --domain sub1.domain-name.com,sub3.domain-name.com.To remove the existing domain, set "". Example: --domain ""Note: Modifying the domain parameter value automatically triggers a restart of all S3 containers, resulting in I/O disruption. |  |
 | container* | Container IDs associated with a frontend process responsible for serving the S3 service.For redundancy and fault tolerance, a minimum of two containers is necessary for the S3 cluster. Nevertheless, it is possible to create a single-container S3 cluster, which means there will be no redundancy.If you include all-servers in the command, do not specify a list of containers in the container parameter. |  |
 | all-servers* | Use all backend servers to serve S3 commands.If you add all-servers to the command, do not specify the list of containers in the container parameter. | None |

Note: Instead of using the `weka s3 cluster update` command for adding or removing containers, use the commands `weka s3 cluster containers add` or `weka s3 cluster containers remove`. It is more convenient when managing an S3 cluster with many containers.

## Add containers to the S3 cluster

**Command:** `weka s3 cluster container add`

Use the following command line to add containers to the S3 cluster:

`weka s3 cluster container add <container-ids>`

The following command example adds two containers with the IDs 8 and 9:

`weka s3 cluster container add 8 9`

**Parameters**

 | Name | Value |
 | --- | --- |
 | container-ids* | Container IDs to add to the S3 cluster.Space-separated list of numbers. |

## Remove containers from the S3 cluster

**Command:** `weka s3 cluster container remove`

Use the following command line to remove containers from the S3 cluster:

`weka s3 cluster container remove <container-ids>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | container-ids* | Container IDs to remove from the S3 cluster.Space-separated list of numbers |

## Remove an S3 cluster

**Command:** `weka s3 cluster remove`

Use this command to remove an S3 cluster managed by the WEKA system.

Removing an existing S3 cluster removes the S3 service and configuration, such as IAM policies, buckets, and ILM rules. S3 access is no longer available for clients. Data that resides within the buckets is not deleted. Internal users with S3 roles are deleted from the system.

<!-- ============================================ -->
<!-- File 125/259: additional-protocols_s3_s3-users-and-authentication.md -->
<!-- ============================================ -->

---
description:
---

# S3 users and authentication

## S3 user role

A user with an S3 user role must access the WEKA cluster through the S3 protocol and run S3 commands and S3 APIs. The S3 user operates within the limits of the IAM policy attached to it.

When accessing data with S3 and other protocols (such as POSIX), you can control the POSIX UID/GID of the underlying file representation of objects created with specific S3 user access/secret keys.

Use `--posix-uid` and `--posix-gid` flags for a local user with an S3 user role.

Note: The S3 user name and password serve as the S3 access key and secret key, respectively.

**Related topics**

#create-users

#creating-a-new-iam-policies-1

## IAM policy

Once an S3 user is created, the Cluster Admin must attach an IAM policy to allow this user to operate (within the policy limits). Without an attached IAM policy, the S3 user cannot run any S3 command or API.

The Cluster Admin can attach to an S3 user one of the following:

* A pre-defined policy
* A new custom policy

To create a custom policy, you can use _AWS Policy Generator_ and select `IAM Policy` as the policy type and `Amazon S3` as the AWS service.

Note: The IAM policy size is limited to 2KB. If a larger policy is required, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system).

**Related information**

AWS Policy Generator

## IAM temporary credentials (STS)

Once an S3 user is created and an IAM policy is attached, the Assume Role command can be used to obtain temporary credentials to access the S3 API.

The result of calling the API is an access key, secret key, and session token that can be used to access S3 APIs. The permissions for the temporary credentials are the permissions induced by the user's IAM policy. Furthermore, it is possible to supply a different IAM policy (with reduced capabilities only) for the temporary credentials request.

Note: If the STS credentials are compromised, revoke them by deleting the associated S3 user. This action will invalidate all STS credentials linked to that user.

Note: Some S3 clients and SDKs, when provided with an access key and secret key pair, automatically support the AssumeRole API. They use STS credentials and automatically regenerate a new STS when the previous one expires.

## S3 service accounts

S3 service accounts are child identities of a single parent S3 user. Each service account inherits its privileges based on the IAM policies attached to its parent user. S3 service accounts also support an optionally attached IAM policy that restricts its access to a _subset_ of the actions and resources available to the parent user (S3 APIs and S3-related CLI commands).

S3 service accounts enable the management of specific object store buckets and S3 APIs (as defined by the IAM policy) without relying on the S3 user administrative action.

Unlike IAM temporary credentials (STS), the S3 service account is not temporary and has no expiration date. It is used to manage the object store buckets and S3 APIs.

Only an S3 user can manage S3 service accounts (Cluster Admin cannot). An S3 user can create up to 100 S3 service accounts. Managing S3 service accounts is only available through the CLI.

**Related topics**

#supported-s3-apis

<!-- ============================================ -->
<!-- File 126/259: additional-protocols_s3_s3-users-and-authentication_s3-users-and-authentication.md -->
<!-- ============================================ -->

---
description:
---

# Manage S3 users and authentication using the CLI

With the CLI, you can:

* View existing IAM policies
* Add an IAM policy
* Delete an IAM policy
* Attach a policy to an S3 user
* Detach a policy from an S3 user
* Generate a temporary security token

## View existing IAM policies

**Command:** `weka s3 policy list`

Use this command to list the existing IAM policies.

The command lists the pre-defined and custom policies the Cluster Admin has added.

**Command:** `weka s3 policy show <policy-name>`

Use this command to see the JSON definition of the selected IAM policy.

The pre-defined policy values are:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListAllMyBuckets",
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads",
        "s3:ListMultipartUploadParts",
        "s3:GetBucketLocation",
        "s3:GetBucketPolicy",
        "s3:GetBucketTagging",
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::*"
      ]
    }
  ]
}
```

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject"
      ],
      "Resource": [
        "arn:aws:s3:::*"
      ]
    }
  ]
}
```

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:*"
      ],
      "Resource": [
        "arn:aws:s3:::*"
      ]
    }
  ]
}
```

## Add an IAM policy

**Command:** `weka s3 policy add`

Use the following command line to add an S3 IAM policy:

`weka s3 policy add <policy-name> <policy-file>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | policy-name* | Name of the IAM policy to add. |
 | policy-file* | Path to the custom JSON file representing an IAM policy for anonymous access. See #supported-s3-policy-actions. |

## Delete an IAM policy <a href="#creating-a-new-iam-policies" id="creating-a-new-iam-policies"></a>

**Command:** `weka s3 policy remove`

Use the following command line to delete an S3 IAM policy:‚Äå

`weka s3 policy remove <policy-name>`‚Äå

**Parameters**

 | Name | Value |
 | --- | --- |
 | policy-name* | Name of the IAM policy to remove. |

## Attach a policy to an S3 user <a href="#creating-a-new-iam-policies" id="creating-a-new-iam-policies"></a>

**Command:** `weka s3 policy attach`

Use the following command line to attach an IAM policy to an S3 user:‚Äå

`weka s3 policy attach <policy> <user>`‚Äå

**Parameters**

 | Name | Value |
 | --- | --- |
 | policy* | Name of an existing IAM policy. |
 | user* | Name of an existing S3 user. |

## Detach a policy from an S3 user <a href="#creating-a-new-iam-policies-1" id="creating-a-new-iam-policies-1"></a>

**Command:** `weka s3 policy detach`

Use the following command line to detach an IAM policy from an S3 user:‚Äå‚Äå

`weka s3 policy detach <user>`‚Äå‚Äå

**Parameters**

 | Name | Value |
 | --- | --- |
 | user* | Name of an existing S3 user. |

## Generate a temporary security token

**Command:** `weka s3 sts assume-role`

Use the following command line to generate a temporary security token:

`weka s3 sts assume-role <--access-key access-key> [--secret-key secret-key] [--policy-file policy-file] <--duration duration>`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | access-key* | An S3 user access key |  |
 | secret-key | An S3 user secret key | If not supplied, the command prompts to supply the secret-key. |
 | policy-file | Path to a custom JSON file representing an IAM policy for anonymous access.You cannot gain additional capabilities to the IAM policy attached to this S3 user.See Supported Policy Actions. | ‚Äã |
 | duration* | Duration for the token validity.Possible values between 15 minutes and 1 week. Format: 900s, 60m, 2d, 1w | ‚Äã |

An example response:

```
Access-Key: JR9O0U6V42KLPFQDO2Z3
Secret-Key: wM0QMWuQ04WHlByj2SlEyuNrWoliMaCoVPmRsKbH
Session-Token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJKUjlPMFU2VjQyS0xQRlFETzJaMyIsImV4cCI6NjA0ODAwMDAwMDAwMDAwLCJwb2xpY3kiOiJyZWFkd3JpdGUifQ.-rzf78OHdKv-25NFls1SaUvNKST5SoVSG8iR2hQrTQC1K05ZZlHBFfU-6N3_boF9c5P70y5Pa10YBHseh4DkVA
```

<!-- ============================================ -->
<!-- File 127/259: additional-protocols_s3_s3-users-and-authentication_s3-users-and-authentication-1.md -->
<!-- ============================================ -->

---
description: This page describes how to add and control S3 service accounts using the CLI.
---

# Manage S3 service accounts using the CLI

With the CLI, as an S3 user, you can:

* View S3 service accounts
* Add an S3 service account
* Show S3 service account details
* Remove an S3 service account

**Related topics**

#s3-service-accounts

## View existing S3 service accounts

**Command:** `weka s3 service-account list`

Use this command to list the existing S3 service accounts.

The command lists only the access keys of the S3 service accounts added by the S3 user.

## Add an S3 service account

**Command:** `weka s3 service-account add`

Use the following command line to add an S3 user account:

`weka s3 service-account add <policy-file>`

The system returns an access key and a secret key. If you do not specify a `policy-file`, the S3 service account inherits the IAM policy from the parent S3 user.

Note: The secret key is visible **only once** when adding the S3 service account. You must save the secret key in a safe place for later use.

**Parameters**

 | Name | Type | Value | Limitations | Mandatory | Default |
 | --- | --- | --- | --- | --- | --- |
 | Name | Type | Value | Limitations | Mandatory | Default |
 | policy-file | String | The IAM policy file to attach to the S3 service account |  |  | Inherits the IAM policy from the parent S3 user |

## Show an S3 service account details

**Command:** `weka s3 service-account show`

Use the following command line to display the policy details attached to the specified S3 service account:

`weka s3 service-account show <access-key>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | access-key* | The access key of the S3 service account. |

## Remove S3 service account <a href="#creating-a-new-iam-policies" id="creating-a-new-iam-policies"></a>

**Command:** `weka s3 service-account remove`

Use the following command line to remove an S3 service account:‚Äå

`weka s3 service-account remove <access-key>`‚Äå

**Parameters**

 | Name | Value |
 | --- | --- |
 | access-key* | The access key of the S3 service account to remove. |

<!-- ============================================ -->
<!-- File 128/259: additional-protocols_s3_s3-buckets-management.md -->
<!-- ============================================ -->

---
description:
---

# S3 buckets management

Manage your buckets using WEKA, either through standard S3 API calls or the WEKA GUI/API/CLI. Determine bucket permissions by using IAM policies for authorized users or setting bucket policies for anonymous access.

Buckets and objects created through the S3 protocol come with default root POSIX permissions. For more precise control, create a user with an S3 role and set specific POSIX permissions for objects generated with their access/secret keys.

Objects created through anonymous access (enabled by IAM policy) are assigned the anonymous UID/GID. By default, all buckets are created within the specified filesystem during S3 cluster configuration. To place a bucket in a different filesystem, use a straightforward call to the WEKA GUI/API/CLI.

**Related topics**

#s3-user-role

#naming-limitations

<!-- ============================================ -->
<!-- File 129/259: additional-protocols_s3_s3-buckets-management_s3-buckets-management.md -->
<!-- ============================================ -->

---
description: This page describes how to manage S3 buckets using the GUI.
---

# Manage S3 buckets using the GUI

Using the GUI, you can:

* Create a bucket
* View a bucket details
* Edit a bucket hard quota
* Edit a bucket policy
* Delete a bucket

## Create a bucket <a href="#create-a-bucket" id="create-a-bucket"></a>

**Before you begin**

S3 does not support creating buckets on filesystems with names containing the characters ' ', '`(`', '`)`', or '`&`'. Verify that the filesystem name excludes these characters. Rename the filesystem if needed before creating the S3 bucket.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. Select the **Buckets** tab.
4. Select  **+Create**.
5. In the Add S3 Bucket dialog, do the following:
   * **Bucket Name:** Set a bucket name according to the naming conventions.
   * **Filesystem:** Set the filesystem to host the bucket.
   * **Use Existing Directory:** If you want to expose an existing directory, set its path. Make sure that the directory is not below the hierarchy of the already configured S3 bucket.
   * **Hard Quota:** Set the maximum capacity for the bucket. If you want to remove the hard quota setting, enter 0.
   * **Bucket Policy:** Select the policy to attach to the bucket: none, download, upload, public, or custom. If you select a custom policy, add it in JSON format.
6. Select **Save**.

## View a bucket details <a href="#view-a-bucket-details" id="view-a-bucket-details"></a>

You can view the details of the bucket.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. Select the **Buckets** tab.
4. Select the three dots of the bucket and select **View**.

## Edit a bucket hard quota <a href="#edit-a-bucket-hard-quota" id="edit-a-bucket-hard-quota"></a>

The hard quota determines the maximum capacity of the bucket. Initially, you can only set the hard quota for an empty bucket. If the hard quota of the bucket is already set, you can modify it or remove it.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. Select the **Buckets** tab.
4. Select the three dots of the bucket and select **Edit Hard Quota**.
5. Set the maximum capacity for the bucket. If you want to remove the hard quota setting, enter 0.

## Edit a bucket policy <a href="#edit-a-bucket-policy" id="edit-a-bucket-policy"></a>

You can edit the bucket policy according to your needs.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. Select the **Buckets** tab.
4. Select the three dots of the bucket you want to delete, and select **Edit Bucket Policy**.
5. Select the policy to attach to the bucket: none, download, upload, public, or custom. If you select a custom policy, add it in JSON format.

## Delete a bucket <a href="#remove-a-bucket" id="remove-a-bucket"></a>

You can delete an existing bucket from the filesystem only if the bucket is empty. If the bucket is not empty, you can detach the bucket from the S3 configuration and keep the data and metadata in place. Consequently, you can recreate the bucket while preserving the data and metadata (see Create a bucket using the Use Existing Directory switch.

Note: If the intent is to keep the data files for use outside of the S3 configuration and delete only the S3 metadata, contact the Customer Success Team for assistance.

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. Select the **Buckets** tab.
4. Select the three dots of the bucket you want to delete and select **Remove**.

5. In the confirmation message, if the bucket is not empty, switch **Keep Data** to **ON**.\
   Then, select **Remove**.

<!-- ============================================ -->
<!-- File 130/259: additional-protocols_s3_s3-buckets-management_s3-buckets-management-1.md -->
<!-- ============================================ -->

---
description: This page describes how to manage S3 buckets using the CLI.
---

# Manage S3 buckets using the CLI

Using the CLI, you can:

* Add a bucket
* List buckets
* Set a bucket quota
* Unset a bucket quota
* Remove a bucket
* Manage bucket policies

## Add a bucket

**Command:** `weka s3 bucket add`

Use the following command line to add an S3 bucket:

`weka s3 bucket add <name> [--policy policy] [--policy-json policy-json] [--hard-quota hard-quota] [--fs-name fs-name] [--fs-id fs-id] [--existing-path existing-path]`

Note: S3 does not support creating buckets on filesystems with names containing the characters ' ', '`(`', '`)`', or '`&`'. Verify that the filesystem name excludes these characters. Rename the filesystem if needed before creating the S3 bucket.

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | name* | The name for the S3 bucket to add.Refer to the Bucket Naming Limitations section. |  |
 | policy | The name of a pre-defined bucket policy for anonymous access. Possible values: none, download, upload, public. | none |
 | policy-json | A path to a custom policy JSON file representing an S3 bucket policy for anonymous access. |  |
 | hard-quota | Hard quota for the S3 bucket.You can only set on a new bucket without existing data. You cannot set it when using existing-path to an existing directory with data. |  |
 | fs-name | Existing filesystem name to create the bucket within.Possible values: fs-name, fs-id. | The default filesystem specified when creating the S3 cluster. |
 | fs-id | Existing filesystem ID to create the bucket within.Possible values: fs-name, fs-id. | The default filesystem specified when creating the S3 cluster. |
 | existing-path | Existing directory path relative to the filesystem root to expose a bucket from. |  |

## List buckets

**Command:** `weka s3 bucket list`

Use this command to list existing buckets.

## Set a bucket quota

**Command:** `weka s3 bucket quota set`

Use the following command line to set an S3 bucket quota:

`weka s3 bucket quota set <bucket-name> <hard-quota>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | bucket-name* | The name of an existing S3 bucket. |
 | hard-quota* | Hard quota for the S3 bucket.You can only set it initially on an empty bucket. Calling this command on a bucket that already has a quota changes the quota limitation. |

## Reset a bucket quota

**Command:** `weka s3 bucket quota reset <bucket-name>`

Use this command to reset an existing bucket quota.

Note: If the bucket point to a directory shared with other protocols, changing the quota affects all protocols (changes the associated directory quota).

## Remove a bucket

**Command:** `weka s3 bucket remove`

Use this command to remove an existing bucket from the filesystem only if the bucket is empty. If the bucket is not empty, you can keep the data on the filesystem and remove the bucket from the S3 configuration.

`weka s3 bucket remove <name> [--unlink]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | name* | The name of an existing S3 bucket. |
 | unlink | Detaches the bucket from the S3 configuration and keeps the data and metadata in place. Consequently, you can recreate the bucket while preserving the data and metadata (see weka s3 bucket create using the existing-path option).Note: If the intent is to keep the data files for use outside of the S3 configuration and delete only the S3 metadata, contact the Customer Success Team for assistance. |

## Manage bucket policies

It is possible to set bucket policies for anonymous access. You can choose a pre-defined policy or add a customized policy.

### Set a pre-defined bucket policy

A bucket is automatically created without any anonymous access permissions. You can use one of the pre-defined policies: `download`, `upload`, or `public`.

Example: For a bucket named `mybucket`, the following are the pre-defined policy values:

```json
{
  "Statement": [
    {
      "Action": [
        "s3:GetBucketLocation",
        "s3:ListBucket"
      ],
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "*"
        ]
      },
      "Resource": [
        "arn:aws:s3:::mybucket"
      ]
    },
    {
      "Action": [
        "s3:GetObject"
      ],
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "*"
        ]
      },
      "Resource": [
        "arn:aws:s3:::mybucket/*"
      ]
    }
  ],
  "Version": "2012-10-17"
}
```

```json
{
  "Statement": [
    {
      "Action": [
        "s3:GetBucketLocation",
        "s3:ListBucketMultipartUploads"
      ],
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "*"
        ]
      },
      "Resource": [
        "arn:aws:s3:::mybucket"
      ]
    },
    {
      "Action": [
        "s3:DeleteObject",
        "s3:ListMultipartUploadParts",
        "s3:PutObject",
        "s3:AbortMultipartUpload"
      ],
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "*"
        ]
      },
      "Resource": [
        "arn:aws:s3:::mybucket/*"
      ]
    }
  ],
  "Version": "2012-10-17"
}
```

```json
{
  "Statement": [
    {
      "Action": [
        "s3:GetBucketLocation",
        "s3:ListBucket",
        "s3:ListBucketMultipartUploads"
      ],
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "*"
        ]
      },
      "Resource": [
        "arn:aws:s3:::mybucket"
      ]
    },
    {
      "Action": [
        "s3:ListMultipartUploadParts",
        "s3:PutObject",
        "s3:AbortMultipartUpload",
        "s3:DeleteObject",
        "s3:GetObject"
      ],
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "*"
        ]
      },
      "Resource": [
        "arn:aws:s3:::mybucket/*"
      ]
    }
  ],
  "Version": "2012-10-17"
}
```

**Command:** `weka s3 bucket policy set`

Use the following command line to set a pre-defined bucket policy:

`weka s3 bucket policy set <bucket-name>  <bucket-policy>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | bucket-name* | Name of an existing S3 bucket. |
 | bucket-policy* | Name of a pre-defined bucket policy for anonymous access.Possible values: none, download, upload, public. |

### Set a custom bucket policy

To create a custom policy, you can use AWS Policy Generator and select `S3 Bucket Policy` type. With a custom policy, it is possible to limit anonymous access only to specific prefixes.

For example, to set a custom policy for `mybucket` to allow read-only access for objects with a `public/` prefix, the custom policy, as generated with the calculator, is:

```
{
  "Id": "Policy1624778813411",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1624778790840",
      "Action": [
        "s3:ListBucket"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::mybucket",
      "Condition": {
        "StringEquals": {
          "s3:prefix": "public/"
        }
      },
      "Principal": "*"
    },
    {
      "Sid": "Stmt1624778812360",
      "Action": [
        "s3:GetObject"
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::mybucket/public/*",
      "Principal": "*"
    }
  ]
}
```

**Command:** `weka s3 bucket policy set-custom`

Use the following command line to set a custom bucket policy:

`weka s3 bucket policy set-custom <bucket-name> <policy-file>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | bucket-name* | Name of an existing S3 bucket. |
 | policy-file* | A path to a custom JSON file representing an S3 bucket policy for anonymous access.Wildcards (such as s3:*) are not allowed as an Action in the custom policy file.See Supported Policy Actions. |

### View a bucket policy

**Command:** `weka s3 bucket policy get / weka s3 bucket policy get-json`

Use the following command line to view an S3 bucket policy name/JSON:

`weka s3 bucket policy get <bucket-name> / weka s3 bucket policy get-json <bucket-name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | bucket-name* | Name of an existing S3 bucket. |

### Unset a bucket policy

**Command:** `weka s3 bucket policy unset`

Use the following command line to unset an S3 bucket policy:

`weka s3 bucket policy unset <bucket-name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | Name | Value |
 | bucket-name* | Name of an existing S3 bucket. |

<!-- ============================================ -->
<!-- File 131/259: additional-protocols_s3_s3-information-lifecycle-management.md -->
<!-- ============================================ -->

---
description:
---

# S3 lifecycle rules management

S3 lifecycle rules management in WEKA enable you to automate the organization and expiration of objects in S3 buckets by defining rules. These rules operate consistently across all objects in a bucket, regardless of the access protocol.

**Key features and considerations:**

* **Customizable object expiration:** Define rules to automatically expire objects based on prefixes or tags, providing precise control over data retention.
* **Extensive rule support:** Apply up to 1,000 rules per bucket to address diverse data lifecycle requirements.
* **Priority handling:** If multiple rules apply to the same object, the rule with the earliest expiration takes precedence.

**Related topics**

<!-- ============================================ -->
<!-- File 132/259: additional-protocols_s3_s3-information-lifecycle-management_s3-information-lifecycle-management.md -->
<!-- ============================================ -->

---
description:
---

# Manage S3 lifecycle rules using the GUI

Using the GUI, you can:

* Add a lifecycle rule
* View lifecycle rules
* Remove a lifecycle rule
* Remove all lifecycle rules

## **Add** a lifecycle rule

You can add a lifecycle rule to an object (bucket) that defines an expiration duration per object prefix and tags.

**Procedure**

1. From the S3 buckets page, select the three dots of the required bucket, and select **Lifecycle Rules**.

2\. In the Add a Lifecycle Rule dialog set the following:

* **Expiration days:** The minimum number of days before the object is eligible for expiration. ILM processes the object shortly after this period based on its modified timestamp, but processing may be delayed if the queue is long.
* **Prefix:** The object prefix to which the rule applies. Wildcards are not supported.
* **Tags:** One or more object tags to apply the ILM policy rule. The tags are key-value pairs. Example: \<k1>=\<v1>.

3\. Select **Save**.

## View lifecycle rules <a href="#viewing-ilm-rules" id="viewing-ilm-rules"></a>

You can view the lifecycle rules defined for a bucket and filter according to expiration days, prefixes, or tags.

**Procedure**

1. From the S3 buckets page, select the three dots of the required bucket, and select **Lifecycle Rules**.

## Remove a lifecycle rule

You can remove a specific lifecycle rule of a specified bucket if it is no longer required.

**Procedure**

1. From the S3 buckets page, select the three dots of the required bucket, and select **Lifecycle Rules**.
2. In the Lifecycle Rules dialog, select the three dots of the required rule and select **Remove**.

## Remove all lifecycle rules

You can remove all the lifecycle rules of a specified bucket if they are no longer required.

**Procedure**

1. From the S3 buckets page, select the three dots of the required bucket, and select **Lifecycle Rules**.
2. In the Lifecycle Rules dialog, select **Clear all rules**.

<!-- ============================================ -->
<!-- File 133/259: additional-protocols_s3_s3-information-lifecycle-management_s3-information-lifecycle-management-1.md -->
<!-- ============================================ -->

---
description:
---

# Manage S3 lifecycle rules using the CLI

Using the CLI, you can:

* Add a lifecycle rule
* View lifecycle rules
* Remove a lifecycle rule
* Reset the lifecycle rules of a bucket

## Add a lifecycle rule

**Command:** `weka s3 bucket lifecycle-rule add`

Use the following command line to add a lifecycle rule:

`weka s3 bucket lifecycle-rule add <bucket> <expiry-days>  [--prefix prefix] [--tags tags]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | bucket* | Name of the S3 bucket. |
 | expiry-days* | The minimum number of days before the object is eligible for expiration. ILM processes the object shortly after this period based on its modified timestamp, but processing may be delayed if the queue is long.Minimum: 1 day |
 | prefix | The object prefix to which the rule applies. Wildcards are not supported. |
 | tags | Key value pair of object tags to apply the rule to.Pairs of key values: '<k1>=<v1>&#x26;<k2=<v2>' |

## View lifecycle rules <a href="#viewing-ilm-rules" id="viewing-ilm-rules"></a>

**Command:** `weka s3 bucket lifecycle-rule list`‚Äå

Use the following command line to view a bucket's existing lifecycle rules:‚Äå

`weka s3 bucket lifecycle-rule list <bucket>`‚Äå

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | bucket* | The name of the S3 bucket. | ‚ÄãContent |

## Remove a lifecycle rule

**Command:** `weka s3 bucket lifecycle-rule remove`

Use the following command line to remove an lifecycle rule of a specified bucket:

`weka s3 bucket lifecycle-rule remove <bucket> <rule>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | bucket* | The name of the S3 bucket. |
 | rule* | The ID of the rule to delete. |

## Remove all lifecycle rules

**Command:** `weka s3 bucket lifecycle-rule reset`

Use the following command line to remove all the lifecycle rules of a specified bucket:

`weka s3 bucket lifecycle-rule reset <bucket>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | bucket* | The name of the S3 bucket. |

<!-- ============================================ -->
<!-- File 134/259: additional-protocols_s3_audit-s3-apis.md -->
<!-- ============================================ -->

---
description: This page describes how to set up an HTTP webhook for S3  audit purposes.
---

# Audit S3 APIs

Configuring HTTP webhooks for S3 API operations enables the capture of detailed audit logs, which are essential for analyzing access patterns, supporting security and compliance initiatives, and troubleshooting issues in S3 interactions.

S3 API calls can generate JSON-formatted audit events, which are streamed to target applications such as Splunk for real-time monitoring and analysis. This approach replaces the legacy `BucketLogging` S3 APIs with a more robust and scalable auditing solution. Each event provides granular details about the operation, including the request type, object affected, requester identity, and network metadata.

For example, an audit event generated by a `PutObject` operation includes fields such as the bucket name, object key, operation status, client IP address, user agent, and the WEKA cluster information that processed the request. These elements are crucial for tracing user activity, validating policy compliance, and performing forensic investigations. By understanding the structure and key fields in these logs, users can ensure that operations conform to expected behavior and promptly identify unauthorized access.

Note: If the WEKA cluster loses connectivity to the configured webhook application or if the internal event buffer reaches capacity, audit events are discarded. To avoid data loss, it is strongly recommended that the availability and performance of the external webhook target be continuously monitored.

**Related topics**

<!-- ============================================ -->
<!-- File 135/259: additional-protocols_s3_audit-s3-apis_audit-s3-apis.md -->
<!-- ============================================ -->

---
description:
---

# Configure audit webhook using the CLI

Using the CLI, you can:

* Enable an audit webhook for S3 APIs
* Disable an audit webhook for S3 APIs
* View the audit webhook configuration

## Enable an audit webhook for S3 APIs

**Command:** `weka s3 cluster audit-webhook enable`

Use the following command line to enable an audit webhook for the S3 cluster:

`weka s3 cluster audit-webhook enable <--endpoint endpoint>  <--auth-token auth-token>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | endpoint* | The webhook endpoint. |
 | auth-token* | The authentication token obtained from the webhook service. |

## Disable an audit webhook for S3 APIs

**Command:** `weka s3 cluster audit-webhook disable`

Use this command to disable the audit webhook.

## View the audit webhook configuration

**Command:** `weka s3 cluster audit-webhook show`

Use this command to view the audit webhook configuration.

<!-- ============================================ -->
<!-- File 136/259: additional-protocols_s3_audit-s3-apis_audit-s3-apis-1.md -->
<!-- ============================================ -->

---
description: This page describes an example for using Splunk to audit S3.
---

# Example: How to use Splunk to audit S3

Setting up an HTTP Event Collector (HEC).

#### Step 1: Configure the HEC

Follow the steps in Enable HTTP Event Collector on Splunk. Since the S3 event stream is provided in JSON  format, choose `_json` as the data source type.

#### Step 2: Create a token

Follow the steps in Create an Event Collector token on Splunk to create a token WEKA will use to access Splunk as an HTTP webhook. You can create a new index or use an existing one for easy discovery/monitor/query.

Copy the created token for later use.

#### Step 3: Test the configuration

To validate the configuration, send a test event as suggested in the JSON request and response section.

```
curl -k  https://hec.example.com:8088/services/collector/raw -H "Authorization: Splunk B5A79AAD-D822-46CC-80D1-819F80D7BFB0" -d '{"event": "hello world"}'
{"text": "Success", "code": 0}
```

Once completed, you can search the index you have created in Splunk and see this event.

#### Step 4: Configure the audit webhook in WEKA

As a cluster admin, run the following CLI command to enable the audit webhook:

```
weka s3 cluster audit-webhook enable --endpoint=https://splunk-server:8088/services/collector/raw --auth-token='\"Splunk B5A79AAD-D822-46CC-80D1-819F80D7BFB0\"'
```

<!-- ============================================ -->
<!-- File 137/259: additional-protocols_s3_audit-s3-apis_configure-audit-webhook-using-the-gui.md -->
<!-- ============================================ -->

# Configure audit webhook using the GUI

The audit webhook sends S3 APIs audit events to a remote system (for example, Splunk). These events provide a better understanding of the traffic nature.

When creating an S3 cluster, the audit webhook is default enabled but not configured.

**Before you begin**

#create-an-s3-cluster

**Procedure**

1. From the menu, select **Manage > Protocols**.
2. From the Protocols pane, select **S3**.
3. On the S3 Cluster Configuration, select the **Configure audit webhook** icon.

4. On the Audit Webhook Configuration dialog, set the following:
   * **Endpoint:** The webhook endpoint URL that receives the events stream.
   * **Auth Token:** The webhook authentication token.
5. Select **Save**.

<!-- ============================================ -->
<!-- File 138/259: additional-protocols_s3_audit-s3-apis_example-how-to-use-s3-audit-events-for-tracking-and-security.md -->
<!-- ============================================ -->

---
description:
---

# Example: How to use S3 audit events for tracking and security

The S3 audit events are essential for tracking access and modifications to data, ensuring compliance with organizational and regulatory requirements, detecting unauthorized activity, and troubleshooting suspicious or failed S3 operations. By understanding the structure and content of these logs, users can conduct forensic analysis and validate that operations were executed according to policy.

The following example illustrates a `PutObject` operation and describes the key elements in the event log.

```json
{
  "api": {
    "bucket": "phg-sandman",
    "name": "PutObject",
    "object": "cat-and-dog.jpg",
    "status": "OK",
    "statusCode": 200,
    "timeToResponse": "10531825ns"
  },
  "auditVersion": "1.weka",
  "deploymentid": "079f7a1f-be3b-44c8-b36f-4484fe1ae4b2",
  "remotehost": "216.58.114.14",
  "requestHeader": {
    "Authorization": "AWS4-HMAC-SHA256 Credential=AKIAIOSFODNN7EXAMPLE...",
    "User-Agent": "aws-sdk-go/1.44.235 (go1.18.10; linux; amd64) S3Manager",
    "Content-Type": "image/jpeg"
  },
  "requestID": "1773CE9A70A978BB",
  "responseHeader": {
    "Content-Length": "0",
    "ETag": "5d64dcd326aa93f6542e27f757ec8146",
    "Server": "S3"
  },
  "time": "2025-03-21T06:37:27.915055685Z",
  "userAgent": "aws-sdk-go/1.44.235 (go1.18.10; linux; amd64) S3Manager",
  "wekaInfo": {
    "clusterGUID": "b28b4f9b-5d62-4c0b-97ef-6a72037930e7",
    "clusterName": "DAD08-B",
    "release": "4.4.6.11",
    "serverIP": "10.26.211.72",
    "serverName": "obj-115-07.dad08.tcp.target.net",
    "version": "4.4.6"
  }
}
```

#### Key elements and descriptions

* **bucket:** Identifies the S3 bucket involved in the event.
* **name:** Specifies the S3 operation type (for example, `PutObject`, `GetObject`).
* **object:** Name of the object on which the operation was performed.
* **status / statusCode:** Indicates the result of the operation (for example, `OK`, HTTP status `200`).
* **remotehost:** The IP address from which the request originated.
* **Authorization:** Credentials used for API authorization.
* **userAgent:** The user agent string from the requesting client, useful for identifying the client software.
* **clusterName / serverIP / serverName:** Provides information about the WEKA cluster and access point.
* **version:** The software version of the WEKA system handling the request.

<!-- ============================================ -->
<!-- File 139/259: operation-guide.md -->
<!-- ============================================ -->

# Operation Guide

## Topics in this section

### Alerts

This page describes the alerts that can be received in this version of the WEKA system.

### Events

WEKA system events provide timestamped information about cluster operations and changes.

### Statistics

This page describes the statistics available in the WEKA system and how to work with them.

### Insights

### System congestion

This page describes possible congestion issues in the WEKA system.

### User management

The WEKA system enables managing user access and roles locally and through organizational directories like LDAP or AD. This topic covers user types, authentication methods, and management.

### Organizations management

Understand how WEKA supports multi-tenancy by logically separating users and resources using organizations.

### Expand and shrink cluster resources

Expand and shrink a cluster in a homogeneous WEKA system configuration.

### Background tasks

This page describes the management of background tasks running on the WEKA system.‚Äå

### Audit and forwarding management

Manage the forwarding of data access audit events to external monitoring systems.

### Upgrade WEKA versions

Upgrade your WEKA system with the latest version.

### Manage WEKA drivers

Learn how to manage WEKA drivers to ensure they are properly built, installed, signed, and distributed for the appropriate kernel version.

<!-- ============================================ -->
<!-- File 140/259: operation-guide_user-management.md -->
<!-- ============================================ -->

---
description:
---

# User management

## User types and roles

Access to the WEKA system is managed through user accounts, each uniquely identified by a username and authenticated using a password. The system supports up to 1,152 local users. User permissions and access levels are determined by predefined roles.

### Role descriptions

 | Role | Purpose | Key permissions | Restrictions |
 | --- | --- | --- | --- |
 | Cluster Admin | Advanced administrative tasks for managing the cluster. | Full access to system configuration, user management, and performance tuning. | Cannot delete their own account or change their role to a regular user role. |
 | CSI | Interfacing with the WEKA cluster through the weka-csi-plugin for Kubernetes. | Provisioning, mounting, and unmounting file systems.Storage management tasks through CLI and API. | Limited to storage management.No access to broader administrative functions. |
 | Organization Admin | Administrative tasks within a single organization. | Privileges limited to managing the assigned organization. | Cannot perform cluster-wide administrative tasks.For details, see #organization-admin-role-privileges |
 | Read-only | Viewing system configurations and data without making changes. | View system settings and data through GUI, CLI, and API.Authenticate and write data to mounted locations (exception for authenticated mounts). | Cannot modify system settings or create file systems, protocols, or user accounts. |
 | Regular | Basic role for mounting filesystems. | Sign in to obtain an access token.Change own password. | No GUI access.No CLI or API commands beyond mounting tasks. |
 | S3 | Running S3 commands and APIs. | Perform S3 operations within the limits of the assigned IAM policy.Create S3 service accounts with specific policies. | Limited to actions allowed by the attached S3 IAM policy. |

### **Special case: Cluster Admin (first user)**

When a WEKA cluster is created, a default **Cluster Admin** user (`admin`) is generated with a default password. This user must change their password on the first login. The first user has full administrative privileges across the cluster.\
Key responsibilities and restrictions include:

* **Responsibilities**: Managing cluster-wide operations, global configurations, hardware, and resources.
* **Restrictions**: Cannot delete their account or downgrade their role.

Cluster Admin accounts must adhere to a strict password policy:

* Minimum of 8 characters.
* At least one uppercase letter, one lowercase letter, and one number or special character.

You can create additional Cluster Admin accounts with unique usernames. You can rename or delete the default `admin` user if at least one other Cluster Admin account exists. To ensure system continuity, maintain at least one internal Cluster Admin account for support purposes.

Note: When multiple organizations exist, Organization Admins manage specific organizations, while Cluster Admins handle cluster-wide and infrastructure-level tasks.

## Authentication and login process

The WEKA user login process involves authenticating users and managing access. The following steps outline the key components:

* **Local user login**: The system first searches for the user among local accounts created using the GUI or the `weka user add` command.
* **LDAP or AD integration**: If the user is not found locally but exists in an integrated LDAP or AD directory, the system verifies their credentials using LDAP. Integration must be configured beforehand.
* **Login events**:
  * **Successful login**: Triggers a `UserLoggedIn` event, logging the username, role, and user type (local or LDAP).
  * **Failed login**: Prompts an "Invalid username or password" message and triggers a `UserLoginFailed` event with details of the failure.
* **GUI login**: Users log in by entering their username and password in the GUI. The `WEKA_USERNAME` and `WEKA_PASSWORD` environment variables can pass this information to the CLI.
* **CLI login**: Users authenticate through the CLI using the `weka user login <username> <password>` command. This generates an authentication token file, defaulting to `~/.weka/auth-token.json`.
  * Use `weka user whoami` to verify the currently logged-in CLI user.
  * Adjust the token file path with the `--path` option or the `WEKA_TOKEN` environment variable.
* **Persistence and defaults**:
  * The `weka user login` command's persistence is server-specific.
  * If `WEKA_USERNAME` and `WEKA_PASSWORD` are not set, the CLI uses the token file.
  * If no CLI user is logged in and no token file exists, the CLI defaults to `admin/admin` credentials.
* **Custom token path**: Use the `WEKA_TOKEN` environment variable to specify a custom path for the authentication token file.

**Related topics**

<!-- ============================================ -->
<!-- File 141/259: operation-guide_user-management_user-management.md -->
<!-- ============================================ -->

---
description:
---

# Manage users using the GUI

Using the GUI, you can:

* Manage local users
* Manage user directory

## Manage local users

Local users are created in the local system instead of domain users that the organization's User Directory manages. You can create up to 1152 local users to work with a WEKA system cluster.

### Create a local user

**Procedure**

1. From the menu, select **Configure > User Management**.
2. In the Local Users tab, select **+Create**.
3. In the Create New User dialog, set the following properties:
   * **Username:** Set the user name for the local user.
   * **Password:** Set a password according to the requirements. The password must contain at least 8 characters: an uppercase letter, a lowercase letter, and a number or a special character.
   * **Confirm Password:** Type the same password again.
   * **Role:** Select the role for the local user. If you select an S3 user role, select the relevant S3 policy and, optionally, the POSIX UID and POSIX GID**.** For role details, see #role-descriptions.
4. Select **Save**.

### Edit a local user

You can modify the role of a local user but not your role (the signed-in user). For an S3 user, you can only modify the S3 policy, POSIX UID, and POSIX GID.

**Procedure**

1. In the Local Users tab, select the three dots of the local user you want to edit, then select **Edit User**.
2. From the Role property, select the required role. If you modify the role to S3, you can set the S3 policy, POSIX UID, and POSIX GID.
3. Select **Save**.

### Change a local user password

As a Cluster Admin or Organization Admin, you can change the password of a local user and revoke the user's tokens.

Note: To regain access to the system after changing the password, the user must re-authenticate using the new password.

**Procedure**

1. In the Local Users tab, select the three dots of the local user whose password you want to change, then select **Change Password**.
2. In the Change Password for a user dialog, set the following properties:
   * **Old password:** Set the old password.
   * **Password:** Set a new password according to the requirements.
   * **Confirm Password:** Type the same new password again.
   * **Revoke Tokens:** If the user's existing tokens are compromised, you can revoke all of the user's tokens and change their password. To regain access to the system, the user must re-authenticate with the new password or obtain new tokens through the API.
3. Select **Save**.

### Change your password

You can change your password at any time.

**Procedure**

1. From the top bar, select the signed-in user, then select **Change Password**.

2. In the Change Password dialog, set the properties described in the Change a local user password topic.
3. Select **Save**.

### Revoke local user tokens

If the user's existing tokens are compromised, you can revoke all the user's tokens, regardless of changing the user's password. To re-access the system, the user re-authenticates with the new password, or the user needs to obtain new tokens using the API.

**Procedure**

1. In the Local Users tab, select the three dots of the local user you want to revoke the user tokens, then select **Revoke User Tokens**.

2. In the confirmation message, select **Revoke Tokens**.

### Remove a local user

You can remove a local user that is no longer required.

**Procedure**

1. In the Local Users tab, select the three dots of the local user to remove, then select **Remove User**.

In the confirmation message, select **Yes**.

## Configure LDAP/AD in WEKA

You can set the user access to the WEKA system from the organization user directory, either by Light Access Directory Protocol (LADP) or Active Directory (AD).

### Configure LDAP

To use LDAP for authenticating users, set the property values based on your specific LDAP environment and configuration.

<details>

<summary>LDAP property descriptions</summary>

* **Server URI:** The URI or address of the LDAP server, including the protocol (in this case, LDAP), the server's hostname or IP address, and the port number.\
  Example value: `ldap://ldap.example.com:389`
* **Protocol Version:** The version of the LDAP protocol being used. Common versions include LDAPv2 and LDAPv3.\
  Example value: `3`
* **Start TLS:** When enabled, this option initiates a Transport Layer Security (TLS) connection with the LDAP server. TLS provides encryption and secure communication between the client and server, protecting the confidentiality and integrity of data transmitted over the network.
*   **Ignore Certificate Failures:** When enabled, this option instructs the LDAP client to ignore certificate validation failures during the TLS/SSL handshake process. Certificate validation failures can include expired, self-signed, or mismatched certificates. Enabling this option allows the client to establish a connection even if the server's certificate cannot be fully validated. Use this option cautiously, as it may expose the connection to potential security risks.

    Enabling _Start TLS_ and _Ignore Certificate Failures_ must be done based on your specific security requirements and the configuration of your LDAP server.
* **Server Timeout Seconds:** The maximum amount of time, in seconds, the client waits for a response from the LDAP server before timing out.\
  Example value: `30`
* **Base DN :** The base distinguished name (DN) is the starting point for searching the directory tree. It represents the top-level entry in the LDAP directory.\
  Example Value: `dc=example,dc=com`
* **Reader Username:** The username or distinguished name (DN) of a dedicated reader user account used for authenticating and reading data from the LDAP server.\
  Example value: `cn=reader,dc=example,dc=com`
* **Reader Password:** The password is associated with the reader user account for authentication purposes.\
  Example Value: `********`
* **User ID Attribute:** The attribute in the LDAP schema that represents the unique identifier or username for user entries.\
  Example value: `uid`
* **User Object Class:** The object class or object type in the LDAP schema defines the structure and attributes of user entries.\
  Example value: `person`
* **User Revocation Attribute:** An attribute indicates a user account's revocation status, typically a boolean attribute set to true or false.\
  Example value: `isRevoked`
* **Group ID Attribute:** The attribute in the LDAP schema represents the unique identifier or name for group entries.\
  Example value: `cn`
* **Group Membership Attribute:** The attribute establishes the membership relationship between users and groups, specifying which users are members of a particular group.\
  Example value: `member`
* **Group Object Class:** The object class or object type in the LDAP schema defines the structure and attributes of group entries.\
  Example value: `groupOfNames`
* **Cluster Admin Group:** The LDAP group granted administrative privileges for managing the LDAP cluster.\
  Example value: `cn=cluster_admins,ou=groups,dc=example,dc=com`\
  `sAMAccountName: cluster_admins`
* **Organization Admin Role Group:** The LDAP group granted administrative privileges for managing specific organizations or units within the LDAP directory.\
  Example value: `cn=org_admins,ou=groups,dc=example,dc=com`\
  `sAMAccountName: org_admins`
* **Regular User Role Group:** The group in LDAP represents regular users with standard access privileges.\
  Example value: `cn=regular_users,ou=groups,dc=example,dc=com`\
  `sAMAccountName: regular_users`
* **Read-only User Role Group:** The group in LDAP represents users with read-only access privileges restricted from making modifications.\
  Example value: `cn=read_only_users,ou=groups,dc=example,dc=com`\
  `sAMAccountName: read_only_users`

**Note:** The `sAMAccountName` (user logon name) in the Cluster Admin, Organization Admin, Regular User, and Read-only User Role Groups can be up to 20 characters long.

</details>

**Procedure**

1. From the menu, select **Configure > User Management**.
2. Select the User Directory tab.
3. Select **Configure LDAP**.
4. Set all properties based on your specific LDAP environment and configuration.
5. Select **Save**.

Once the LDAP configuration is completed, the User Directory tab displays the details. You can disable the LDAP configuration, update the configuration, or reset the configuration values.

### Configure Active Directory

To use Active Directory for authenticating users, set the property values based on your specific Active Directory environment and configuration.

<details>

<summary>Active Directory property descriptions</summary>

* **Domain:** The domain name of the Active Directory environment. It represents the network boundary and provides a way to organize and manage resources, users, and groups.\
  Example value: `example.com`
* **Server URI:** The URI or address of the Active Directory server, including the protocol (in this case, LDAP) and the server's hostname or IP address.\
  Example value: `ldap://ad.example.com`
* **Reader Username:** A dedicated reader user account's username or user principal name (UPN) used for authenticating and reading data from the Active Directory.\
  Example value: `readeruser@ad.example.com`
* **Reader Password:** The password associated with the reader user account for authentication purposes.\
  Example Value: `********`
* **Cluster Admin Role Group:** The group in Active Directory granted administrative privileges for managing the cluster or server infrastructure.\
  Example value: `CN=ClusterAdmins,CN=Users,DC=example,DC=com`\
  `sAMAccountName: ClusterAdmins`
* **Organization Admin Role Group:** The group in Active Directory granted administrative privileges for managing specific organizations or units within the Active Directory environment.\
  Example value: `CN=OrgAdmins,CN=Users,DC=example,DC=com`\
  `sAMAccountName: OrgAdmins`
* **Regular User Role Group:** The group in Active Directory represents regular users with standard access privileges.\
  Example value: `CN=RegularUsers,CN=Users,DC=example,DC=com`\
  `sAMAccountName:  RegularUsers`
* **Read-only User Role Group:** The group in Active Directory represents users with read-only access privileges, restricted from making modifications.\
  Example value: `CN=ReadOnlyUsers,CN=Users,DC=example,DC=com`\
  `sAMAccountName:  ReadOnlyUsers`

**Note:** The `sAMAccountName` (user logon name) in the Cluster Admin, Organization Admin, Regular User, and Read-only User Role Groups can be up to 20 characters long.

</details>

**Procedure**

1. From the menu, select **Configure > User Management**.
2. Select the User Directory tab.
3. Select **Configure Active Directory**.
4. Set all properties based on your specific Active Directory environment and configuration.
5. Select **Save**.

Once the Active Directory configuration is completed, the User Directory tab displays the details. You can disable the Active Directory configuration, update the configuration, or reset the configuration values.

    For S3 user roles only.

    For S3 user roles only.

<!-- ============================================ -->
<!-- File 142/259: operation-guide_user-management_user-management-1.md -->
<!-- ============================================ -->

---
description:
---

# Manage users using the CLI

Using the CLI, you can:

* Create a local user
* Log-in to the WEKA cluster
* Change a local user password
* Revoke user access
* Update a local user
* Delete a local user
* Authenticate users from an LDAP user directory

## Create a local user

**Command:** `weka user add`

Use the following command line to create a local user:

`weka user add <username> <role> [password] [--posix-uid uid] [--posix-gid gid]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | username* | Name for the new user |  |
 | role | Role of the new created user.Possible values: clusteradmin, csi, orgadmin, readonly, regular, s3 |  |
 | password | New user password.If not supplied, the command prompts to supply the password. |  |
 | posix-uid | POSIX UID of underlying files representing objects created by this S3 user access/keys credentials.For S3 user roles only. | 0 |
 | posix-gid | POSIX GID of underlying files representing objects created by this S3 user access/keys credentials.For S3 user roles only. | 0 |

Note: **Example:**
`$ weka user add my_new_user regular S3cret`
This command line creates a user with a username of `my_new_user`, a password of `S3cret` and a role of a Regular user.

### Display list of users

Run the `weka user` command to display the list of users defined in WEKA.

```
$ weka user
| Username | Source | Role |
------------+----------+--------
| my_new_user | Internal | Regular |
| admin | Internal | Admin |
```

### Display current user information

Run the `weka user whoami` command to receive information about the current user running the command.

To use the new user credentials, use the`WEKA_USERNAME` and `WEKA_PASSWORD`environment variables:

```
$ WEKA_USERNAME=my_new_user WEKA_PASSWORD=S3cret weka user whoami
| Username | Source | Role |
------------+----------+--------
| my_new_user | Internal | Regular |
```

## Log-in to the WEKA cluster

**Command:** `weka user login`

Use the following command to log a user into the WEKA cluster. If login is successful, the user credentials are saved to the user's home directory.

`weka user login [username] [password] [--org org] [--path path]`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | username* | User's username |
 | password* | User's password |
 | org | Organization name or ID |
 | path | The path where the login token will be saved (default: ~/.weka/auth-token.json). This path can also be specified using the WEKA_TOKEN environment variable.After logging-in, use the WEKA_TOKEN environment variable to specify where the login token is located. |

Note: **Manage authentication tokens in WEKA**
The `--path` parameter is used to control the directory and file where the authentication token is written. The specified path, which includes the filename, can then be assigned to the `WEKA_TOKEN` environment variable.
**Example 1: Using the `--path` parameter**
The following example demonstrates how to log in and specify the path for the authentication token. After logging in, the path is set to the `WEKA_TOKEN` environment variable.
```sh
weka user login user1 password1 --path /home/user1/.weka/user1-token.json
export WEKA_TOKEN=/home/user1/.weka/user1-token.json
```
**Example 2: Using the `WEKA_TOKEN` environment variable**
Alternatively, you can set the `WEKA_TOKEN` environment variable first, which removes the need to use the `--path` parameter during the login process.
```sh
export WEKA_TOKEN=/home/user1/.weka/user1-token.json
weka user login user1 password1
```

**Related topic**

## Change a local user password

**Command:** `weka user passwd`

Use the following command to change a local user password:

`weka user passwd <password> [--username username]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | password* | New password |  |
 | username | Name of the user to change the password for.It must be a valid local user. | The current logged-in user |

Note: * If necessary, provide or set`WEKA_USERNAME` or `WEKA_PASSWORD.`
* To regain access to the system after changing the password, the user must re-authenticate using the new password.

## Revoke user access

**Command:** `weka user revoke-tokens`

Use the following command to revoke internal user access to the system and mounting filesystems:

`weka user revoke-tokens <username>`

You can revoke the access for LDAP users by changing the `user-revocation-attribute` defined in the LDAP server configuration.

**Parameters**

 | Name | Value |
 | --- | --- |
 | username* | A valid user in the organization of the Organization Admin running the command. |

Note: NFS and SMB are different protocols from WekaFS, which require additional security considerations when used. For example, The system grants NFS permissions per server. Therefore, manage the permissions for accessing these servers for NFS export carefully.

## Update a local user

**Command:** `weka user update`

Use the following command line to update a local user:

`weka user update <username> [--role role] [--posix-uid uid] [--posix-gid gid]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | username* | Name of an existing user.It must be a valid local user. |
 | role | Updated user role.Possible values: regular, s3,readonly, orgadmin or clusteradmin |
 | posix-uid | POSIX UID of underlying files representing objects created by this S3 user access/keys credentials.For S3 user roles only. |
 | posix-gid | POSIX GID of underlying files representing objects created by this S3 user access/keys credentials.For S3 user roles only. |

## Delete a local user

**Command:** `weka user delete`

To delete a user, use the following command line:

`weka user delete <username>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | username* | Name of the user to delete.It must be a valid local user. |

Note: **Example:**
`$ weka user add my_new_user`
Then run the`weka user` command to verify that the user was deleted:
```
$ weka user
| Username | Source | Role |
---------+----------+------
| admin | Internal | Admin |
```

## Authenticate users from an LDAP user directory

To authenticate users from an LDAP user directory, the LDAP directory must first be configured to the Weka system. This is performed as follows.

### Configure an LDAP user directory

**Command:**\
`weka user ldap setup`\
`weka user ldap setup-ad`

One of two CLI commands is used to configure an LDAP user directory for user authentication. The first is for configuring a general LDAP server and the second is for configuring an Active Directory server.

To configure an LDAP server, use the following command line:

`weka user ldap setup <server-uri> <base-dn> <user-object-class> <user-id-attribute> <group-object-class> <group-membership-attribute> <group-id-attribute> <reader-username> <reader-password> <cluster-admin-group> <org-admin-group> <regular-group> <readonly-group> [--start-tls start-tls] [--ignore-start-tls-failure ignore-start-tls-failure] [--server-timeout-secs server-timeout-secs] [--protocol-version protocol-version] [--user-revocation-attribute user-revocation-attribute]`

To configure an Active Directory server, use the following command line:

`weka user ldap setup-ad <server-uri> <domain> <reader-username> <reader-password> <cluster-admin-group> <org-admin-group> <regular-group> <readonly-group> [--start-tls start-tls] [--ignore-start-tls-failure ignore-start-tls-failure] [--server-timeout-secs server-timeout-secs] [--user-revocation-attribute user-revocation-attribute]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | server-uri* | Either the LDAP server hostname/IP or a URI.Format: ldap://hostname:port or ldaps://hostname:port |  |
 | base-dn* | Base DN under which users are stored.It must be a valid name. |  |
 | user-id-attribute* | Attribute storing user IDs.It must be a valid name. |  |
 | user-object-class* | Object class of users.It must be a valid name. |  |
 | group-object-class* | Object class of groups.It must be a valid name. |  |
 | group-membership-attribute* | Attribute of group containing the DN of a user membership in the group.It must be a valid name. |  |
 | group-id-attribute* | Attribute storing the group name.The name must match the names used in the <admin-group>, <regular group> and <readonly group> |  |
 | reader-username and reader-password* | Credentials of a user with read access to the directory.The password is kept in the Weka cluster configuration in plain text, as it is used to authenticate against the directory during user authentication. |  |
 | cluster-admin-group* | Name of group containing users defined with cluster admin role.It must be a valid name. |  |
 | org-admin-group* | Name of group containing users defined with organization admin role.It must be a valid name. |  |
 | regular-group* | Name of group containing users defined with regular privileges.It must be a valid name. |  |
 | readonly-group* | Name of group containing users defined with read only privileges.It must be a valid name. |  |
 | server-timeout-secs | Server connection timeout in seconds. |  |
 | protocol-version | Selection of LDAP version.Possible values: LDAP v2 or LDAP v3 | LDAP v3 |
 | user-revocation-attribute | The LDAP attribute; when its value changes in the LDAP directory, user access and mount tokens are revoked.The user must re-login after a change is detected. |  |
 | start-tls | Issue StartTLS after connecting.Possible values: yes or noDo not use with ldaps:// | no |
 | ignore-start-tls-failure | Ignore start TLS failure.Possible values: yes or no | no |

Note: The `sAMAccountName` (user logon name) in the Cluster Admin, Organization Admin, Regular User, and Read-only User Role Groups can be up to 20 characters long.

### View a configured LDAP User Directory

**Command:**\
`weka user ldap`

This command is used for viewing the current LDAP configuration used for authenticating users.

### Disable or enable a configured LDAP user directory

**Command:**\
`weka user ldap disable`\
`weka user ldap enable`

These commands are used for disabling or enabling user authentication through a configured LDAP user directory.

Note: You can only disable an LDAP configuration, but not delete it.

<!-- ============================================ -->
<!-- File 143/259: operation-guide_organizations.md -->
<!-- ============================================ -->

---
description:
---

# Organizations management

## Overview

Organizations enable separation of duties between user groups within the same WEKA system. Each organization is isolated from others. Users in one organization cannot access or manage data from another.

* Up to 256 organizations can be created.
* Each organization is managed by an Organization Admin.
* A Cluster Admin oversees the overall system but cannot access organization-specific data.

## Cluster Admin responsibilities

Cluster Admins manage the system-wide configuration and can:

* Create and delete organizations.
* Assign an Organization Admin to each organization.
* Monitor total capacity used by each organization.

Although Cluster Admins have backend access (for example, root on servers), they cannot access user data within organizations. They may still view events across all organizations.

Note: * **QoS between organizations**: Data is not physically separated at the hardware level. While the system balances IO fairly, there is no QoS guarantee between organizations. One organization‚Äôs activity can affect cluster-wide performance.
* **Mount configuration:** Mounts can be configured with maximum and preferred throughput settings. For more information, see [Set mount option default values](../../weka-filesystems-and-object-stores/mounting-filesystems#set-mount-option-default-values).

## Organization use cases

### Private cloud multi-tenancy

Organizations can be used to logically separate departments (for example, IT, Finance, Genomics). While setup may require extra configuration, such as per-organization LDAP, the underlying cluster infrastructure remains shared and trusted.

### Logical separation of external groups

For environments with multiple independent user groups, organizations provide stronger data isolation and management boundaries.

## System entity management

### Cluster-level entities

Managed by the Cluster Admin:

* Hardware
* NFS service (including NFS groups and IP interfaces)
* SMB service
* S3 service
* Filesystem groups (used by Organization Admins when creating filesystems)
* Encryption settings (KMS)
* User management for the root organization

Note: Protocol services (NFS, SMB, S3) are only available in the root organization. Filesystems cannot be moved between organizations, including into or out of the root organization.

### Organization-level entities

Managed exclusively by the Organization Admin:

* Filesystems (including encryption)
* Object store buckets
* LDAP server configuration
* NFS exports and client permissions
* User management for their specific organization

Note: In an organization, only authenticated users with the Regular or Organization Admin role can mount the filesystems.

## Manage organizations

Only Cluster Admins can create or delete organizations. If no organizations are configured, the root organization is used by default, and mounts do not require authentication.

After creating an organization, users must specify the organization name when logging in, using the `--org` flag in the `weka user login` command.

## Usage and quota management

Cluster Admins can:

* Monitor per-organization SSD and total usage.
* Set quotas to limit usage by capacity type.

This supports chargeback models based on actual or allocated storage usage.

## Organization admin privileges

When an organization is created, the Cluster Admin assigns an Organization Admin who manages the organization-level resources.

Organization Admins can:

* Create, delete, and manage users
* Set user roles and change passwords
* Manage the organization‚Äôs LDAP configuration

#### Restrictions

To ensure Organization Admins do not lose access:

* They cannot delete their own user account.
* They cannot change their own role.

<!-- ============================================ -->
<!-- File 144/259: operation-guide_organizations_organizations.md -->
<!-- ============================================ -->

---
description: Explore how to manage organizations using the GUI.
---

# Manage organizations using the GUI

Using the GUI, you can:

* Create an organization
* View organizations
* Edit an organization
* Delete an organization

## Create an organization

Only a Cluster Admin can create an organization.

**Procedure**

1. From the menu, select **Configure > Organizations**.
2. On the Organizations page, select **+Create**.
3. In the Create Organization dialog, set the following properties:
   * **Organization Name:** A name for the organization.
   * **Org. Admin Username**: The user with an Organization Admin role created for the organization.
   * **Org. Admin Password**: The password of the user with an Organization Admin role created for the organization.
   * **Confirm Password**: The same password as set in the Org. Admin Password.
   * **Set Organization SSD Quota**: Turn on the switch and set the SSD capacity limitation for the organization.
   * **Set Organization Total Quota**: Turn on the switch and set the total capacity limitation for the organization (SSD and object store bucket).
4. Select **Save**.

## View organizations

As a Cluster Admin, you can view all organizations in the cluster.

As an Organization Admin, you can view only the organization you are assigned to.

**Procedure**

1. From the menu, select **Configure > Organizations**.

## Edit an organization

You can modify an organization's SSD and total quota to meet the capacity demand changes.

**Procedure**

1. From the menu, select **Configure > Organizations**.
2. On the Organizations tab, select the three dots of the organization to edit and select **Edit**.

3\. In the Edit Organization dialog, set the following properties:

* **Set Organization SSD Quota**: Turn on the switch and set the SSD capacity limitation for the organization.
* **Set Organization Total Quota**: Turn on the switch and set the total capacity limitation for the organization (SSD and object store bucket).

4\. Select **Save**.

## Delete an organization

If an organization is no longer required, you can remove it. You cannot remove the root organization.

Note: Deleting an organization is irreversibl&#x65;**.** It removes all entities related to the organization, such as filesystems, object stores, and users.

**Procedure**

1. From the menu, select **Configure > Organizations**.
2. On the Organizations tab, select the three dots of the organization to edit and select **Remove**.

3\. In the confirmation message, select **Yes**.

<!-- ============================================ -->
<!-- File 145/259: operation-guide_organizations_organizations-1.md -->
<!-- ============================================ -->

---
description: Explore how to manage organizations using the CLI.
---

# Manage organizations using the CLI

Using the CLI, you can:

* Add an organization
* View organizations
* Rename an organization
* Update the quota of an organization
* Remove an organization

## Add an organization

**Command:** `weka org add`

Use the following command line to create an organization:

`weka org add <name> <username> <password> [--ssd-quota ssd-quota] [--total-quota total-quota]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | Name | Value | Default |
 | name* | Valid organization name. |  |
 | username* | Valid username of the created Organization Admin. |  |
 | password* | Password of the created Organization Admin. |  |
 | ssd-quota | Allowed quota out of the system SSDs to be used by the organization. | 0 (not limited) |
 | total-quota | Total allowed quota for the organization (SSD and object store). | 0 (not limited) |

## View organizations

**Command:** `weka org`

```
# weka org

| ID | Name | Allocated SSD | SSD Quota | Allocated Total | Total Quota |
---+------------+---------------+-----------+-----------------+-------------
| 0 | Root | 0 B | 0 B | 0 B | 0 B |
| 1 | Local IT | 500.00 GB | 500.00 GB | 500.00 GB | 0 B |
| 2 | CUSTOMER_1 | 100.00 GB | 300.00 GB | 200.00 GB | 900.00 GB |
```

## **Rename an organization**

**Command:** `weka org rename`

Use the following command line to rename an organization:

`weka org rename <org> <new-name>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | org* | Current organization name or ID. |
 | new-name* | New organization name. |

## Update the quota of an organization

**Command:** `weka org set-quota`

Use the following command line to update an organization's quota:

`weka org set-quota <org> [--ssd-quota ssd-quota] [--total-quota total-quota]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | Name | Value |
 | org* | Organization name or ID.The root organization (org ID = 0 cannot be limited) |
 | ssd-quota | Allowed quota out of the system SSDs to be used by the organization |
 | total-quota | Total allowed quota for the organization (SSD and object store) |

## Remove an organization

**Command:** `weka org remove`

Use the following command line to remove an organization:

`weka org remove <org>`

Note: Removing an organization is irreversible. It removes all entities related to the organization, such as filesystems, object stores, and users.

**Parameters**

 | Name | Value |
 | --- | --- |
 | org* | Organization name or ID. |

**Related topics**

<!-- ============================================ -->
<!-- File 146/259: operation-guide_organizations_organizations-2.md -->
<!-- ============================================ -->

---
description:
---

# Mount authentication for organization filesystems

Once the Cluster Admin has created an organization and the Organization Admin has created filesystems, users, or configured the LDAP for the organization, regular organization users can mount filesystems.

The purpose of organizations is to provide separation and security for organization data, which requires authentication of the WEKA system filesystem mounts. This authentication of mounts prevents users of other organizations and even the Cluster Admin from accessing organization filesystems.

Mounting filesystems in an organization (other than the Root organization) is only supported using a stateless client. A login prompt appears as part of the mount command if the user is not logged into the system.

## Mount a filesystem

To securely mount a filesystem, first log into the WEKA system:

```
weka user login my_user my_password --org my_org -H backend-host-0
```

Then mount the filesystem:

```
mount -t wekafs backend-host-0/my_fs /mnt/weka/my_fs
```

For all mount options, see [Mount Command Options](../../../weka-filesystems-and-object-stores/mounting-filesystems#mount-command-options).

## Authentication‚Äå

Authentication is achieved by obtaining a mount token and including it in the mount command. Logging into the WEKA system using the CLI (the `weka user login` command) creates an authentication token and saves it in the client (default to `~/.weka/auth-token.json,` which can be changed using the`--path`attribute).

The WEKA system assigns the token that relates to a specific organization. Only mounts that pass the path to a correct token can successfully access the filesystems of the organization.

Once the system authenticates a user, the mount command uses the default location of the authentication token. It is possible to change the token location/name and pass it as a parameter in the mount command using the `auth_token_path` mount option, or the`WEKA_TOKEN` environment variable.

```
mount -t wekafs backend-host-0/my_fs /mnt/weka/my_fs -o auth_token_path=<path>
```

This option is useful when mounting several filesystems for several users/organizations on the same server or when using Autofs.

When a token is compromised or no longer required, such as when a user leaves the organization, the Organization Admin can prevent using that token for new mounts by revoking the user access.

**Related topics**

<!-- ============================================ -->
<!-- File 147/259: operation-guide_events.md -->
<!-- ============================================ -->

---
description:
---

# Events

WEKA events provide essential information about the WEKA cluster and customer environment. Cluster and customer operations and changes in the background can trigger these timestamped events. They may convey information, signal a system issue, or report a previously resolved problem. The system stores a maximum of 300,000 notifications in the events folder.

The WEKA cluster sends all events to a predefined central monitoring system, WEKA Home or Local WEKA Home. The GUI displays the events retrieved from the central monitoring system.

### Events triggered by alerts

In the monitoring and alerting system, alerts are closely tied to specific events, providing valuable insights into problematic states' occurrence and underlying causes. The events associated with alerts encompass a comprehensive log, including triggered, cleared, and continuous alerts.

The following processes ensure systematic and efficient handling of events triggered by alerts:

* Alerts that are active for at least 1 minute generate an `AlertTriggered` event. Once they are inactive for at least 1 minute, an `AlertCleared` event is generated with details, including the alert type, title, and description.
* Every 30 minutes, all active alerts are logged as part of an `AlertContinuousEvent` with details, including the alert type and the entry count.
* These events are internal with severity level DEBUG and category ALERTS.

**Related topics**

Broken link

<!-- ============================================ -->
<!-- File 148/259: operation-guide_events_events.md -->
<!-- ============================================ -->

---
description: This page describes how to manage events using the GUI.
---

# Manage events using the GUI

With the GUI, you can:

* View events
* Filter events

## View events

The events enable you to investigate issues that occur in the system.

The System Events page provides the following details:

* **Severity**: The severity of the event. The options are Info (lowest), Warning, Minor, Major, and Critical (highest).
* **Timestamp**: The date and time the event occurred. You can switch the display time between local and system time through the top bar.
* **Process ID:** The process ID created the event.
* **Origin**: The event's originator, such as a user, backend, or cluster. For example, when a user creates a filesystem, the username appears as the event's originator.
* **Category**: The category options include Alerts, Cloud, Clustering, Config, Custom, Drive, Events, Filesystem, InterfaceGroup, Kms, Licensing, NFS, Network, Node, ObjectStorage, Org, Raid, Resources, S3, Security, Smb, System, Traces, Upgrade, and User.
* **Name**: The name of the event.
* **Description**: The description of the event.

You can select the **Advanced** switch to display internal events. This option is helpful for experts investigating internal issues.

 **Procedure**

1. From the menu, select **Investigate > Events**.

## Filter events

You can filter the events according to the event severity, timestamp, category, or event name. You can also filter events by multiple categories and multiple event names.

#### Procedures

<details>

<summary>Display events of a specific minimum severity</summary>

1. Select the filter icon of the **Severity** column.
2. Select the required minimum severity.\
   For example, the Critical events are displayed if you select the Major severity.

</details>

<details>

<summary>Display events of a specific period</summary>

1. Select the filter icon of the **Timestamp** column.
2. In the **From** field, select the timestamp of the beginning of the period to display.
3. In the **To** field, select the timestamp of the end of the period to display or select **Now**.
4. Select **OK**.
5. Select **Filter**.

</details>

<details>

<summary>Display events of specific categories</summary>

1. Select the filter icon of the **Category** column.
2. In the **Filter Categories**, select the category you want to display. You can select multiple categories.
3. Select **Filter**.

</details>

<details>

<summary>Display events with specific event names</summary>

1. Select the filter icon of the **Event** column.
2. In the **Events Filter**, select the event name you want to display. You can select multiple event names.
3. Select **Filter**.

</details>

## Display events by a predefined template

You can display events according to predefined templates based on a combination of event names with the same logical context. For example, selecting the **Processes** template displays all events related to processes. A predefined template enables focusing on certain areas of the system.

The predefined templates include protocols, object store, cluster-wide tasks, filesystems, quota, snapshots, clients, and processes.

**Procedure**

1. In the Events page, select **Predefined Templates.**
2. Select from the list the required template to display.

**Related topic**

#switch-the-display-time

<!-- ============================================ -->
<!-- File 149/259: operation-guide_events_events-1.md -->
<!-- ============================================ -->

---
description: This page describes how to manage events using the CLI.
---

# Manage events using the CLI

With the CLI, you can:

* View events
* View events of a specific container
* Trigger a custom event

## View events

**Command:** `weka events`

Use the following command line to list events in the Weka cluster:

`weka events [--num-results num-results] [--start-time <start-time>] [--end-time <end-time>] [--severity severity] [--direction direction] [--fetch-order fetch-order] [--type-list type-list] [--exclude-type-list exclude-type-list] [--category-list category-list] [--cloud-time] [--show-internal] [--raw-units] [--UTC]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | num-results | Maximum number of events to display.Positive integer. 0 shows all events. | 50 |
 | start-time | Include events that occurred at this start time and later.Format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 9:15Z, 10:00+2:00. | -365 days |
 | end-time | Include events that occurred up to this time.Format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 9:15Z, 10:00+2:00. | Set to a time represents 'now' |
 | severity | Include events with this level of severity and higher.Possible values: info, warning, minor, major, critical. | info |
 | direction | Sort events by ascending or descending time.Possible values: asc, dsc. | asc |
 | fetch-order | Fetch from end-time and backward or start-time and forward.Possible values: fw, bw | bw |
 | type-list | Filter events by type (can be used multiple times).Use weka events list-types to see available types. | None |
 | exclude-type-list | Filter-out events by type (can be used multiple times).Use weka events list-types to see available types. |  |
 | category-list | Include only events matching the defined category.Possible values: Alerts, Cloud, Clustering, Config, Custom, Drive, Events, Filesystem, InterfaceGroup, Kms, Licensing, NFS, Network, Node, ObjectStorage, Org, Raid, Resources, S3, Security, Smb, System, Traces, Upgrade, User. | All |
 | cloud-time | Query and sort results by the digested time in the cloud | False |
 | show-internal | Also displays internal events | False |
 | raw-units | Print values in raw units such as bytes and seconds. | Readable format. Example: 1KiB 234MiB 2GiB |
 | UTC | Print times in UTC | Host's local time |

## View events of a specific container

**Command:** `weka events list-local`

Use the following command line to list recent events on the specific container running the command from.

This command is helpful for the following cases:

* No connectivity to the central monitoring site
* No connectivity from a specific container
* Containers that are not part of the cluster

`weka events list-local [--start-time <start-time>] [--end-time <end-time>] [--next next] [--stem-mode] [--show-internal] [--raw-units] [--UTC]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | start-time | Include events that occurred at this start time and later.Format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 9:15Z, 10:00+2:00. | -365 days |
 | end-time | Include events that occurred up to this time.Format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 9:15Z, 10:00+2:00. | Set to a time represents 'now' |
 | next | Identifier to the next page of events.As returned in the previous call to weka events list-local. |  |
 | stem-mode | Displays events when the container has not been attached to the cluster | False |
 | show-internal | Also displays internal events | False |
 | raw-units | Print values in raw units, such as bytes and seconds. | Readable format. Examples: 1KiB 234MiB 2GiB. |
 | UTC | Print times in UTC. | Server's local time. |

## Trigger a custom event

**Command:** `weka events trigger-event`

It can be useful to mark specific activities, maintenance work, or important changes/new usage of the system, and see that as part of the system events timeline.

To trigger a custom event, use `weka events trigger-event <text>`

<!-- ============================================ -->
<!-- File 150/259: operation-guide_events_events-list.md -->
<!-- ============================================ -->

---
description:
---

# Events list

### Agent

 | **Type** | **Severity** | **Description** |
 | --------------------------------- | ------------ | --------------------------------------- |
 | ContainerStateEvent | INFO | Container state |
 | WCGroupInvalidResourceConfigEvent | WARNING | Container CGroup resource Configuration |
 | WCGroupStateDisabledEvent | MAJOR | Container CGroup state |
 | WCGroupStateEnabledEvent | INFO | Container CGroup state |
 | WCGroupValidResourceConfigEvent | INFO | Container CGroup resource configuration |

### Alerts

 | **Type** | **Severity** | **Description** |
 | -------------------- | ------------ | ----------------------------------------- |
 | AlertCleared | DEBUG | The system has {action} an alert |
 | AlertContinuousEvent | DEBUG | The system has reported continuous alerts |
 | AlertMuted | INFO | Alert muted |
 | AlertTriggered | DEBUG | The system has {action} an alert |
 | AlertUnmuted | INFO | Alert unmuted |

### Cloud

 | **Type** | **Severity** | **Description** |
 | ----------------------------------- | ------------ | --------------------------------------------------- |
 | ClientsReportingViaBackendsDisabled | DEBUG | Clients reporting to cloud via backends disabled |
 | ClientsReportingViaBackendsEnabled | DEBUG | Clients reporting to cloud via backends enabled |
 | CloudDisabled | INFO | Cloud disabled |
 | CloudEnabled | INFO | Cloud enabled |
 | CloudProxyUpdated | INFO | Cloud proxy updated |
 | CloudSetUploadRate | INFO | Cloud upload rate changed |
 | CloudStatsErrorClearedEvent | WARNING | Cloud stats have been written successfully |
 | CloudStatsErrorEvent | WARNING | Error writing cloud stats for upload |
 | DiagsUploaded | INFO | Diagnostics are uploaded |
 | LowDiskSpaceClearedEvent | WARNING | The host no longer has low disk space |
 | LowDiskSpaceEvent | WARNING | The host has low disk space |
 | WekaHomeProxyUploaderStartFailed | DEBUG | Failed to start the Weka Home Proxy Uploader Server |

### Clustering

 | **Type** | **Severity** | **Description** |
 | ----------------------------------- | ------------ | --------------------------------------------------------------------------------------------- |
 | AllBucketsResponsive | INFO | All compute resources are responding |
 | BucketRedist | INFO | The buckets were redistributed in the cluster |
 | ClientConnected | INFO | Client connected |
 | ClientDisconnected | INFO | Client disconnected |
 | ClientRemoved | INFO | Start removing a disconnected client from the cluster |
 | ClientsUnavailable | CRITICAL | Some clients are unavailable because too many backends are down |
 | ClockSkewedHostJoin | MINOR | The container (host) cannot join because of a clock skew |
 | ClusteringFailure | MINOR | Container clustering failed |
 | ClusterInitializationFailed | MAJOR | The cluster initialization has failed |
 | ClusterInitialized | INFO | The cluster is successfully initialized |
 | ConfigChangeSetsSliderFull | MINOR | Configuration changeset slider is full while the process (node) pulls the configuration |
 | ConfigGenerationHasNoFirstChunk | MINOR | Applying a partial configuration generation is prohibited |
 | ConfigSnapshotPulled | MINOR | Configuration snapshot is pulled. |
 | DoubleUnmatchingMachineIdentifier | MAJOR | There is a container with the same Agent-Machine-ID, but different SMBIOS UID |
 | GrimReaperFencingNode | MINOR | A partially connected process (node) is selected to be fenced by the grim reaper |
 | HostActivated | INFO | Host configuration change |
 | HostAdded | INFO | Host configuration change |
 | HostAdding | INFO | Host configuration change |
 | HostDeactivated | INFO | Host configuration change |
 | HostDeactivating | INFO | Host configuration change |
 | HostDrained | INFO | Host configuration change |
 | HostDraining | INFO | Host configuration change |
 | HostRemoved | INFO | Host configuration change |
 | HostRemoving | INFO | Host configuration change |
 | HostRemovingFailed | INFO | Host configuration change |
 | LeaderChanged | WARNING | The cluster leader has changed |
 | LeaderSteppingUpAfterUpgrade | INFO | The cluster leader is stepping up after upgrade |
 | NodeNetworkUnstable | MAJOR | A process (node) with an unstable network is detected |
 | NodePartiallyConnected | MINOR | A partially connected process (node) was removed |
 | NodeRejoined | INFO | The process (node) has rejoined the cluster |
 | NodesNotInExpectedState | MAJOR | Some processes (nodes) are not in the expected state |
 | NodeUnreachable | MINOR | An unreachable process (node) was removed |
 | OperationTookTooLong | WARNING | The operation took too long |
 | PersistentUnresponsiveBuckets | CRITICAL | Some compute resources are not responding for more than {longUnresponsivenessMinutes} minutes |
 | PreviousCluster | INFO | This host was part of another cluster before |
 | RejoinFailureReport | MINOR | Containers (nodes) failed to rejoin |
 | UnresponsiveBuckets | MAJOR | Some compute resources are not responding |
 | WrongConfigSignatureForRaftSnapshot | MINOR | Tried to load a RAFT snapshot with an unsupported configuration root snapshot signature |
 | WrongSchemaVersionForRaftSnapshot | MINOR | Tried to load a RAFT snapshot with an unsupported schema version |

### Config

 | **Type** | **Severity** | **Description** |
 | ------------------------------------------------------------ | ------------ | ---------------------------------------------------------------------- |
 | BlockTaskStateChanged | DEBUG | Block task state changed |
 | CachedSnapshotIsNewerThanStreamed | INFO | The streamed snapshot is older than the cached snapshot |
 | CachedSnapshotIsOlderThanStreamed | INFO | Requested snapshot doesn't exist yet |
 | CannotAddAnotherContainerWithTheSameMachineId | MAJOR | Too many containers with the same machine ID |
 | CannotAddAnotherFeContainerWithTheSameMachineId | MAJOR | Too many FE containers with the same machine ID |
 | ClientTargetVersionChange | INFO | Client target version has been set |
 | ConfigAddedKeyManually | INFO | A configuration value is added manually by the cluster administrator |
 | ConfigCapabilityFormatChanged!"max_supported_test_format" | INFO | Cluster capability max_supported_test_format has been updated |
 | ConfigOverridden | INFO | A configuration value is overridden by the cluster administrator |
 | ConfigOverrideChanged | INFO | A config override is changed |
 | ConfigOverrideDiscarded | WARNING | A config override was disabled automatically |
 | ConfigPropagationTookTooLong | MAJOR | Config propagation took too long |
 | ConfigRemovedKeyManually | INFO | A configuration value is removed manually by the cluster administrator |
 | ContainerBlacklistToggle | MAJOR | A container is added or removed to/from the denylist |
 | DirectoryQuotasDisabled | INFO | Directory quotas were disabled |
 | DirectoryQuotasEnabled | INFO | Directory Quotas were enabled |
 | HostRequestedActionChange | INFO | Host requested action changed |
 | IOStatusChanged | INFO | IO status changed |
 | LeaderIterationTooSlow | MAJOR | Leader iteration took too long between iterations |
 | LoginBannerCleared | INFO | Login banner has been cleared |
 | LoginBannerDisabled | INFO | Login banner disabled |
 | LoginBannerEnabled | INFO | Login banner enabled |
 | LoginBannerSet | INFO | Login banner has been set |
 | ProcessBlacklistToggle | MAJOR | A process is added or removed to/from the denylist |
 | ProcessLimitExpanded | INFO | Process limit expanded to allow 24 bits of node ID |
 | TooManyContainersWithSameMachineId | MAJOR | Too many containers with the same machine ID |
 | UpgradeBlockTaskStartInvoked | DEBUG | Block task upgrade task start invoked |
 | WrongVersionForRaftSnapshot | MINOR | Tried to load a RAFT snapshot with an unsupported version |

### Custom

 | **Type** | **Severity** | **Description** |
 | ----------- | ------------ | ------------------ |
 | Custom | INFO | Custom event |
 | CustomMajor | MAJOR | Custom major event |

### Drive

 | **Type** | **Severity** | **Description** |
 | ---------------------------- | ------------ | ----------------------------------------------------------------------------------------------- |
 | AioVecMaxLeakedExceded | DEBUG | AioVec {leaked} netbuf leaked |
 | CorruptedDrive | MAJOR | A drive has a valid header but is corrupted |
 | DriveAdded | INFO | Drive provisioned |
 | DriveCorrupted | MAJOR | A drive has a valid header but is corrupted |
 | DriveDeactivated | INFO | Drive deactivated |
 | DriveDead | MAJOR | The drive is unresponsive and fails to return IOs for an extended period |
 | DriveExcessiveErrors | WARNING | The drive has an excessive error rate and will be phased out. Contact the Customer Success Team |
 | DriveFormatUpgraded | INFO | The drive format was upgraded |
 | DriveImmediateShutdown | MAJOR | The drive had to be shut down immediately. Contact the Customer Success Team |
 | DriveInitFailed | MAJOR | A drive failed to initialize |
 | DriveIoError | MAJOR | A drive has an IO error |
 | DriveIoErrorBMS | MAJOR | A drive found an IO error in a background media scan |
 | DriveLimitExceeded | WARNING | An attempt was made to add more drives than were supported |
 | DriveMediumError | MINOR | A drive has a medium error |
 | DriveNotUnderIOMMU | MAJOR | The drive is not under IOMMU, but the host IOMMU is enabled. Contact the Customer Success Team |
 | DriveNvmeErrorLog | WARNING | An NVMe drive error log entry |
 | DriveNvmeSmartChange | MINOR | The NVMe drive SMART status has changed |
 | DriveOutOfNvkvChunks | MAJOR | A drive is out of NVKV chunks |
 | DriveRemoved | INFO | Drive removed |
 | DriveSignatureUnknown | MINOR | A drive has an unknown signature |
 | DriveSmartCriticalWarning | MINOR | The drive SMART reports a critical warning and fails immediately |
 | DriveStateChangesReport | MINOR | Drive state changes |
 | DriveStuckIOs | MAJOR | The IOs are stuck for an extended period |
 | DriveUnresponsive | MAJOR | The drive is unresponsive and fails to return IOs for an extended period |
 | DriveWrongFailureDomain | MINOR | A drive is attached to a container (host) from an incorrect failure domain |
 | MBufPoison | MAJOR | MBUf got poison error |
 | NvmeBindTimingOut | MAJOR | The NVMe device binding is stuck, and the server needs a power cycle to recover |
 | SpdkCuseFilterBlockedCommand | WARNING | SPDK CUSE driver blocked command |

### Events

 | **Type** | **Severity** | **Description** |
 | --------------------- | ------------ | ----------------------------------------------------------------------------------------- |
 | DedupEventsDiscarded | WARNING | Deduplicated events discarded. |
 | EventsDedupReport | INFO | Event deduplication ended. |
 | EventsDiscarded | MINOR | Too many events were generated in a short period, so some of them were discarded and lost |
 | Example | INFO | Example |
 | ExampleAggregated | INFO | Example aggregated |
 | ExampleDebug | DEBUG | Example debug |
 | TracesDumperDownEvent | MAJOR | The Traces Dumper is inactive |

### Filesystem

 | **Type** | **Severity** | **Description** |
 | ------------------------------------------ | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | BlockReadFailure | CRITICAL | Failed to read a block |
 | BlockSeekFinished | MAJOR | Block seek finished |
 | BlockSeekStarted | MAJOR | Block seek started for a secondary metadata block that could not be read |
 | BrokenFile | MAJOR | File metadata corruption |
 | CacheFlushHanging | MAJOR | Host is hanging while trying to sync a file's write cache to the cluster |
 | ChecksumErrorInBackgroundWrite | MAJOR | Checksum error detected by SSD node in a committing block |
 | ChecksumErrorInCommit | MAJOR | Checksum error detected by SSD node in a committing block |
 | ChecksumErrorInWrite | CRITICAL | Checksum error detected by COMPUTE node in a write |
 | ClusterwideJobAborting | INFO | Clusterwide Job configuration change |
 | ClusterwideJobAdded | INFO | Clusterwide Job configuration change |
 | ClusterwideJobRemoved | INFO | Clusterwide Job configuration change |
 | CWTaskTemplateFinished | INFO | A cluster-wide task (CWTask) template finished |
 | DefaultDirectoryQuotaSet | INFO | A default directory quota was set |
 | DefaultDirectoryQuotaUnset | INFO | A default directory quota was unset |
 | DestageBlocked | CRITICAL | Destage of a bucket cannot start |
 | DestageHanging | CRITICAL | Destage of a bucket is hanging |
 | DirectoryQuotaSet | INFO | The directory quota was set |
 | DirectoryQuotaUnset | INFO | The directory quota was unset |
 | DumpSnapHashCompleted | INFO | Finished a snap hash manifest scan |
 | FailedToAddClusterwideJob | MAJOR | Failed to add clusterwide job because the queue has reached maximum limit |
 | FailedToSplitSliceNoRetry | CRITICAL | Failed to split a directory slice. Will not retry |
 | FilesystemAdded | INFO | Filesystem configuration change |
 | FilesystemDeleted | INFO | Filesystem configuration change |
 | FilesystemDownloadStarted | INFO | Filesystem download started |
 | FilesystemGroupAdded | INFO | Filesystem group configuration change |
 | FilesystemGroupDeleted | INFO | Filesystem group configuration change |
 | FilesystemGroupUpdated | INFO | Filesystem group configuration change |
 | FilesystemRemoved | INFO | Filesystem configuration change |
 | FilesystemUpdated | INFO | Filesystem configuration change |
 | FlockSyncHanging | CRITICAL | A bucket cannot finish flock resync with all the clients |
 | ForcedBucketStepdown | MINOR | Bucket forced to step down |
 | FreeBlockStillUsed | CRITICAL | Found a block falsely marked as free |
 | FsCapacityLimitReached | WARNING | Filesystem capacity limit has been reached |
 | HangingBackendIosDetected | CRITICAL | Some IOs are hanging |
 | HangingBackendIosNoLongerDetected | INFO | IOs are no longer hanging |
 | HangingBucketStepDown | WARNING | Bucket step-down is hanging |
 | HangingDirectorySplit | MAJOR | Directory split hasn't made any progress for a long time |
 | HangingDriverFrontendIosDetected | CRITICAL | Some IOs are hanging |
 | HangingDriverFrontendIosNoLongerDetected | INFO | IOs are no longer hanging |
 | HangingNFSFrontendIosDetected | CRITICAL | Some IOs are hanging |
 | HangingNFSFrontendIosNoLongerDetected | INFO | IOs are no longer hanging |
 | IntegrityCheckFinished | DEBUG | Integrity check has finished. |
 | IntegrityCheckIssueCritical | CRITICAL | Found a data integrity issue |
 | IntegrityCheckIssueCriticalNoDedup | CRITICAL | Found a data integrity issue |
 | IntegrityCheckIssueDebug | DEBUG | Found a data integrity issue |
 | IntegrityCheckIssueDebugNoDedup | DEBUG | Found a data integrity issue |
 | IntegrityCheckIssueMajor | MAJOR | Found a data integrity issue |
 | IntegrityCheckIssueMajorNoDedup | MAJOR | Found a data integrity issue |
 | IntegrityCheckIssueMinor | MINOR | Found a data integrity issue |
 | IntegrityCheckIssueMinorNoDedup | MINOR | Found a data integrity issue |
 | IntegrityCheckIssueWarning | WARNING | Found a data integrity issue |
 | IntegrityCheckIssueWarningNoDedup | WARNING | Found a data integrity issue |
 | IntegrityCheckStarted | DEBUG | Integrity check has started |
 | IntegrityCheckTransientIssue | DEBUG | Found a transient state which is expected to be encountered. Can be ignored, unless it persists. In which case, a non-transient issue event will be produced |
 | IntegrityCheckTransientIssueNoDedup | DEBUG | Found a transient state which is expected to be encountered. Can be ignored, unless it persists. In which case, a non-transient issue event will be produced |
 | ManualOverrideStall | WARNING | A service has been manually overridden and stalled. |
 | MetadataCommitQueueHang | MINOR | Bucket step down due to hanging metadata commit queue |
 | ObjectStoreAttachedToFilesystem | INFO | The object store is attached to the filesystem |
 | ObjectStoreFinishedDetachingFromFilesystem | INFO | The object store finished detaching from the filesystem |
 | ObjectStoreStartedDetachingFromFilesystem | INFO | The object store started detaching from the filesystem |
 | QuotaGraceExpired | WARNING | Directory soft capacity quota has been reached, and the grace period has expired |
 | QuotaHardLimitReached | WARNING | Directory hard capacity quota has been reached |
 | RAIDMDReadFailureInSnaphashDump | WARNING | Failed to read metadata block from RAID when dumping the snapshot manifest |
 | SnapshotContentCopied | INFO | Snapshot content copied |
 | SnapshotCreated | INFO | Snapshot created |
 | SnapshotCreationFailed | MAJOR | Snapshot creation failed |
 | SnapshotDeleted | INFO | Snapshot deleted |
 | SnapshotDownloadStarted | INFO | Snapshot download started |
 | SnapshotFilesystemRestored | INFO | Filesystem restored from snapshot |
 | SnapshotParamsUpdated | INFO | Snapshot updated |
 | SnapshotPolicyAdded | INFO | Snapshot policy configuration change |
 | SnapshotPolicyAttached | INFO | The snapshot policy is attached to the filesystem |
 | SnapshotPolicyDeleted | INFO | Snapshot policy configuration change |
 | SnapshotPolicyDetached | INFO | The snapshot policy is detached from the filesystem |
 | SnapshotPolicyUpdated | INFO | Snapshot policy configuration change |
 | SnapshotUploadFailed | MAJOR | Snapshot upload failed |
 | SnapshotUploadFinished | INFO | Snapshot upload finished |
 | SnapshotUploadStarted | INFO | Snapshot upload started |
 | SquelchBlockIdSetAbortedFlushed | DEBUG | While setting a squelch block's block ID for upgrade, it was already changed to invalid |
 | SquelchBlockIdSetAbortedRewritten | WARNING | While setting a squelch block's block id for upgrade was already rewritten to something else |
 | SuperblockUnreadable | CRITICAL | Superblock of a bucket could not be loaded |
 | UnflushedOpOnDeletingSnapview | MAJOR | Unflushed IO on a deleting snapshot |

### InterfaceGroup

 | **Type** | **Severity** | **Description** |
 | ----------------------------- | ------------ | ------------------------------------------------ |
 | FloatingIpAcquired | INFO | A floating IP was acquired by the process (node) |
 | FloatingIpReleased | INFO | A floating IP was released by the process (node) |
 | FloatingIpRemoveStateTimedout | WARNING | Timeout occurred during floating IP removal |
 | InterfaceGroupAdded | INFO | Interface group configuration change |
 | InterfaceGroupDeleted | INFO | Interface group configuration change |
 | InterfaceGroupIpsAdded | INFO | Interface group IPs configuration change |
 | InterfaceGroupIpsDeleted | INFO | Interface group IPs configuration change |
 | InterfaceGroupPortAdded | INFO | Interface group port configuration change |
 | InterfaceGroupPortDeleted | INFO | Interface group port configuration change |
 | InterfaceGroupUpdated | INFO | Interface group configuration change |

### IO

 | **Type** | **Severity** | **Description** |
 | -------------------- | ------------ | ------------------------------- |
 | SystemDriveIsTooSlow | MAJOR | System drive is slow to respond |

### KDriver

 | **Type** | **Severity** | **Description** |
 | ----------- | ------------ | --------------- |
 | DriverAlert | WARNING | Driver Alert |

### Kms

 | **Type** | **Severity** | **Description** |
 | ----------------------- | ------------ | ------------------------ |
 | KmsConfigurationAdded | INFO | KMS configuration change |
 | KmsConfigurationRemoved | INFO | KMS configuration change |
 | KmsConfigurationUpdated | INFO | KMS configuration change |

### Licensing

 | **Type** | **Severity** | **Description** |
 | ---------------------- | ------------ | ------------------------------ |
 | LicensingReset | INFO | Licensing state has been reset |
 | LicensingWorkerStarted | DEBUG | Licensing worker started |
 | NewLicenseInstalled | INFO | New license installed |
 | PaygLicensingEnabled | INFO | PAYG licensing enabled |

### ManualOverride

 | **Type** | **Severity** | **Description** |
 | --------------------- | ------------ | ----------------------------- |
 | ManualOverrideChanged | INFO | A manual override is changed. |

### Network

 | **Type** | **Severity** | **Description** |
 | --------------------------- | ------------ | -------------------------------------------------------------------------------------------- |
 | ClientNodeDisconnected | INFO | A client disconnected from the cluster |
 | CloudMoveIpFail | MINOR | Move IP on cloud failed |
 | DefaultDataNetworkingChange | INFO | The default data networking configuration has changed |
 | DpdkIBQkeyMismatch | MAJOR | DPDK IB qkey Mismatch |
 | DpdkInitFailed | MINOR | DPDK initialization failed |
 | DpdkPoolSummary | DEBUG | Summary of DPDK pool status |
 | FipIsNoLongerOnDevice | MAJOR | IP is no longer on device |
 | HangingRPCs | MAJOR | Some RPCs are hanging too long |
 | HugepagesAllocationFailure | MINOR | Hugepages allocation failure |
 | IONodeCannotFetchConfig | WARNING | The IO node cannot join the cluster for too long |
 | L6PacketFormatNotInSync | MAJOR | L6 packet format not in sync with process limits flags, indicates a non-fatal internal error |
 | MemoryAllocFailed | MINOR | Memory allocation failed |
 | MemoryClaimFailed | MINOR | Memory claim failed |
 | MemoryMigratedAfterPin | MAJOR | Hugepage mapping migrated after it was pinned |
 | MemoryMigratedBeforePin | MINOR | Hugepage mapping migrated before it was pinned |
 | MemoryPinningIoctlFailed | MINOR | Memory pinning ioctl failed |
 | MgmtNodeCannotFetchConfig | WARNING | The management process (node) cannot join the cluster for too long |
 | NetDeviceLinkDown | MINOR | Network interface DOWN |
 | NetDeviceLinkUp | MINOR | Network interface UP |
 | NetSlaveDeviceLinkDown | MINOR | Network slave interface DOWN |
 | NetSlaveDeviceLinkUp | MINOR | Network slave interface UP |
 | NetworkBan | MAJOR | Ban Network Peer |
 | NetworkPortConfigFail | MINOR | Network port configuration failed |
 | NetworkPortDead | MAJOR | Network Port hasn't passed packets for an extended period, it is likely dead |
 | NetworkUnban | INFO | Unban network peer |
 | NICNotFound | INFO | NIC not found when initializing |
 | NoConnectivityToLivingNode | MAJOR | A process (node) is disconnected from living peers |
 | NodeCannotJoinCluster | WARNING | The process (node) cannot join the cluster for too long |
 | NodeCannotSendJumboFrames | MINOR | A process (node) cannot send jumbo packets |
 | NodeDisconnected | MINOR | A process (node) disconnected from the cluster |
 | NoHardwareWatchdog | MAJOR | No hardware watchdog found |
 | NoJumboFrames | MINOR | The network does not allow jumbo packets through |
 | RDMAClientDisabled | MINOR | RDMA optimization disabled |
 | RDMAClientEnabled | MINOR | RDMA optimization enabled |
 | RDMADeviceDead | MAJOR | RDMA device did not receive completions for an extended period, it is likely dead |

### NFS

 | **Type** | **Severity** | **Description** |
 | ----------------------------------- | ------------ | ------------------------------------------------ |
 | NfsAclConfigurationChangeEvent | INFO | NFS Acl Configuration changed |
 | NfsAuthTypeChangeEvent | INFO | NFS Authentication Types Configuration |
 | NfsClientGroupAdded | INFO | NFS client group configuration change |
 | NfsClientGroupDeleted | INFO | NFS client group configuration change |
 | NfsClientGroupRuleAdded | INFO | NFS client group rule configuration change |
 | NfsClientGroupRuleDeleted | INFO | NFS client group rule configuration change |
 | NfsClusterStatusActiveEvent | INFO | NFS Cluster is active |
 | NfsClusterStatusInactiveEvent | CRITICAL | NFS Cluster is inactive |
 | NfsCustomOptionsUpdated | INFO | NFS custom options configuration change |
 | NfsDirectIOConfigurationChangeEvent | INFO | NFS DirectIO Configuration changed |
 | NfsExportsPermissionsAdded | INFO | NFS export permissions for configuration change |
 | NfsExportsPermissionsDeleted | INFO | NFS export permissions for configuration change |
 | NfsExportsPermissionsUpdated | INFO | NFS export permissions for configuration change |
 | NfsKerberosSetupEvent | INFO | NFS Kerberos Service setup is done |
 | NfsLdapSetupEvent | INFO | NFS LDAP setup is done |
 | NfsLocksConfigurationChangeEvent | INFO | NFS Locks Configuration changed |
 | NfsMountFail | WARNING | NFS mount request failed |
 | NfsPortmapFail | MAJOR | The NFS server failed to register in the portmap |
 | NfsServiceDown | CRITICAL | NFS service down |
 | NfsStatsConfigurationChangeEvent | INFO | NFS Extended stats Configuration changed |

### Node

 | **Type** | **Severity** | **Description** |
 | ---------------------- | ------------ | ------------------------------------------------- |
 | AssertionFailed | MAJOR | Assertion failed. |
 | GCCrashReport | MINOR | Node has crashed in GC on the previous run |
 | MemoryAllocationFailed | MAJOR | Memory allocation failed |
 | NodeAbruptExitReport | MINOR | Node has crashed on the previous run |
 | NodeExceptionExit | MAJOR | A process (node) exited with an exception |
 | NodeKernelStack | WARNING | Kernel stack of node before reset |
 | NodeSignalExit | MAJOR | A process (node) exited due to receiving a signal |
 | NodeStarted | INFO | A process (node) started |
 | NodeStopped | INFO | A process (node) stopped |
 | NodeTraceback | WARNING | Traceback of node before reset |
 | SoftAssertionFailed | MAJOR | Assertion failed (soft) |

### ObjectStorage

 | **Type** | **Severity** | **Description** |
 | ---------------------------------------------------- | ------------ | ----------------------------------------------------------------------------------------------------------------------------- |
 | ChecksumErrorInDownloadedObject | MINOR | Checksum error detected by COMPUTE node in a downloaded OBS data block |
 | ChecksumErrorOnObjectUpload | MAJOR | Checksum error detected by COMPUTE node when uploading an OBS data block (corrupted after verifying data read from the drive) |
 | DataBlobDownloadFailed | WARNING | Failed downloading data blob header |
 | DownloadedExtentHasInvalidBlobId | MAJOR | Downloaded extent has invalid blob id |
 | DownloadedExtentMissingExpectedBlock | MAJOR | Downloaded extent missing expected block |
 | ExtentHasFakeRetentionTag | MAJOR | Extent has non-local tag but has disk-only blocks |
 | InvalidDataBlobHeader | MAJOR | Invalid header detected by COMPUTE node in a downloaded OBS data blob |
 | ObjectStoreBucketAdded | INFO | Object store bucket configuration change |
 | ObjectStoreBucketDeleted | INFO | Object store bucket configuration change |
 | ObjectStoreBucketUpdated | INFO | Object store bucket configuration change |
 | ObjectStoreGroupAdded | INFO | Object store configuration change |
 | ObjectStoreGroupDeleted | INFO | Object store configuration change |
 | ObjectStoreGroupUpdated | INFO | Object store configuration change |
 | ObjectStoreHasHighLevelOfUnreclaimedCapacity | WARNING | The object store has a high level of unreclaimed capacity |
 | ObjectStoreIsFull | CRITICAL | Object store is full |
 | ObjectStoreNoLongerHasHighLevelOfUnreclaimedCapacity | INFO | The object store no longer has a high level of unreclaimed capacity |
 | ObjectStoreStatusDown | MAJOR | The object store status is down |
 | ObjectStoreStatusUp | INFO | The object store status is up |
 | ObsIsMissingObject | CRITICAL | Permanently failed to download an object from object storage - The object was not found |
 | PersistentChecksumErrorInDownloadedObject | MAJOR | Checksum error detected by COMPUTE node in a downloaded OBS data block |

### Org

 | **Type** | **Severity** | **Description** |
 | -------------------- | ------------ | --------------------------------------- |
 | OrgCreated | INFO | The organization is created |
 | OrgDeleted | INFO | The organization is deleted |
 | OrgRenamed | INFO | The organization is renamed |
 | OrgSsdQuotaChanged | INFO | The organization SSD quota is changed |
 | OrgTotalQuotaChanged | INFO | The organization total quota is changed |

### Raft

 | **Type** | **Severity** | **Description** |
 | --------------------------------- | ------------ | ---------------------------------------------------------------- |
 | IndexChangeDuringStream | INFO | The RAFT index changed during the streaming of the raft snapshot |
 | OutOfMemoryErrorOnLeadershipAgent | MAJOR | Out of memory error on the leadership agent |

### Raid

 | **Type** | **Severity** | **Description** |
 | ------------------------------------ | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |
 | BitmapChecksumMismatch | MINOR | Bitmap checksum mismatch detected |
 | DataGenerationNumberBug | WARNING | An issue with advancing the applied data generation number report from a bucket |
 | DataProtectionLevelDecreased | MINOR | The data protection level decreased |
 | DataProtectionLevelIncreased | INFO | The data protection level increased |
 | DiskNvkvHighUtilization | WARNING | Disk's internal resource (NVKV) has high utilization |
 | DiskWritableStateChange | INFO | Disk's writable state changed |
 | DrivesProcessConnectionLost | MINOR | Disk connection lost |
 | DrivesProcessConnectionRecovered | INFO | Disks quick recovery from lost connection detected |
 | EnoughActiveFailureDomains | MINOR | Sufficient active failure domains |
 | EvictionOfPlacementFailed | MAJOR | Eviction of placement encountered a potentially corrupt block marked as used |
 | FailedRecoveringData | MINOR | Detected unexpected data, not enough redundant copies are available to recover it |
 | FoundCorruptedBlockInStripe | CRITICAL | Detected corrupt block in a RAID stripe |
 | HotSpareFailureDomainsUpdated | INFO | Hot spare failure domains updated |
 | IncorrectScannedBlockChecksum | CRITICAL | Detected used block with a mismatching checksum |
 | InFlightCorruptionDetectedByScrubber | MINOR | Detected in-flight corrupt read result from drive |
 | NoDataProtection | MAJOR | No data protection |
 | PersistentNoDataProtection | CRITICAL | The Cluster has not had data protection for a significant period. The problem is likely not ephemeral |
 | PersistentTooManyFailures | CRITICAL | The Cluster has been experiencing too many failures when accessing drives for a significant period, and the problem is likely not ephemeral |
 | QuorumGenerationNumberBug | WARNING | An issue with advancing the applied quorum generation number report from a bucket |
 | RaidReadFreeBlock | MAJOR | RAID had read a block marked as free |
 | RaidScrubbingRateUpdated | INFO | RAID scrubber limit updated |
 | RaidSpaceFull | MINOR | A RAID space is full |
 | RaidsStarted | INFO | RAIDs started |
 | RepairedCorruptDataFromDrive | CRITICAL | Detected corrupt data from drive\[s]. The system will rewrite with the correct data |
 | ScrubberHanging | MAJOR | RAID Scrubber is hanging on the placement. |
 | SingleHopReadCorruptionDetected | MINOR | Single-hop read corruption detected |
 | SwitchPlacementHanging | MINOR | An active placement to write to is unavailable because of capacity constraints or disk failures |
 | TooFewActiveFailureDomains | MAJOR | Too few active failure domains |
 | TooManyFailures | MAJOR | Too many failures, and some data is unavailable |
 | UsedSSDCapacityCriticalOverflow | CRITICAL | SSD capacity usage is overflowing, and the internal spares are running out. The cluster may soon become unavailable for writing |
 | UsedSSDCapacityNoLongerOverflows | INFO | SSD capacity usage is no longer overflowing |
 | UsedSSDCapacityOverflow | MAJOR | SSD capacity usage is overflowing, and the internal capacity spares are used |

### Resources

 | **Type** | **Severity** | **Description** |
 | ------------------------------------------ | ------------ | ----------------------------------------------------------------- |
 | APIServerStarted | INFO | Successfully started the API server |
 | APIServerStartFailed | WARNING | Failed to start the API server |
 | BandwidthSelected | INFO | Bandwidth set for host |
 | CoreAllocated | INFO | Allocated core |
 | DeviceIsNotAValidNetworkDevice | WARNING | Device is not a valid network device |
 | DisabledNumaBalancing | INFO | Disabled NUMA Balancing |
 | DriverLoaded | INFO | Driver attached |
 | FailedToLoadDriver | WARNING | Failed to load the wekafs driver |
 | HangingHTTPRequest | MAJOR | Hanging HTTP request detected. |
 | HttpServerFibersExhausted | MAJOR | Hanging HTTP requests exhausted all available fibers |
 | HugepagesAllocated | INFO | Hugepages allocated |
 | HugepagesAllocationRetries | WARNING | Hugepages allocation retried |
 | HugepagesAllocationStarted | INFO | Hugepages allocation started |
 | HugepagesAllocationTookTooLong | WARNING | Hugepages allocation takes an unexpectedly long duration |
 | InactiveHostCannotJoinCluster | INFO | Inactive host cannot join the cluster |
 | LoadingStableResourcesFailed | INFO | Failed loading stable resources |
 | NetBufsExhausted | MAJOR | netbufs exhausted |
 | NetDevDriverReloadFailed | MINOR | Net device driver reload failed |
 | NetworkDeviceAllocated | INFO | Allocated network device |
 | NetworkDeviceHasNoIp | MAJOR | Network device has no IP address |
 | NetworkDeviceNotUsedByAnySlots | MINOR | Network device not used by any slots |
 | NoIPsConfiguredForHostJoinWithNoDefaultNet | WARNING | No IP configured for the process (node) {nid} with no default-net |
 | RevertToStableResources | INFO | Reverted to stable resources |
 | UnlimitedBandwidthSelected | INFO | Bandwidth set to unlimited |
 | WCGroupContainerEvent | MAJOR | Container Status |
 | WCGroupUsageMajorEvent | MAJOR | Container {resource} Status |
 | WCGroupUsageWarningEvent | WARNING | Container {resource} Status |

### S3

 | **Type** | **Severity** | **Description** |
 | --------------------------------------- | ------------ | ----------------------------------- |
 | S3AddBucketILMRuleEvent | INFO | S3 add bucket ILM rule |
 | S3AsssumeRoleEvent | INFO | S3 STS token created |
 | S3AttachIAMPolicyEvent | INFO | S3 attach IAM policy |
 | S3AuditWebhookDisabledEvent | INFO | The S3 Audit Webhook disabled |
 | S3AuditWebhookEnabledEvent | INFO | The S3 Audit Webhook enabled |
 | S3BucketDestroyedEvent | INFO | The S3 bucket destroyed |
 | S3ClusterCreated | INFO | An S3 cluster created |
 | S3ClusterDestroyed | INFO | The S3 cluster destroyed |
 | S3ClusterDestroyFailed | MAJOR | Failed to destroy the S3 cluster |
 | S3ClusterStatusActiveEvent | INFO | S3 Cluster is active |
 | S3ClusterStatusInactiveEvent | CRITICAL | S3 Cluster is inactive |
 | S3ClusterUpdated | INFO | The S3 cluster updated |
 | S3ContainerStateChangesEvent | INFO | S3 container status change |
 | S3ContainerStatusActiveEvent | INFO | S3 container active |
 | S3ContainerStatusInactiveEvent | MAJOR | S3 container inactive |
 | S3ContainerStatusOnlineEvent | INFO | S3 container online |
 | S3ContainerStatusSaturatedEvent | MAJOR | S3 container saturated |
 | S3CreateBucketEvent | INFO | The S3 bucket created |
 | S3CreateIAMPolicyEvent | INFO | The S3 created an IAM Policy |
 | S3CreateServiceAccountEvent | INFO | S3 create service account |
 | S3DestroyBucketEvent | INFO | The S3 bucket destroyed |
 | S3DetachIAMPolicyEvent | INFO | S3 detach IAM policy |
 | S3DrainEvent | INFO | S3 container drain |
 | S3KVAddedEvent | INFO | S3 Config Add Key |
 | S3KVRemovedEvent | INFO | S3 configuration remove key |
 | S3KVResetEvent | INFO | S3 KV store configuration reset |
 | S3MultipleContainersStatusInactiveEvent | CRITICAL | multiple S3 containers are inactive |
 | S3NotificationDropped | CRITICAL | S3 notifications dropped. |
 | S3RemoveBucketILMRuleEvent | INFO | S3 remove bucket ILM rule |
 | S3RemoveIAMPolicyEvent | INFO | The S3 removed the IAM policy |
 | S3RemoveServiceAccountEvent | INFO | S3 remove service account |
 | S3ResetBucketILMRuleEvent | INFO | S3 reset bucket ILM rules |
 | S3SetBucketPolicyEvent | INFO | S3 set bucket policy |
 | S3UnDrainEvent | INFO | S3 container undrain |
 | SLBContainerStatusActiveEvent | INFO | SLB container active |
 | SLBContainerStatusInactiveEvent | MAJOR | SLB container inactive |

### Security

 | **Type** | **Severity** | **Description** |
 | ----------------------------------- | ------------ | ------------------------------------------------------------------------- |
 | BackendJoinTLSVerificationFailure | MINOR | A backend tried to join the cluster but failed the TLS verification check |
 | BackendNodeJoinSecurityPolicyDenied | WARNING | Backend join denied per security policy |
 | CaCertSet | INFO | The CA certificate was added to the cluster |
 | CaCertUnset | INFO | The CA certificate was unset |
 | ClientNodeJoinSecurityPolicyDenied | WARNING | Client join denied per security policy |
 | ContainerJoinSecretDenied | WARNING | The container failed to rejoin due to missing or incorrect join-secret |
 | FileSystemSecurityPoliciesChange | INFO | Filesystem security policies changed. |
 | JoinSecurityPoliciesUpdated | INFO | Join Security Policies for {mode} configuration change |
 | LocalTLSCertAdded | INFO | CertificateChange configuration change |
 | LocalTLSCertRemoved | INFO | CertificateChange configuration change |
 | LocalTLSCertUpdated | INFO | CertificateChange configuration change |
 | MountAccessDenied | WARNING | Mount access denied |
 | MountSecurityPolicyAllowed | INFO | Mount access allowed by security policy |
 | MountSecurityPolicyDenied | WARNING | Mount access denied by security policy |
 | OrgSecurityPoliciesChanged | INFO | The organization security policies have changed |
 | OrgTokensRevoked | INFO | The organization API tokens have been revoked |
 | SecurityPolicyAccessDenied | WARNING | Access denied per security policy |
 | SecurityPolicyCreated | INFO | Security policy configuration change |
 | SecurityPolicyDeleted | INFO | Security policy configuration change |
 | SecurityPolicyUpdated | INFO | Security policy configuration change |
 | TLSSet | INFO | TLS was set |
 | TLSStrictnessUpdated | INFO | TLS strictness updated |
 | TLSUnset | INFO | TLS was unset |
 | TokenTimeoutsChange | INFO | Token timeouts for authentication were updated |

### Smb

 | **Type** | **Severity** | **Description** |
 | --------------------------------- | ------------ | ------------------------------------- |
 | SmbAdJoined | INFO | Active Directory configuration change |
 | SmbAdLeft | INFO | Active Directory configuration change |
 | SmbClusterConfigured | INFO | SMB cluster configuration change |
 | SmbClusterCreateCreated | INFO | SMB cluster configuration change |
 | SmbClusterDestroyed | INFO | SMB cluster configuration change |
 | SmbConfigGenerationUpdated | INFO | SMB Config configuration change |
 | SmbShareAdded | INFO | Share configuration change |
 | SmbShareConfigured | INFO | Share configuration change |
 | SmbShareHostnameACERemovedRemoved | INFO | SambaHostnameACE configuration change |
 | SmbShareHostnameACEResetDestroyed | INFO | SambaHostnameACE configuration change |
 | SmbShareRemoved | INFO | Share configuration change |
 | SmbTrustedDomainAdded | INFO | TrustedDomain configuration change |
 | SmbTrustedDomainRemoved | INFO | TrustedDomain configuration change |

### Statistics

 | **Type** | **Severity** | **Description** |
 | ----------------- | ------------ | -------------------------------------- |
 | StatLimitExceeded | WARNING | A set limit on a statistic is exceeded |

### System

 | **Type** | **Severity** | **Description** |
 | ---------------------------------- | ------------ | ------------------------------------------------------------------ |
 | BlockTaskAborted | INFO | The bucket task was aborted successfully |
 | BlockTaskComplete | INFO | A bucket task completed successfully |
 | BucketsCreated | INFO | The system has created buckets |
 | ClusterTaskAborted | INFO | Cluster task aborted |
 | ClusterTaskAborting | INFO | Cluster task started aborting |
 | ClusterTaskPaused | INFO | Cluster task paused |
 | ClusterTaskResumed | INFO | Cluster task resumed |
 | ClusterTasksCpuLimitUpdated | INFO | Cluster tasks CPU limit set. |
 | ClusterwideTaskChanged | DEBUG | The cluster-wide task has changed |
 | DataServiceTaskFailedWithError | MINOR | Data service task failure |
 | DataServiceTaskWaitingForChildTask | INFO | Data service task waiting |
 | DsShardViewChanged | INFO | There is a change in the data service shard view |
 | HaveEnoughSSDCapacity | MINOR | Sufficient SSD capacity exists for all the provisioned filesystems |
 | IOStarted | INFO | The system has started. |
 | IOStopped | INFO | The system has stopped. |
 | NotEnoughSSDCapacity | CRITICAL | Need more SSD capacity for all the provisioned filesystems |
 | QOSConfigReset | INFO | QoS configuration reset |
 | QOSConfigSet | INFO | QoS configuration set |
 | StartIORequested | INFO | The user has requested to start the IO |
 | StopIORequested | INFO | The user has requested to stop the IO |
 | SystemInfoReport | INFO | The management process (node) started, reporting OS info |
 | ThreadPoolCanNotStartThread | MINOR | The reactor's thread pool failed to start a thread |
 | TooManyFibers | MINOR | Too many fiber allocations |

### Telemetry

 | **Type** | **Severity** | **Description** |
 | ---------------------------------- | ------------ | -------------------------------------- |
 | AuditTracesStatusChange | INFO | Audit traces status changed |
 | TelemetryExportAdded | INFO | Telemetry Export configuration change |
 | TelemetryExportDisabled | INFO | Telemetry Export Disabled |
 | TelemetryExportEnabled | INFO | Telemetry Export Enabled |
 | TelemetryExportRemoved | INFO | Telemetry Export configuration change |
 | TelemetryExportUpdated | INFO | Telemetry Export configuration change |
 | TelemetrySourcesAttachedToExport | INFO | Telemetry Sources Attached To Export |
 | TelemetrySourcesDetachedFromExport | INFO | Telemetry Sources Detached From Export |

### Traces

 | **Type** | **Severity** | **Description** |
 | ---------------------------------- | ------------ | ------------------------------------------------ |
 | RemoteTracesDisabled | INFO | Remote traces disabled |
 | RemoteTracesEnabled | INFO | Remote traces enabled |
 | RemoteTraceStreamerEndpointInvalid | INFO | Invalid remote trace streamer endpoint |
 | TracesConfigurationActivated | INFO | Traces configuration change |
 | TracesConfigurationDeactivated | INFO | Traces configuration change |
 | TracesConfigurationReset | INFO | Traces configuration change |
 | TracesConfigurationUpdated | INFO | Traces configuration change |
 | TracesFreezeOnEventOfInterest | MAJOR | Traces have been frozen on the event of interest |
 | TracesFreezePeriodReset | INFO | Traces freeze period has been reset |
 | TracesFreezePeriodSet | INFO | Traces freeze period has been set |

### Upgrade

 | **Type** | **Severity** | **Description** |
 | --------------------------- | ------------ | ------------------------------------------------- |
 | CleanupUpgradePhaseSkipped | MAJOR | Skipped Cleanup in Upgrade Phase: {currentPhase} |
 | ClientUpgradeRequested | INFO | A client upgrade is requested |
 | ComputeUpgradeFinished | INFO | The compute containers upgrade has finished |
 | ComputeUpgradeInvoked | INFO | The compute containers upgrade has started |
 | ComputeUpgradeStarted | INFO | The compute containers upgrade has started |
 | ContainerUpgradeFailed | MAJOR | Container upgrade failed |
 | ContainerUpgradeFinished | INFO | Container upgrade finished |
 | ContainerUpgradeStarted | INFO | Container upgrade started |
 | DataservUpgradeFinished | INFO | The dataserv containers upgrade has finished |
 | DataservUpgradeStarted | INFO | The dataserv containers upgrade has started |
 | DrivesUpgradeFinished | INFO | The drives containers upgrade has finished |
 | DrivesUpgradeStarted | INFO | The drives containers upgrade has started |
 | ExternalUpgradeCancelled | INFO | The external upgrade has been cancelled |
 | ExternalUpgradeFinished | INFO | The external upgrade has finished |
 | ExternalUpgradeStarting | INFO | An external upgrade has started |
 | FailureDomainUpgradeStarted | INFO | Started to upgrade containers of a failure domain |
 | FinishedExternalHostUpgrade | INFO | The external server (host) upgrade has finished |
 | FrontendUpgradeFinished | INFO | The frontend containers upgrade has finished |
 | FrontendUpgradeStarted | INFO | The frontend containers upgrade has started |
 | StartingExternalHostUpgrade | INFO | The external server (host) upgrade has started |
 | TargetVersionChange | DEBUG | Target version has changed |
 | UpgradePaused | INFO | Upgrade paused |
 | UpgradeResumed | INFO | Upgrade resumed |
 | UpgradeStatusChange | DEBUG | Upgrade status has changed |
 | WekaVersionDowngraded | WARNING | The cluster is running a lower version |

### User

 | **Type** | **Severity** | **Description** |
 | -------------------------------- | ------------ | ------------------------------------------- |
 | LDAPAuthDisabled | INFO | LDAP authentication disabled |
 | LDAPAuthEnabled | INFO | LDAP authentication enabled |
 | LDAPConfigUpdated | INFO | LDAP configuration updated |
 | UserCreated | INFO | The user is created |
 | UserDeleted | INFO | The user is deleted |
 | UserLoggedIn | INFO | User logged in |
 | UserLoginFailed | INFO | User login failed |
 | UserLoginLocked | MINOR | User login locked |
 | UserPasswordChanged | INFO | The user changed the password |
 | UserPasswordChangedByAnotherUser | INFO | The administrator changed the user password |
 | UserRoleChanged | INFO | The user role is changed |

<!-- ============================================ -->
<!-- File 151/259: operation-guide_alerts.md -->
<!-- ============================================ -->

---
description:
---

# Alerts

Alerts indicate problematic ongoing states that the cluster is suffering from. To dismiss an alert, you need to resolve the root cause of the alert.

The system provides the alert name, description, and corrective action for each alert.

Usually, an alert is introduced alongside an equivalent event. This can help identify the point when the problematic state occurred and its root cause.

**Related topics**

<!-- ============================================ -->
<!-- File 152/259: operation-guide_alerts_alerts.md -->
<!-- ============================================ -->

---
description: This page describes how to manage alerts using the GUI.
---

# Manage alerts using the GUI

Using the GUI, you can:

* View alerts
* Mute alerts
* Unmute alerts

## View alerts

The bell icon on the top bar indicates the number of existing active alerts in the system. The alerts pane in the system dashboard also provides the name of the alerts.

If there are no alerts (active or muted), the alerts pane is empty, and the bell does not specify any number.

**Procedure**

1. To display the alert details, select the bell icon or select any alert.

## Mute alerts

If for any reason, it is not possible to resolve the root cause of an alert in a reasonable time and you want to hide it temporarily, you can mute the alert for a specified period. Then later, you can unmute the alert and resolve it.

The system automatically unmutes the muted alerts after the expiry period.

**Procedure**

1. On the Active Alerts page, select the bell next to the alert.
2. Set the mute duration (number and units) and select **Mute**.

The muted alert is moved to the Muted Alerts area. The total number of active alerts is deducted by the number of muted alerts.

## Unmute alerts

Muted alerts appear under the Muted Alerts area. You can unmute an alert manually before the expiry duration.

**Procedure**

1. Under the Muted Alerts area, select the bell of the alert you want to unmute.

<!-- ============================================ -->
<!-- File 153/259: operation-guide_alerts_alerts-1.md -->
<!-- ============================================ -->

---
description: This page describes how to manage alerts using the CLI.
---

# Manage alerts using the CLI

Using the CLI, you can:

* Display alert types
* View alerts
* Mute alerts
* Unmute alerts

## **Display alert types**

**Command:** `weka alerts types`

Use this command to list all possible types of alerts that the WEKA cluster can return.

**Command:**`weka alerts describe`

Use this command to describe all the alert types the WEKA cluster can return, along with possible corrective actions for each alert.

## **View alerts**

**Command:** `weka alerts`

Use the following command line to list all alerts (muted and unmuted) in the WEKA cluster:

`weka alerts [--muted]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | muted | List muted alerts alongside unmuted ones. | False |

## **Mute alerts**

**Command:** `weka alerts mute`

Use the following command line to mute an alert type:

`weka alerts mute <alert-type> <duration>`

The system does not prompt muted alerts when listing active alerts. You must specify the duration in which the alert-type is muted. After the expiry of the specified duration, the system unmutes the alert-type automatically.

**Parameters**

 | Name | Value |
 | --- | --- |
 | alert-type* | An alert-type to mute, use weka alerts types to list types. |
 | duration* | Expiration time for muting this alert type.Format: 3s, 2h, 4m, 1d, 1d5h, 1w. |

## **Unmute alerts**

**Command:** `weka alerts unmute`

Use the following command line to unmute a muted alert-type:

`weka alerts unmute <alert-type>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | alert-type* | An alert-type to unmute, use weka alerts types to list types. |

<!-- ============================================ -->
<!-- File 154/259: operation-guide_alerts_list-of-alerts-and-corrective-actions.md -->
<!-- ============================================ -->

---
description:
---

# List of alerts and corrective actions

Note: The `LicenseError` alert includes a 5% grace threshold. It is triggered only when used capacity exceeds 105% of the licensed limit, which allows for minor overages without generating an immediate alert.

 | Alert name | Description | Corrective actions | Severity |
 | --- | --- | --- | --- |
 | AdminDefaultPassword | Default admin password in use | Change the admin password to restrict unauthorized access. | INFO |
 | AgentNotRunning | The local agent does not run | Restart the local agent on the specified server using the command ‚Äòservice weka-agent start‚Äô. | DEBUG |
 | ApproachingClientsUnavailability | Approaching connected clients limit | Ensure all backend containers are up or expand the cluster with more backend containers or servers. | DEBUG |
 | ApproachingSystemLimit | Approaching a system limit | Follow the information specified in the {action_item}. | MAJOR |
 | AutoRemoveTimeoutTooLow | Stateless Client auto-remove timeout too low. | Remount the host with a higher auto-remove timeout value. | WARNING |
 | AvailableMemory | Not enough available memory | Check your system. | MAJOR |
 | BackendNumaBalancingEnabled | NUMA balancing is enabled on a backend server | Disable the automatic NUMA balancing by running the command line 'echo 0 > /proc/sys/kernel/numa_balancing' on the backend server. | WARNING |
 | BackendVersionsMismatch | Backends mismatch cluster version | Upgrade all the backends to match the cluster's version | WARNING |
 | BadDisksCapacityRatio | Large discrepancy between the smallest and largest drive | There is a large discrepancy between the sizes of the smallest and largest drives in the system. Replace drives with comparable capacities to minimize the differences between the drives' sizes. | MAJOR |
 | BlockedJrpcMethod | JRPC method is blocked | Unblock the JRPC method by running the command 'blocked_jrpc_methods_remove' or 'blocked_jrpc_methods_clear' manhole. | DEBUG |
 | BondInterfaceCompromised | Network high availability interface compromised | Ensure a proper operation of the network configuration, cables, and NICs. | MINOR |
 | BucketCapacityExhausting | Buckets are nearing exhaustion of their maximum capacity | Consider migration to a cluster with a higher number of buckets | DEBUG |
 | BucketHasNoQuorum | Too many compute processes are down | The number of inactive compute processes exceed the threshold required for the bucket to function properly, resulting in the bucket being unavailable. Ensure the compute processes on the containers {hosts} are up and running and connected. If the issue is not resolved contact the Customer Success Team. | DEBUG |
 | BucketUnresponsive | Compute resource failure | Check the connectivity and status of the drives of the leader container. and ensure the compute processes are running and connected. If the issue is not resolved, contact the Customer Success Team. | CRITICAL |
 | CPUFrequentStarvation | CPU frequent starvation detected in the last minute | Check the logs of the relevant containers for potential hardware or core allocation problems. | DEBUG |
 | CPUStarvation | CPU starvation detected in the last minute | Check the logs of the relevant containers for potential hardware problems. For specific hang address run /weka/weka_addr2line within the reported weka container on the address to convert them into a symbol name. | DEBUG |
 | CWTaskAbortionStuck | CWTask stuck in aborting state | Start IO to allow the task to complete aborting | DEBUG |
 | ChokingDetected | High congestion level | In some situations, the system may slow down IOs when reaching some limits (or even block new IOs at higher limits) until the congested resource is relieved. Such situations may be transient, and the issue will be resolved on its own after a short time. However, some cases suggest an issue that needs to be addressed, such as a workload maxing out the cluster's resources. In such cases, the cluster resources must be expanded, as described in Expanding &#x26; Shrinking Cluster Resources. Contact the Customer Success Team. | DEBUG |
 | ClientVersionsMismatch | Clients mismatch cluster version | Upgrade the clients to the same version as the cluster by running 'weka local upgrade' locally. | INFO |
 | ClockSkew | Clock skew on server | Ensure the NTP is configured correctly on the containers and that their clocks are synchronized. | MINOR |
 | CloudHealth | WEKA Home disconnected | Check that the server has Internet connectivity and is connected to the Weka Home. See the Weka Home - The Weka support cloud topic in the documentation. | MINOR |
 | CloudStatsError | Statistics upload failed | See the event details in the System Events. | DEBUG |
 | ClusterInitializationError | Cluster initialization error | Diagnose the root cause and resolve it. Use weka cluster stop-io to clear the alert. | INFO |
 | ClusterIsUpgrading | Cluster is upgrading | If the upgrade fails, contact the Customer Success Team. | DEBUG |
 | ConfigOverridesActive | Config overrides are active | Contact the Customer Success Team. | DEBUG |
 | CoreOverlapping | Core Overlapping | Contact the Customer Success Team. | MAJOR |
 | DataIntegrity | Data integrity problem found | A scan identifies a certain number of data integrity problems. It highlights issues such as data corruption or inconsistencies that need immediate investigation and resolution. Contact the Customer Success Team. | CRITICAL |
 | DataProtection | Partial data protection | The cluster's data protection status changes, often due to failing containers or drives. It highlights that the system‚Äôs redundancy is compromised and requires immediate attention to restore full data protection. If the cluster is still resilient to 1 failure, this is okay, but still requires checking which process container or drive is down and acting accordingly. | MINOR |
 | DedicatedWatchdog | A dedicated server requires the installation of a hardware watchdog driver. | Ensure a hardware watchdog driver is available at /dev/watchdog. For details, search the Knowledge Base in the WEKA support portal. | DEBUG |
 | DrainingStuck | Stuck draining | Check the host status and logs for more information. | MINOR |
 | DriveCriticalWarnings | Drive has critical warnings | Deactivate the drive using the command 'weka cluster drive deactivate' and replace it. | MAJOR |
 | DriveDown | Drive down | Contact the Customer Success Team to check if the drive requires a replacement. | MINOR |
 | DriveEndurancePercentageUsed | Drive has reached its endurance threshold | Replace the drive before failure. | MAJOR |
 | DriveEnduranceSparesRemaining | Drive internal spares run too low | Replace the specified drive before it fails. | MAJOR |
 | DriveNVKVRunningLow | Drive nearing exhaustion of internal resource | Contact the Customer Success Team. | DEBUG |
 | DriveNeedsPhaseout | Drive is showing excessive errors | Deactivate the drive using the command 'weka cluster drive deactivate', and replace it. | MAJOR |
 | FaultsEnabled | Faults are enabled | Contact the Customer Success Team. | DEBUG |
 | FilesystemKMSError | Filesystem KMS Error | Review the filesystem's KMS customization and the KMS configuration and connectivity. | DEBUG |
 | FilesystemsThinProvisioningLowSpace | Filesystems thin provisioning low space | Thinly provisioned filesystems are nearing capacity limits, potentially leading to storage shortages. Consider adding more SSD capacity to the organization containing these filesystems. | WARNING |
 | FilesystemsThinProvisioningReserveReached | Filesystems thin provisioning capacity reserve reached | The reserved capacity for thin provisioning is exhausted. Create a new filesystem or expand the filesystem's capacity using the reserved capacity. | DEBUG |
 | HangingCacheSync | Cache sync is hanging | Consider using weka debug fs drop-dirty-cache to drop the cache and enable other clients to access the file (unsynchronized writes will be lost). | MINOR |
 | HangingClusterTasks | Cluster background task progress is hanging | If a task, which is expected to show progress or complete within a certain timeframe, stops progressing, it can trigger this alert. This could be due to various reasons like resource contention, system errors, or issues with the task itself. Contact the Customer Success Team. | DEBUG |
 | HangingIos | Some IOs stop responding | I/O operations have stopped responding on specific nodes, which could be due to storage issues, network problems, resource exhaustion, or process deadlocks. Ensure the compute processes are up and running and connected. If a backend object store is configured ensure it is connected and responsive. If the issue is not resolved contact the Customer Success Team. | DEBUG |
 | HighDrivesCapacity | SSD capacity overflow | The SSD‚Äôs used capacity reaches a critical level, exceeding a predefined threshold of internal reserves. Free up space on the SSDs or add more SSDs to the cluster. Refer to the "Expand Specific Resources of a Container" topic in the documentation. | MAJOR |
 | HighLevelOfUnreclaimedCapacityInObjectStore | High level of unreclaimed space in an object store |  | DEBUG |
 | HighSSDToRAMRatio | High SSD to RAM Ratio | Consider increasing RAM cluster wide, or removing unneeded drives to ensure Filesystems(RAID) requirements and lower SSD to RAM ratio | DEBUG |
 | HotspotInodes | IOs for some files have a long waiting queue | Contact the Customer Success Team. | DEBUG |
 | IBNotEnhanced | Enhanced IB mode disabled | Contact the Customer Success Team. | DEBUG |
 | ImbalancedCpuUsage | Imbalanced CPU usage detected in cluster processes | Check system configuration: Examine the system configuration for abnormalities that may be causing the CPU usage imbalance. | DEBUG |
 | JumboConnectivity | A container cannot send jumbo frames | Check the container network settings and the switch to which the container is connected, and ensure to enable jumbo frames. This setting improves performance. | WARNING |
 | KMSError | KMS Error | Review the KMS configuration and connectivity. | MAJOR |
 | LeaderPreparedForUpgrade | Leader prepared for upgrade | After the upgrade, the leader state automatically returns to normal. If this alert persists, contact the Customer Success Team. | DEBUG |
 | LegacyManualOverridesActive | Legacy manual overrides are active | Contact the Customer Success Team. | DEBUG |
 | LicenseError | License error | Ensure the cluster uses the correct license, the license has not expired, and the allocated space does not exceed the license limits. | WARNING |
 | LocalTLSCertificateExpired | Local TLS certificate expired | Update the local certificate. | DEBUG |
 | LocalTLSCertificateExpiringSoon | Local TLS certificate is expiring soon | Update the local certificate. | DEBUG |
 | LocalTLSConnectivityToNeighbors | Outgoing TLS connectivity to backends is down | Fix the TLS issue. One possibility is errors in the local cacert in /etc/wekaio/certs. | DEBUG |
 | LongestWaitInodes | Elevated IO wait time detected for some files | The file path can be retrieved using the weka debug fs resolve-inode {inodeIdValue} --snap-view-id {snapViewIdValue} command. A file to get lists of clients with open handles can be retrieved using the weka debug fs lsof {inodeIdValue} --snap-view-id {snapViewIdValue} command.By understanding the access pattern, you may want to optimize the application's caching mechanism. For additional assistance, contact the Customer Success Team. | DEBUG |
 | LowDiskSpace | Low disk space | See the event details in the System Events. | MINOR |
 | ManualOverridesActive | Manual overrides are active | Contact the Customer Success Team. | DEBUG |
 | ManualOverridesForced | Manual overrides are forced | Contact the Customer Success Team. | DEBUG |
 | MismatchedDriveFailureDomain | A drive failure domain does not match the failure domain of its attached container | One or more SSD drives in the system fail, reducing the total available capacity below the provisioned level. When the remaining capacity becomes too low, it can lead to hanging I/O operations. Check for down drives. | MAJOR |
 | MismatchedJoinSecrets | Backend containers do not have the same join secrets | This may create problems rejoining or reforming the cluster. Make sure all backend containers have the same join-secrets. | DEBUG |
 | NegativeUnprovisionedCapacity | Negative unprovisioned capacity | Resize one or more of the filesystems to reclaim capacity. For more information, contact the Customer Success Team. | DEBUG |
 | NetworkFailedToStartPorts | Network ports failed to start | Run weka debug net ports $NODE to see the current status. | DEBUG |
 | NetworkInterfaceLinkDown | Network interface link status down | Check the connectivity to the specified network interface. Verify that nothing blocks it. | MINOR |
 | NfsLocksDisabled | NFS Locks disabled | Configure config fs using weka nfs global-config set --config-fs=. | INFO |
 | NfsServiceDownAlert | NFS Service Down | If down services persist, contact the Customer Success Team. | MAJOR |
 | NoCgroupsConfigured | No cgroups configured warnings | Disabled or improperly configured Cgroups can cause system instability and performance degradation. Enable and configure Cgroups (v1/v2) following the Cgroups configuration section at https://docs.weka.io. | WARNING |
 | NoClusterLicense | No license assigned | Obtain and install a license from get.weka.io. | WARNING |
 | NodeBlacklisted | A process cannot rejoin the cluster | To enable the process to rejoin the cluster, whitelist it by running the command ‚Äòweka debug blacklist disable‚Äô. | DEBUG |
 | NodeDisconnected | Process disconnected | Check network connectivity to ensure the processes can communicate with the cluster. | MINOR |
 | NodeNetworkUnstable | A process with an unstable network detected | Ensure proper network connectivity in the cluster. If the problem is not resolved, contact the Customer Success Team. | WARNING |
 | NodeRDMANotActive | RDMA support for process is Inactive | Ensure that at least one RDMA-capable device exists. | DEBUG |
 | NodeTieringConnectivity | A process cannot Connect to an object store | A process cannot connect to the ObjectStore, due to either network connectivity, node / process health, or the OBS vendor equipment itself. Check the connectivity with the object store and ensure the process communicates with it. | MAJOR |
 | NonTlsApisAllowed | Non-TLS APIs are allowed | Update TLS strictness to enforce encrypted TLS APIs over HTTP | DEBUG |
 | NotEnoughActiveDrives | Reduced data protection | Check the connectivity and server status. Activate drives in more FDs. | MAJOR |
 | NotEnoughMemoryForFilesystemOperation | Insufficient cluster-wide RAM for proper Filesystem's Operation | Increase RAM cluster-wide to meet Filesystems(RAID) requirements for RAM or remove drives contributing to SSD capacity | DEBUG |
 | NotEnoughSSDCapacity | Some provisioned capacity is unavailable due to failed drives | Check for down drives. | MAJOR |
 | NotificationQueueHighLoad | S3NotificationQueueReachedHighWatermark | S3 notifications kafka queue on {hostIds} containers is {HighThreshold}% full. Action required. | MINOR |
 | NotificationSendFailure | S3NotificationSendFailure | Failures occurred when sending S3 notifications to Kafka on the following hosts during the past {windowMinutes} minutes: {hostIds}. Check system logs for details and restore Kafka service availability. | MAJOR |
 | PartialConnectivityTrackingDisabled | Partial connectivity tracking is disabled | Contact the Customer Success Team to turn on the grim reaper. | DEBUG |
 | PartialHugepageAllocation | Not enough memory | Check your system. | MAJOR |
 | PartiallyConnectedNode | A partially connected process detected | Ensure proper network connectivity in the cluster. If the problem is not resolved, contact the Customer Success Team. | MINOR |
 | PassedClientsAvailabilityThreshold vReached connected clients limit | Add more backend containers or servers to the cluster, check whether the backends are down, or disconnect some clients. | DEBUG |  |
 | PathsDegraded | Degraded Paths | Contact the Customer Success Team to review path connectivity. | MINOR |
 | PerformanceDegradedLowRAM | Low Server RAM | Add more servers to the cluster, add RAM to the backend servers or increase the memory allocation to the compute processes. | MAJOR |
 | QuotasHardLimitReached | Directory quota hard limit exceeded | Run the 'weka fs quota list' cluster commnad to get the list of directories exceeding their hard quota limits. Clear some space for these directories or increase their hard quota limit. | WARNING |
 | QuotasSoftLimitReached | Directory quota soft limit exceeded | Run the 'weka fs quota list' cluster command to get the list of directories exceeding their soft quota limits. Clear some space for these directories or increase their hard quota limit. | INFO |
 | RAIDCapacityExhaustion | RAID capacity exhaustion | If this situation does not resolve it self within a short period of time (~5 minutes) contact the Customer Success Team. | MAJOR |
 | RequestedActionFailure | Requested action failure | Check the logs for more information. | DEBUG |
 | ResourcesNotApplied | Resource changes are not applied | Apply the resource changes by running the command 'weka cluster container apply '. | DEBUG |
 | SSDCapacityDiscrepancy | Mismatch between the actual SSD capacity usage and the expected range | There is a mismatch between the actual SSD capacity usage and the expected range. The discrepancy could be caused by misconfiguration, inefficient tiering, data overgrowth, or other underlying issues. Monitor the compute processes' stability and contact the Customer Success Team. | DEBUG |
 | SSDCapacityTooHigh | Available capacity cannot be fully utilized | The SSD capacity is being underutilized due to an insufficient number of configured WEKA buckets. As a result, only a percentage of the available SSD space is usable. The message dynamically includes: usable_capacity: The amount of SSD space that can currently be utilized, percentage: The percentage of the total SSD capacity that is available for use, and full_capacity: The total SSD capacity that is theoretically available if fully configured. Contact the Customer Success Team for assistance in optimizing the SSD capacity usage. | INFO |
 | SystemDefinedTLS | TLS certificate is not user-defined | Replace the auto-generated self-signed certificate with a user-defined certificate by running the command 'weka security tls set'. | INFO |
 | TLSCertificateExpired | TLS certificate expired | Replace the existing certificate by running the command 'weka security tls set'. | MAJOR |
 | TLSCertificateExpiresSoon | TLS certificate is about to expire | Replace the existing certificate by running the command 'weka security tls set'. | MAJOR |
 | TelemetryStatusFault | Telemetry status is not streaming | Check your telemetry sinks configuartion. | DEBUG |
 | TieredFilesystemOverfillingSSD | Tiered filesystems' SSD capacity overfilling | A tiered filesystem exceeds a predefined threshold of SSD usage. In a tiered system, data should be offloaded (tiered) from the SSDs to object storage when SSD capacity starts to fill up. Resolve tiering connectivity problems or increase the upload bandwidth. | WARNING |
 | TooManyPendingClusterwideJobs | Too many pending cluster wide jobs | Consider changing the policy configuration. | DEBUG |
 | TraceDumperDown | Trace dumper is down | Contact the Customer Success Team to restart the trace dumper.' | DEBUG |
 | TracesDisabled | Traces are disabled | To turn the cluster traces, run the command 'weka debug traces start'. For more information, see the Traces management topic in the documentation. | DEBUG |
 | TracesFreezePeriodActive | Freeze traces is active | If the problem persists after the case is resolved, contact the Customer Success Team. | DEBUG |
 | UdpModePerformanceWarning | A backend container is configured in UDP mode | If this is a misconfiguration, add network devices to the specified backend container using the command ‚Äòweka cluster container net add‚Äô. | DEBUG |
 | UnwritableDisksConfigured | A drive is set to unwritable | If the drive remains unwritable after maintenance, contact the Customer Success Team. | DEBUG |
 | WTracerDaemonWriteIOFailures | WTracer Daemon has writes IO failures | See the event details in the System Events. | WARNING |
 | WTracerLostTraces | WTracer Lost Traces | See the event details in the System Events. | MINOR |

<!-- ============================================ -->
<!-- File 155/259: operation-guide_statistics.md -->
<!-- ============================================ -->

---
description:
---

# Statistics

As the WEKA system runs, it collects hundreds of statistics on system performance. These statistics help analyze the WEKA system performance and determine the source of any issue.

The statistics categories of the basic charts include:

* CPU
* Object Store
* Operations
* Operations (Driver)
* Operations (NFS)
* Operations (NFSw)
* SSD

When you select each category, a list of the possible statistics related to the category is displayed, from which you can select a specific chart.

The system also provides advanced statistic charts aimed to be used by the Customer Success Team.

The default statistics page displays charts of the last hour of operation, presenting the system operation average value per second in one minute range.

## **Drill-down options**

This Statistics page provides several options to drill down into the charts according to the selected category.

The options include:

* Move the mouse over the scrollable chart area to view the performance metrics of the WEKA cluster.
* Troubleshoot or obtain a correlation between events and performance using links to events that occurred.
* Add charts to the Statistics page, or remove charts.
* Display different charts of up to five on the statistics page. The default statistics page shows OPS (total), Throughput (total), and read/write latency for the last hour. You can change the interval by selecting the Hour, Day, or Week buttons or specifying a timeframe.
* Display and zoom in on statistics from defined timelines and dates.
* Bookmark specific statistics for future reference and share them with others (using the URL).

Note: The page shows only the statistics of the backend servers and clients in the cluster. The page does not show statistics in the following cases:
* A backend server is removed.
* A client is not connected to the cluster for more than the [retention period](statistics-1#set-statistics-retention).
The WEKA cluster does not hold historical statistics data. For historical statistics data, use `weka-mon` (see [Set up the WEKAmon external monitoring](../monitor-the-weka-cluster/external-monitoring)).

**Related topics**

<!-- ============================================ -->
<!-- File 156/259: operation-guide_statistics_statistics.md -->
<!-- ============================================ -->

---
description: This page describes how to manage the statistics using the GUI.
---

# Manage statistics using the GUI

Using the GUI, you can:

* View the statistics page
* Add a chart to the statistics page
* Remove a chart from the statistics page
* Set the timeframe
* Display events from a chart

## View the statistics page

**Procedure**

1. From the menu, select **Investigate > Statistics**.

2\. To view statistics of predefined templates for commonly used categories, select \
    **Predefined Templates** and from the list select the required statistics template.\
    The following predefined templates are available:

* **Protocols:** System throughput breakdown by protocol access.
* **Driver:** Throughput, latency, and OPS generated by native clients.
* **S3:** Throughput, latency, and OPS generated by S3 clients
* **NFS-W:** Throughput, latency, and OPS generated by NFS clients.
* **OBS:** Throughput and latency generated while interacting with object store buckets.

## Add a chart to the statistics page <a href="#add-a-chart-to-the-statistics-page" id="add-a-chart-to-the-statistics-page"></a>

You can add charts to the statistics page to display up to a maximum of five charts.

**Procedure**

1. From the menu, select **Investigate > Statistics**.
2. On the Statistics page, select **+Add**.
3. In the Add Chart dialog, do one of the following:
   * From the Categories pane, select a category, and then from the Statistics Name pane select the required chart. You multiple charts (up to 5) at once. The number next to the category name indicates the number of charts already selected.
   * Search for a chart using the **Filter**. Type a keyword or two related to the chart, and then from the Statistics Name pane select the required chart.

## Remove a chart from the statistics page <a href="#remove-a-chart-from-the-statistics-page" id="remove-a-chart-from-the-statistics-page"></a>

You can remove a chart that is no longer required to free space for adding another chart to the statistics page. For example, if the Statistics page already has the maximum number of five charts.

**Procedure**

1. On the upper left corner of the chart, select **X**.

## Set the timeframe  <a href="#set-the-timeframe" id="set-the-timeframe"></a>

The Statistics page contains a time axis for all the displayed charts. To investigate charts in a specific timeframe, you can set the interval in the time axis to the last hour, last day, or last week. You can also set a timeframe for a specific period (start and end time).

**Procedure**

1. To display the charts for the last period: **Hour**, **Day**, or **Week**, in the **Last** line, select the relevant button.
2. To display the charts for a specific period, in the **Range** line select the calendar, and set the start time and end time for the timeframe.

## Display events from a chart <a href="#display-events-from-a-chart" id="display-events-from-a-chart"></a>

If events occur during the period of the displayed charts, a purple box indicates the number of the events. To investigate the events, show and correlate them with the statistics data.

**Procedure**

1. On the time axis, select the purple box (it only appears if events occur).
2. From the popup box, select **Show All**.

<!-- ============================================ -->
<!-- File 157/259: operation-guide_statistics_statistics-1.md -->
<!-- ============================================ -->

---
description: This page describes how to manage the statistics using the CLI.
---

# Manage statistics using the CLI

Using the CLI, you can:

* List statistics types
* View statistics in real-time
* View statistics over time
* Set statistics retention

## List statistics types

**Command:** `weka stats list-types`

Use the following command line to obtain statistics definition information:\
`weka stats list-types [<name-or-category>] [--show-internal]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | name-or-category | Limit the output to a specific statistic name, category, or category label (in parenthesis). |  |
 | show-internal | Include internal statistics in the output. | False |

**Examples**

* Filter by category label ( `"api statistics"` )

```bash
$ weka stats list-types "api statistics"
CATEGORY  CATEGORY LABEL  IDENTIFIER    DESCRIPTION         LABEL                       TYPE      UNIT      PARAMETERS  RELATED RATE PARAMETER  PERMISSION  FOR NODE TYPE  CAN ACCUMULATE  HISTOGRAM  HISTOGRAM UNIT
api       api statistics  TOTAL_2xx_RQ  Total 2xx requests  Total 2xx requests (total)  Absolute  Requests                                      USER        MANAGEMENT     True            False
api       api statistics  TOTAL_3xx_RQ  Total 3xx requests  Total 3xx requests (total)  Absolute  Requests                                      USER        MANAGEMENT     True            False
api       api statistics  TOTAL_429_RQ  Total 429 requests  Total 429 requests (total)  Absolute  Requests                                      USER        MANAGEMENT     True            False
api       api statistics  TOTAL_4xx_RQ  Total 4xx requests  Total 4xx requests (total)  Absolute  Requests                                      USER        MANAGEMENT     True            False
api       api statistics  TOTAL_5xx_RQ  Total 5xx requests  Total 5xx requests (total)  Absolute  Requests                                      USER        MANAGEMENT     True            False
```

* Filter by name and category ( `api` )

```bash
$ weka stats list-types api
CATEGORY  CATEGORY LABEL   IDENTIFIER    DESCRIPTION                 LABEL                       TYPE         UNIT          PARAMETERS  RELATED RATE PARAMETER  PERMISSION  FOR NODE TYPE  CAN ACCUMULATE  HISTOGRAM  HISTOGRAM UNIT
api       api statistics   TOTAL_2xx_RQ  Total 2xx requests          Total 2xx requests (total)  Absolute     Requests                                          USER        MANAGEMENT     True            False
api       api statistics   TOTAL_3xx_RQ  Total 3xx requests          Total 3xx requests (total)  Absolute     Requests                                          USER        MANAGEMENT     True            False
api       api statistics   TOTAL_429_RQ  Total 429 requests          Total 429 requests (total)  Absolute     Requests                                          USER        MANAGEMENT     True            False
api       api statistics   TOTAL_4xx_RQ  Total 4xx requests          Total 4xx requests (total)  Absolute     Requests                                          USER        MANAGEMENT     True            False
api       api statistics   TOTAL_5xx_RQ  Total 5xx requests          Total 5xx requests (total)  Absolute     Requests                                          USER        MANAGEMENT     True            False
ops_s3    Operations (S3)  API_FAILURES  Total of failures per API   Failures per API (total)    Accumulated  Ops           api                                 USER        MANAGEMENT     False           False
ops_s3    Operations (S3)  API_OPS       Total of Ops per API        Ops per API (total)         Accumulated  Ops           api                                 USER        MANAGEMENT     False           False
ops_s3    Operations (S3)  API_TTFB      Time To First Byte per API  TTFB per API (sum)          Accumulated  Milliseconds  api                                 USER        MANAGEMENT     False           False
ops_s3    Operations (S3)  API_TTLB      Time To Last Byte per API   TTLB per API (sum)          Accumulated  Milliseconds  api                                 USER        MANAGEMENT     False           False
```

* Include internal stats and filter by name and category ( `config` )

```bash
$ weka stats list-types config --show-internal
CATEGORY  CATEGORY LABEL  IDENTIFIER                                                  DESCRIPTION                                                                                                     LABEL                                                                                           TYPE         UNIT                                                   PARAMETERS                                           RELATED RATE PARAMETER       PERMISSION  FOR NODE TYPE  CAN ACCUMULATE  HISTOGRAM  HISTOGRAM UNIT
cluster   Processes       PEER_CONFIGURE_FAILURES                                     How many times the node failed to configure peers to sync with them                                             Peer configure failures detected (total)                                                        Absolute     Peer configure failures                                                                                                                  WEKA        ANY            True            False
config    Config          AVERAGE_CHANGES_IN_CHANGESET                                The average number of changes in a changeset                                                                    Average (total)                                                                                 Rate         Changes/Sec                                                                                                                              WEKA        MANAGEMENT     True            False
config    Config          AVERAGE_CHANGES_IN_GENERATION                               The average number of changes in a generation                                                                   Average (total)                                                                                 Rate         Changes/Sec                                                                                                                              WEKA        MANAGEMENT     True            False
config    Config          BACKEND_NODE_REJOIN_TIME                                    The number of backends rejoin attempts per completion time range                                                Backend node rejoin time (total)                                                                Histogram    Number of rejoins                                                                                                                        WEKA        MANAGEMENT     True            True       mSecs
config    Config          CHANGESET_COMMIT_LATENCY                                    The average latency of committing a configuration changeset                                                     Commit changeset latency (per changeset commits)                                                Measured     Microseconds                                                                                                TOTAL_CHANGESETS_COMMITTED   WEKA        MANAGEMENT     True            False
config    Config          CLIENT_NODE_REJOIN_TIME                                     The number of clients rejoin attempts per completion time range                                                 Client node rejoin time (total)                                                                 Histogram    Number of rejoins                                                                                                                        WEKA        MANAGEMENT     True            True       mSecs
config    Config          CONFIG_PROPAGATION_LATENCY                                  The latencies of propagation of a configuration generation                                                      Config Propagation latency (total)                                                              Histogram    Generation                                                                                                                               WEKA        MANAGEMENT     True            True       Milliseconds
config    Config          FetchLocalStateChangesCallType_INTERNAL_CONTINUE            Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type                         Number of RPC calls to fetch local-state LRU changes per INTERNAL & CONTINUE (total)            Absolute     RPC Calls                                              fetchLocalStateChangesCallType, overlayTreeNodeType                               WEKA        MANAGEMENT     True            False
config    Config          FetchLocalStateChangesCallType_INTERNAL_RESTART_FROM_TAIL   Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type                         Number of RPC calls to fetch local-state LRU changes per INTERNAL & RESTART_FROM_TAIL (total)   Absolute     RPC Calls                                              fetchLocalStateChangesCallType, overlayTreeNodeType                               WEKA        MANAGEMENT     True            False
config    Config          FetchLocalStateChangesCallType_INTERNAL_RETRY_LAST_REQUEST  Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type                         Number of RPC calls to fetch local-state LRU changes per INTERNAL & RETRY_LAST_REQUEST (total)  Absolute     RPC Calls                                              fetchLocalStateChangesCallType, overlayTreeNodeType                               WEKA        MANAGEMENT     True            False
config    Config          FetchLocalStateChangesCallType_LEAF_CONTINUE                Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type                         Number of RPC calls to fetch local-state LRU changes per LEAF & CONTINUE (total)                Absolute     RPC Calls                                              fetchLocalStateChangesCallType, overlayTreeNodeType                               WEKA        MANAGEMENT     True            False
config    Config          FetchLocalStateChangesCallType_LEAF_RESTART_FROM_TAIL       Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type                         Number of RPC calls to fetch local-state LRU changes per LEAF & RESTART_FROM_TAIL (total)       Absolute     RPC Calls                                              fetchLocalStateChangesCallType, overlayTreeNodeType                               WEKA        MANAGEMENT     True            False
config    Config          FetchLocalStateChangesCallType_LEAF_RETRY_LAST_REQUEST      Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type                         Number of RPC calls to fetch local-state LRU changes per LEAF & RETRY_LAST_REQUEST (total)      Absolute     RPC Calls                                              fetchLocalStateChangesCallType, overlayTreeNodeType                               WEKA        MANAGEMENT     True            False
config    Config          GENERATION_COMMIT_LATENCY                                   The average latency of committing a configuration generation to the RAFT log                                    Commit changeset latency (per changeset commits)                                                Measured     Microseconds                                                                                                TOTAL_GENERATIONS_COMMITTED  WEKA        MANAGEMENT     True            False
config    Config          HEARTBEAT_PROCESSING_TIME                                   The number of non-leader heartbeats per processing time range                                                   Non-leader heartbeat processing time (total)                                                    Histogram    Number of heartbeats                                                                                                                     WEKA        MANAGEMENT     True            True       secs
config    Config          HEARTBEAT_PROCESSING_TIME_OLD                               The number of non-leader heartbeats per processing time range (OLD)                                             Non-leader heartbeat processing time (total)                                                    Histogram    Number of heartbeats                                                                                                                     WEKA        MANAGEMENT     True            True       secs
config    Config          HISTOGRAM_LEADER_ITERATION_WAIT_DURATION_CONFIG_ALIGNMENT   Wait duration of leader iteration for all nodes to align on latest configuration generation                     Leader Iteration Wait for Configuration Propagation (total)                                     Histogram    Leader iteration wait time                                                                                                               WEKA        MANAGEMENT     True            True       mSecs
config    Config          LEADER_HEARTBEAT_PROCESSING_TIME                            The number of leader heartbeats per processing time range                                                       Leader heartbeat processing time (total)                                                        Histogram    Number of heartbeats                                                                                                                     WEKA        MANAGEMENT     True            True       secs
config    Config          LEADER_HEARTBEAT_PROCESSING_TIME_OLD                        The number of leader heartbeats per processing time range (OLD)                                                 Leader heartbeat processing time (total)                                                        Histogram    Number of heartbeats                                                                                                                     WEKA        MANAGEMENT     True            True       secs
config    Config          LOCALSTATE_AGGREGATION_LATENCY                              This period between clockSkewReportTime table's update by a management process and the time the leader sees it  LocalState aggregation latency (total)                                                          Histogram    Time is taken to aggregate LocalState in milliseconds                                                                                    WEKA        MANAGEMENT     True            True       msecs
config    Config          LOCAL_STATS_FETCH_GENERATION_LAGGING                        The number of local-state generations that the parent fetch request still needs to read                         local-state generation dirty LRU length (average)                                               Accumulated  local-state generations                                overlayParentIndex                                                                WEKA        MANAGEMENT     False           False
config    Config          OVERLAY_FULL_SHIFTS                                         The number of entire overlay shifts                                                                             Full overlay shifts (total)                                                                     Absolute     Changes                                                                                                                                  WEKA        MANAGEMENT     True            False
config    Config          OVERLAY_INCREMENTAL_SHIFTS                                  The number of incremental overlay shifts                                                                        Incremental overlay shifts (total)                                                              Absolute     Changes                                                                                                                                  WEKA        MANAGEMENT     True            False
config    Config          OVERLAY_TRACKER_INCREMENTALS                                The number of incremental OverlayTracker applications                                                           Incremental OverlayTracker applications (total)                                                 Absolute     Changes                                                                                                                                  WEKA        MANAGEMENT     True            False
config    Config          OVERLAY_TRACKER_RESYNCS                                     The number of OverlayTracker full-resyncs                                                                       OverlayTracker resyncs (total)                                                                  Absolute     Changes                                                                                                                                  WEKA        MANAGEMENT     True            False
config    Config          TOTAL_CHANGESETS_COMMITTED                                  The total number of committed changesets                                                                        Changeset commits (total)                                                                       Absolute     Change Sets                                                                                                                              WEKA        MANAGEMENT     True            False
config    Config          TOTAL_COMMITTED_CHANGES                                     The total number of committed configuration change sets                                                         Changes (total)                                                                                 Absolute     Changes                                                                                                                                  WEKA        MANAGEMENT     True            False
config    Config          TOTAL_CONFIG_SNAPSHOT_PULLS                                 The total number of config snapshot pulls                                                                       Config snapshot pulls (total)                                                                   Absolute     Pulls                                                                                                                                    WEKA        MANAGEMENT     True            False
config    Config          TOTAL_GENERATIONS_COMMITTED                                 The number of committed generations                                                                             Config generation commits (total)                                                               Absolute     Generations                                                                                                                              WEKA        MANAGEMENT     True            False
raft      RAFT            Configuration_LEADER_CHANGES                                Changes of leader                                                                                               Configuration Leader Changes (total)                                                            Absolute     Changes                                                councilType                                                                       WEKA        ANY            True            False
raft      RAFT            Configuration_REQUESTS_COMPLETED                            Requests to leader completed successfully                                                                       Configuration Requests Completed (total)                                                        Absolute     Requests                                               councilType                                                                       WEKA        ANY            True            False
```

## View statistics in real-time

**Command:** `weka stats realtime`

Use the following command line to obtain the current performance-related statistics of the processes in a one-second interval:\
`weka stats realtime [<process-ids>] [--raw-units] [--UTC]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | process-ids | Only show real-time stats of the specified processes in a comma-separated list. |  |
 | raw-units | Print values in raw units such as bytes and seconds. | Readable format.Examples: 1KiB 234MiB 2GiB. |
 | UTC | Print times in UTC. | Server's local time. |

## **View statistics over time**

**Command:** `weka stats`

The collected statistics can help analyze system performance and determine the source of issues that may occur during WEKA system runs. Statistics are divided according to categories. When selecting a category, a list of the possible statistics is displayed, from which you can select the specific statistics.

With the exception of real-time statistics, WEKA averages all statistics over one-minute intervals. Therefore, the total value or other aggregates relate to a specific minute.

Use the following command line to manage filters and read statistics:

`weka stats [--start-time <start-time>] [--end-time <end-time>] [--interval interval] [--resolution-secs resolution-secs] [--category category][--stat stat] [--process-ids process-ids] [--param param] [--accumulated] [--per-process] [--no-zeros] [--show-internal] [--raw-units] [--UTC]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | start-time | Start time of the reported period.Format examples: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 9:15Z, 10:00+2:00. | -1m |
 | end-time | End time of the reported period.Format examples: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 9:15Z, 10:00+2:00. | Current time |
 | interval* | Period of time to be reported.Valid interval in seconds (positive integer). |  |
 | resolution-secs | Length of each interval in the reported period.The value must be a multiple of 60 seconds | 60 |
 | category | A specific category for retrieving statistics.Valid categories: CPU, Object Store, Operations, Operations (NFS), Operations (Driver), SSD. | All |
 | stat | The names of the statistics to retrieve. | All |
 | process-ids | A valid process ID. | All |
 | param | For parameterized statistics, it retrieves only the instantiations where the specified parameter has the specified value.Format: key:valExample for multiple values:'--param method:putBlocks --param method:initBlock' |  |
 | accumulated | Display accumulated statistics instead of rate statistics. | False |
 | per-process | Do not aggregate statistics across processes. | False |
 | no-zeros | Filters out results where the value is 0. | False |
 | show-internal | Display internal statistics. | False |
 | raw-units | Print values in raw units, such as bytes and seconds. | Readable format.(for example: 1KiB 234MiB 2GiB) |
 | UTC | Print times in UTC. | Server's local time. |

## Set statistics retention

**Command:** `weka stats retention set`

Use the following command line to set the statistics retention period.\
`weka stats retention set <--days days> [--dry-run]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | days* | Number of days to keep the statistics.Ensure sufficient free disk space per server and the specified number of days. |
 | dry-run | Only tests the required capacity per the retention period. |

Use `weka stats retention status` to view the current retention and `weka stats retention restore-default` to restore the default retention settings.

<!-- ============================================ -->
<!-- File 158/259: operation-guide_statistics_statistics-list.md -->
<!-- ============================================ -->

---
description:
---

# Statistics list

### api statistics

 | **Type** | **Description** | **Units** |
 | -------------- | ------------------ | --------- |
 | TOTAL_2xx_RQ | Total 2xx requests | Requests |
 | TOTAL_3xx_RQ | Total 3xx requests | Requests |
 | TOTAL_429_RQ | Total 429 requests | Requests |
 | TOTAL_4xx_RQ | Total 4xx requests | Requests |
 | TOTAL_5xx_RQ | Total 5xx requests | Requests |

### Assert failures

 | **Type** | **Description** | **Units** |
 | ------------------------------------------------- | ---------------------------------------------------------------------- | ------------------ |
 | ASSERTION_FAILURES_IGNORE_SOFT | Assertion failures count with "IGNORE_SOFT" behaviour | Assertion failures |
 | ASSERTION_FAILURES_IGNORE | Assertion failures count with "IGNORE" behaviour | Assertion failures |
 | ASSERTION_FAILURES_KILL_BUCKET | Assertion failures count with "KILL_BUCKET" behaviour | Assertion failures |
 | ASSERTION_FAILURES_KILL_FIBER | Assertion failures count with "KILL_FIBER" behaviour | Assertion failures |
 | ASSERTION_FAILURES_KILL_NODE_OOM | Assertion failures count with "KILL_NODE_OOM" behaviour | Assertion failures |
 | ASSERTION_FAILURES_KILL_NODE_WITH_CORE_DUMP | Assertion failures count with "KILL_NODE_WITH_CORE_DUMP" behaviour | Assertion failures |
 | ASSERTION_FAILURES_KILL_NODE | Assertion failures count with "KILL_NODE" behaviour | Assertion failures |
 | ASSERTION_FAILURES_STALL_AND_KILL_NODE | Assertion failures count with "STALL_AND_KILL_NODE" behaviour | Assertion failures |
 | ASSERTION_FAILURES_STALL | Assertion failures count with "STALL" behaviour | Assertion failures |
 | ASSERTION_FAILURES_THROW_EXCEPTION | Assertion failures count with "THROW_EXCEPTION" behaviour | Assertion failures |
 | ASSERTION_FAILURES | Assertion failures count of all available types | Assertion failures |

### Attribute Cache

 | **Type** | **Description** | **Units** |
 | ------------------------ | --------------------------------------------------------- | --------- |
 | GP_GETATTR_CACHE_MISS | Number of general purpose getAttr cache misses per second | Ops/Sec |
 | GP_GETATTR | Number of general purpose getAttr calls per second | Ops/Sec |

### Audit

 | **Type** | **Description** | **Units** |
 | ------------------------------ | ---------------------------------------------------------- | --------- |
 | AUDIT_ACQUIRE_CHARTER | Number of audit traces created for ACQUIRE_CHARTER | Audits |
 | AUDIT_CREATE_DIRENT | Number of audit traces created for CREATE_DIRENT | Audits |
 | AUDIT_CREATE_INODE | Number of audit traces created for CREATE_INODE | Audits |
 | AUDIT_CREATE_OPEN_DIRENT | Number of audit traces created for CREATE_OPEN_DIRENT | Audits |
 | AUDIT_CREATE_UNLINKED_INODE | Number of audit traces created for CREATE_UNLINKED_INODE | Audits |
 | AUDIT_DEREFERENCE_DIRENT | Number of audit traces created for DEREFERENCE_DIRENT | Audits |
 | AUDIT_GET_ATTR | Number of audit traces created for GET_ATTR | Audits |
 | AUDIT_GET_XATTR | Number of audit traces created for GET_XATTR | Audits |
 | AUDIT_HEARTBEAT | Number of audit traces created for HEARTBEAT | Audits |
 | AUDIT_LINK | Number of audit traces created for LINK | Audits |
 | AUDIT_LIST_XATTR | Number of audit traces created for LIST_XATTR | Audits |
 | AUDIT_MOUNT | Number of audit traces created for MOUNT | Audits |
 | AUDIT_READDIR | Number of audit traces created for READDIR | Audits |
 | AUDIT_READLINK | Number of audit traces created for READLINK | Audits |
 | AUDIT_REMOVE_XATTR | Number of audit traces created for REMOVE_XATTR | Audits |
 | AUDIT_RENAME | Number of audit traces created for RENAME | Audits |
 | AUDIT_SET_ATTR | Number of audit traces created for SET_ATTR | Audits |
 | AUDIT_SET_XATTR | Number of audit traces created for SET_XATTR | Audits |
 | AUDIT_UMOUNT | Number of audit traces created for UMOUNT | Audits |
 | AUDIT_UNLINK_DIR | Number of audit traces created for UNLINK_DIR | Audits |
 | AUDIT_UNLINK_FILE | Number of audit traces created for UNLINK_FILE | Audits |
 | AUDITS | Number of audits traces created for all types | Audits |

### Audit Enhancer Statistics

 | **Type** | **Description** | **Units** |
 | ----------------------------------------- | ------------------------------------------------------------------------------------------------ | -------------------- |
 | BACKEND_NAME_RESOLVE_BATCH_REQUESTS | Number of resolves to backend batches | Requests |
 | CACHE_ADDED_ENTRIES | Number of entries added to cache | Entries |
 | CACHE_EVICT_ENTRIES | Number of entries evicted from cache | Entries |
 | CACHE_NUM_ENTRIES | Number of entries in cache | Entries in cache |
 | CACHE_OVERWRITTEN_ENTRIES | Number of entries overwritten in the cache | Entries |
 | CACHE_PRELOAD_HITS | Number of cache hits when trying to preload | Entries |
 | CACHE_PRELOAD_MISSES | Number of cache misses when trying to preload | Entries |
 | CACHE_RESOLVE_HITS | Number of cache hits to resolve | Entries |
 | CACHE_RESOLVE_MISSES | Number of cache misses when trying to resolve | Entries |
 | ENHANCE_BACKEND_CALL_AVG_TIME | Per backend call average time | hnsecs |
 | ENHANCE_BACKEND_CALL_MAX_TIME | Per backend call maximum time | hnsecs |
 | ENHANCE_BACKEND_CALL_MIN_TIME | Per backend call minimum time | hnsecs |
 | ENHANCE_BACKEND_CALL_TOTAL_TIME | Total backend call time | hnsecs |
 | ENHANCE_RESOLVE_PATH_ATTEMPTS_COUNT | Number of resolve paths | Count |
 | ENHANCE_RESOLVE_PATH_AVG_TIME | Average resolve path time | hnsecs |
 | ENHANCE_RESOLVE_PATH_MAX_TIME | Maximum resolve path time | hnsecs |
 | ENHANCE_RESOLVE_PATH_MIN_TIME | Minimum resolve path time | hnsecs |
 | ENHANCE_RESOLVE_PATH_TOTAL_TIME | Total resolve path time | hnsecs |
 | ENHANCE_SLEEP_TIME | Sleep time when backend call fails | hnsecs |
 | ENHANCER_BATCH_COUNT | Total batch count | Batches |
 | ENHANCER_ENHANCED_TOTAL_ENTRIES | Total entries enhanced | Entries |
 | ENHANCER_PER_BATCH_AVG_ENTRIES | Average entries in a batch | Entries/Batch |
 | ENHANCER_PER_BATCH_MAX_ENTRIES | Maximum entries in a batch | Entries/Batch |
 | ENHANCER_PER_BATCH_MIN_ENTRIES | Minimum entries in a batch | Entries/Batch |
 | INODES_TO_RESOLVE_NAME | Number of inodes to resolve name | Inodes |
 | NAME_RESOLVE_FAIL_NAME_NOT_IN_CACHE | Number of times a path could not be resolved, as one of the directory names was not in the cache | Failed path resolves |
 | NAME_RESOLVE_REQUESTS_FAIL | Number of resolve name queries failed | Requests failed |
 | NAME_RESOLVE_REQUESTS_SUCCESS | Number of resolved name queries succeeded | Requests succeeded |
 | NAME_RESOLVE_REQUESTS | Number of resolve name queries issued to backend | Requests |
 | PER_BATCH_ENHANCE_AVG_TIME | Per batch enhance average processing time | hnsecs |
 | PER_BATCH_ENHANCE_MAX_TIME | Per batch enhance maximum processing time | hnsecs |
 | PER_BATCH_ENHANCE_MIN_TIME | Per batch enhance minimum processing time | hnsecs |
 | PER_BATCH_ENHANCE_PRELOAD_AVG_TIME | Per batch enhance average preload time | hnsecs |
 | PER_BATCH_ENHANCE_PRELOAD_MAX_TIME | Per batch enhance maximum preload time | hnsecs |
 | PER_BATCH_ENHANCE_PRELOAD_MIN_TIME | Per batch enhance minimum preload time | hnsecs |
 | PER_BATCH_ENHANCE_PRELOAD_TOTAL_TIME | Total enhance preload time | hnsecs |
 | PER_BATCH_ENHANCE_TOTAL_TIME | Total enhance batch processing time | hnsecs |
 | PRELOAD_CACHE_COUNT | Number of cache preloads | Number of attempts |

### Block Cache

 | **Type** | **Description** | **Units** |
 | ------------------------------------ | ------------------------------------------ | --------- |
 | BUCKET_CACHE_METADATA_HITS | Bucket block cache metadata hits | Queries |
 | BUCKET_CACHE_METADATA_MISSES | Bucket block cache metadata misses | Queries |
 | BUCKET_CACHE_REGISTRY_L2_HITS | Bucket block cache registry L2 hits | Queries |
 | BUCKET_CACHE_REGISTRY_L2_MISSES | Bucket block cache registry L2 misses | Queries |
 | BUCKET_CACHED_METADATA_BLOCKS | Bucket number of cached metadata blocks | Blocks |
 | BUCKET_CACHED_REGISTRY_L2_BLOCKS | Bucket number of cached registry L2 blocks | Blocks |
 | BUCKET_REGISTRY_L2_BLOCKS_NUM | Bucket number of registry L2 blocks | Blocks |

### Block Writes

 | **Type** | **Description** | **Units** |
 | ---------------------- | ------------------------------ | --------- |
 | BLOCK_FULL_WRITES | Number of full block writes | Writes |
 | BLOCK_PARTIAL_WRITES | Number of partial block writes | Writes |

### Bucket

 | **Type** | **Description** | **Units** |
 | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ------------ |
 | BUCKET_SESSION_VALIDATION_LATENCY | Average latency of bucket session validation | Microseconds |
 | BUCKET_SESSION_VALIDATIONS | Number of bucket session validations per second | RPCs/Sec |
 | BUCKET_START_TIME | Duration of bucket activation on step up | Startups |
 | CHOKING_LEVEL_ALL | Throttling level applied on all types of IOs | % |
 | CHOKING_LEVEL_NON_MUTATING | Throttling level applied on non-mutating only types of IOs | % |
 | COALESCED_MAY_CREATE_EXTENT | Number of mayCreateExtent calls coalesced | Calls |
 | DESTAGE_COUNT | Number of destages per second | Destages/Sec |
 | DESTAGED_BLOCKS_COUNT | Number of destaged blocks per second | Blocks/Sec |
 | DIR_MOVE_TIME | Time to complete a directory move | Ops |
 | EXTENT_BLOCK_SEQUENCES | Histogram of the number of consecutive sequences of blocks in a single extent | Extents |
 | EXTENT_BLOCKS_COUNT | Difference in number of EXTENT blocks | Blocks |
 | FAIRNESS_DELAYED_MAY_CREATE_EXTENT | Number of mayCreateExtent calls not coalesced to prevent starvation | Calls |
 | FREEABLE_LRU_BUFFERS | Number of unused blocks in LRU cache | Buffers |
 | HASH_BLOCKS_COUNT | Difference in number of HASH blocks | Blocks |
 | INODE_BLOCKS_COUNT | Difference in number of INODE blocks | Blocks |
 | INTEGRITY_ISSUES | Number of filesystem integrity issues detected | Issues |
 | JOURNAL_BLOCKS_COUNT | Difference in number of JOURNAL blocks | Blocks |
 | NOOP_JOURNALS | Number of NOOP journals | Ops/Sec |
 | NOOP_REPLAYS | Number of NOOP replays | Ops |
 | ODH_COLLISIONS_ACCESS_CLOCK_STATES | Number of ODH items created with colliding hash in ACCESS_CLOCK_STATES ODH | Collisions |
 | ODH_COLLISIONS_BIG_BLOB_MANIFEST | Number of ODH items created with colliding hash in BIG_BLOB_MANIFEST ODH | Collisions |
 | ODH_COLLISIONS_DEFAULT_DIR_QUOTA | Number of ODH items created with colliding hash in DEFAULT_DIR_QUOTA ODH | Collisions |
 | ODH_COLLISIONS_DIR_QUOTA | Number of ODH items created with colliding hash in DIR_QUOTA ODH | Collisions |
 | ODH_COLLISIONS_DIRECTORY | Number of ODH items created with colliding hash in DIRECTORY ODH | Collisions |
 | ODH_COLLISIONS_FLOCK_EXPIRED_FRONTENDS_WNID | Number of ODH items created with colliding hash in FLOCK_EXPIRED_FRONTENDS_WNID ODH | Collisions |
 | ODH_COLLISIONS_FLOCK_EXPIRED_FRONTENDS | Number of ODH items created with colliding hash in FLOCK_EXPIRED_FRONTENDS ODH | Collisions |
 | ODH_COLLISIONS_GRAVEYARD | Number of ODH items created with colliding hash in GRAVEYARD ODH | Collisions |
 | ODH_COLLISIONS_INODES_PENDING_VALIDATIONS | Number of ODH items created with colliding hash in INODES_PENDING_VALIDATIONS ODH | Collisions |
 | ODH_COLLISIONS_INODES_POTENTIAL_PENDING_DELETION | Number of ODH items created with colliding hash in INODES_POTENTIAL_PENDING_DELETION ODH | Collisions |
 | ODH_COLLISIONS_MODIFY_CLOCK_STATES | Number of ODH items created with colliding hash in MODIFY_CLOCK_STATES ODH | Collisions |
 | ODH_COLLISIONS_OBS_IMMEDIATE_RELEASE | Number of ODH items created with colliding hash in OBS_IMMEDIATE_RELEASE ODH | Collisions |
 | ODH_COLLISIONS_OBS_RECLAMATION | Number of ODH items created with colliding hash in OBS_RECLAMATION ODH | Collisions |
 | ODH_COLLISIONS_REFERENCE_RELOCATIONS | Number of ODH items created with colliding hash in REFERENCE_RELOCATIONS ODH | Collisions |
 | ODH_COLLISIONS_SNAP_LAYER_CAPACITY | Number of ODH items created with colliding hash in SNAP_LAYER_CAPACITY ODH | Collisions |
 | ODH_COLLISIONS_SNAP_LAYER_SIZE_V4_3 | Number of ODH items created with colliding hash in SNAP_LAYER_SIZE_V4_3 ODH | Collisions |
 | ODH_COLLISIONS_SNAPSHOT_MEMBERS | Number of ODH items created with colliding hash in SNAPSHOT_MEMBERS ODH | Collisions |
 | ODH_COLLISIONS_STOW_DOWNLOAD_REDISTRIBUTE_PULL_STATE_V4_3 | Number of ODH items created with colliding hash in STOW_DOWNLOAD_REDISTRIBUTE_PULL_STATE_V4_3 ODH | Collisions |
 | ODH_COLLISIONS_STOW_DOWNLOAD_REDISTRIBUTE_V4_3 | Number of ODH items created with colliding hash in STOW_DOWNLOAD_REDISTRIBUTE_V4_3 ODH | Collisions |
 | ODH_COLLISIONS_STOW_UPLOAD_MANIFEST | Number of ODH items created with colliding hash in STOW_UPLOAD_MANIFEST ODH | Collisions |
 | ODH_COLLISIONS_SV_CAPACITY_LEADER | Number of ODH items created with colliding hash in SV_CAPACITY_LEADER ODH | Collisions |
 | ODH_COLLISIONS_UNLINKED_INODES | Number of ODH items created with colliding hash in UNLINKED_INODES ODH | Collisions |
 | ODH_COLLISIONS | Number of ODH items created with colliding hash in all ODHs | Collisions |
 | ODL_BLOCKS_COUNT | Difference in number of ODL blocks | Blocks |
 | ODL_PAYLOAD_BLOCKS_COUNT | Difference in number of ODL_PAYLOAD blocks | Blocks |
 | READ_BYTES | Number of bytes read per second | Bytes/Sec |
 | READ_LATENCY | Average latency of READ operations | Microseconds |
 | READS | Number of read operations per second | Ops/Sec |
 | REGISTRY_COLLISIONS | Number of registry items created with colliding key | Collisions |
 | REGISTRY_L1_BLOCKS_COUNT | Difference in number of REGISTRY_L1 blocks | Blocks |
 | REGISTRY_L2_BLOCKS_COUNT | Difference in number of REGISTRY_L2 blocks | Blocks |
 | REGISTRY_SEARCHES_COUNT | Number of registry searches per second | Queries/Sec |
 | REJECTED_STALE_PUT_BLOCKS_FALSE_POSITIVES | Number of putBlocks RPCs falsely rejected due to stale serial number | RPCs/Sec |
 | REJECTED_STALE_PUT_BLOCKS | Number of putBlocks RPCs rejected due to stale serial number | RPCs/Sec |
 | RESIDENT_BLOCKS_COUNT | Number of blocks in the resident blocks table | Blocks |
 | SINGLE_HOP_MISMATCH_RECOVERY | Number of single-hop read prefix mismatch recoveries | Issues |
 | SINGLE_HOP_RDMA_MISMATCH_DPDK_FALLBACK | Number of single-hop read prefix mismatch RDMA failures | Issues |
 | SINGLE_HOP_WRITE_ATTEMPTS | Number of single hop write attempts | Ops/Sec |
 | SINGLE_HOP_WRITE_BYTES | Total single hop write bytes | Bytes/Sec |
 | SINGLE_HOP_WRITE_FAILURES | Number of single-hop write operation failures per second | Ops/Sec |
 | SINGLE_HOP_WRITES_BAD_CSUM | Number of single hop write operation (BAD_CSUM) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_FE_CALLBACK_FAIL | Number of single hop write operation (FE_CALLBACK_FAIL) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_FE_FAILOVER | Number of single hop write operation (FE_FAILOVER) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_MANUAL_OVERRIDE_DENY | Number of single hop write operation (MANUAL_OVERRIDE_DENY) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_NO_BYPASSING_STRIPES | Number of single hop write operation (NO_BYPASSING_STRIPES) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_OTHER_ERROR | Number of single hop write operation (OTHER_ERROR) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_SKIP | Number of single-hop write operations (SKIP) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_SSD_FAIL | Number of single hop write operation (SSD_FAIL) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_SUCCESS | Number of single-hop write operations (SUCCESS) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_TOO_MANY_PLACEMENTS | Number of single hop write operation (TOO_MANY_PLACEMENTS) per second | Ops/Sec |
 | SINGLE_HOP_WRITES_UNEXPECTED_FAIL | Number of single hop write operation (UNEXPECTED_FAIL) per second | Ops/Sec |
 | SNAPSHOT_CREATION_TIME | Time to complete a snapshot creation | Snapshots |
 | SPATIAL_DIGEST_BLOCKS_COUNT | Difference in number of SPATIAL_DIGEST blocks | Blocks |
 | SPATIAL_SQUELCH_BLOCKS_COUNT | Difference in number of SPATIAL_SQUELCH blocks | Blocks |
 | SUCCESSFUL_DATA_WEDGINGS | Number of successful attempts to wedge data blocks in journal per second | Attempts/Sec |
 | SUPERBLOCK_BLOCKS_COUNT | Difference in number of SUPERBLOCK blocks | Blocks |
 | TEMPORAL_SQUELCH_BLOCKS_COUNT | Difference in number of TEMPORAL_SQUELCH blocks | Blocks |
 | TRANSIENT_INTEGRITY_ISSUES | Number of transient filesystem integrity issues detected | Issues |
 | UNSUCCESSFUL_DATA_WEDGINGS | Number of unsuccessful attempts to wedge data blocks in journal per second | Attempts/Sec |
 | USED_L2_RESERVED_ENTRY | Number of uses of L2 reserved entries | Occurrences |
 | USER_DATA_BUFFERS_IN_USE | Number of data buffers used for serving ongoing IOs | Buffers |
 | WRITE_BYTES | Number of byte writes per second | Bytes/Sec |
 | WRITE_LATENCY | Average latency of WRITE operations | Microseconds |
 | WRITES | Number of write operations per second | Ops/Sec |

### Bucket Failovers

 | **Type** | **Description** | **Units** |
 | ----------------------------- | ---------------------------------------------------------------------------- | ---------- |
 | BUCKET_FAILOVERS | Number of failovers detected in remote buckets | Failovers |
 | REMOTE_BUCKET_IS_SECONDARY | Number of times a remote bucket reported it is secondary and cannot serve us | Exceptions |

### Bucket Rebalances

 | **Type** | **Description** | **Units** |
 | -------------------------------------- | --------------------------------------------------------- | --------------- |
 | BUCKET_INIT_LATENCY_HIST | Duration of bucket initialization | Initializations |
 | BUCKET_INIT_LATENCY | Average latency of bucket initialization | Seconds |
 | BUCKET_INITS | Number of bucket initializations | Times |
 | BUCKET_REBALANCER_STEPDOWN_REQUESTS | Number of bucket rebalancer stepdown requests of a bucket | Times |
 | INFORMATIVE_DENY_BUCKET_ACCESS | Number of new-style NotBucketLeaderEx exceptions | Exceptions |
 | LEGACY_DENY_BUCKET_ACCESS | Number of old-style NotBucketLeader exceptions | Exceptions |

### Charters

 | **Type** | **Description** | **Units** |
 | ----------------------------------- | ------------------------------ | --------- |
 | DEDGRADED_TO_READER_RELINQUISHES | Charter relinquishes by reason | charters |
 | EAGER_RELINQUISHES | Charter relinquishes by reason | charters |
 | LRU_EXPIRED_RELINQUISHES | Charter relinquishes by reason | charters |
 | LRU_LENGTH_RELINQUISHES | Charter relinquishes by reason | charters |
 | OUT_OF_SPACE_RELINQUISHES | Charter relinquishes by reason | charters |

### Choking

 | **Type** | **Description** | **Units** |
 | ----------------------------- | ---------------------------------------------------------------------------- | --------- |
 | CHOKING_LEVEL_ALL | Throttling level applied on all types of IOs, both mutating and non-mutating | Processes |
 | CHOKING_LEVEL_NON_MUTATING | Throttling level applied on non-mutating only types of IOs | Processes |

### Clients

 | **Type** | **Description** | **Units** |
 | --------------------- | ------------------------------------------------------------------------------- | ----------- |
 | CLIENTS_CONNECTED | Clients connected | Clients/Sec |
 | CLIENTS_DISCONNECTED | The number of clients left or removed | Clients/Sec |
 | CLIENTS_LEFT | The number of clients left | Clients/Sec |
 | CLIENTS_RECONNECTED | The number of clients reconnected instead of their previous connection instance | Clients/Sec |
 | CLIENTS_REMOVED | The number of clients removed | Clients/Sec |

### Cloud

 | **Type** | **Description** | **Units** |
 | ---------------------- | ------------------------------------------------------- | --------- |
 | TOTAL_PROXY_REQUESTS | Number of times the process used other nodes as a proxy | Times |

### Config

 | **Type** | **Description** | **Units** |
 | --------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
 | AVERAGE_CHANGES_IN_CHANGESET | The average number of changes in a changeset | Changes/Sec |
 | AVERAGE_CHANGES_IN_GENERATION | The average number of changes in a generation | Changes/Sec |
 | BACKEND_NODE_REJOIN_TIME | The number of backends rejoin attempts per completion time range | Number of rejoins |
 | CHANGESET_COMMIT_LATENCY | The average latency of committing a configuration changeset | Microseconds |
 | CLIENT_NODE_REJOIN_TIME | The number of clients rejoin attempts per completion time range | Number of rejoins |
 | CONFIG_PROPAGATION_LATENCY | The latencies of propagation of a configuration generation | Generation |
 | FetchLocalStateChangesCallType_INTERNAL_CONTINUE | Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type | RPC Calls |
 | FetchLocalStateChangesCallType_INTERNAL_RESTART_FROM_TAIL | Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type | RPC Calls |
 | FetchLocalStateChangesCallType_INTERNAL_RETRY_LAST_REQUEST | Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type | RPC Calls |
 | FetchLocalStateChangesCallType_LEAF_CONTINUE | Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type | RPC Calls |
 | FetchLocalStateChangesCallType_LEAF_RESTART_FROM_TAIL | Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type | RPC Calls |
 | FetchLocalStateChangesCallType_LEAF_RETRY_LAST_REQUEST | Number of RPC calls to fetch local-state LRU changes, per overlay node type & call type | RPC Calls |
 | GENERATION_COMMIT_LATENCY | The average latency of committing a configuration generation to the RAFT log | Microseconds |
 | HEARTBEAT_PROCESSING_TIME_OLD | The number of non-leader heartbeats per processing time range (OLD) | Number of heartbeats |
 | HEARTBEAT_PROCESSING_TIME | The number of non-leader heartbeats per processing time range | Number of heartbeats |
 | HISTOGRAM_LEADER_ITERATION_WAIT_DURATION_CONFIG_ALIGNMENT | Wait duration of leader iteration for all nodes to align on the latest configuration generation | Leader iteration wait time |
 | LEADER_HEARTBEAT_PROCESSING_TIME_OLD | The number of leader heartbeats per processing time range (OLD) | Number of heartbeats |
 | LEADER_HEARTBEAT_PROCESSING_TIME | The number of leader heartbeats per processing time range | Number of heartbeats |
 | LOCAL_STATS_FETCH_GENERATION_LAGGING | The number of local-state generations that the parent fetch request still needs to read | local-state generations |
 | LOCALSTATE_AGGREGATION_LATENCY | This period between clockSkewReportTime table's update by a management process and the time the leader sees it | Time is taken to aggregate LocalState in milliseconds |
 | OVERLAY_FULL_SHIFTS | The number of entire overlay shifts | Changes |
 | OVERLAY_INCREMENTAL_SHIFTS | The number of incremental overlay shifts | Changes |
 | OVERLAY_TRACKER_INCREMENTALS | The number of incremental OverlayTracker applications | Changes |
 | OVERLAY_TRACKER_RESYNCS | The number of OverlayTracker full-resyncs | Changes |
 | TOTAL_CHANGESETS_COMMITTED | The total number of committed changesets | Change Sets |
 | TOTAL_COMMITTED_CHANGES | The total number of committed configuration change sets | Changes |
 | TOTAL_CONFIG_SNAPSHOT_PULLS | The total number of config snapshot pulls | Pulls |
 | TOTAL_GENERATIONS_COMMITTED | The number of committed generations | Generations |

### CPU

 | **Type** | **Description** | **Units** |
 | ---------------- | ----------------------------------------------------- | --------- |
 | CPU_UTILIZATION | The percentage of the CPU time used for handling I/Os | % |

### Data Reduction

 | **Type** | **Description** | **Units** |
 | ----------------------------------- | -------------------------------------------------------------------------------------------------- | -------------- |
 | ACCEPTED_INGESTS | Number of ingests accepted by the extent | Blocks/Sec |
 | ACCEPTED_RELOCATES | Number of relocation accepted by the extent | Blocks/Sec |
 | ACCEPTED_SEGMENTS | Number of blocks accepted for clusterization | Blocks/Sec |
 | AVG_DELTAS | Average deltas per reference during ingestion (excluding history) | deltas/ref |
 | boxSize | Box sizes histogram | Segments |
 | CLUSTERIZE_CALLS | Clusterize Calls | Calls/Sec |
 | CLUSTERIZE_TIME | Average time to clusterize | Milliseconds |
 | COMPRESS_TASK_CALLS | Compress Task Calls | Calls/Sec |
 | COMPRESS_TASK_TIME | Average time to complete the compress task | Milliseconds |
 | COMPRESSED_DELTA_SIZE | Average size of new compressed delta segments | Bytes |
 | COMPRESSED_ETERNAL_SINGLE_SIZE | Average size of new compressed eternal-single segments | Bytes |
 | COMPRESSED_REF_ABLE_SIZE | Average size of new compressed referencable segments | Bytes |
 | COMPRESSED_SELF_DELTA_SIZE | Average self-compressed size of new delta segments | Bytes |
 | COMPRESSED_SIZE | Average size of new compressed segments | Bytes |
 | CROSS_BLOCKS_READ_ERRS | Number of failed reads due to wrong crossBlocks flag | Reads/Sec |
 | DELTA_BACKPTR_COLLISIONS | Number of times delta blocks with the same backptr were encountered during GC | Blocks/Sec |
 | DELTA_PROMOTES | Number of delta blocks promoted by GC | Blocks/Sec |
 | DELTA_RELOCS | Number of delta blocks relocated by GC | Blocks/Sec |
 | DELTA_REMOVAL_BACKPTR_COLLISIONS | Number of times delta blocks with the same backptr were encountered during deletions flush | Blocks/Sec |
 | DELTA_SIZE_PER_SIMILARITY | Average size of new compressed delta segments per similarity value | Bytes |
 | DELTA_TOTAL_PER_SIMILARITY | Total size of new compressed delta segments per similarity value | Bytes |
 | DELTAS_COMPLETE_RELOCS | Number of delta blocks notified about a relocation of both delta and ref segments at the same time | Blocks/Sec |
 | DELTAS_GC | Number of delta blocks removed by GC | Blocks/Sec |
 | DELTAS_PER_SIMILARITY | Number of new compressed delta segments per similarity value | Segments |
 | DELTAS_REF_RELOCS | Number of delta blocks notified about reference relocations | Blocks/Sec |
 | DISCOVERED_FREE_BYTES | Free bytes discovered by scrub | Bytes/Sec |
 | DROPPED_HISTORY_UPDATES | Number of History Updated Dropped | Segments/Sec |
 | DROPPED_SEGMENTS | Number of blocks dropped during clusterization | Blocks/Sec |
 | ENQUEUED_FP_CALCS | Written blocks to data-reduction filesystems, requiring fingerprint calculations | Blocks/Sec |
 | ETERNAL_SINGLE_PROMOTES | Number of eternal single blocks promoted by GC | Blocks/Sec |
 | ETERNAL_SINGLE_RELOCS | Number of eternal single blocks relocated by GC | Blocks/Sec |
 | ETERNAL_SINGLE_TOTAL_SIZE | Total size of new compressed eternal-single segments | Bytes |
 | ETERNAL_SINGLES_UNIQUES_EST_LOG | Log2 of number of unique hashes for new eternal single blocks | Blocks |
 | GC_PROMOTIONS | Number of times data was rewritten to the next GC tree level | Blocks/Sec |
 | HISTORY_DOUBLE_ADDS | Number of double-adds encountered in history | Errs/Sec |
 | HISTORY_READ_ERRS | Number of failed reference reads from history | Reads/Sec |
 | historyLogHist | historyLogHist | Segments |
 | historySegsInSliceShard | History segments in slice shard | Segments |
 | improvedFrom | Similarity improvements old values | Segments |
 | improvedTo | Similarity improvements, new values | Segments |
 | INGEST_PERFORMED_FP_CALCS | Delayed data-reduction fingerprint calculations performed during ingest | Blocks/Sec |
 | INGEST_START_CALLS | Ingest Start Calls | Calls/Sec |
 | INGEST_START_TIME | Average time to start ingest | Milliseconds |
 | inheritedRefs | Inherited Refs | Segments |
 | intoFilter | Segments inserted into filter | Segments |
 | MEMORY_HISTORY_TRUNCATES | Number of History Truncations due to low memory | Reads/Sec |
 | NEW_DELTAS_FROM_HISTORY | Number of new delta blocks created with references from history | Blocks/Sec |
 | NEW_DELTAS_FROM_INGEST | Number of new delta blocks created with references from the same ingest batch | Blocks/Sec |
 | NEW_DELTAS | Number of new delta blocks created | Blocks |
 | NEW_ETERNAL_SINGLES | Number of new eternal single blocks created | Blocks/Sec |
 | NEW_INCOMPRESSIBLE_DELTAS | Number of new incompressible delta segments ingested | Blocks/Sec |
 | NEW_INCOMPRESSIBLE_REF_ABLES | Number of new incompressible referencable segments ingested | Blocks/Sec |
 | NEW_INGESTED | Ingested Blocks | Blocks |
 | NEW_REF_ABLES | Number of new referencable blocks created | Blocks |
 | NEW_REFERENCES | Number of new reference blocks created | Blocks/Sec |
 | NEW_SINGLES | Number of new (non-eternal) single blocks created | Blocks/Sec |
 | PERFORMED_FP_CALCS | Executed data-reduction fingerprint calculations | Blocks/Sec |
 | REF_BACKPTR_COLLISIONS | Number of times blocks with the same reference-backptr were encountered during GC | Blocks/Sec |
 | REFERENCE_GC | Number of reference blocks removed by GC | Blocks/Sec |
 | REFERENCE_PROMOTES | Number of reference blocks promoted by GC | Blocks/Sec |
 | REFERENCE_RELOCS | Number of reference blocks relocated by GC | Blocks/Sec |
 | refsBySource | Refs by ref source | Refs |
 | REJECTED_INGESTS | Number of ingests rejected by the extent | Blocks/Sec |
 | REJECTED_RELOCATES | Number of relocation rejected by the extent | Blocks/Sec |
 | REPLACED_FP_CALCS | Block re-writes replacing the data for fingerprint calculations | Blocks/Sec |
 | SCRUBBED_PLACEMENTS | Number of scrubbed placeents | Placements/Sec |
 | SEGMENT_PROMOTES_BYTES | Number of segment bytes promoted by GC | Bytes/Sec |
 | SEGMENT_PROMOTES | Promoted Compressed Blocks | Blocks |
 | SEGMENT_RELOCS_BYTES | Number of segment bytes relocated by GC | Bytes/Sec |
 | SEGMENT_RELOCS | Relocated Compressed Blocks | Blocks |
 | segsInSliceShard | Segments in slice shard | Segments |
 | similarityByRefSource | Similarity histogram per ref source | Score |
 | SINGLES_MARKED_AS_REFS | Number of single blocks marked as references due to new matches | Blocks/Sec |
 | SKIPPED_FP_CALCS | Writes to data-reduction filesystems that skipped fingerprint calculations | Blocks/Sec |
 | STALE_HISTORY_USE | Number of segments used as references | Reads/Sec |
 | typicalCandidatesPerHist | typicalCandidatesPerHist | Segments |
 | typicalRefsPerHist | typicalRefsPerHist | Segments |

### Dataservice

 | **Type** | **Description** | **Units** |
 | ---------------------------------------------- | ------------------------------------------------------------ | ------------ |
 | DIFFLIST_GET_LATENCY | Average latency of getDifflist | Microseconds |
 | DIFFLIST_GET_MANIFEST_LATENCY | Average latency of getDifflist getManifest | Microseconds |
 | DIFFLIST_GET_MANIFEST_OPS | Number of getDifflist getManifest | Ops/Sec |
 | DIFFLIST_GET_MANIFEST_PER_GETLIST_LATENCY | Average latency of getDifflist getmanifest per getdifflist | Microseconds |
 | DIFFLIST_GET_MANIFEST_PER_GETLIST_OPS | Number of getDifflist getmanifest per getdifflist | Ops/Sec |
 | DIFFLIST_GET_OPS | Number of getDifflist | Ops/Sec |
 | DIFFLIST_RESOLVE_PATH_BATCH_LATENCY | Average latency of getDifflist resolve-path per batch | Microseconds |
 | DIFFLIST_RESOLVE_PATH_BATCH_OPS | Number of getDifflist resolve-path per batch | Ops/Sec |
 | DIFFLIST_RESOLVEPATH_LATENCY | Average latency of getDifflist resolvepath | Microseconds |
 | DIFFLIST_RESOLVEPATH_OPS | Number of getDifflist resolvepath | Ops/Sec |
 | QUOTA_TASK_ADD_DIR_ENTRIES | Number of entries added for directory quota task | Ops |
 | QUOTA_TASK_CREATES | Number of directory quota tasks created | Ops |
 | QUOTA_TASK_DELETE_DIR_ENTRIES | Number of entries removed for directory quota task | Ops |
 | QUOTA_TASK_DELETES | Number of directory quota tasks removed | Ops |
 | QUOTA_TASK_FAILED_STAMPS | Number of failed quota coloring stamp operations | Ops |
 | QUOTA_TASK_FIBERS | Number of directory quota task fibers spawned per second | Fibers |
 | QUOTA_TASK_READDIR_LATENCY | Average latency of directory quota task readdir operations | Microseconds |
 | QUOTA_TASK_READDIR_OPS | Number of directory quota task readdir operations per second | Ops/Sec |
 | QUOTA_TASK_RUNTIME | Average runtime of directory quota task fibers | Microseconds |
 | QUOTA_TASK_STAMP_LATENCY | Average latency of directory quota task stamp operations | Microseconds |
 | QUOTA_TASK_STAMPS | Number of directory quota stamp operations per second | Ops/Sec |
 | QUOTA_TASK_SUCCESSFUL_STAMPS | Number of successful directory quota stamp operations | Ops |
 | QUOTAS_MARKED | Number of directory quotas marked | Quotas |

### Decisions about buckets from the cluster leader

 | **Type** | **Description** | **Units** |
 | ------------------------------- | ------------------------------------------------------------------------------ | ------------------------------ |
 | TOTAL_COUNCIL_CLEANUPS | The number of times a bucket council's toRemove/toAdd field member was cleared | Bucket council cleanups |
 | TOTAL_COUNCIL_REDISTRIBUTIONS | The number of times a bucket council was changed for any bucket | Bucket council redistributions |

### ExecTime

 | **Type** | **Description** | **Units** |
 | --------------- | ------------------------------------------------------------------------------------ | ------------ |
 | EXECTIME_AVG | Average execution time (usec) of function calls that ran for over the threshold time | Microseconds |
 | EXECTIME_COUNT | Number of times the function was called and ran for over the threshold time | Executions |

### Filesystem OBS

 | **Type** | **Description** | **Units** |
 | ------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------ |
 | BACKPRESSURED_BUCKETS_IN_FSS | Number of backpressured buckets | Buckets |
 | CONCURRENT_DEMOTES | Number of demotes executed concurrently | Demotes |
 | DEMOTE_EXTENT_OBS_FETCH_BACKPRESSURE | Number of extent BACKPRESSURE object-store fetch operations per second | Ops/Sec |
 | DEMOTE_EXTENT_OBS_FETCH_IMMEDIATE_RELEASE | Number of extent IMMEDIATE_RELEASE object-store fetch operations per second | Ops/Sec |
 | DEMOTE_EXTENT_OBS_FETCH_MANHOLE | Number of extent MANHOLE object-store fetch operations per second | Ops/Sec |
 | DEMOTE_EXTENT_OBS_FETCH_MIGRATE | Number of extent MIGRATE object-store fetch operations per second | Ops/Sec |
 | DEMOTE_EXTENT_OBS_FETCH_POLICY | Number of extent POLICY object-store fetch operations per second | Ops/Sec |
 | DEMOTE_EXTENT_OBS_FETCH_RECLAMATION_REUPLOAD | Number of extent RECLAMATION_REUPLOAD object-store fetch operations per second | Ops/Sec |
 | DEMOTE_EXTENT_OBS_FETCH_STOW | Number of extent STOW object-store fetch operations per second | Ops/Sec |
 | DEMOTE_EXTENT_OBS_FETCH | Number of extent object-store fetch operations per second | Ops/Sec |
 | DEMOTE_WAITING_FOR_SLOT | Average time waiting for a demotion concurrency slot | Microseconds |
 | DESERIALIZED_EXTENTS_WITH_INVALID_BLOBS | Number of deserialized extents with invalid blob id | Extents |
 | DOWNLOAD_LATENCY | Average latency of downloads | Microseconds |
 | DOWNLOADS | Number of downloads per second | Ops/Sec |
 | EXTENTS_WITH_FAKE_RETENTION_TAG | Number of scanned extents with fake retention tag | Extents |
 | FAILED_DOWNLOADS | Number of failed downloads per second | Ops/Sec |
 | FAILED_UPLOADS | Number of failed uploads per second | Ops/Sec |
 | OBS_4K_IOPS_READ | Number of object store dedicated 4K read operations per second | Ops/Sec |
 | OBS_BACKPRESSURE_FREED | Number of bytes freed from disk due to backpressure per second | Bytes/Sec |
 | OBS_BLOB_HEADER_DOWNLOAD_LATENCY | Average latency of blob header download | Microseconds |
 | OBS_BLOB_SCAVENGE_LATENCY | Average latency of blob scavenges | Microseconds |
 | OBS_BLOB_TIERING_DURATION | Duration of tiering blobs to object-store | Ops |
 | OBS_COMPLETELY_ALIVE_BLOBS | Percentage of blobs with only live extents linked to them | % |
 | OBS_COMPLETELY_DEAD_BLOBS | Percentage of blobs with no live extent linked to them | % |
 | OBS_EXTENTS_PREFETCH | Number of extents prefetched from object-store per second | Extents/Sec |
 | OBS_FREED | Number of bytes freed from disk because they are in the object-store per second | Bytes/Sec |
 | OBS_IMMEDIATE_RELEASE_FREED | Number of bytes freed from disk due to immediate release per second | Bytes/Sec |
 | OBS_INODES_PREFETCH | Number of files prefetched from object-store per second | Ops/Sec |
 | OBS_INODES_RELEASE | Number of files released to object store per second | Ops/Sec |
 | OBS_ONGOING_RECLAMATIONS | Number of ongoing reclamations | Ops |
 | OBS_POLICY_FREED | Number of bytes freed from disk due to policy per second | Bytes/Sec |
 | OBS_PROMOTE_EXTENT_WRITE_LATENCY | Average latency of extent promote writes | Microseconds |
 | OBS_PROMOTE_EXTENT_WRITE | Number of extents promoted from object-store per second | Extents/Sec |
 | OBS_PROMOTE_WRITE | Number of bytes promoted from object-store per second | Bytes/Sec |
 | OBS_READ | Number of reads that needed data from the object-store per second | Ops/Sec |
 | OBS_RECLAMATION_PURGED_BYTES | Number of bytes purged per second | Bytes/Sec |
 | OBS_RECLAMATION_SCAVENGED_BLOBS | Number of blobs scavenged per second | Ops/Sec |
 | OBS_RECLAMATION_SCAVENGED_BYTES | Number of bytes scavenged per second | Bytes/Sec |
 | OBS_RECLAMATION_WAIT_FOR_DESTAGE | Average time waiting for destage on space reclamation | Microseconds |
 | OBS_RELOC_DOWNLOAD | Number of relocation blobs downloaded per second | Ops/Sec |
 | OBS_RELOC_UPLOAD | Number of relocation blobs uploaded per second | Ops/Sec |
 | OBS_SCAVENGED_BLOB_WASTE_LEVEL | Waste level found in blobs | Blobs |
 | OBS_SHARED_DOWNLOADS_LATENCY | Average latency of shared downloads from object-store | Microseconds |
 | OBS_SHARED_DOWNLOADS | Number of shared downloads from object-store per second | Ops/Sec |
 | OBS_TRUNCATE | Number of truncates that needed data from the object-store per second | Ops/Sec |
 | OBS_UNEXPECTED_TAG_ON_DOWNLOAD | Number of unexpected tags found when downloading extents | Occurrences |
 | OBS_WRITE | Number of writes that needed data from the object-store per second | Ops/Sec |
 | STOW_COMMIT_QUEUE_HANG | Number of times metadata download queue was hanging full | Occurrences |
 | STOW_METADATA_DESERIALIZATION_LATENCY | Average latency of metadata blob deserialization | Milliseconds |
 | STOW_METADATA_SEED_DOWNLOADS | Number of seed downloads per second | Ops/Sec |
 | STOW_SERIALIZED_EXTENT_DATA | Number of extent descriptors uploaded that contain data | Extent Descriptors |
 | STOW_SERIALIZED_EXTENT_DESCS | Number of extent descriptors uploaded | Extent Descriptors |
 | STOW_SERIALIZED_EXTENT_REDIRECTS | Number of extent descriptors uploaded that redirect to previous snapshot | Extent Descriptors |
 | TIERED_FS_BREAKING_POLICY | Number of tiered filesystems breaking policy | Activations |
 | TIMEOUT_DOWNLOADS | Number of timed out downloads per second | Ops/Sec |
 | TIMEOUT_OPERATIONS | Total number of timed-out operations per second | Ops/Sec |
 | TIMEOUT_UPLOADS | Number of timed out uploads per second | Ops/Sec |
 | UNEXPECTED_BLOCK_VERSION_POST_UPGRADE | Number of unexpected block versions found after upgrade completed | Occurrences |
 | UPLOAD_CHOKING_LATENCY | Average latency of waiting for upload choking budget | Microseconds |
 | UPLOAD_LATENCY | Average latency of uploads | Microseconds |
 | UPLOADS | Number of upload attempts per second | Ops/Sec |

### Frontend

 | **Type** | **Description** | **Units** |
 | ---------------- | ------------------------------------------------------------------------- | ---------- |
 | FE_IDLE_CYCLES | The number of idle cycles on the frontend | Cycles/Sec |
 | FE_IDLE_TIME | The percentage of the CPU time not used for handling I/Os on the frontend | % |

### Frontend Encryption

 | **Type** | **Description** | **Units** |
 | ------------------------------- | --------------------------------------------------- | ------------ |
 | FE_BLOCK_CRYPTO_LATENCY | Average latency of frontend block crypto | Microseconds |
 | FE_BLOCK_DECRYPT_DURATION | Duration of decryption of blocks in the frontend | Microseconds |
 | FE_BLOCK_ENCRYPT_DURATION | Duration of encryption of blocks in the frontend | Microseconds |
 | FE_BLOCKS_DECRYPTED | Number of blocks decrypted in the frontend | Blocks |
 | FE_BLOCKS_ENCRYPTED | Number of blocks encrypted in the frontend | Blocks |
 | FE_FILENAME_CRYPTO_LATENCY | Average latency of frontend filename crypto | Microseconds |
 | FE_FILENAME_DECRYPT_DURATION | Duration of decryption of filenames in the frontend | Microseconds |
 | FE_FILENAME_ENCRYPT_DURATION | Duration of encryption of filenames in the frontend | Microseconds |
 | FE_FILENAMES_DECRYPTED | Number of filenames decrypted in the frontend | Filenames |
 | FE_FILENAMES_ENCRYPTED | Number of filenames encrypted in the frontend | Filenames |

### Garbage Collection

 | **Type** | **Description** | **Units** |
 | ---------------------------- | ----------------------------------- | --------- |
 | GC_ALLOC_BYTES | Number of bytes allocated from GC | Bytes |
 | GC_FREE_SIZE_AFTER_SCAN | GC pool size after the scan ends | Bytes |
 | GC_FREE_SIZE_BEFORE_SCAN | GC pool size before the scan starts | Bytes |
 | GC_SCAN_TIME | GC scan time | Msec |
 | GC_SCANS | Number of GC scans | Scans |
 | GC_USED_SIZE_AFTER_SCAN | GC used size after the scan ends | Bytes |
 | GC_USED_SIZE_BEFORE_SCAN | GC used size before the scan starts | Bytes |

### JRPC

 | **Type** | **Description** | **Units** |
 | ---------------------------------------------------- | ------------------------------------------------------------------------------------------------ | ------------ |
 | JRPC_SERVER_CALLS_CLIENT_DOES_NOT_SUPPORT_QOS | The number of JRPC calls made from a client that does not support JRPC QoS | Requests/Sec |
 | JRPC_SERVER_CALLS_CLIENT_SUPPORTS_QOS | The number of JRPC calls made from a client that supports JRPC QoS | Requests/Sec |
 | JRPC_SERVER_CALLS_QOS_DECLINED | The number of JRPC calls where server returns TOO_MANY_REQUESTS (QoS declined to run a method) | Requests/Sec |
 | JRPC_SERVER_PROCESSING_AVG | The average time the JRPC server processed the JRPC requests. | Microseconds |
 | JRPC_SERVER_PROCESSING_TIME | The number of JRPC requests processed by the server per time range. | Requests |

### Memory

 | **Type** | **Description** | **Units** |
 | ------------ | -------------------------------------------------------------------- | --------- |
 | GC_CURRENT | The process (node) GC memory size, current in sample time. | Bytes |
 | GC_PEAK | The process (node) GC memory size, peak over 1-minute intervals. | Bytes |
 | RSS_CURRENT | The process (node) memory resident size, current in sample time. | MB |
 | RSS_PEAK | The process (node) memory resident size, peak over process lifetime. | MB |

### Network

 | **Type** | **Description** | **Units** |
 | --------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ---------------------- |
 | ACKS_LOST | Number of lost ACK packets | Packets/Sec |
 | ACKS_REORDERED | Number of reordered ACK packets | Packets/Sec |
 | BAD_RECV_CSUM | Number of packets received with a bad checksum | Packets/Sec |
 | CORRUPT_PACKETS | Number of packets received and deemed corrupted | Packets/Sec |
 | DOUBLY_RECEIVED_PACKETS | Number of packets that were received multiple times | Packets/Sec |
 | DROPPED_LARGE_PACKETS | Number of large packets dropped in the socket backend | Packets/Sec |
 | DROPPED_PACKETS | Number of packets received that we dropped | Packets/Sec |
 | ECN_ENCOUNTERED | Number of ECN Encountered packets | Packets/Sec |
 | FAULT_RECV_DELAYED_PACKETS | Number of received packets delayed due to a fault injection | Packets/Sec |
 | FAULT_RECV_DROPPED_PACKETS | Number of received packets dropped due to a fault injection | Packets/Sec |
 | FAULT_SENT_DELAYED_PACKETS | Number of sent packets delayed due to a fault injection | Packets/Sec |
 | FAULT_SENT_DROPPED_PACKETS | Number of sent packets dropped due to a fault injection | Packets/Sec |
 | FRAGMENTATION_DUPS | Number of packets duplicated during fragmentation | Packets/Sec |
 | GOODPUT_RX_RATIO | Percentage of goodput RX packets out of total data packets received | % |
 | GOODPUT_TX_RATIO | Percentage of goodput TX packets out of total data packets sent | % |
 | GW_MAC_RESOLVE_FAILURES | Number of times we failed to ARP resolve the gateway IP | Failures |
 | GW_MAC_RESOLVE_SUCCESSES | Number of times we succeeded in ARP resolving the gateway IP | Successes |
 | INVALID_FIRST_FRAGMENT | Number of times we got an invalid first fragment | Packets/Sec |
 | MBUF_DUP_COUNT | Number of Duplicate mbufs found | Occurrences |
 | MBUF_DUP_ITER | Duplicate mbuf check completions | Occurrences |
 | NDP_DAD_RECV_ADDR_CONFLICTS | NDP DAD Receive Address Conflict Detected | Packets/Sec |
 | NDP_DAD_RECV_NO_CONFLICTS | NDP DAD Receive No Conflict | Packets/Sec |
 | NODE_RECONNECTED | Number of reconnections | Reconnects/Sec |
 | PACKET_ALIGN_BYTES_COPIED | Number of bytes copied during receive for packet alignment | Bytes/Sec |
 | PACKET_COMBINE_BYTES_COPIED | Number of bytes copied during receive packet buffer combining | Bytes/Sec |
 | PACKETS_FAILING_COMBINE | Number of packets received that failed buffer combining | Packets/Sec |
 | PACKETS_NEEDING_ALIGN | Number of packets received that needed alignment adjustment | Packets/Sec |
 | PACKETS_NEEDING_COMBINE | Number of packets received that needed buffer combining | Packets/Sec |
 | PACKETS_PUMPED | Number of packets received in each call to recvPackets | Batches |
 | PACKETS_RX_EXTMBUF | Number of packets received with external mbuf | Packets |
 | PACKETS_VLAN_INSERTED_HW | Number of packets sent with hardware inserted VLAN tag | Packets/Sec |
 | PACKETS_VLAN_INSERTED_SW | Number of packets sent with software-inserted VLAN tag | Packets/Sec |
 | PACKETS_VLAN_STRIPPED_HW | Number of packets received with hardware stripped VLAN tag | Packets/Sec |
 | PACKETS_VLAN_STRIPPED_SW | Number of packets received with software stripped VLAN tag | Packets/Sec |
 | PEER_RTT_BACKEND | RTT histogram | Microseconds |
 | PEER_RTT_CLIENT | RTT histogram | Microseconds |
 | POISON_DETECTED_EXPECTED | Expected number of poisoned netbufs detected | Occurrences |
 | POISON_DETECTED_UNEXPECTED | Unexpected number of poisoned netbufs detected | Occurrences |
 | POISON_DETECTED | Number of poisoned netbufs detected | Occurrences |
 | PORT_EXT_RX_PACKETS | Number of external packets received | Packets/Sec |
 | PORT_RX_BYTES | Number of bytes received | Bytes/Sec |
 | PORT_RX_ERRORS | Number of packet RX errors | Packets/Sec |
 | PORT_RX_MISSED | Number of packets lost due to RX queue full | Packets/Sec |
 | PORT_RX_NO_MBUFS | Number of packets lost due to no mbufs | Packets/Sec |
 | PORT_RX_PACKETS | Number of packets received | Packets/Sec |
 | PORT_TX_BYTES | Number of bytes transmitted | Bytes/Sec |
 | PORT_TX_ERRORS | Number of packet TX errors | Packets/Sec |
 | PORT_TX_PACKETS | Number of packets transmitted | Packets/Sec |
 | PUMP_DURATION | Duration of each pump | Requests |
 | PUMP_INTERVAL | Interval between pumps | Requests |
 | PUMPS_TXQ_FULL | Number of times we couldn't send any new packets to the NIC queue | Pumps/Sec |
 | PUMPS_TXQ_PARTIAL | Number of times we only sent some of our queued packets to the NIC queue | Pumps/Sec |
 | RDMA_ADD_CHUNK_FAILURES | Number of RDMA cookie setting failures | Failures/Sec |
 | RDMA_AHCACHE_POPULATIONS | Number of RDMA RDMA AH cache population attempts | Attempts/Sec |
 | RDMA_BINDING_FAILOVERS | Number of RDMA High-Availability fail-overs | Fail-overs/Sec |
 | RDMA_CANCELED_COMPLETIONS | Number of RDMA completions that were canceled | Completions/Sec |
 | RDMA_CLIENT_BINDING_INVALIDATIONS | Number of RDMA client binding invalidations | Invalidations/Sec |
 | RDMA_COMP_DURATION | Histogram of RDMA completion duration times | Requests |
 | RDMA_COMP_FAILURES | Number of RDMA requests that were completed with an error | Failures/Sec |
 | RDMA_COMP_LATENCY | Average time of RDMA requests completion | Microseconds |
 | RDMA_COMP_STATUSES | Histogram of RDMA completion statuses | Completions/Sec |
 | RDMA_COMPLETIONS | Number of RDMA requests that were completed | Completions/Sec |
 | RDMA_FAILED_AHCACHE_POPULATIONS | Number of failed RDMA AH cache population attempts | Failed Attempts/Sec |
 | RDMA_FALLBACK_WHILE_AH_POPULATE | Number of fallbacks from RDMA due to AH cache population in progress | Fallbacks/Sec |
 | RDMA_NET_ERR_RETRY_EXCEEDED | Number of RDMA requests with error retries exceeded | Occurrences/Sec |
 | RDMA_NO_LINK_LAYER | Number of RDMA lid parsing due to no link layer | RDMA-No-Link-Layer/Sec |
 | RDMA_POOL_ALLOC_FAILED | Number of times an RDMA request was not issued due to a pool allocation failure | Failures/Sec |
 | RDMA_POOL_LOW_CAPACITY | Number of times an RDMA request was not issued due to low RDMA pool memory | Failures/Sec |
 | RDMA_POOL_MBUF_LEAKED | RDMA leaked mbufs | Occurrences |
 | RDMA_PORT_WAITING_FIBERS | Number of fibers pending to send an RDMA request | Waiting fibers |
 | RDMA_REQUESTS | Number of RDMA requests sent to the NIC | Requests/Sec |
 | RDMA_RX_BYTES | Number of bytes received with RDMA | Bytes/Sec |
 | RDMA_SERVER_BINDING_RESTARTS | Number of RDMA server binding restarts | Restarts/Sec |
 | RDMA_SERVER_FAILED_BINDING_RESTARTS | Number of failed RDMA server binding restarts | Failed Restarts/Sec |
 | RDMA_SERVER_RECV_FAILURES | Number of failed RDMA server-side receive attempts | Failures/Sec |
 | RDMA_SERVER_SEND_FAILURES | Number of failed RDMA server-side send attempts | Failures/Sec |
 | RDMA_SUBMIT_FAILURES | Number of RDMA submit failures, likely indicating a fabric issue | Failures/Sec |
 | RDMA_SUBMIT_TIMEOUTS | Number of RDMA submit timeouts | Timeouts/Sec |
 | RDMA_TX_BYTES | Number of bytes sent with RDMA | Bytes/Sec |
 | RDMA_WAIT_INTERRUPTED | RDMA Wait interruptions | Issues |
 | RDMA_WAIT_PREMATURE_WAKEUP | RDMA Wait for premature wakeup | Issues |
 | RDMA_WAIT_TIMEOUT | RDMA Wait timeouts | Issues/Sec |
 | RECEIVED_ACK_PACKETS | Number of received ack packets | Packets/Sec |
 | RECEIVED_CONTROL_PACKETS | Number of received control packets | Packets/Sec |
 | RECEIVED_DATA_PACKETS | Number of received data packets | Packets/Sec |
 | RECEIVED_PACKET_GENERATIONS | The generation ("resend count") of the first incarnation of the packet seen by the receiver (indicates packet loss) | Packets |
 | RECEIVED_PACKETS | Number of packets received | Packets/Sec |
 | RECEIVED_PING_PACKETS | Number of received ping packets | Packets/Sec |
 | RECEIVED_PONG_PACKETS | Number of received pong packets | Packets/Sec |
 | RECEIVED_REJECT_PACKETS | Number of received reject packets | Packets/Sec |
 | RECEIVED_SYNC_PACKETS | Number of received sync packets | Packets/Sec |
 | REORDERED_PACKETS | Number of reordered packets | Packets/Sec |
 | RESEND_BATCH_SIZE | Number of packets sent in a resend batch | Batches |
 | RESENT_DATA_PACKETS | Number of data packets resent | Packets/Sec |
 | SEND_BATCH_SIZE_BYTES | Number of bytes sent in a first send batch | Batches |
 | SEND_BATCH_SIZE | Number of packets sent in a first send batch | Batches |
 | SEND_QUEUE_TIMEOUTS | Number of packets canceled due to envelope timeout and were not in the send window | Packets/Sec |
 | SEND_WINDOW_TIMEOUTS | Number of packets canceled due to envelope timeout while in the send window | Packets/Sec |
 | SENT_ACKS | Number of ACK packets sent | Packets/Sec |
 | SENT_CONTROL_PACKETS | Number of control packets sent | Packets/Sec |
 | SENT_DATA_PACKETS | Number of data packets sent | Packets/Sec |
 | SENT_PACKETS | Number of sent packets | Packets/Sec |
 | SENT_REJECTS | Number of rejects sent | Packets/Sec |
 | SHORT_CIRCUIT_SENDS | Number of packets sent to the same node | Packets/Sec |
 | SLOW_PATH_CSUM | Number of packets that went through checksum calculation on the CPU | Packets/Sec |
 | TIME_TO_ACK | Histogram of time to acknowledge a data packet | Requests |
 | TIME_TO_FIRST_SEND | Time from queueing to first send | Requests |
 | TIMELY_RESENDS | Number of packets resent due to timely resend | Packets/Sec |
 | UCX_RXQ_FULL | UCX Drop RXQ Full | Packets/Sec |
 | UCX_SEND_CB | UCX Send Callback | Packets/Sec |
 | UCX_SEND_ERROR | UCX Send Error | Packets/Sec |
 | UCX_SENT_PACKETS_ASYNC | UCX Sent Asynchronously | Packets/Sec |
 | UCX_SENT_PACKETS_IMMEDIATE | UCX Sent Immediately | Packets/Sec |
 | UCX_TXQ_FULL | UCX Drop TXQ Full | Packets/Sec |
 | UDP_SENDMSG_FAILED_EAGAIN | Number of packets that failed to be sent on the socket backend with EAGAIN | Packets/Sec |
 | UDP_SENDMSG_FAILED_OTHER | Number of packets that failed to be sent on the socket backend with an unknown error | Packets/Sec |
 | UDP_SENDMSG_PARTIAL_SEND | Number of packets that we failed to send, but in the same pump, some packets were sent | Packets/Sec |
 | UNACKED_RESENDS | Number of packets resent after receiving an ack | Packets/Sec |
 | ZERO_CSUM_OVERWRITE | Number of packets with zero checksum that were overwritten with 0xFFFF | Packets/Sec |
 | ZERO_CSUM | Number of checksum zero received | Packets/Sec |

### NODE_TRANSITIONS

 | **Type** | **Description** | **Units** |
 | --------------------------------- | ----------------------------------------------------------- | --------------- |
 | JOINING_FENCED_REASON_COUNTS | Counts of reasons JOINING nodes were fenced | Occurrences/Sec |
 | JOINING_TO_UP_TRANSITIONS | Number of nodes transitioned from JOINING to UP. | Nodes |
 | SYNC_TO_JOIN_FAILURE_COUNTS | Counts of SYNCING to JOINING failures categorized by reason | Occurrences/Sec |
 | SYNCING_TO_JOINING_TRANSITIONS | Number of nodes transitioned from SYNCING to JOINING | Nodes |
 | UP_FENCED_REASON_COUNTS | Counts of reasons UP nodes were fenced | Occurrences/Sec |

### Object Storage

 | **Type** | **Description** | **Units** |
 | ---------------------------------------------------- | ------------------------------------------------------------------------- | ------------- |
 | FAILED_OBJECT_DELETES | Number of failed object deletes per second (any failure reason) | Ops/Sec |
 | FAILED_OBJECT_DOWNLOADS | Number of failed object downloads per second (any failure reason) | Ops/Sec |
 | FAILED_OBJECT_HEAD_QUERIES | Number of failed object head queries per second (any failure reason) | Ops/Sec |
 | FAILED_OBJECT_OPERATIONS | Total number of failed operations per second | Ops/Sec |
 | FAILED_OBJECT_UPLOADS | Number of failed object uploads per second (any failure reason) | Ops/Sec |
 | OBJECT_DELETE_DURATION | Duration of object delete request | Ops |
 | OBJECT_DELETE_LATENCY | Average latency of deleting an object | Microseconds |
 | OBJECT_DELETES | Number of object deletes per second | Ops/Sec |
 | OBJECT_DOWNLOAD_BYTES_BACKGROUND | Number of BACKGROUND bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_DOWNLOAD_BYTES_FOREGROUND | Number of FOREGROUND bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_DOWNLOAD_DURATION | Duration of object download request | Ops |
 | OBJECT_DOWNLOAD_LATENCY | Average latency of downloading an object | Microseconds |
 | OBJECT_DOWNLOAD_SIZE | Size of downloaded object ranges | Ops |
 | OBJECT_DOWNLOADS_BACKGROUND | Number of BACKGROUND objects downloaded per second | Ops/Sec |
 | OBJECT_DOWNLOADS_FOREGROUND | Number of FOREGROUND objects downloaded per second | Ops/Sec |
 | OBJECT_DOWNLOADS | Number of objects downloaded per second | Ops/Sec |
 | OBJECT_HEAD_DURATION | Duration of object head request | Ops |
 | OBJECT_HEAD_LATENCY | Average latency of deleting an object | Microseconds |
 | OBJECT_HEAD_QUERIES | Number of object head queries per second | Ops/Sec |
 | OBJECT_OPERATIONS | Total number of operations per second | Ops/Sec |
 | OBJECT_REMOVE_SIZE | Size of removed objects | Ops |
 | OBJECT_UPLOAD_BYTES_BACKPRESSURE | Number of BACKPRESSURE bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_UPLOAD_BYTES_IMMEDIATE_RELEASE | Number of IMMEDIATE_RELEASE bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_UPLOAD_BYTES_MANHOLE | Number of MANHOLE bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_UPLOAD_BYTES_MIGRATE | Number of MIGRATE bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_UPLOAD_BYTES_POLICY | Number of POLICY bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_UPLOAD_BYTES_RECLAMATION_REUPLOAD | Number of RECLAMATION_REUPLOAD bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_UPLOAD_BYTES_STOW | Number of STOW bytes sent to the object store per second | Bytes/Sec |
 | OBJECT_UPLOAD_DURATION | Duration of object upload request | Ops |
 | OBJECT_UPLOAD_LATENCY | Average latency of uploading an object | Microseconds |
 | OBJECT_UPLOAD_SIZE | Size of uploaded objects | Ops |
 | OBJECT_UPLOADS_BACKPRESSURE | Number of BACKPRESSURE upload attempts per second | Ops/Sec |
 | OBJECT_UPLOADS_IMMEDIATE_RELEASE | Number of IMMEDIATE_RELEASE upload attempts per second | Ops/Sec |
 | OBJECT_UPLOADS_MANHOLE | Number of MANHOLE upload attempts per second | Ops/Sec |
 | OBJECT_UPLOADS_MIGRATE | Number of MIGRATE upload attempts per second | Ops/Sec |
 | OBJECT_UPLOADS_POLICY | Number of POLICY upload attempts per second | Ops/Sec |
 | OBJECT_UPLOADS_RECLAMATION_REUPLOAD | Number of RECLAMATION_REUPLOAD upload attempts per second | Ops/Sec |
 | OBJECT_UPLOADS_STOW | Number of STOW upload attempts per second | Ops/Sec |
 | OBJECT_UPLOADS | Number of object uploads per second | Ops/Sec |
 | OBS_READ_BYTES | Number of bytes read from object storage | Bytes/Sec |
 | OBS_WRITE_BYTES | Number of bytes sent to object storage | Bytes/Sec |
 | ONGOING_DOWNLOADS | Number of ongoing downloads | Ops |
 | ONGOING_REMOVES | Number of ongoing removes | Ops |
 | ONGOING_UPLOADS | Number of ongoing uploads | Ops |
 | READ_BYTES | Number of bytes read from object storage | Bytes/Sec |
 | REMOVE_BYTES | Number of bytes removed from object storage | Bytes/Sec |
 | REQUEST_COUNT_DELETE | Number of HTTP DELETE requests per second | Requests/Sec |
 | REQUEST_COUNT_GET | Number of HTTP GET requests per second | Requests/Sec |
 | REQUEST_COUNT_HEAD | Number of HTTP HEAD requests per second | Requests/Sec |
 | REQUEST_COUNT_INVALID | Number of HTTP INVALID requests per second | Requests/Sec |
 | REQUEST_COUNT_POST | Number of HTTP POST requests per second | Requests/Sec |
 | REQUEST_COUNT_PUT | Number of HTTP PUT requests per second | Requests/Sec |
 | RESPONSE_COUNT_ACCEPTED | Number of HTTP ACCEPTED responses per second | Responses/Sec |
 | RESPONSE_COUNT_BAD_GATEWAY | Number of HTTP BAD_GATEWAY responses per second | Responses/Sec |
 | RESPONSE_COUNT_BAD_REQUEST | Number of HTTP BAD_REQUEST responses per second | Responses/Sec |
 | RESPONSE_COUNT_CONFLICT | Number of HTTP CONFLICT responses per second | Responses/Sec |
 | RESPONSE_COUNT_CONTINUE | Number of HTTP CONTINUE responses per second | Responses/Sec |
 | RESPONSE_COUNT_CREATED | Number of HTTP CREATED responses per second | Responses/Sec |
 | RESPONSE_COUNT_EXPECTATION_FAILED | Number of HTTP EXPECTATION_FAILED responses per second | Responses/Sec |
 | RESPONSE_COUNT_FORBIDDEN | Number of HTTP FORBIDDEN responses per second | Responses/Sec |
 | RESPONSE_COUNT_FOUND | Number of HTTP FOUND responses per second | Responses/Sec |
 | RESPONSE_COUNT_GATEWAY_TIMEOUT | Number of HTTP GATEWAY_TIMEOUT responses per second | Responses/Sec |
 | RESPONSE_COUNT_GONE | Number of HTTP GONE responses per second | Responses/Sec |
 | RESPONSE_COUNT_HTTP_VERSION_NOT_SUPPORTED | Number of HTTP HTTP_VERSION_NOT_SUPPORTED responses per second | Responses/Sec |
 | RESPONSE_COUNT_INSUFFICIENT_STORAGE | Number of HTTP INSUFFICIENT_STORAGE responses per second | Responses/Sec |
 | RESPONSE_COUNT_INVALID | Number of HTTP INVALID responses per second | Responses/Sec |
 | RESPONSE_COUNT_LENGTH_REQUIRED | Number of HTTP LENGTH_REQUIRED responses per second | Responses/Sec |
 | RESPONSE_COUNT_METHOD_NOT_ALLOWED | Number of HTTP METHOD_NOT_ALLOWED responses per second | Responses/Sec |
 | RESPONSE_COUNT_MOVED_PERMANENTLY | Number of HTTP MOVED_PERMANENTLY responses per second | Responses/Sec |
 | RESPONSE_COUNT_NO_CONTENT | Number of HTTP NO_CONTENT responses per second | Responses/Sec |
 | RESPONSE_COUNT_NON_AUTH_INFO | Number of HTTP NON_AUTH_INFO responses per second | Responses/Sec |
 | RESPONSE_COUNT_NOT_ACCEPTABLE | Number of HTTP NOT_ACCEPTABLE responses per second | Responses/Sec |
 | RESPONSE_COUNT_NOT_FOUND | Number of HTTP NOT_FOUND responses per second | Responses/Sec |
 | RESPONSE_COUNT_NOT_IMPLEMENTED | Number of HTTP NOT_IMPLEMENTED responses per second | Responses/Sec |
 | RESPONSE_COUNT_NOT_MODIFIED | Number of HTTP NOT_MODIFIED responses per second | Responses/Sec |
 | RESPONSE_COUNT_OK | Number of HTTP OK responses per second | Responses/Sec |
 | RESPONSE_COUNT_PARTIAL_CONTENT | Number of HTTP PARTIAL_CONTENT responses per second | Responses/Sec |
 | RESPONSE_COUNT_PAYMENT_REQUIRED | Number of HTTP PAYMENT_REQUIRED responses per second | Responses/Sec |
 | RESPONSE_COUNT_PRECONDITION_FAILED | Number of HTTP PRECONDITION_FAILED responses per second | Responses/Sec |
 | RESPONSE_COUNT_PROXY_AUTH_REQUIRED | Number of HTTP PROXY_AUTH_REQUIRED responses per second | Responses/Sec |
 | RESPONSE_COUNT_REDIRECT_MULTIPLE_CHOICES | Number of HTTP REDIRECT_MULTIPLE_CHOICES responses per second | Responses/Sec |
 | RESPONSE_COUNT_REQUEST_HEADER_FIELDS_TOO_LARGE | Number of HTTP REQUEST_HEADER_FIELDS_TOO_LARGE responses per second | Responses/Sec |
 | RESPONSE_COUNT_REQUEST_TIMEOUT | Number of HTTP REQUEST_TIMEOUT responses per second | Responses/Sec |
 | RESPONSE_COUNT_REQUEST_TOO_LARGE | Number of HTTP REQUEST_TOO_LARGE responses per second | Responses/Sec |
 | RESPONSE_COUNT_REQUESTED_RANGE_NOT_SATISFIABLE | Number of HTTP REQUESTED_RANGE_NOT_SATISFIABLE responses per second | Responses/Sec |
 | RESPONSE_COUNT_RESET_CONTENT | Number of HTTP RESET_CONTENT responses per second | Responses/Sec |
 | RESPONSE_COUNT_SEE_OTHER | Number of HTTP SEE_OTHER responses per second | Responses/Sec |
 | RESPONSE_COUNT_SERVER_ERROR | Number of HTTP SERVER_ERROR responses per second | Responses/Sec |
 | RESPONSE_COUNT_SERVICE_UNAVAILABLE | Number of HTTP SERVICE_UNAVAILABLE responses per second | Responses/Sec |
 | RESPONSE_COUNT_SWITCHING_PROTOCOL | Number of HTTP SWITCHING_PROTOCOL responses per second | Responses/Sec |
 | RESPONSE_COUNT_TEMP_REDIRECT | Number of HTTP TEMP_REDIRECT responses per second | Responses/Sec |
 | RESPONSE_COUNT_UNAUTHORIZED | Number of HTTP UNAUTHORIZED responses per second | Responses/Sec |
 | RESPONSE_COUNT_UNPROCESSABLE_ENTITY | Number of HTTP UNPROCESSABLE_ENTITY responses per second | Responses/Sec |
 | RESPONSE_COUNT_UNSUPPORTED_MEDIA_TYPE | Number of HTTP UNSUPPORTED_MEDIA_TYPE responses per second | Responses/Sec |
 | RESPONSE_COUNT_URI_TOO_LONG | Number of HTTP URI_TOO_LONG responses per second | Responses/Sec |
 | RESPONSE_COUNT_USE_PROXY | Number of HTTP USE_PROXY responses per second | Responses/Sec |
 | WAITING_FOR_BUCKET_DOWNLOAD_BANDWIDTH | Time requests wait for the object store bucket download bandwidth | Ops |
 | WAITING_FOR_BUCKET_DOWNLOAD_FLOW | Time requests wait for the object store bucket download flow | Ops |
 | WAITING_FOR_BUCKET_REMOVE_BANDWIDTH | Time requests wait for the object store bucket remove bandwidth | Ops |
 | WAITING_FOR_BUCKET_REMOVE_FLOW | Time requests wait for the object store bucket remove flow | Ops |
 | WAITING_FOR_BUCKET_UPLOAD_BANDWIDTH | Time requests wait for the object store bucket upload bandwidth | Ops |
 | WAITING_FOR_BUCKET_UPLOAD_FLOW | Time requests wait for the object store bucket upload flow | Ops |
 | WAITING_FOR_GROUP_DOWNLOAD_BANDWIDTH | Time requests wait for the object store group download bandwidth | Ops |
 | WAITING_FOR_GROUP_DOWNLOAD_FLOW | Time requests wait for the object store group download flow | Ops |
 | WAITING_FOR_GROUP_REMOVE_BANDWIDTH | Time requests wait for the object store group remove bandwidth | Ops |
 | WAITING_FOR_GROUP_REMOVE_FLOW | Time requests wait for the object store group remove flow | Ops |
 | WAITING_FOR_GROUP_UPLOAD_BANDWIDTH | Time requests wait for the object store group upload bandwidth | Ops |
 | WAITING_FOR_GROUP_UPLOAD_FLOW | Time requests wait for the object store group upload flow | Ops |
 | WAITING_IN_BUCKET_DOWNLOAD_QUEUE | Time requests wait in the object store bucket download queue | Ops |
 | WAITING_IN_BUCKET_REMOVE_QUEUE | Time requests wait in the object store bucket remove queue | Ops |
 | WAITING_IN_BUCKET_UPLOAD_QUEUE | Time requests wait in the object store bucket upload queue | Ops |
 | WAITING_IN_GROUP_DOWNLOAD_QUEUE | Time requests wait in the object store group download queue | Ops |
 | WAITING_IN_GROUP_REMOVE_QUEUE | Time requests wait in the object store group remove queue | Ops |
 | WAITING_IN_GROUP_UPLOAD_QUEUE | Time requests wait in object-store group upload queue | Ops |
 | WRITE_BYTES | Number of bytes sent to object storage | Bytes/Sec |

### Operations

 | **Type** | **Description** | **Units** |
 | ----------------------- | ---------------------------------------------- | ------------ |
 | ACCESS_LATENCY | Average latency of ACCESS operations | Microseconds |
 | ACCESS_OPS | Number of ACCESS operations per second | Ops/Sec |
 | COMMIT_LATENCY | Average latency of COMMIT operations | Microseconds |
 | COMMIT_OPS | Number of COMMIT operations per second | Ops/Sec |
 | CREATE_LATENCY | Average latency of CREATE operations | Microseconds |
 | CREATE_OPS | Number of CREATE operations per second | Ops/Sec |
 | FILEATOMICOPEN_LATENCY | Average latency of FILEATOMICOPEN operations | Microseconds |
 | FILEATOMICOPEN_OPS | Number of FILEATOMICOPEN operations per second | Ops/Sec |
 | FILECLOSE_LATENCY | Average latency of FILECLOSE operations | Microseconds |
 | FILECLOSE_OPS | Number of FILECLOSE operations per second | Ops/Sec |
 | FILEOPEN_LATENCY | Average latency of FILEOPEN operations | Microseconds |
 | FILEOPEN_OPS | Number of FILEOPEN operations per second | Ops/Sec |
 | FLOCK_LATENCY | Average latency of FLOCK operations | Microseconds |
 | FLOCK_OPS | Number of FLOCK operations per second | Ops/Sec |
 | FSINFO_LATENCY | Average latency of FSINFO operations | Microseconds |
 | FSINFO_OPS | Number of FSINFO operations per second | Ops/Sec |
 | GETATTR_LATENCY | Average latency of GETATTR operations | Microseconds |
 | GETATTR_OPS | Number of GETATTR operations per second | Ops/Sec |
 | LINK_LATENCY | Average latency of LINK operations | Microseconds |
 | LINK_OPS | Number of LINK operations per second | Ops/Sec |
 | LOOKUP_LATENCY | Average latency of LOOKUP operations | Microseconds |
 | LOOKUP_OPS | Number of LOOKUP operations per second | Ops/Sec |
 | MKDIR_LATENCY | Average latency of MKDIR operations | Microseconds |
 | MKDIR_OPS | Number of MKDIR operations per second | Ops/Sec |
 | MKNOD_LATENCY | Average latency of MKNOD operations | Microseconds |
 | MKNOD_OPS | Number of MKNOD operations per second | Ops/Sec |
 | OPS | Total number of operations | Ops/Sec |
 | PATHCONF_LATENCY | Average latency of PATHCONF operations | Microseconds |
 | PATHCONF_OPS | Number of PATHCONF operations per second | Ops/Sec |
 | READ_BYTES | Number of bytes read per second | Bytes/Sec |
 | READ_DURATION | The number of reads per completion duration | Reads |
 | READ_LATENCY | Average latency of READ operations | Microseconds |
 | READDIR_LATENCY | Average latency of READDIR operations | Microseconds |
 | READDIR_OPS | Number of READDIR operations per second | Ops/Sec |
 | READLINK_LATENCY | Average latency of READLINK operations | Microseconds |
 | READLINK_OPS | Number of READLINK operations per second | Ops/Sec |
 | READS | Number of read operations per second | Ops/Sec |
 | REMOVE_LATENCY | Average latency of REMOVE operations | Microseconds |
 | REMOVE_OPS | Number of REMOVE operations per second | Ops/Sec |
 | RENAME_LATENCY | Average latency of RENAME operations | Microseconds |
 | RENAME_OPS | Number of RENAME operations per second | Ops/Sec |
 | RMDIR_LATENCY | Average latency of RMDIR operations | Microseconds |
 | RMDIR_OPS | Number of RMDIR operations per second | Ops/Sec |
 | SETATTR_LATENCY | Average latency of SETATTR operations | Microseconds |
 | SETATTR_OPS | Number of SETATTR operations per second | Ops/Sec |
 | STATFS_LATENCY | Average latency of STATFS operations | Microseconds |
 | STATFS_OPS | Number of STATFS operations per second | Ops/Sec |
 | SYMLINK_LATENCY | Average latency of SYMLINK operations | Microseconds |
 | SYMLINK_OPS | Number of SYMLINK operations per second | Ops/Sec |
 | THROUGHPUT | Number of bytes read/writes per second | Bytes/Sec |
 | UNLINK_LATENCY | Average latency of UNLINK operations | Microseconds |
 | UNLINK_OPS | Number of UNLINK operations per second | Ops/Sec |
 | WRITE_BYTES | Number of byte writes per second | Bytes/Sec |
 | WRITE_DURATION | The number of writes per completion duration | Writes |
 | WRITE_LATENCY | Average latency of WRITE operations | Microseconds |
 | WRITES | Number of write operations per second | Ops/Sec |

### Operations (driver)

 | **Type** | **Description** | **Units** |
 | ----------------------------------------------- | ------------------------------------------------------------------------------------------ | ------------ |
 | DIRECT_READ_SIZES_RATE | The number of O_DIRECT reads per each read size range per second | Reads |
 | DIRECT_READ_SIZES | The number of O_DIRECT reads per each read size range | Reads |
 | DIRECT_WRITE_SIZES_RATE | The number of O_DIRECT writes per each read size range per second | Writes |
 | DIRECT_WRITE_SIZES | The number of O_DIRECT writes per each read size range | Writes |
 | DOORBELL_RING_COUNT | The number of times the driver queue's doorbell was ringed | Ops |
 | FAILED_1HOP_READS | Number of failed single hop reads per second | Ops/Sec |
 | FILEATOMICOPEN_LATENCY | Average latency of FILEATOMICOPEN operations | Microseconds |
 | FILEATOMICOPEN_OPS | Number of FILEATOMICOPEN operations per second | Ops/Sec |
 | FILEATOMICOPEN_QOS_DELAY | Average QoS delay for FILEATOMICOPEN operations | Microseconds |
 | FILECLOSE_LATENCY | Average latency of FILECLOSE operations | Microseconds |
 | FILECLOSE_OPS | Number of FILECLOSE operations per second | Ops/Sec |
 | FILECLOSE_QOS_DELAY | Average QoS delay for FILECLOSE operations | Microseconds |
 | FILEOPEN_LATENCY | Average latency of FILEOPEN operations | Microseconds |
 | FILEOPEN_OPS | Number of FILEOPEN operations per second | Ops/Sec |
 | FILEOPEN_QOS_DELAY | Average QoS delay for FILEOPEN operations | Microseconds |
 | FLOCK_LATENCY | Average latency of FLOCK operations | Microseconds |
 | FLOCK_OPS | Number of FLOCK operations per second | Ops/Sec |
 | FLOCK_QOS_DELAY | Average QoS delay for FLOCK operations | Microseconds |
 | GETATTR_LATENCY | Average latency of GETATTR operations | Microseconds |
 | GETATTR_OPS | Number of GETATTR operations per second | Ops/Sec |
 | GETATTR_QOS_DELAY | Average QoS delay for GETATTR operations | Microseconds |
 | GETXATTR_LATENCY | Average latency of GETXATTR operations | Microseconds |
 | GETXATTR_OPS | Number of GETXATTR operations per second | Ops/Sec |
 | GETXATTR_QOS_DELAY | Average QoS delay for GETXATTR operations | Microseconds |
 | IOCTL_OBS_PREFETCH_LATENCY | Average latency of IOCTL_OBS_PREFETCH operations | Microseconds |
 | IOCTL_OBS_PREFETCH_OPS | Number of IOCTL_OBS_PREFETCH operations per second | Ops/Sec |
 | IOCTL_OBS_PREFETCH_QOS_DELAY | Average QoS delay for IOCTL_OBS_PREFETCH operations | Microseconds |
 | IOCTL_OBS_RELEASE_LATENCY | Average latency of IOCTL_OBS_RELEASE operations | Microseconds |
 | IOCTL_OBS_RELEASE_OPS | Number of IOCTL_OBS_RELEASE operations per second | Ops/Sec |
 | IOCTL_OBS_RELEASE_QOS_DELAY | Average QoS delay for IOCTL_OBS_RELEASE operations | Microseconds |
 | KEEPALIVES_NO_LEASE | Number of driver keepalives sent while we have no lease | Ops/Sec |
 | LINK_LATENCY | Average latency of LINK operations | Microseconds |
 | LINK_OPS | Number of LINK operations per second | Ops/Sec |
 | LINK_QOS_DELAY | Average QoS delay for LINK operations | Microseconds |
 | LISTXATTR_LATENCY | Average latency of LISTXATTR operations | Microseconds |
 | LISTXATTR_OPS | Number of LISTXATTR operations per second | Ops/Sec |
 | LISTXATTR_QOS_DELAY | Average QoS delay for LISTXATTR operations | Microseconds |
 | LOOKUP_LATENCY | Average latency of LOOKUP operations | Microseconds |
 | LOOKUP_OPS | Number of LOOKUP operations per second | Ops/Sec |
 | LOOKUP_QOS_DELAY | Average QoS delay for LOOKUP operations | Microseconds |
 | MKNOD_LATENCY | Average latency of MKNOD operations | Microseconds |
 | MKNOD_OPS | Number of MKNOD operations per second | Ops/Sec |
 | MKNOD_QOS_DELAY | Average QoS delay for MKNOD operations | Microseconds |
 | OPS | Total number of operations | Ops/Sec |
 | PENDING_IOS_COUNT | Pending IO count per FE container | Ops |
 | RDMA_WRITE_REQUESTS | Number of RDMA write request operations per second | Ops/Sec |
 | READ_BYTES_1HOP | Number of bytes read per second via single hop | Bytes/Sec |
 | READ_BYTES | Number of bytes read per second | Bytes/Sec |
 | READ_CHECKSUM_ERRORS | The number of times the driver's checksum validation failed upon the read's content | Ops |
 | READ_CORRUPTIONS_DETECTED_IN_1HOP | The number of corrupt data blocks encountered during 1-hop read | Ops |
 | READ_DURATION | The number of reads per time duration | Reads |
 | READ_LATENCY_NO_QOS | Average latency of READ operations without QoS delay | Microseconds |
 | READ_LATENCY | Average latency of READ operations | Microseconds |
 | READ_PARENT_SELINUX_ATTRIBUTE | The number of times we could not get SELinux attribute from parent | Ops |
 | READ_QOS_DELAY | Average QoS delay for READ operations | Microseconds |
 | READ_RDMA_SIZES_RATE | The number of RDMA reads per each read size range per second | Reads |
 | READ_RDMA_SIZES | The number of RDMA reads per each read size range | Reads |
 | READ_SIZES_RATE | The number of reads per read size range per second | Reads |
 | READ_SIZES | The number of reads per each read size range | Reads |
 | READDIR_LATENCY | Average latency of READDIR operations | Microseconds |
 | READDIR_OPS | Number of READDIR operations per second | Ops/Sec |
 | READDIR_QOS_DELAY | Average QoS delay for READDIR operations | Microseconds |
 | READLINK_LATENCY | Average latency of READLINK operations | Microseconds |
 | READLINK_OPS | Number of READLINK operations per second | Ops/Sec |
 | READLINK_QOS_DELAY | Average QoS delay for READLINK operations | Microseconds |
 | READS_NO_LEASE | Number of direct reads while we have no lease | Ops/Sec |
 | READS | Number of read operations per second | Ops/Sec |
 | RENAME_LATENCY | Average latency of RENAME operations | Microseconds |
 | RENAME_OPS | Number of RENAME operations per second | Ops/Sec |
 | RENAME_QOS_DELAY | Average QoS delay for RENAME operations | Microseconds |
 | REQUESTS_COMPLETED | The number of completions frontends sent to the driver's queue | Ops |
 | REQUESTS_FETCHED | The number of operations frontends fetched from the driver's queue | Ops |
 | RMDIR_LATENCY | Average latency of RMDIR operations | Microseconds |
 | RMDIR_OPS | Number of RMDIR operations per second | Ops/Sec |
 | RMDIR_QOS_DELAY | Average QoS delay for RMDIR operations | Microseconds |
 | RMXATTR_LATENCY | Average latency of RMXATTR operations | Microseconds |
 | RMXATTR_OPS | Number of RMXATTR operations per second | Ops/Sec |
 | RMXATTR_QOS_DELAY | Average QoS delay for RMXATTR operations | Microseconds |
 | SETATTR_LATENCY | Average latency of SETATTR operations | Microseconds |
 | SETATTR_OPS | Number of SETATTR operations per second | Ops/Sec |
 | SETATTR_QOS_DELAY | Average QoS delay for SETATTR operations | Microseconds |
 | SETXATTR_LATENCY | Average latency of SETXATTR operations | Microseconds |
 | SETXATTR_OPS | Number of SETXATTR operations per second | Ops/Sec |
 | SETXATTR_QOS_DELAY | Average QoS delay for SETXATTR operations | Microseconds |
 | SINGLE_HOP_WRITE_ATTEMPTS | Number of single-hop write operation attempts per second | Ops/Sec |
 | SINGLE_HOP_WRITE_PUT_BLOCKS_RDMA_FAILURES | Number of single hop write putBlock calls per second that failed RDMA | Calls/Sec |
 | SINGLE_HOP_WRITE_RDMA_FAILURES | Number of single-hop write operations per second that failed RDMA | Ops/Sec |
 | SINGLE_HOP_WRITE_RETRY_NO_RDMA | Number of single hop writes per second that were retried without RDMA | Ops/Sec |
 | SINGLE_HOP_WRITE_SKIPS_DISABLED | Number of single-hop write operation skips (DISABLED) per second | Ops/Sec |
 | SINGLE_HOP_WRITE_SKIPS_EXTERNAL_RDMA | Number of single hop write operation skips (EXTERNAL_RDMA) per second | Ops/Sec |
 | SINGLE_HOP_WRITE_SKIPS_MAX_CONCURRENCY | Number of single hop write operation skips (MAX_CONCURRENCY) per second | Ops/Sec |
 | SINGLE_HOP_WRITE_SKIPS_PARTIAL_BLOCKS | Number of single hop write operation skips (PARTIAL_BLOCKS) per second | Ops/Sec |
 | SINGLE_HOP_WRITE_SKIPS_SMALL_WRITE | Number of single hop write operation skips (SMALL_WRITE) per second | Ops/Sec |
 | SINGLE_HOP_WRITE_SKIPS_UNINIT_CHECKSUM | Number of single hop write operation skips (UNINIT_CHECKSUM) per second | Ops/Sec |
 | SINGLE_HOP_WRITE_SKIPS | Number of single-hop write operation skips per second | Ops/Sec |
 | SKIPPED_1HOP_READS_DISABLED | Number of skipped single hop reads per second because it is disabled | Ops/Sec |
 | SKIPPED_1HOP_READS_EXTERNAL_RDMA_SPARSE | Number of skipped single hop reads per second because it is a sparse read on external RDMA | Ops/Sec |
 | SKIPPED_1HOP_READS_GET_EXTENT_FAILED | Number of skipped single hop reads per second due to get extent failed | Ops/Sec |
 | SKIPPED_1HOP_READS_PARTIAL_READ | Number of skipped single hop reads per second due to partial read failure | Ops/Sec |
 | SKIPPED_1HOP_READS_SSD_LOAD | Number of skipped single hop reads per second due to drive load | Ops/Sec |
 | SKIPPED_1HOP_READS_TOO_MANY_DESCRIPTORS | Number of skipped single hop reads per second due to too many descriptors | Ops/Sec |
 | SKIPPED_1HOP_READS_TOO_MANY_DRIVES | Number of skipped single hop reads per second due to too many drives | Ops/Sec |
 | SKIPPED_1HOP_READS_TOO_SMALL | Number of skipped single hop reads per second because IO is too small | Ops/Sec |
 | STATFS_LATENCY | Average latency of STATFS operations | Microseconds |
 | STATFS_OPS | Number of STATFS operations per second | Ops/Sec |
 | STATFS_QOS_DELAY | Average QoS delay for STATFS operations | Microseconds |
 | SUCCEEDED_1HOP_READS | Number of successful single hop reads per second | Ops/Sec |
 | SYMLINK_LATENCY | Average latency of SYMLINK operations | Microseconds |
 | SYMLINK_OPS | Number of SYMLINK operations per second | Ops/Sec |
 | SYMLINK_QOS_DELAY | Average QoS delay for SYMLINK operations | Microseconds |
 | THROUGHPUT | Number of read/write bytes per second | Bytes/Sec |
 | UNLINK_LATENCY | Average latency of UNLINK operations | Microseconds |
 | UNLINK_OPS | Number of UNLINK operations per second | Ops/Sec |
 | UNLINK_QOS_DELAY | Average QoS delay for UNLINK operations | Microseconds |
 | WRITE_BYTES | Number of byte writes per second | Bytes/Sec |
 | WRITE_DURATION | The number of writes per time duration | Writes |
 | WRITE_LATENCY_NO_QOS | Average latency of WRITE operations without QoS delay | Microseconds |
 | WRITE_LATENCY | Average latency of WRITE operations | Microseconds |
 | WRITE_QOS_DELAY | Average QoS delay for WRITE operations | Microseconds |
 | WRITE_RDMA_SIZES_RATE | The number of RDMA writes per each read size range per second | Writes |
 | WRITE_RDMA_SIZES | The number of RDMA writes per each read size range | Writes |
 | WRITE_SIZES_RATE | The number of writes per each read size range per second | Writes |
 | WRITE_SIZES | The number of writes per each read size range | Writes |
 | WRITES_NO_LEASE | Number of direct writes while we have no lease | Ops/Sec |
 | WRITES | Number of write operations per second | Ops/Sec |

### Operations (Filesystem)

 | **Type** | **Description** | **Units** |
 | -------------- | ----------------------------------------------------- | ------------ |
 | READ_BYTES | Total read bytes per filesystem | Bytes/Sec |
 | READ_LATENCY | Average latency of read operations per filesystem | Microseconds |
 | READS | Number of read operations per second per filesystem | Ops/Sec |
 | THROUGHPUT | Number of bytes read/writes per second per filesystem | Bytes/Sec |
 | WRITE_BYTES | Total write bytes per filesystem | Bytes/Sec |
 | WRITE_LATENCY | Average latency of write operations per filesystem | Microseconds |
 | WRITES | Number of write operations per second per filesystem | Ops/Sec |

### Operations (NFS)

 | **Type** | **Description** | **Units** |
 | ----------------- | -------------------------------------------- | ------------ |
 | ACCESS_LATENCY | Average latency of ACCESS operations | Microseconds |
 | ACCESS_OPS | Number of ACCESS operations per second | Ops/Sec |
 | COMMIT_LATENCY | Average latency of COMMIT operations | Microseconds |
 | COMMIT_OPS | Number of COMMIT operations per second | Ops/Sec |
 | CREATE_LATENCY | Average latency of CREATE operations | Microseconds |
 | CREATE_OPS | Number of CREATE operations per second | Ops/Sec |
 | FSINFO_LATENCY | Average latency of FSINFO operations | Microseconds |
 | FSINFO_OPS | Number of FSINFO operations per second | Ops/Sec |
 | GETATTR_LATENCY | Average latency of GETATTR operations | Microseconds |
 | GETATTR_OPS | Number of GETATTR operations per second | Ops/Sec |
 | LINK_LATENCY | Average latency of LINK operations | Microseconds |
 | LINK_OPS | Number of LINK operations per second | Ops/Sec |
 | LOOKUP_LATENCY | Average latency of LOOKUP operations | Microseconds |
 | LOOKUP_OPS | Number of LOOKUP operations per second | Ops/Sec |
 | MKDIR_LATENCY | Average latency of MKDIR operations | Microseconds |
 | MKDIR_OPS | Number of MKDIR operations per second | Ops/Sec |
 | MKNOD_LATENCY | Average latency of MKNOD operations | Microseconds |
 | MKNOD_OPS | Number of MKNOD operations per second | Ops/Sec |
 | OPS | Total number of operations | Ops/Sec |
 | PATHCONF_LATENCY | Average latency of PATHCONF operations | Microseconds |
 | PATHCONF_OPS | Number of PATHCONF operations per second | Ops/Sec |
 | READ_BYTES | Number of bytes read per second | Bytes/Sec |
 | READ_DURATION | The number of reads per completion duration | Reads |
 | READ_LATENCY | Average latency of READ operations | Microseconds |
 | READ_SIZES | NFS read sizes histogram | Reads |
 | READDIR_LATENCY | Average latency of READDIR operations | Microseconds |
 | READDIR_OPS | Number of READDIR operations per second | Ops/Sec |
 | READLINK_LATENCY | Average latency of READLINK operations | Microseconds |
 | READLINK_OPS | Number of READLINK operations per second | Ops/Sec |
 | READS | Number of read operations per second | Ops/Sec |
 | REMOVE_LATENCY | Average latency of REMOVE operations | Microseconds |
 | REMOVE_OPS | Number of REMOVE operations per second | Ops/Sec |
 | RENAME_LATENCY | Average latency of RENAME operations | Microseconds |
 | RENAME_OPS | Number of RENAME operations per second | Ops/Sec |
 | SETATTR_LATENCY | Average latency of SETATTR operations | Microseconds |
 | SETATTR_OPS | Number of SETATTR operations per second | Ops/Sec |
 | STATFS_LATENCY | Average latency of STATFS operations | Microseconds |
 | STATFS_OPS | Number of STATFS operations per second | Ops/Sec |
 | SYMLINK_LATENCY | Average latency of SYMLINK operations | Microseconds |
 | SYMLINK_OPS | Number of SYMLINK operations per second | Ops/Sec |
 | THROUGHPUT | Number of bytes read/writes per second | Bytes/Sec |
 | WRITE_BYTES | Number of byte writes per second | Bytes/Sec |
 | WRITE_DURATION | The number of writes per completion duration | Writes |
 | WRITE_LATENCY | Average latency of WRITE operations | Microseconds |
 | WRITE_SIZES | NFS write sizes histogram | Writes |
 | WRITES | Number of write operations per second | Ops/Sec |

### Operations (NFSw)

 | **Type** | **Description** | **Units** |
 | ------------------------------------------ | ------------------------------------------------------------- | ------------ |
 | ACCESS_LATENCY | Average latency of ACCESS operations | Microseconds |
 | ACCESS_OPS | Number of ACCESS operations per second | Ops/Sec |
 | COMMIT_LATENCY | Average latency of COMMIT operations | Microseconds |
 | COMMIT_OPS | Number of COMMIT operations per second | Ops/Sec |
 | CREATE_LATENCY | Average latency of CREATE operations | Microseconds |
 | CREATE_OPS | Number of CREATE operations per second | Ops/Sec |
 | GETATTR_LATENCY | Average latency of GETATTR operations | Microseconds |
 | GETATTR_OPS | Number of GETATTR operations per second | Ops/Sec |
 | LINK_LATENCY | Average latency of LINK operations | Microseconds |
 | LINK_OPS | Number of LINK operations per second | Ops/Sec |
 | LOOKUP_LATENCY | Average latency of LOOKUP operations | Microseconds |
 | LOOKUP_OPS | Number of LOOKUP operations per second | Ops/Sec |
 | NFS3_ACCESS_ID_LATENCY | Average latency of NFS3_ACCESS operations | Microseconds |
 | NFS3_ACCESS_ID_OPS | Number of NFS3_ACCESS operations per second | Ops/Sec |
 | NFS3_COMMIT_ID_LATENCY | Average latency of NFS3_COMMIT operations | Microseconds |
 | NFS3_COMMIT_ID_OPS | Number of NFS3_COMMIT operations per second | Ops/Sec |
 | NFS3_CREATE_ID_LATENCY | Average latency of NFS3_CREATE operations | Microseconds |
 | NFS3_CREATE_ID_OPS | Number of NFS3_CREATE operations per second | Ops/Sec |
 | NFS3_FSINFO_ID_LATENCY | Average latency of NFS3_FSINFO operations | Microseconds |
 | NFS3_FSINFO_ID_OPS | Number of NFS3_FSINFO operations per second | Ops/Sec |
 | NFS3_FSINFO_LATENCY | Average latency of NFS3_FSINFO operations | Microseconds |
 | NFS3_FSINFO_OPS | Number of NFS3_FSINFO operations per second | Ops/Sec |
 | NFS3_GETATTR_ID_LATENCY | Average latency of NFS3_GETATTR operations | Microseconds |
 | NFS3_GETATTR_ID_OPS | Number of NFS3_GETATTR operations per second | Ops/Sec |
 | NFS3_LINK_ID_LATENCY | Average latency of NFS3_LINK operations | Microseconds |
 | NFS3_LINK_ID_OPS | Number of NFS3_LINK operations per second | Ops/Sec |
 | NFS3_LOOKUP_ID_LATENCY | Average latency of NFS3_LOOKUP operations | Microseconds |
 | NFS3_LOOKUP_ID_OPS | Number of NFS3_LOOKUP operations per second | Ops/Sec |
 | NFS3_MKDIR_ID_LATENCY | Average latency of NFS3_MKDIR operations | Microseconds |
 | NFS3_MKDIR_ID_OPS | Number of NFS3_MKDIR operations per second | Ops/Sec |
 | NFS3_MKDIR_LATENCY | Average latency of NFS3_MKDIR operations | Microseconds |
 | NFS3_MKDIR_OPS | Number of NFS3_MKDIR operations per second | Ops/Sec |
 | NFS3_MKNOD_ID_LATENCY | Average latency of NFS3_MKNOD operations | Microseconds |
 | NFS3_MKNOD_ID_OPS | Number of NFS3_MKNOD operations per second | Ops/Sec |
 | NFS3_MKNOD_LATENCY | Average latency of NFS3_MKNOD operations | Microseconds |
 | NFS3_MKNOD_OPS | Number of NFS3_MKNOD operations per second | Ops/Sec |
 | NFS3_OPS_ID | Number of NFS3_OPS per second | Ops/Sec |
 | NFS3_PATHCONF_ID_LATENCY | Average latency of NFS3_PATHCONF operations | Microseconds |
 | NFS3_PATHCONF_ID_OPS | Number of NFS3_PATHCONF operations per second | Ops/Sec |
 | NFS3_PATHCONF_LATENCY | Average latency of NFS3_PATHCONF operations | Microseconds |
 | NFS3_PATHCONF_OPS | Number of NFS3_PATHCONF operations per second | Ops/Sec |
 | NFS3_READ_BYTES_ID | Number of NFS3_READ_BYTES per second | Bytes/Sec |
 | NFS3_READ_ID_LATENCY | Average latency of NFS3_READ operations | Microseconds |
 | NFS3_READ_ID_OPS | Number of NFS3_READ operations per second | Ops/Sec |
 | NFS3_READDIR_ID_LATENCY | Average latency of NFS3_READDIR operations | Microseconds |
 | NFS3_READDIR_ID_OPS | Number of NFS3_READDIR operations per second | Ops/Sec |
 | NFS3_READLINK_ID_LATENCY | Average latency of NFS3_READLINK operations | Microseconds |
 | NFS3_READLINK_ID_OPS | Number of NFS3_READLINK operations per second | Ops/Sec |
 | NFS3_REMOVE_ID_LATENCY | Average latency of NFS3_REMOVE operations | Microseconds |
 | NFS3_REMOVE_ID_OPS | Number of NFS3_REMOVE operations per second | Ops/Sec |
 | NFS3_RENAME_ID_LATENCY | Average latency of NFS3_RENAME operations | Microseconds |
 | NFS3_RENAME_ID_OPS | Number of NFS3_RENAME operations per second | Ops/Sec |
 | NFS3_SETATTR_ID_LATENCY | Average latency of NFS3_SETATTR operations | Microseconds |
 | NFS3_SETATTR_ID_OPS | Number of NFS3_SETATTR operations per second | Ops/Sec |
 | NFS3_STATFS_ID_LATENCY | Average latency of NFS3_STATFS operations | Microseconds |
 | NFS3_STATFS_ID_OPS | Number of NFS3_STATFS operations per second | Ops/Sec |
 | NFS3_STATFS_LATENCY | Average latency of NFS3_STATFS operations | Microseconds |
 | NFS3_STATFS_OPS | Number of NFS3_STATFS operations per second | Ops/Sec |
 | NFS3_SYMLINK_ID_LATENCY | Average latency of NFS3_SYMLINK operations | Microseconds |
 | NFS3_SYMLINK_ID_OPS | Number of NFS3_SYMLINK operations per second | Ops/Sec |
 | NFS3_SYMLINK_LATENCY | Average latency of NFS3_SYMLINK operations | Microseconds |
 | NFS3_SYMLINK_OPS | Number of NFS3_SYMLINK operations per second | Ops/Sec |
 | NFS3_WRITE_BYTES_ID | Number of NFS3_WRITE_BYTES per second | Bytes/Sec |
 | NFS3_WRITE_ID_LATENCY | Average latency of NFS3_WRITE operations | Microseconds |
 | NFS3_WRITE_ID_OPS | Number of NFS3_WRITE operations per second | Ops/Sec |
 | NFS4_ACCESS_ID_LATENCY | Average latency of NFS4_ACCESS operations | Microseconds |
 | NFS4_ACCESS_ID_OPS | Number of NFS4_ACCESS operations per second | Ops/Sec |
 | NFS4_BACKCHANNEL_CTL_ID_LATENCY | Average latency of NFS4_BACKCHANNEL_CTL operations | Microseconds |
 | NFS4_BACKCHANNEL_CTL_ID_OPS | Number of NFS4_BACKCHANNEL_CTL operations per second | Ops/Sec |
 | NFS4_BACKCHANNEL_CTL_LATENCY | Average latency of NFS4_BACKCHANNEL_CTL operations | Microseconds |
 | NFS4_BACKCHANNEL_CTL_OPS | Number of NFS4_BACKCHANNEL_CTL operations per second | Ops/Sec |
 | NFS4_BIND_CONN_TO_SESSION_ID_LATENCY | Average latency of NFS4_BIND_CONN_TO_SESSION operations | Microseconds |
 | NFS4_BIND_CONN_TO_SESSION_ID_OPS | Number of NFS4_BIND_CONN_TO_SESSION operations per second | Ops/Sec |
 | NFS4_BIND_CONN_TO_SESSION_LATENCY | Average latency of NFS4_BIND_CONN_TO_SESSION operations | Microseconds |
 | NFS4_BIND_CONN_TO_SESSION_OPS | Number of NFS4_BIND_CONN_TO_SESSION operations per second | Ops/Sec |
 | NFS4_CLOSE_ID_LATENCY | Average latency of NFS4_CLOSE operations | Microseconds |
 | NFS4_CLOSE_ID_OPS | Number of NFS4_CLOSE operations per second | Ops/Sec |
 | NFS4_CLOSE_LATENCY | Average latency of NFS4_CLOSE operations | Microseconds |
 | NFS4_CLOSE_OPS | Number of NFS4_CLOSE operations per second | Ops/Sec |
 | NFS4_COMMIT_ID_LATENCY | Average latency of NFS4_COMMIT operations | Microseconds |
 | NFS4_COMMIT_ID_OPS | Number of NFS4_COMMIT operations per second | Ops/Sec |
 | NFS4_CREATE_ID_LATENCY | Average latency of NFS4_CREATE operations | Microseconds |
 | NFS4_CREATE_ID_OPS | Number of NFS4_CREATE operations per second | Ops/Sec |
 | NFS4_CREATE_SESSION_ID_LATENCY | Average latency of NFS4_CREATE_SESSION operations | Microseconds |
 | NFS4_CREATE_SESSION_ID_OPS | Number of NFS4_CREATE_SESSION operations per second | Ops/Sec |
 | NFS4_CREATE_SESSION_LATENCY | Average latency of NFS4_CREATE_SESSION operations | Microseconds |
 | NFS4_CREATE_SESSION_OPS | Number of NFS4_CREATE_SESSION operations per second | Ops/Sec |
 | NFS4_DELEGPURGE_ID_LATENCY | Average latency of NFS4_DELEGPURGE operations | Microseconds |
 | NFS4_DELEGPURGE_ID_OPS | Number of NFS4_DELEGPURGE operations per second | Ops/Sec |
 | NFS4_DELEGPURGE_LATENCY | Average latency of NFS4_DELEGPURGE operations | Microseconds |
 | NFS4_DELEGPURGE_OPS | Number of NFS4_DELEGPURGE operations per second | Ops/Sec |
 | NFS4_DELEGRETURN_ID_LATENCY | Average latency of NFS4_DELEGRETURN operations | Microseconds |
 | NFS4_DELEGRETURN_ID_OPS | Number of NFS4_DELEGRETURN operations per second | Ops/Sec |
 | NFS4_DELEGRETURN_LATENCY | Average latency of NFS4_DELEGRETURN operations | Microseconds |
 | NFS4_DELEGRETURN_OPS | Number of NFS4_DELEGRETURN operations per second | Ops/Sec |
 | NFS4_DESTROY_CLIENTID_ID_LATENCY | Average latency of NFS4_DESTROY_CLIENTID operations | Microseconds |
 | NFS4_DESTROY_CLIENTID_ID_OPS | Number of NFS4_DESTROY_CLIENTID operations per second | Ops/Sec |
 | NFS4_DESTROY_CLIENTID_LATENCY | Average latency of NFS4_DESTROY_CLIENTID operations | Microseconds |
 | NFS4_DESTROY_CLIENTID_OPS | Number of NFS4_DESTROY_CLIENTID operations per second | Ops/Sec |
 | NFS4_DESTROY_SESSION_ID_LATENCY | Average latency of NFS4_DESTROY_SESSION operations | Microseconds |
 | NFS4_DESTROY_SESSION_ID_OPS | Number of NFS4_DESTROY_SESSION operations per second | Ops/Sec |
 | NFS4_DESTROY_SESSION_LATENCY | Average latency of NFS4_DESTROY_SESSION operations | Microseconds |
 | NFS4_DESTROY_SESSION_OPS | Number of NFS4_DESTROY_SESSION operations per second | Ops/Sec |
 | NFS4_EXCHANGE_ID_LATENCY | Average latency of NFS4_EXCHANGE_ID operations | Microseconds |
 | NFS4_EXCHANGE_ID_OPS | Number of NFS4_EXCHANGE_ID operations per second | Ops/Sec |
 | NFS4_EXCHANGEID_ID_LATENCY | Average latency of NFS4_EXCHANGEID operations | Microseconds |
 | NFS4_EXCHANGEID_ID_OPS | Number of NFS4_EXCHANGEID operations per second | Ops/Sec |
 | NFS4_FREE_STATEID_ID_LATENCY | Average latency of NFS4_FREE_STATEID operations | Microseconds |
 | NFS4_FREE_STATEID_ID_OPS | Number of NFS4_FREE_STATEID operations per second | Ops/Sec |
 | NFS4_FREE_STATEID_LATENCY | Average latency of NFS4_FREE_STATEID operations | Microseconds |
 | NFS4_FREE_STATEID_OPS | Number of NFS4_FREE_STATEID operations per second | Ops/Sec |
 | NFS4_GET_DIR_DELEGATION_ID_LATENCY | Average latency of NFS4_GET_DIR_DELEGATION operations | Microseconds |
 | NFS4_GET_DIR_DELEGATION_ID_OPS | Number of NFS4_GET_DIR_DELEGATION operations per second | Ops/Sec |
 | NFS4_GET_DIR_DELEGATION_LATENCY | Average latency of NFS4_GET_DIR_DELEGATION operations | Microseconds |
 | NFS4_GET_DIR_DELEGATION_OPS | Number of NFS4_GET_DIR_DELEGATION operations per second | Ops/Sec |
 | NFS4_GETATTR_ID_LATENCY | Average latency of NFS4_GETATTR operations | Microseconds |
 | NFS4_GETATTR_ID_OPS | Number of NFS4_GETATTR operations per second | Ops/Sec |
 | NFS4_GETDEVICEINFO_ID_LATENCY | Average latency of NFS4_GETDEVICEINFO operations | Microseconds |
 | NFS4_GETDEVICEINFO_ID_OPS | Number of NFS4_GETDEVICEINFO operations per second | Ops/Sec |
 | NFS4_GETDEVICEINFO_LATENCY | Average latency of NFS4_GETDEVICEINFO operations | Microseconds |
 | NFS4_GETDEVICEINFO_OPS | Number of NFS4_GETDEVICEINFO operations per second | Ops/Sec |
 | NFS4_GETDEVICELIST_ID_LATENCY | Average latency of NFS4_GETDEVICELIST operations | Microseconds |
 | NFS4_GETDEVICELIST_ID_OPS | Number of NFS4_GETDEVICELIST operations per second | Ops/Sec |
 | NFS4_GETDEVICELIST_LATENCY | Average latency of NFS4_GETDEVICELIST operations | Microseconds |
 | NFS4_GETDEVICELIST_OPS | Number of NFS4_GETDEVICELIST operations per second | Ops/Sec |
 | NFS4_GETFH_ID_LATENCY | Average latency of NFS4_GETFH operations | Microseconds |
 | NFS4_GETFH_ID_OPS | Number of NFS4_GETFH operations per second | Ops/Sec |
 | NFS4_GETFH_LATENCY | Average latency of NFS4_GETFH operations | Microseconds |
 | NFS4_GETFH_OPS | Number of NFS4_GETFH operations per second | Ops/Sec |
 | NFS4_LAYOUTCOMMIT_ID_LATENCY | Average latency of NFS4_LAYOUTCOMMIT operations | Microseconds |
 | NFS4_LAYOUTCOMMIT_ID_OPS | Number of NFS4_LAYOUTCOMMIT operations per second | Ops/Sec |
 | NFS4_LAYOUTCOMMIT_LATENCY | Average latency of NFS4_LAYOUTCOMMIT operations | Microseconds |
 | NFS4_LAYOUTCOMMIT_OPS | Number of NFS4_LAYOUTCOMMIT operations per second | Ops/Sec |
 | NFS4_LAYOUTGET_ID_LATENCY | Average latency of NFS4_LAYOUTGET operations | Microseconds |
 | NFS4_LAYOUTGET_ID_OPS | Number of NFS4_LAYOUTGET operations per second | Ops/Sec |
 | NFS4_LAYOUTGET_LATENCY | Average latency of NFS4_LAYOUTGET operations | Microseconds |
 | NFS4_LAYOUTGET_OPS | Number of NFS4_LAYOUTGET operations per second | Ops/Sec |
 | NFS4_LAYOUTRETURN_ID_LATENCY | Average latency of NFS4_LAYOUTRETURN operations | Microseconds |
 | NFS4_LAYOUTRETURN_ID_OPS | Number of NFS4_LAYOUTRETURN operations per second | Ops/Sec |
 | NFS4_LAYOUTRETURN_LATENCY | Average latency of NFS4_LAYOUTRETURN operations | Microseconds |
 | NFS4_LAYOUTRETURN_OPS | Number of NFS4_LAYOUTRETURN operations per second | Ops/Sec |
 | NFS4_LINK_ID_LATENCY | Average latency of NFS4_LINK operations | Microseconds |
 | NFS4_LINK_ID_OPS | Number of NFS4_LINK operations per second | Ops/Sec |
 | NFS4_LOCK_ID_LATENCY | Average latency of NFS4_LOCK operations | Microseconds |
 | NFS4_LOCK_ID_OPS | Number of NFS4_LOCK operations per second | Ops/Sec |
 | NFS4_LOCK_LATENCY | Average latency of NFS4_LOCK operations | Microseconds |
 | NFS4_LOCK_OPS | Number of NFS4_LOCK operations per second | Ops/Sec |
 | NFS4_LOCKT_ID_LATENCY | Average latency of NFS4_LOCKT operations | Microseconds |
 | NFS4_LOCKT_ID_OPS | Number of NFS4_LOCKT operations per second | Ops/Sec |
 | NFS4_LOCKT_LATENCY | Average latency of NFS4_LOCKT operations | Microseconds |
 | NFS4_LOCKT_OPS | Number of NFS4_LOCKT operations per second | Ops/Sec |
 | NFS4_LOCKU_ID_LATENCY | Average latency of NFS4_LOCKU operations | Microseconds |
 | NFS4_LOCKU_ID_OPS | Number of NFS4_LOCKU operations per second | Ops/Sec |
 | NFS4_LOCKU_LATENCY | Average latency of NFS4_LOCKU operations | Microseconds |
 | NFS4_LOCKU_OPS | Number of NFS4_LOCKU operations per second | Ops/Sec |
 | NFS4_LOOKUP_ID_LATENCY | Average latency of NFS4_LOOKUP operations | Microseconds |
 | NFS4_LOOKUP_ID_OPS | Number of NFS4_LOOKUP operations per second | Ops/Sec |
 | NFS4_LOOKUPP_ID_LATENCY | Average latency of NFS4_LOOKUPP operations | Microseconds |
 | NFS4_LOOKUPP_ID_OPS | Number of NFS4_LOOKUPP operations per second | Ops/Sec |
 | NFS4_LOOKUPP_LATENCY | Average latency of NFS4_LOOKUPP operations | Microseconds |
 | NFS4_LOOKUPP_OPS | Number of NFS4_LOOKUPP operations per second | Ops/Sec |
 | NFS4_NVERIFY_ID_LATENCY | Average latency of NFS4_NVERIFY operations | Microseconds |
 | NFS4_NVERIFY_ID_OPS | Number of NFS4_NVERIFY operations per second | Ops/Sec |
 | NFS4_NVERIFY_LATENCY | Average latency of NFS4_NVERIFY operations | Microseconds |
 | NFS4_NVERIFY_OPS | Number of NFS4_NVERIFY operations per second | Ops/Sec |
 | NFS4_OPEN_CONFIRM_ID_LATENCY | Average latency of NFS4_OPEN_CONFIRM operations | Microseconds |
 | NFS4_OPEN_CONFIRM_ID_OPS | Number of NFS4_OPEN_CONFIRM operations per second | Ops/Sec |
 | NFS4_OPEN_CONFIRM_LATENCY | Average latency of NFS4_OPEN_CONFIRM operations | Microseconds |
 | NFS4_OPEN_CONFIRM_OPS | Number of NFS4_OPEN_CONFIRM operations per second | Ops/Sec |
 | NFS4_OPEN_DOWNGRADE_ID_LATENCY | Average latency of NFS4_OPEN_DOWNGRADE operations | Microseconds |
 | NFS4_OPEN_DOWNGRADE_ID_OPS | Number of NFS4_OPEN_DOWNGRADE operations per second | Ops/Sec |
 | NFS4_OPEN_DOWNGRADE_LATENCY | Average latency of NFS4_OPEN_DOWNGRADE operations | Microseconds |
 | NFS4_OPEN_DOWNGRADE_OPS | Number of NFS4_OPEN_DOWNGRADE operations per second | Ops/Sec |
 | NFS4_OPEN_ID_LATENCY | Average latency of NFS4_OPEN operations | Microseconds |
 | NFS4_OPEN_ID_OPS | Number of NFS4_OPEN operations per second | Ops/Sec |
 | NFS4_OPEN_LATENCY | Average latency of NFS4_OPEN operations | Microseconds |
 | NFS4_OPEN_OPS | Number of NFS4_OPEN operations per second | Ops/Sec |
 | NFS4_OPENATTR_ID_LATENCY | Average latency of NFS4_OPENATTR operations | Microseconds |
 | NFS4_OPENATTR_ID_OPS | Number of NFS4_OPENATTR operations per second | Ops/Sec |
 | NFS4_OPENATTR_LATENCY | Average latency of NFS4_OPENATTR operations | Microseconds |
 | NFS4_OPENATTR_OPS | Number of NFS4_OPENATTR operations per second | Ops/Sec |
 | NFS4_OPS_ID | Number of NFS4_OPS per second | Ops/Sec |
 | NFS4_PUTFH_ID_LATENCY | Average latency of NFS4_PUTFH operations | Microseconds |
 | NFS4_PUTFH_ID_OPS | Number of NFS4_PUTFH operations per second | Ops/Sec |
 | NFS4_PUTFH_LATENCY | Average latency of NFS4_PUTFH operations | Microseconds |
 | NFS4_PUTFH_OPS | Number of NFS4_PUTFH operations per second | Ops/Sec |
 | NFS4_PUTPUBFH_ID_LATENCY | Average latency of NFS4_PUTPUBFH operations | Microseconds |
 | NFS4_PUTPUBFH_ID_OPS | Number of NFS4_PUTPUBFH operations per second | Ops/Sec |
 | NFS4_PUTPUBFH_LATENCY | Average latency of NFS4_PUTPUBFH operations | Microseconds |
 | NFS4_PUTPUBFH_OPS | Number of NFS4_PUTPUBFH operations per second | Ops/Sec |
 | NFS4_PUTROOTFH_ID_LATENCY | Average latency of NFS4_PUTROOTFH operations | Microseconds |
 | NFS4_PUTROOTFH_ID_OPS | Number of NFS4_PUTROOTFH operations per second | Ops/Sec |
 | NFS4_PUTROOTFH_LATENCY | Average latency of NFS4_PUTROOTFH operations | Microseconds |
 | NFS4_PUTROOTFH_OPS | Number of NFS4_PUTROOTFH operations per second | Ops/Sec |
 | NFS4_READ_BYTES_ID | Number of NFS4_READ_BYTES per second | Bytes/Sec |
 | NFS4_READ_ID_LATENCY | Average latency of NFS4_READ operations | Microseconds |
 | NFS4_READ_ID_OPS | Number of NFS4_READ operations per second | Ops/Sec |
 | NFS4_READDIR_ID_LATENCY | Average latency of NFS4_READDIR operations | Microseconds |
 | NFS4_READDIR_ID_OPS | Number of NFS4_READDIR operations per second | Ops/Sec |
 | NFS4_READLINK_ID_LATENCY | Average latency of NFS4_READLINK operations | Microseconds |
 | NFS4_READLINK_ID_OPS | Number of NFS4_READLINK operations per second | Ops/Sec |
 | NFS4_RECLAIM_COMPLETE_ID_LATENCY | Average latency of NFS4_RECLAIM_COMPLETE operations | Microseconds |
 | NFS4_RECLAIM_COMPLETE_ID_OPS | Number of NFS4_RECLAIM_COMPLETE operations per second | Ops/Sec |
 | NFS4_RECLAIM_COMPLETE_LATENCY | Average latency of NFS4_RECLAIM_COMPLETE operations | Microseconds |
 | NFS4_RECLAIM_COMPLETE_OPS | Number of NFS4_RECLAIM_COMPLETE operations per second | Ops/Sec |
 | NFS4_RELEASE_LOCKOWNER_ID_LATENCY | Average latency of NFS4_RELEASE_LOCKOWNER operations | Microseconds |
 | NFS4_RELEASE_LOCKOWNER_ID_OPS | Number of NFS4_RELEASE_LOCKOWNER operations per second | Ops/Sec |
 | NFS4_RELEASE_LOCKOWNER_LATENCY | Average latency of NFS4_RELEASE_LOCKOWNER operations | Microseconds |
 | NFS4_RELEASE_LOCKOWNER_OPS | Number of NFS4_RELEASE_LOCKOWNER operations per second | Ops/Sec |
 | NFS4_REMOVE_ID_LATENCY | Average latency of NFS4_REMOVE operations | Microseconds |
 | NFS4_REMOVE_ID_OPS | Number of NFS4_REMOVE operations per second | Ops/Sec |
 | NFS4_RENAME_ID_LATENCY | Average latency of NFS4_RENAME operations | Microseconds |
 | NFS4_RENAME_ID_OPS | Number of NFS4_RENAME operations per second | Ops/Sec |
 | NFS4_RENEW_ID_LATENCY | Average latency of NFS4_RENEW operations | Microseconds |
 | NFS4_RENEW_ID_OPS | Number of NFS4_RENEW operations per second | Ops/Sec |
 | NFS4_RENEW_LATENCY | Average latency of NFS4_RENEW operations | Microseconds |
 | NFS4_RENEW_OPS | Number of NFS4_RENEW operations per second | Ops/Sec |
 | NFS4_RESTOREFH_ID_LATENCY | Average latency of NFS4_RESTOREFH operations | Microseconds |
 | NFS4_RESTOREFH_ID_OPS | Number of NFS4_RESTOREFH operations per second | Ops/Sec |
 | NFS4_RESTOREFH_LATENCY | Average latency of NFS4_RESTOREFH operations | Microseconds |
 | NFS4_RESTOREFH_OPS | Number of NFS4_RESTOREFH operations per second | Ops/Sec |
 | NFS4_SAVEFH_ID_LATENCY | Average latency of NFS4_SAVEFH operations | Microseconds |
 | NFS4_SAVEFH_ID_OPS | Number of NFS4_SAVEFH operations per second | Ops/Sec |
 | NFS4_SAVEFH_LATENCY | Average latency of NFS4_SAVEFH operations | Microseconds |
 | NFS4_SAVEFH_OPS | Number of NFS4_SAVEFH operations per second | Ops/Sec |
 | NFS4_SECINFO_ID_LATENCY | Average latency of NFS4_SECINFO operations | Microseconds |
 | NFS4_SECINFO_ID_OPS | Number of NFS4_SECINFO operations per second | Ops/Sec |
 | NFS4_SECINFO_LATENCY | Average latency of NFS4_SECINFO operations | Microseconds |
 | NFS4_SECINFO_NO_NAME_ID_LATENCY | Average latency of NFS4_SECINFO_NO_NAME operations | Microseconds |
 | NFS4_SECINFO_NO_NAME_ID_OPS | Number of NFS4_SECINFO_NO_NAME operations per second | Ops/Sec |
 | NFS4_SECINFO_NO_NAME_LATENCY | Average latency of NFS4_SECINFO_NO_NAME operations | Microseconds |
 | NFS4_SECINFO_NO_NAME_OPS | Number of NFS4_SECINFO_NO_NAME operations per second | Ops/Sec |
 | NFS4_SECINFO_OPS | Number of NFS4_SECINFO operations per second | Ops/Sec |
 | NFS4_SEQUENCE_ID_LATENCY | Average latency of NFS4_SEQUENCE operations | Microseconds |
 | NFS4_SEQUENCE_ID_OPS | Number of NFS4_SEQUENCE operations per second | Ops/Sec |
 | NFS4_SEQUENCE_LATENCY | Average latency of NFS4_SEQUENCE operations | Microseconds |
 | NFS4_SEQUENCE_OPS | Number of NFS4_SEQUENCE operations per second | Ops/Sec |
 | NFS4_SET_SSV_ID_LATENCY | Average latency of NFS4_SET_SSV operations | Microseconds |
 | NFS4_SET_SSV_ID_OPS | Number of NFS4_SET_SSV operations per second | Ops/Sec |
 | NFS4_SET_SSV_LATENCY | Average latency of NFS4_SET_SSV operations | Microseconds |
 | NFS4_SET_SSV_OPS | Number of NFS4_SET_SSV operations per second | Ops/Sec |
 | NFS4_SETATTR_ID_LATENCY | Average latency of NFS4_SETATTR operations | Microseconds |
 | NFS4_SETATTR_ID_OPS | Number of NFS4_SETATTR operations per second | Ops/Sec |
 | NFS4_SETCLIENTID_CONFIRM_ID_LATENCY | Average latency of NFS4_SETCLIENTID_CONFIRM operations | Microseconds |
 | NFS4_SETCLIENTID_CONFIRM_ID_OPS | Number of NFS4_SETCLIENTID_CONFIRM operations per second | Ops/Sec |
 | NFS4_SETCLIENTID_CONFIRM_LATENCY | Average latency of NFS4_SETCLIENTID_CONFIRM operations | Microseconds |
 | NFS4_SETCLIENTID_CONFIRM_OPS | Number of NFS4_SETCLIENTID_CONFIRM operations per second | Ops/Sec |
 | NFS4_SETCLIENTID_ID_LATENCY | Average latency of NFS4_SETCLIENTID operations | Microseconds |
 | NFS4_SETCLIENTID_ID_OPS | Number of NFS4_SETCLIENTID operations per second | Ops/Sec |
 | NFS4_SETCLIENTID_LATENCY | Average latency of NFS4_SETCLIENTID operations | Microseconds |
 | NFS4_SETCLIENTID_OPS | Number of NFS4_SETCLIENTID operations per second | Ops/Sec |
 | NFS4_TEST_STATEID_ID_LATENCY | Average latency of NFS4_TEST_STATEID operations | Microseconds |
 | NFS4_TEST_STATEID_ID_OPS | Number of NFS4_TEST_STATEID operations per second | Ops/Sec |
 | NFS4_TEST_STATEID_LATENCY | Average latency of NFS4_TEST_STATEID operations | Microseconds |
 | NFS4_TEST_STATEID_OPS | Number of NFS4_TEST_STATEID operations per second | Ops/Sec |
 | NFS4_VERIFY_ID_LATENCY | Average latency of NFS4_VERIFY operations | Microseconds |
 | NFS4_VERIFY_ID_OPS | Number of NFS4_VERIFY operations per second | Ops/Sec |
 | NFS4_VERIFY_LATENCY | Average latency of NFS4_VERIFY operations | Microseconds |
 | NFS4_VERIFY_OPS | Number of NFS4_VERIFY operations per second | Ops/Sec |
 | NFS4_WANT_DELEGATION_ID_LATENCY | Average latency of NFS4_WANT_DELEGATION operations | Microseconds |
 | NFS4_WANT_DELEGATION_ID_OPS | Number of NFS4_WANT_DELEGATION operations per second | Ops/Sec |
 | NFS4_WANT_DELEGATION_LATENCY | Average latency of NFS4_WANT_DELEGATION operations | Microseconds |
 | NFS4_WANT_DELEGATION_OPS | Number of NFS4_WANT_DELEGATION operations per second | Ops/Sec |
 | NFS4_WRITE_BYTES_ID | Number of NFS4_WRITE_BYTES per second | Bytes/Sec |
 | NFS4_WRITE_ID_LATENCY | Average latency of NFS4_WRITE operations | Microseconds |
 | NFS4_WRITE_ID_OPS | Number of NFS4_WRITE operations per second | Ops/Sec |
 | OPS | Total number of operations | Ops/Sec |
 | READ_BYTES | Number of bytes read per second | Bytes/Sec |
 | READ_LATENCY | Average latency of READ operations | Microseconds |
 | READ_OPS | Number of READ operations per second | Ops/Sec |
 | READDIR_LATENCY | Average latency of READDIR operations | Microseconds |
 | READDIR_OPS | Number of READDIR operations per second | Ops/Sec |
 | READLINK_LATENCY | Average latency of READLINK operations | Microseconds |
 | READLINK_OPS | Number of READLINK operations per second | Ops/Sec |
 | REMOVE_LATENCY | Average latency of REMOVE operations | Microseconds |
 | REMOVE_OPS | Number of REMOVE operations per second | Ops/Sec |
 | RENAME_LATENCY | Average latency of RENAME operations | Microseconds |
 | RENAME_OPS | Number of RENAME operations per second | Ops/Sec |
 | SETATTR_LATENCY | Average latency of SETATTR operations | Microseconds |
 | SETATTR_OPS | Number of SETATTR operations per second | Ops/Sec |
 | THROUGHPUT | Number of bytes read/written per second | Bytes/Sec |
 | WRITE_BYTES | Number of bytes written per second | Bytes/Sec |
 | WRITE_LATENCY | Average latency of WRITE operations | Microseconds |
 | WRITE_OPS | Number of WRITE operations per second | Ops/Sec |

### Operations (S3)

 | **Type** | **Description** | **Units** |
 | -------------------------- | ----------------------------------------------- | ------------ |
 | API_FAILURES | Total of failures per API | Ops |
 | API_OPS | Total of Ops per API | Ops |
 | API_TTFB | Time To First Byte per API | Milliseconds |
 | API_TTLB | Time To Last Byte per API | Milliseconds |
 | AVG_TTLB_HIST | TTLB (Time To Last Byte) Milliseconds Histogram | Count |
 | AVG_TTLB_PERCENT | TTLB (Time To Last Byte) Percentile | Milliseconds |
 | FS_READ_BYTES | Total Read Bytes per FS | Bytes/Sec |
 | FS_RQ | Total Requests per FS | Ops |
 | FS_STATUS | Count HTTP Status Code per FS | Ops |
 | FS_WRITE_BYTES | Total Write Bytes per FS | Bytes/Sec |
 | TOTAL_BUCKET_CREATE_OPS | Total bucket create operations per second | Ops/Sec |
 | TOTAL_BUCKET_DELETE_OPS | Total bucket delete operations per second | Ops/Sec |
 | TOTAL_BUCKET_LIST_OPS | Total bucket list operations per second | Ops/Sec |

### Operations (SLB of S3)

 | **Type** | **Description** | **Units** |
 | --------------------------------------- | ------------------------------------------------------- | ----------- |
 | AVG_1xx_RQ | Average 1xx replies per second | Ops/Sec |
 | AVG_2xx_RQ | Average 2xx replies per second | Ops/Sec |
 | AVG_3xx_RQ | Average 3xx replies per second | Ops/Sec |
 | AVG_429_RQ | Average 429 replies per second | Ops/Sec |
 | AVG_4xx_RQ | Average 4xx replies per second | Ops/Sec |
 | AVG_503_RQ | Average 503 replies per second | Ops/Sec |
 | AVG_5xx_RQ | Average 5xx replies per second | Ops/Sec |
 | SLB_1xx_RQ | 1xx responses to traffic originating from adjacent SLBs | Ops |
 | SLB_2xx_RQ | 2xx responses to traffic originating from adjacent SLBs | Ops |
 | SLB_3xx_RQ | 3xx responses to traffic originating from adjacent SLBs | Ops |
 | SLB_4xx_RQ | 4xx responses to traffic originating from adjacent SLBs | Ops |
 | SLB_5xx_RQ | 5xx responses to traffic originating from adjacent SLBs | Ops |
 | TOTAL_1xx_RQ | Total 1xx replies | Ops |
 | TOTAL_2xx_RQ | Total 2xx replies | Ops |
 | TOTAL_3xx_RQ | Total 3xx replies | Ops |
 | TOTAL_429_RQ | Total 429 replies | Ops |
 | TOTAL_4xx_RQ | Total 4xx replies | Ops |
 | TOTAL_503_RQ | Total 503 replies | Ops |
 | TOTAL_5xx_RQ | Total 5xx replies | Ops |
 | TOTAL_active_connection | Total SLB Downstream Active Connections | Connections |
 | TOTAL_max_duration_RQ | Total Max Duration Reached replies | Ops |
 | TOTAL_rejected_via_ip_detection_RQ | Total Rejected by IP Detection replies | Ops |
 | TOTAL_response_before_complete_RQ | Total S3 Responses before Complete replies | Ops |
 | TOTAL_rx_reset_RQ | Total User RX Reset Connection replies | Ops |
 | TOTAL_tx_reset_RQ | Total Envoy TX Reset Connection replies | Ops |

### Platform

 | **Type** | **Description** | **Units** |
 | ----------------------------- | ------------------------------------------------- | ------------- |
 | REPORTED_MMAP_ALLOC_MEM | Memory allocated through reportedMmaps, in bytes. | Bytes |
 | REPORTED_MMAP_ALLOCS | Number of reported mmap allocations | Allocations |
 | REPORTED_MMAP_DEALLOCS | Number of reported mmap deallocations | Deallocations |
 | REPORTED_MMAP_RESERVED_MEM | Memory reserved for reportedMmaps, in bytes. | Bytes |

### Processes

 | **Type** | **Description** | **Units** |
 | ------------------------- | ---------------------------------------------------------------------------------- | ----------------------- |
 | ABRUPT_EXITS | Tracks the number of times a process exits unexpectedly | Abrupt process exits |
 | PEER_CONFIGURE_FAILURES | Tracks the number of times a process fails to configure a peer for synchronization | Peer configure failures |

### RAFT

 | **Type** | **Description** | **Units** |
 | ---------------------------------- | --------------------------------------------------- | --------- |
 | Bucket_LEADER_CHANGES | Changes of leader | Changes |
 | Bucket_REQUESTS_COMPLETED | Requests to leader completed successfully | Requests |
 | Configuration_LEADER_CHANGES | Changes of leader | Changes |
 | Configuration_REQUESTS_COMPLETED | Requests to leader completed successfully | Requests |
 | Invalid_LEADER_CHANGES | Changes of leader | Changes |
 | Invalid_REQUESTS_COMPLETED | Requests to leader completed successfully | Requests |
 | RAFT_BYTES_WRITTEN | Number of writes written to disk for RAFT | Bytes |
 | SYNCLOG_TIMEOUTS | The number of timeouts of syncing logs to a process | Timeouts |
 | Test_LEADER_CHANGES | Changes of leader | Changes |
 | Test_REQUESTS_COMPLETED | Requests to leader completed successfully | Requests |

### RAID

 | **Type** | **Description** | **Units** |
 | ------------------------------------------------------ | -------------------------------------------------------------------------- | --------------- |
 | IS_BLOCK_USED_FREE_LATENCY | Average latency of handling an isBlockUsed of a free block | Micros |
 | IS_BLOCK_USED_FREE | Number of isBlockUsed returning free | Blocks/Sec |
 | IS_BLOCK_USED_USED_LATENCY | Average latency of handling an isBlockUsed of a used block | Micros |
 | IS_BLOCK_USED_USED | Number of isBlockUsed returning used | Blocks/Sec |
 | NVKV_RECOVERY_NETBUF_REREAD_UNEQUAL | Number of unequal netbufs encountered that caused NVKV recovery to restart | Blocks/Sec |
 | RAID_ALLOCATION_FAILED_HOLES | Slots failed to be allocated and were left as holes | Holes/Sec |
 | RAID_BLOCKS_IN_PREPARED_STRIPE | Free blocks in prepared stripe | Blocks |
 | RAID_CHUNKS_CLEANED_BY_SHIFT | Dirty chunks cleaned by being shifted out | Occurrences |
 | RAID_CHUNKS_SHIFTED | Dirty chunks that shifted out | Occurrences |
 | RAID_COMMITTED_STRIPES | Number of stripes written | Stripes |
 | RAID_COMPRESSED_BLOCKS_WRITTEN | Physical blocks are written containing compressed data | Blocks/Sec |
 | RAID_COMPRESSED_PADDING | Zero-Blocks written to compressed space for alignment | Blocks/Sec |
 | RAID_CORRUPTION_RECOVERY_FAILURE | Corrupt data could not be recovered | Occurrences |
 | RAID_DRIVE_FAILURE | Drive failures viewed from compute or front-end nodes | Occurrences/Sec |
 | RAID_PLACEMENT_ALLOC_PlacementSpace0 | Number of placement allocations | Occurrences/Sec |
 | RAID_PLACEMENT_ALLOC_PlacementSpaceN_Compressed | Number of placement allocations | Occurrences/Sec |
 | RAID_PLACEMENT_ALLOC_PlacementSpaceN_Uncompressed | Number of placement allocations | Occurrences/Sec |
 | RAID_PLACEMENT_RETIRE_PlacementSpace0 | Number of placement retirements | Occurrences/Sec |
 | RAID_PLACEMENT_RETIRE_PlacementSpaceN_Compressed | Number of placement retirements | Occurrences/Sec |
 | RAID_PLACEMENT_RETIRE_PlacementSpaceN_Uncompressed | Number of placement retirements | Occurrences/Sec |
 | RAID_PLACEMENT_SWITCHES | Number of placement switches | Switches |
 | RAID_READ_BATCHES_PER_REQUEST_HISTOGRAM | Histogram of the number of batches of stripes read in a single request | Request |
 | RAID_READ_BLOCKS_STRIPE_HISTOGRAM | Histogram of the number of blocks read from a single stripe | Reads |
 | RAID_READ_BLOCKS | Number of blocks read by the RAID | Blocks/Sec |
 | RAID_READ_DEGRADED | Degraded mode reads | Blocks/Sec |
 | RAID_READ_FREE | Read Free | Occurences |
 | RAID_READ_IOS | Raw read blocks performed by the RAID | Blocks/Sec |
 | RAID_STALE_WRITES_DETECTED | Stale write detected in read | Occurrences |
 | RAID_STALE_WRITES_REPROTECTIONS | Stale write reprotections in read | Occurrences |
 | WRONG_DRIVE_DELTAS | Delta segments are written to the wrong drive | Blocks/Sec |
 | WRONG_DRIVE_REFS | Reference segments are written to the wrong drive | Blocks/Sec |

### Reactor

 | **Type** | **Description** | **Units** |
 | ----------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------- |
 | AVG_QUEUE_TIME_FLEX_TASKS | Average queue time of deferred flex tasks | Cycles |
 | BACKGROUND_CYCLES | Number of cycles spent in background fibers | Cycles/Sec |
 | BACKGROUND_FIBERS | Number of background fibers that are ready to run and eager to get CPU cycles | Fibers |
 | BACKGROUND_TIME | The percentage of the CPU time used for background operations | % |
 | BucketInvocationState_CAPACITY | Number of data structures allocated to the BucketInvocationState pool | Structs |
 | BucketInvocationState_STRUCT_SIZE | Number of bytes in each struct of the BucketInvocationState pool | Bytes |
 | BucketInvocationState_USED | Number of structs in the BucketInvocationState pool that are currently being used | Structs |
 | CPU_HANGS_AND_KNOWN_HOGGER | Number of CPU hangs detected while known hogger | Hangs/Sec |
 | CPU_HANGS | Number of CPU hangs detected | Hangs/Sec |
 | CYCLES_PER_SECOND | Number of cycles the CPU runs per second | Cycles/Sec |
 | DeferredTask2_CAPACITY | Number of data structures allocated to the DeferredTask2 pool | Structs |
 | DeferredTask2_STRUCT_SIZE | Number of bytes in each struct of the DeferredTask2 pool | Bytes |
 | DeferredTask2_USED | Number of structs in the DeferredTask2 pool that are currently being used | Structs |
 | DEFUNCT_FIBERS | Number of defunct buffers, which are just memory structures allocated for future fiber needs | Fibers |
 | EXCEPTIONS | Number of exceptions caught by the reactor | Exceptions/Sec |
 | FLEX_TASKS_INLINE_FULLQUEUE | Number of flex tasks run inline on main thread due to full defer queue | Invocations/Sec |
 | FLEX_TASKS_INLINE_LIGHTLOAD | Number of flex tasks run inline on main thread due to light load | Invocations/Sec |
 | FLEX_TASKS_INLINE_OVERRIDE | Number of flex tasks run inline on main thread due to override | Invocations/Sec |
 | FLEX_TASKS_LATENCY | Histogram of flex tasks latency | usecs |
 | FLEX_TASKS_QUEUED_RUN_MAINTHREAD | Number of deferred flex tasks run by main thread | Invocations/Sec |
 | FLEX_TASKS_QUEUED_RUN_THREADPOOL | Number of deferred flex tasks run on thread pool | Invocations/Sec |
 | FLEX_TASKS_QUEUED | Number of deferred flex tasks queued | Invocations/Sec |
 | FLEX_TASKS_SUBMITTED | Number of flex tasks submitted | Invocations/Sec |
 | IDLE_CALLBACK_INVOCATIONS | Number of background work invocations | Invocations/Sec |
 | IDLE_CYCLES | Number of cycles spent in idle | Cycles/Sec |
 | IDLE_TIME | The percentage of the CPU time not used for handling I/Os | % |
 | LINGERING_FIBERS | Number of LINGERING fibers | Fibers |
 | MAIN_THREAD_DEFERRED_FLEX_TASK_CYCLES | Number of cycles main thread spent running deferred flex tasks | Cycles/Sec |
 | MAIN_THREAD_INLINE_FLEX_TASK_CYCLES | Number of cycles main thread spent running inline flex tasks | Cycles/Sec |
 | MAIN_THREAD_INLINE_FLEX_TASKS_AVG_RUNTIME | Average runtime of inline flex tasks run by the main thread | Cycles |
 | MAIN_THREAD_QUEUED_FLEX_TASKS_AVG_RUNTIME | Average runtime of deferred flex tasks run by the main thread | Cycles |
 | networkBuffers_CAPACITY | Number of data structures allocated to the networkBuffers pool | Structs |
 | networkBuffers_USED | Number of structs in the networkBuffers pool that are currently being used | Structs |
 | NODE_CONTEXT_SWITCHES | Number of context switches. | Switches |
 | NODE_HANG | The number of process (node) hangs per hang time range. | Number of hangs |
 | NODE_POLL_TIME | Time of scheduler stats polling. | usecs |
 | NODE_RUN_PERCENTAGE | Percentage of time process is running | percentage |
 | NODE_RUN_TIME | Time process is running. | usecs |
 | NODE_WAIT_PERCENTAGE | Percentage of time process is waiting on waitqueue | percentage |
 | NODE_WAIT_TIME | The Time the process is waiting on the wait queue. | usecs |
 | ObsBucketManagement_CAPACITY | Number of data structures allocated to the ObsBucketManagement pool | Structs |
 | ObsBucketManagement_STRUCT_SIZE | Number of bytes in each struct of the ObsBucketManagement pool | Bytes |
 | ObsBucketManagement_USED | Number of structs in the ObsBucketManagement pool that are currently being used | Structs |
 | ObsGateway_CAPACITY | Number of data structures allocated to the ObsGateway pool | Structs |
 | ObsGateway_STRUCT_SIZE | Number of bytes in each struct of the ObsGateway pool | Bytes |
 | ObsGateway_USED | Number of structs in the ObsGateway pool that are currently being used | Structs |
 | OUTRAGEOUS_HOGGERS | Number of hoggers taking an excessive amount of time to run | Invocations |
 | PENDING_FIBERS | Number of fibers pending for external events, such as a network packet or SSD response. Upon such an external event, they change state to scheduled fibers | Fibers |
 | QUEUE_TIME_FLEX_TASK_CYCLES | Queue time of deferred flex tasks | Cycles/Sec |
 | rdmaNetworkBuffers_CAPACITY | Number of data structures allocated to the rdmaNetworkBuffers pool | Structs |
 | rdmaNetworkBuffers_USED | Number of structs in the rdmaNetworkBuffers pool that are currently being used | Structs |
 | RELENTLESS_CYCLES | Number of cycles spent in relentless fibers | Cycles/Sec |
 | RELENTLESS_FIBERS | Number of relentless fibers that are ready to run and eager to get CPU cycles | Fibers |
 | SCHEDULED_FIBERS | Number of current fibers that are ready to run and eager to get CPU cycles | Fibers |
 | SLEEPY_FIBERS | Number of SLEEPY fibers | Fibers |
 | SLEEPY_RPC_SERVER_FIBERS | Number of SLEEPY RPC server fibers | Sleepy fiber detections |
 | SSD_CAPACITY | Number of data structures allocated to the SSD pool | Structs |
 | SSD_STRUCT_SIZE | Number of bytes in each struct of the SSD pool | Bytes |
 | SSD_USED | Number of structs in the SSD pool that are currently being used | Structs |
 | STEP_CYCLES | Histogram of time spent in a fiber | Fiber steps |
 | THREAD_POOL_DEFERRED_TASK_CYCLES | Number of cycles the thread pool spent running deferred tasks | Cycles/Sec |
 | THREAD_POOL_FLEX_TASK_CYCLES | Number of cycles the thread pool spent running flex tasks | Cycles/Sec |
 | THREAD_POOL_QUEUED_FLEX_TASKS_AVG_RUNTIME | Average runtime of deferred flex tasks run on thread pool | Cycles |
 | THREAD_POOL_TASK_INVOCATIONS | Number of tasks run by thread pool | Invocations/Sec |
 | TimedCallback_CAPACITY | Number of data structures allocated to the TimedCallback pool | Structs |
 | TimedCallback_STRUCT_SIZE | Number of bytes in each struct of the TimedCallback pool | Bytes |
 | TimedCallback_USED | Number of structs in the TimedCallback pool that are currently being used | Structs |
 | TIMER_CALLBACKS | Current number of timer callbacks | Callbacks |
 | TOTAL_FIBERS_COUNT | Number of fibers | Fibers |
 | UploadFileInfo_CAPACITY | Number of data structures allocated to the UploadFileInfo pool | Structs |
 | UploadFileInfo_STRUCT_SIZE | Number of bytes in each struct of the UploadFileInfo pool | Bytes |
 | UploadFileInfo_USED | Number of structs in the UploadFileInfo pool that are currently being used | Structs |

### Resolve Inode Cache

 | **Type** | **Description** | **Units** |
 | -------------------------------------- | ----------------------------- | --------- |
 | RESOLVER_INODE_TO_PATH_CACHE_HITS | resolveInodeToPath cache hits | Queries |
 | RESOLVER_INODE_TO_PATH_CACHE_MISS | resolveInodeToPath cache miss | Queries |

### RPC

 | **Type** | **Description** | **Units** |
 | ---------------------------------- | ------------------------------------------------------------------ | ------------ |
 | CLIENT_CANCELED_REQUESTS | Number of requests canceled by the client | Calls/Sec |
 | CLIENT_DROPPED_RESPONSES | Number of responses dropped by the client | Calls/Sec |
 | CLIENT_ENCRYPTION_AUTH_FAILURES | Number of authentication failures by the client | Calls/Sec |
 | CLIENT_MISSING_ENCRYPTION_KEY | Number of times the client was missing an encryption key | Calls/Sec |
 | CLIENT_RECEIVED_EXCEPTIONS | Number of exceptions received by the client | Calls/Sec |
 | CLIENT_RECEIVED_RESPONSES | Number of responses received by the client | Calls/Sec |
 | CLIENT_RECEIVED_TIMEOUTS | Number of timeouts experienced by the client | Calls/Sec |
 | CLIENT_ROUNDTRIP_AVG_LOW | Round-trip average of client low-priority RPC calls | Microseconds |
 | CLIENT_ROUNDTRIP_AVG_NORM | Round-trip average of client normal priority RPC calls | Microseconds |
 | CLIENT_ROUNDTRIP_AVG | Round-trip average of client normal and low priority RPC calls | Microseconds |
 | CLIENT_RPC_CALLS_DOWNGRADED | Number of client-downgraded RPC calls | RPC/Sec |
 | CLIENT_RPC_CALLS_LOW | Number of low-priority RPC calls | RPC/Sec |
 | CLIENT_RPC_CALLS_NORM | Number of normal priority RPC calls | RPC/Sec |
 | CLIENT_RPC_CALLS | Number of all priorities of RPC calls | RPC/Sec |
 | CLIENT_SENT_REQUESTS | Number of requests sent by the client | Calls/Sec |
 | DEUS_EX_MBUF_LIMITED | Number of RPCs slow down due to low MBuf reserves | Ops/Sec |
 | DEUS_EX_NO_FIBERS | Number of RPCs put in DeusEx due to lack of global fibers | Ops/Sec |
 | DEUS_EX_NOT_EMPTY | Number of RPCs put in DeusEx to preserve RPC order | Ops/Sec |
 | DEUS_EX_RPC_MAX_FIBERS | Number of RPCs put in DeusEx due to RPC max fibers | Ops/Sec |
 | FIRST_RESULTS | Number of first results per second | Ops/Sec |
 | MBUF_LIMITED_SLEEP | Number of times wait due to low MBuf reserves | Actions/Sec |
 | RPC_ENCRYPTION_SETUP_FAILURES | Number of encryption key setup failures | Failures |
 | SERVER_ABORTS | Number of server received aborts | Calls/Sec |
 | SERVER_DROPPED_REQUESTS | Number of requests dropped by the server | Calls/Sec |
 | SERVER_ENCRYPTION_AUTH_FAILURES | Number of encryption authentication failures at the server | Calls/Sec |
 | SERVER_MISSING_ENCRYPTION_KEY | Number of requests missing encryption key at the server | Calls/Sec |
 | SERVER_PROCESSING_AVG | Average time to process server RPC calls | Microseconds |
 | SERVER_PROCESSING_TIME | Histogram of the time it took the server to process a request | RPCs |
 | SERVER_REJECTS | Number of times the server rejected a request | Calls/Sec |
 | SERVER_RPC_CALLS_UPGRADED | Number of server-upgraded RPC calls | RPC/Sec |
 | SERVER_RPC_CALLS | Number of server RPC calls | RPC/Sec |
 | SERVER_SENT_EXCEPTIONS | Number of exceptions sent by the server as a response | Calls/Sec |
 | SERVER_SENT_RESPONSES | Number of responses the server sent | Calls/Sec |
 | SERVER_UNENCRYPTED_REFUSALS | Number of requests refused due to missing encryption at the server | Calls/Sec |
 | TIME_TO_FIRST_RESULT | Average latency to the first result of a MultiCall | Microseconds |

### Scrubber

 | **Type** | **Description** | **Units** |
 | --------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | --------------- |
 | BLOCK_CONSISTENCY_CHECK_LATENCY | Average latency of checking block consistency | Micros |
 | BLOCK_CONSISTENCY_CHECKS | Number of blocks that were checked for consistency against their block-used-state | Blocks/Sec |
 | CLEANED_CHUNKS | Number of chunks that were cleaned by the scrubber | Chunks/Sec |
 | DEGRADED_READS | Number of degraded reads for scrubbing | Requests/Sec |
 | FALSE_USED_CHECK_LATENCY | Average latency of checking false used per block | Micros |
 | FALSE_USED_EXTRA_NOTIFIED | Number of blocks that were notified as used by the mark-extra-used mechanism | Blocks/Sec |
 | INTERRUPTS | Number of scrubs that were interrupted | Occurrences/Sec |
 | NETWORK_BUDGET_WAIT_LATENCY | Average latency of waiting for our network budget | Micros |
 | NOT_REALLY_DIRTY_BLOCKS | Number of marked dirty blocks that ScrubMissingWrites found were clean | Blocks/Sec |
 | NUM_COPY_DISCARDED_BLOCKS | Number of copied blocks that were discarded | Blocks/Sec |
 | NUM_COPY_DISCARDS | Number of times we discarded scrubber copy work | Occurrences/Sec |
 | NUM_INVENTED_STRIPES_DISCARD_BLOCKS | Number of blocks that were discarded due to invented stripes | Blocks/Sec |
 | NUM_INVENTED_STRIPES_DISCARDS | Number of times we discarded all scrubber work due to invented stripes | Occurrences/Sec |
 | NUM_SCRUBBER_DISCARD_INTERMEDIATES | Number of times we discarded all intermediate scrubber work | Occurrences/Sec |
 | NUM_SMW_DISCARDED_BLOCKS | Number of SMW'd blocks that were discarded | Blocks/Sec |
 | NUM_SMW_DISCARDS | Number of times we discarded scrubber SMW work | Occurrences/Sec |
 | NUM_STRIPE_SKIPPED_NOT_FULLY_READ | Number of stripes skipped since stripe is not fully read | Occurrences |
 | PLACEMENT_SELECTION_LATENCY | Average latency of scrubbed placement selection | Micros |
 | RAID_PLACEMENT_SCANS_COMPLETED | Number of placement scan completions | Occurrences |
 | READ_BATCH_SOURCE_BLOCKS | Number of source blocks read per batch | Batches |
 | READ_BLOCKS_LATENCY | Average latency of read blocks | Micros |
 | READS_CALLED | Number of blocks that were read | Blocks/Sec |
 | RELOCATE_BLOCKS_LATENCY | Average latency of relocating blocks | Micros |
 | RELOCATED_BLOCKS | Number of blocks that were relocated for eviction | Blocks/Sec |
 | RETRUSTED_UNPROTECTED_DIRTY_BLOCKS | Number of dirty blocks that ScrubMissingWrites retrusted because they were unprotected | Blocks/Sec |
 | REWRITTEN_DIRTY_BLOCKS | Number of dirty blocks that ScrubMissingWrites rewrote to clean them | Blocks/Sec |
 | SCAN_LIKELY_LEAKED_BLOCKS | Number of free blocks encountered during a scan that was marked as KnownUsed in the RAID | Occurrences |
 | SCRUB_BATCHES_LATENCY | Average latency of scrub batches | Millis |
 | SCRUB_FALSE_USED_FAILED_READS | Number of blocks that we failed to read for scrub-false-used | Blocks/Sec |
 | SCRUB_FALSE_USED_FAILED | Number of placements we failed to fully scrub-false-used | Occurrences/Sec |
 | SCRUB_FALSE_USED_PLACEMENTS | Number of placements we finished scrub-false-used | Occurences/Sec |
 | SCRUB_FALSE_USED_WAS_UNPROTECTED | Number of blocks that were falsely marked used and unprotected | Blocks/Sec |
 | SCRUB_IN_FLIGHT_CORRUPTION_DETECTED | Number of in-flight corruptions detected when scrubbing | Occurrences |
 | SCRUB_PREPARATION_FAILED | Number of times we failed to prepare() a task and aborted scrub of placement | Occurrences/Sec |
 | SFU_CHECK_FREE | Number of blocks that were detected as false-used and freed | Blocks/Sec |
 | SFU_CHECK_SECONDARY | Number of blocks that were detected as secondary | Blocks/Sec |
 | SFU_CHECK_USED_CKSUM_ERR | Number of blocks that were detected as used with checksum error | Blocks/Sec |
 | SFU_CHECK_USED | Number of blocks that were detected as used | Blocks/Sec |
 | SFU_CHECKS | Number of blocks that were scrubbed-false-used | Blocks/Sec |
 | SFU_FREE_STRIPE_LATENCY | Average latency of handling a read of a free stripe | Micros |
 | SFU_FREE_STRIPES | Number of free stripes that were scrubbed-false-used | Stripes/Sec |
 | SFU_USED_STRIPE_LATENCY | Average latency of handling a read of a used stripe | Micros |
 | SFU_USED_STRIPES | Number of used stripes that were scrubbed-false-used | Stripes/Sec |
 | SOURCE_READS | Number of source/committed superset blocks directly read by the scrubber | Blocks/Sec |
 | STRIPE_DATA_IS_BLOCK_USED_LATENCY | Average latency of isBlockUsed during stripe verification | Micros |
 | STRIPE_DATA_IS_BLOCK_USED | Number of isBlockUsed during stripe verification | Blocks/Sec |
 | TARGET_COPIED_CHUNKS | Number of chunks that were copied to the target by the scrubber | Chunks/Sec |
 | UPDATE_PLACEMENT_INFO_LATENCY | Average latency of updating the placement info quorum | Micros |
 | UPDATE_PLACEMENT_INFO | Number of times we ran updatePlacementInfo | Occurrences/Sec |
 | WONT_CLEAN_COPYING | Number of actually dirty blocks that ScrubMissingWrites refused to clean because they will be moved to target anyway | Blocks/Sec |
 | WRITE_BATCH_SOURCE_BLOCKS | Number of source blocks to write in batch | Batches |
 | WRITE_BATCH_TARGET_BLOCKS | Number of target blocks to write in batch | Batches |
 | WRITE_BLOCKS_LATENCY | Average latency of writing blocks | Micros |
 | WRITES_CALLED | Number of blocks that were written | Blocks/Sec |

### Squelch

 | **Type** | **Description** | **Units** |
 | ------------------------------------------------------- | ------------------------------------------------------ | ------------- |
 | BLOCKS_PER_DESQUELCH | Number of squelch blocks per desquelch | Desquelches |
 | EXTENT_DESQUELCHES_NUM | Number of desquelches | Times |
 | EXTENT_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | HASH_DESQUELCHES_NUM | Number of desquelches | Times |
 | HASH_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | INODE_DESQUELCHES_NUM | Number of desquelches | Times |
 | INODE_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | JOURNAL_DESQUELCHES_NUM | Number of desquelches | Times |
 | JOURNAL_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | MAX_BLOCKS_WITH_TEMPORAL_SQUELCH_ITEMS_IN_BUCKET | Number of blocks with temporal squelch items in bucket | Blocks |
 | MAX_TEMPORAL_SQUELCH_ITEMS_IN_BUCKET | Number of temporal squelch items in bucket | Squelch items |
 | ODL_DESQUELCHES_NUM | Number of desquelches | Times |
 | ODL_PAYLOAD_DESQUELCHES_NUM | Number of desquelches | Times |
 | ODL_PAYLOAD_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | ODL_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | REGISTRY_L1_DESQUELCHES_NUM | Number of desquelches | Times |
 | REGISTRY_L1_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | REGISTRY_L2_DESQUELCHES_NUM | Number of desquelches | Times |
 | REGISTRY_L2_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | SPATIAL_DIGEST_DESQUELCHES_NUM | Number of desquelches | Times |
 | SPATIAL_DIGEST_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | SPATIAL_SQUELCH_DESQUELCHES_NUM | Number of desquelches | Times |
 | SPATIAL_SQUELCH_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | SUPERBLOCK_DESQUELCHES_NUM | Number of desquelches | Times |
 | SUPERBLOCK_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |
 | TEMPORAL_SQUELCH_DESQUELCHES_NUM | Number of desquelches | Times |
 | TEMPORAL_SQUELCH_SQUELCH_BLOCKS_READ | Number of squelch blocks desquelched | Blocks |

### SSD

 | **Type** | **Description** | **Units** |
 | -------------------------------------------------- | ---------------------------------------------------------------------------------------------- | -------------- |
 | CLEAN_CHUNK_SKIPPED | Number of clean chunks skipped | Chunks |
 | DRIVE_ACTIVE_IOS | The number of in-flight IOs against the SSD during sampling | IOs |
 | DRIVE_AER_RECEIVED | Number of AER reports | reports |
 | DRIVE_CANCELLED_COMPLETED_BLOCKS | Drive cancelled completed blocks | Blocks/Sec |
 | DRIVE_CANCELLED_NOT_SUBMITTED_BLOCKS | Drive cancelled not submitted blocks | Blocks/Sec |
 | DRIVE_COMPLETED_OVER_COUNT | Drive completed count > 1 detected | Occurrences |
 | DRIVE_E2E_CORRECTION_COUNT | Drive E2E correction count | Error Count |
 | DRIVE_ENDURANCE_USED | Drive endurance percentage used | % |
 | DRIVE_FORFEITS | Number of IOs forfeited due to lack of memory buffers | Operations/Sec |
 | DRIVE_IDLE_CYCLES | Number of cycles spent in idle | Cycles/Sec |
 | DRIVE_IDLE_TIME | Percentage of the CPU time not used for handling I/Os | % |
 | DRIVE_IO_OVERLAPPED | Number of overlapping IOs | Operations |
 | DRIVE_IO_TOO_LONG | Number of IOs that took longer than expected | Operations/Sec |
 | DRIVE_LATENCY | Measure the latencies up to 5ms (higher latencies are grouped) | Requests |
 | DRIVE_LOAD | Drive Load at sampling time | Load |
 | DRIVE_MAX_ERASE_COUNT | Drive maximum block erase count | Erase Count |
 | DRIVE_MEDIA_BLOCKS_READ | Blocks read from the SSD media | Blocks/Sec |
 | DRIVE_MEDIA_BLOCKS_WRITE | Blocks are written to the SSD media | Blocks/Sec |
 | DRIVE_MEDIA_ERRORS | SSD Media Errors | IO/Sec |
 | DRIVE_MIN_ERASE_COUNT | Drive minimum block erase count | Erase Count |
 | DRIVE_NON_MEDIA_ERRORS | SSD Non-Media Errors | IO/Sec |
 | DRIVE_PCI_CORRECTABLE_ERROR_COUNT | Drive PCI Correctable error count | Error Count |
 | DRIVE_PCI_INACCESSIBLE | Number of PCI Inaccessible errors detected | Count |
 | DRIVE_PCI_LINK_RETRAIN_COUNT | Drive PCI link retrain count | Error Count |
 | DRIVE_PENDING_IOS | The number of IOs waiting to start executing during sampling | IOs |
 | DRIVE_PUMP_LATENCY | Latency between SSD pumps | Microseconds |
 | DRIVE_PUMPED_IOS | Number of requests returned in a pump | Pumps |
 | DRIVE_PUMPS_DELAYED | Number of Drive pumps that got delayed | Operations/Sec |
 | DRIVE_PUMPS_SEVERELY_DELAYED | Number of Drive pumps that got severely delayed | Operations/Sec |
 | DRIVE_READ_LATENCY | Drive Read Execution Latency | Microseconds |
 | DRIVE_READ_OPS | Drive Read Operations | IO/Sec |
 | DRIVE_READ_RATIO_PER_SSD_READ | Drive Read OPS Per SSD Request | Ratio |
 | DRIVE_REMAINING_IOS | Number of requests still in the drive after a pump | Pumps |
 | DRIVE_REMAINING_SPARES | Drive remaining spares | % |
 | DRIVE_REQUEST_BLOCKS | Measure drive request size distribution | Requests |
 | DRIVE_SOFT_ECC_COUNT | Drive Soft ECC Error Count | Error Count |
 | DRIVE_SSD_PUMPS | Number of drive pumps that resulted in the data flow from/to drive | Pump/Sec |
 | DRIVE_UNALIGNED_IOS | Drive unaligned IOs count | Error Count |
 | DRIVE_UNCORRECTABLE_READ_COUNT | Drive uncorrectable read count | Error Count |
 | DRIVE_UTILIZATION | Percentage of time the drive had an active IO submitted to it | % |
 | DRIVE_WAF_INTERVAL | Drive Interval write amplification | Factor |
 | DRIVE_WAF_LIFETIME | Drive lifetime write amplification | Factor |
 | DRIVE_WRITE_LATENCY | Drive Write Execution Latency | Microseconds |
 | DRIVE_WRITE_OPS | Drive Write Operations | IO/Sec |
 | DRIVE_WRITE_RATIO_PER_SSD_WRITE | Drive Write OPS Per SSD Request | Ratio |
 | DRIVE_XOR_RECOVERY_COUNT | Drive XOR recovery count | Error Count |
 | LEAKED_AIOVEC_NETBUF | Number of leaked AIOVec netbufs on uncompleted IO | Operations |
 | NVKV_CHUNK_OUT_OF_SPACE | Number of failed attempts to allocate a stripe in an NVKV chunk | Attempts/Sec |
 | NVKV_INVALIDATOR_MATCHED | Number of NVKV invalidators matching the data | Attempts/Sec |
 | NVKV_OUT_OF_CHUNKS | Number of failed attempts to allocate an NVKV chunk | Attempts/Sec |
 | NVKV_OUT_OF_SUPERBLOCK_ENTRIES | Number of failed attempts to allocate a superblock NVKV entry | Attempts/Sec |
 | NVME_NAMESPACE_CAPACITY | NVMe namespace capacity | Blocks |
 | NVME_NAMESPACE_SIZE | The size of the NVMe namespace | Blocks |
 | NVME_NAMESPACE_UTILIZATION | NVMe namespace utilization | Blocks |
 | NVME_SMART_AVAILABLE_SPARE_THRESHOLD | Normalized percentage of the available spare falls below the threshold | % |
 | NVME_SMART_AVAILABLE_SPARE | Normalized percentage when the available spare falls below the threshold | % |
 | NVME_SMART_COMPOSITE_TEMP | Current composite temperature of the container in Kelvins | Kelvin |
 | NVME_SMART_CONTROLLER_BUSY_TIME | The duration the controller is busy with I/O commands | Minutes |
 | NVME_SMART_CRITICAL_COMPOSITE_TEMP_TIME | The time spent in critical composite temperature state | Minutes |
 | NVME_SMART_CRITICAL_WARNING | Critical warnings regarding the drive controller state | BitFields |
 | NVME_SMART_DATA_UNITS_READ | The number of 512-byte data units the server has read from the controller (in millions) | Count |
 | NVME_SMART_DATA_UNITS_WRITTEN | The number of 512-byte data units the host has written to the controller (in millions) | Count |
 | NVME_SMART_ERROR_LOG_ENTRIES | The total number of Error Information log entries over the controller's lifetime | Occurrences |
 | NVME_SMART_HOST_READ_CMDS | The number of read commands completed by the controller | Occurrences |
 | NVME_SMART_HOST_WRITE_CMDS | The number of write commands completed by the controller | Occurrences |
 | NVME_SMART_MEDIA_ERRORS | The number of unrecovered data integrity errors detected by the controller | Occurrences |
 | NVME_SMART_POWER_CYCLES | The number of power cycles | Occurrences |
 | NVME_SMART_TEMP_SENSOR_1 | The current temperature reported by temperature sensor 1 | Kelvin |
 | NVME_SMART_TEMP_SENSOR_2 | The current temperature reported by temperature sensor 2 | Kelvin |
 | NVME_SMART_TEMP_SENSOR_3 | The current temperature reported by temperature sensor 3 | Kelvin |
 | NVME_SMART_TEMP_SENSOR_4 | The current temperature reported by temperature sensor 4 | Kelvin |
 | NVME_SMART_TEMP_SENSOR_5 | The current temperature reported by temperature sensor 5 | Kelvin |
 | NVME_SMART_TEMP_SENSOR_6 | The current temperature reported by temperature sensor 6 | Kelvin |
 | NVME_SMART_TEMP_SENSOR_7 | The current temperature reported by temperature sensor 7 | Kelvin |
 | NVME_SMART_TEMP_SENSOR_8 | The current temperature reported by temperature sensor 8 | Kelvin |
 | NVME_SMART_THERMAL_MGMT_TEMP1_TRANSITION_CNT | The number of times the controller entered lower active power states due to thermal management | Occurrences |
 | NVME_SMART_THERMAL_MGMT_TEMP2_TRANSITION_CNT | The number of times the controller entered lower active power states due to thermal management | Occurrences |
 | NVME_SMART_TOTAL_THERMAL_MGMT_TEMP1_TIME | The total time the controller spent in lower power states for thermal management temperature 1 | Seconds |
 | NVME_SMART_TOTAL_THERMAL_MGMT_TEMP2_TIME | The total time the controller spent in lower power states for thermal management temperature 2 | Seconds |
 | NVME_SMART_UNSAFE_SHUTDOWNS | The number of unsafe shutdown events | Occurrences |
 | NVME_SMART_USED_PERCENTAGE | Vendor-specific estimate of the percentage of NVM subsystem life used | % |
 | NVME_SMART_WARNING_COMPOSITE_TEMP_TIME | The time spent in warning composite temperature state | Minutes |
 | SPDK_CUSE_CMDS_BLOCKED | Number of SPDK CUSE commands blocked | Count |
 | SPDK_CUSE_CMDS_FAILED | Number of SPDK CUSE commands failed | Count |
 | SPDK_CUSE_CMDS_INPROGRESS | Number of SPDK CUSE commands in progress | Count |
 | SPDK_CUSE_CMDS_ISSUED | Number of SPDK CUSE commands issued | Count |
 | SPDK_CUSE_CMDS_SUCCEEDED | Number of SPDK CUSE commands succeeded | Count |
 | SSD_BLOCKS_READ | Number of blocks read from the SSD service | Blocks/Sec |
 | SSD_BLOCKS_WRITTEN | Number of blocks written to the SSD service | Blocks/Sec |
 | SSD_CHUNK_ALLOCS_TRIMMED | Number of chunk allocations from the trimmed queue | Chunks |
 | SSD_CHUNK_ALLOCS_UNTRIMMED | Number of chunk allocations from the untrimmed queue | Chunks |
 | SSD_CHUNK_ALLOCS | Number of chunk allocations | Chunks |
 | SSD_CHUNK_FREE_TRIMMED | Number of free trimmed chunks | Chunks |
 | SSD_CHUNK_FREE_UNTRIMMED | Number of free untrimmed chunks | Chunks |
 | SSD_CHUNK_FREES | Number of chunk frees | Chunks |
 | SSD_CHUNK_TRIMS | Number of trims performed | Chunks |
 | SSD_CHUNKS_IN_USE | Number of allocated chunks | Chunks |
 | SSD_E2E_BAD_CSUM | End-to-End checksum failures | IO/Sec |
 | SSD_READ_ERRORS | Errors in reading blocks from the SSD service | Blocks/Sec |
 | SSD_READ_LATENCY | Avg. latency of read requests from the SSD service | Microseconds |
 | SSD_READ_REQS_LARGE_NORMAL | Number of large normal read requests from the SSD service | IO/Sec |
 | SSD_READ_REQS | Number of read requests from the SSD service | IO/Sec |
 | SSD_SCRATCH_BUFFERS_USED | Number of scratch blocks used | Blocks |
 | SSD_TRIM_TIMEOUTS | Number of trim timeouts | Timeouts |
 | SSD_UNALIGNED_WRITES | Number of unaligned writes | Ops |
 | SSD_WRITE_ERRORS | Errors in writing blocks to the SSD service | Blocks/Sec |
 | SSD_WRITE_LATENCY | Latency of writes to the SSD service | Microseconds |
 | SSD_WRITES_REQS_LARGE_NORMAL | Number of large normal priority write requests to the SSD service | IO/Sec |
 | SSD_WRITES | Number of write requests to the SSD service | IO/Sec |
 | SSDS_IO_ERRORS | IO errors on the SSD service | Blocks/Sec |
 | SSDS_IOS | IOs performed on the SSD service | IO/Sec |

### Statistics

 | **Type** | **Description** | **Units** |
 | -------------------------------- | -------------------------------------------------------------------------------------- | ----------- |
 | AVAILABLE_HOST_MEMORY_MB | Amount of Free Memory | MB |
 | GATHER_FROM_NODE_LATENCY_NET | Time spent on responding to a stats-gathering request (not including metadata) | Seconds/Sec |
 | GATHER_FROM_NODE_LATENCY | Time spent responding to a stats-gathering request (not including metadata) | Seconds/Sec |
 | GATHER_FROM_NODE_SLEEP | Time spent in-between responding to a stats-gathering request (not including metadata) | Seconds/Sec |
 | TIMES_QUERIED_STATS | Number of times the process queried other processes for stats | Times |
 | TIMES_QUERIED | Number of times the process was queried for stats (not including metadata) | Times |

### Telemetry

 | **Type** | **Description** | **Units** |
 | ---------------------------------- | ---------------------------------------------------- | --------- |
 | TOTAL_ADDED_FILES | Total number of files added | Files |
 | TOTAL_BUFFER_RECEIVED_BYTES | Total number of bytes received by vector buffers | Bytes |
 | TOTAL_BUFFER_RECEIVED_EVENTS | Total number of events received to vector buffers | Events |
 | TOTAL_BUFFER_SENT_BYTES | Total number of bytes sent to vector buffers | Bytes |
 | TOTAL_BUFFER_SENT_EVENTS | Total number of events sent to vector buffers | Events |
 | TOTAL_COMPONENT_RECEIVED_BYTES | Total number of bytes received by vector components | Bytes |
 | TOTAL_COMPONENT_RECEIVED_EVENTS | Total number of events received by vector components | Events |
 | TOTAL_COMPONENT_SENT_BYTES | Total number of bytes sent to vector components | Bytes |
 | TOTAL_COMPONENT_SENT_EVENTS | Total number of events sent to vector components | Events |
 | TOTAL_DELETED_FILES | Total number of files deleted | Files |
 | TOTAL_RESUMED_FILES | Total number of files resumed | Files |
 | TOTAL_UNWATCHED_FILES | Total number of files unwatched | Files |

### Unlink Log

 | **Type** | **Description** | **Units** |
 | ------------------------------------------- | --------------------------------------------------------------------- | ----------- |
 | UNLINK_LOG_APPEND_DENIALS | Number of rejects for adding new entry due to ODH marked as non-clean | Denials |
 | UNLINK_LOG_UNEXPECTED_ODH_INDEX_CHANGE | Number of unexpected ODH index changes | Occurrences |

### WTracer Daemon Stats

 | **Type** | **Description** | **Units** |
 | -------------------------------- | ----------------------------------------------------------- | ----------- |
 | DAEMON_EXPORTED_BYTES | Number of exported bytes by the wtracer daemon | Bytes/Sec |
 | DAEMON_FOUND_ENTRIES | Number of found entries by the wtracer daemon | Entries/Sec |
 | DAEMON_IO_WRITE_FAILURES | Number of write I/O operations failed by the wtracer daemon | Errors |
 | DAEMON_NUM_BULK_FLUSHES | Number of bulk flushes of entries in the wtracer daemon | Flushes/Sec |
 | DAEMON_PROCESSED_BLOBS | Number of processed blobs by the wtracer daemon | Blobs/Sec |
 | DAEMON_PROCESSED_ENTRIES | Number of processed entries by the wtracer daemon | Entries/Sec |
 | DAEMON_PROCESSED_LOST_ENTRIES | Number of processed lost entries by the wtracer daemon | Entries/Sec |
 | DAEMON_TIME_SPENT_ENHANCER | Time spent in the wtracer daemon enhancer | hnsecs |

### WTracer Dumper Stats

 | **Type** | **Description** | **Units** |
 | -------------------------- | --------------------------------------------- | ----------- |
 | DUMPER_ACTIVE_AREAS | Number of areas dumpers currently have active | Areas |
 | DUMPER_ACTIVE_HISTOGRAMS | Number of histogram dumpers currently active | Histograms |
 | DUMPER_BYTES_OUT | Number of bytes the dumper outputs to files | Bytes/Sec |
 | DUMPER_ENTRIES_OUT | Number of entries the dumper outputs to files | Entries/Sec |
 | DUMPER_ERRORS | Total number of errors dumper triggered | Errors |
 | DUMPER_LOST_TRACES | Number of lost traces detected by the dumper | Entries/Sec |
 | DUMPER_SKIPPED_BYTES | Total bytes skipped by the dumper | Bytes |
 | DUMPER_TOTAL_AREAS | Total number of areas dumped | Areas |
 | DUMPER_TOTAL_BLOBS | Number of blobs the dumper dumped | Blobs/Sec |
 | DUMPER_TOTAL_CHUNKS | Number of chunks the dumper dumped | Chunks/Sec |
 | DUMPER_TOTAL_HISTOGRAMS | Total number of histograms dumped | Histograms |

<!-- ============================================ -->
<!-- File 159/259: operation-guide_insights.md -->
<!-- ============================================ -->

# Insights

Insights provide the top processes usage, remote procedure calls (RPCs), and drives load and latency. The insights help you make informed decisions about future actions to resolve bottlenecks and improve performance. The WEKA system collects insights data from the CPUs and drives.

To display insights, select **Investigate > Insights**.

## CPU insight

CPU insight provides a quick usage overview of the Compute, Drives, and Frontend processes, which run on stateless clients or backends as a gateway.

CPU insight shows the average load in the last 15 minutes and the top 10 processes with the highest CPU usage every minute.

You can select up to three processes to show the top three RPCs that kept the process busy. You can compare the RPCs between the selected processes.

### Display the top RPC calls of the compute cores

1. From the left pane, select **CPU**.
2. Select the **COMPUTE** tab.
3. Select the compute cores you want to compare (maximum three).

### Display the top RPC calls of the drive cores

1. From the left pane, select **CPU**.
2. Select the **DRIVES** tab.
3. Select the drive cores you want to compare (maximum three).

### Display the top RPC calls of the frontend cores

1. From the left pane, select **CPU**.
2. Select the **FRONTEND** tab
3. Select the frontend cores you want to compare (maximum three).

### Display the top RPC calls of the protocol gateways

1. From the left pane, select **CPU**.
2. Select the **GATEWAY** tab
3. Select the gateway cores you want to compare (maximum three).

## Drive insight

Drives insight provides a load overview of the drives in the cluster. The view of the drives is divided into three groups based on the load level. It shows the drives information when reaching a medium (load level between 128 and 254) and high load (load level 255).

You can compare each drive's latency with the average and other drives.

### Display the top drives' latency

1. From the left pane, select **DRIVES**.
2. Select the backend drives you want to compare (maximum three).

<!-- ============================================ -->
<!-- File 160/259: operation-guide_audit-and-forwarding-management.md -->
<!-- ============================================ -->

---
description:
---

# Audit and forwarding management

## Overview

Effective data management requires a robust auditing and forwarding functionality to ensure data security, compliance with regulatory standards, and integration with advanced workflows. The auditing and forwarding functionality provides continuous event streams that record data access, modifications, and deletions, enabling organizations to monitor and respond to activity across their storage environment.

This auditing and forwarding functionality supports compliance with regulations such as HIPAA and GINA, aids in investigating security incidents, and ensures the integrity of stored data. Additionally, it facilitates operational insights by enabling analysis of user behavior, such as identifying which datasets are accessed, by whom, and at what times.

The audit event stream can also be used to trigger automated workflows. For example, an external system can monitor for specific operations in defined paths and initiate corresponding actions when conditions are met.

The auditing and forwarding functionality is designed to minimize any impact on client performance. To enable this, it is built upon a lightweight and scalable tracing system that monitors core filesystem events. To further reduce any impact on workloads, captured events are enriched asynchronously with contextual metadata, such as full file paths, to provide meaningful and actionable information.

By default, the auditing and forwarding functionality is disabled. It must first be enabled at the system level, a process that sets up the necessary infrastructure and system components required for auditing. Once active, the auditing and forwarding functionality is configurable on a per-filesystem basis, allowing fine-grained control over which datasets are monitored and which operations are of interest.

Audit events are forwarded using a modular, pluggable framework. This design enables flexible integration with a variety of external platforms, addressing diverse customer environments. The forwarding process is managed by the telemetry gateway (telemetry container) and an internal observability pipeline, which processes and routes audit events.

The auditing and forwarding functionality supports exporting audit events to the following platforms:

* Kafka
* Splunk
* Amazon S3

## Audit operation types

The auditing and forwarding functionality records various filesystem activities. The following operations apply to actions originating from POSIX and other protocols like NFS, SMB, and S3, as they all translate into standard filesystem events. For example, deleting a file over S3 generates the same `UNLINK` event as a POSIX `rm` command.

Operations specific to filesystem management, such as `MOUNT` and `UMOUNT`, are generally considered POSIX-only events.

The following table describes each audited operation type.

 | Operation | Description |
 | --- | --- |
 | FILEOPEN | Logs the initial opening of a file for read or write access. For performance reasons, subsequent opens are logged only when the access type changes or moves between system nodes. |
 | ATOMIC_FILEOPEN | Logs the creation and opening of a file as a single, atomic action. Unlike FILEOPEN, this operation is always recorded. |
 | LOOKUP | Logs the action of searching for a file or directory by its name. |
 | READDIR | Logs the reading of a directory's contents, such as when a user lists its files. |
 | MKNOD | Logs the creation of a file, special file, or directory. |
 | RENAME | Logs the renaming of a file or directory. |
 | RMDIR | Logs the removal of a directory. |
 | GETATTR | Logs the retrieval of file attributes (metadata), such as access time or permissions. |
 | SETATTR | Logs the modification of file attributes (metadata), such as changing permissions or file size. |
 | READLINK | Logs the reading of a symbolic link's destination path. |
 | UNLINK | Logs the deletion of a file, symbolic link, or hard link. |
 | SYMLINK | Logs the creation of a symbolic link (a shortcut to another file or directory). |
 | LINK | Logs the creation of a hard link (an additional name for an existing file). |
 | SETXATTR | Logs the addition of a custom attribute (extended metadata) to a file. |
 | LISTXATTR | Logs the listing of all custom attributes for a file. |
 | GETXATTR | Logs the reading of a specific custom attribute from a file. |
 | RMXATTR | Logs the removal of a custom attribute from a file. |
 | MOUNT | Logs the mounting of a filesystem, making it accessible. |
 | UMOUNT | Logs the unmounting of a filesystem, making it inaccessible. |
 | HEARTBEAT | Sends a periodic message from each node to confirm that the audit system is operational. |
 | LOST_AUDIT | Sends a special message to indicate that one or more audit events may have been lost, signaling a potential gap in the audit trail. |

### Operation categories for configuration

When configuring audit logging, the system allows enabling auditing based on high-level operation categories. These categories serve as groupings of multiple specific audit operations. This simplifies the configuration process by allowing users to select broader classes of operations to monitor.

These categories are specified in the command-line and configuration interfaces to control the scope of audit logging efficiently.

 | Category (configurable operation type) | Included audit operations |
 | --- | --- |
 | open | FILEOPEN, ATOMIC_FILEOPEN |
 | create | MKNOD, SYMLINK, LINK, SETATTR, ATOMIC_FILEOPEN |
 | read | READDIR |
 | modify | SETATTR, SETXATTR, RMXATTR |
 | delete | UNLINK, RMDIR |
 | rename | RENAME |
 | session_management | MOUNT, UMOUNT, HEARTBEAT, LOST_AUDIT |

## Audit message format

Each audit event sent to an external system is structured in a consistent message format containing fields that provide detailed information about the audited operation. The audit message can contain the following fields:

 | Field | Description |
 | --- | --- |
 | category | The management category of the audited operation. |
 | recordType | The type of record. For audit events, this is always AUDIT. |
 | recordId | A unique identifier for the specific audit record. |
 | recordVersion | The version number of the audit message format. |
 | operation | The type of filesystem operation that was performed (for example, FILEOPEN, SETATTR). |
 | timestamp | The date and time when the operation occurred. |
 | clusterGuid | The unique identifier of the cluster. |
 | clusterName | The name of the cluster where the event occurred. |
 | clientIp | The IP address of the client machine that initiated the operation. |
 | clientHostname | The hostname of the client machine that initiated the operation. |
 | uid | The user ID (UID) of the user who performed the operation. |
 | gid | The group ID (GID) of the user who performed the operation. |
 | fsId | The unique identifier for the filesystem where the operation occurred. |
 | fsName | The name of the filesystem where the operation occurred. |
 | snapshotId | The ID of the snapshot related to the transaction. An ID of 0 indicates the live filesystem. |
 | snapshotName | The name of the snapshot related to the transaction. |
 | inodeId | The unique inode identifier for the file or directory involved in the operation. |
 | parentinodeID | The unique inode identifier of the parent directory. |
 | fullPath | The complete path used to identify the object in the filesystem. |
 | errorCode | The status code of the operation. A value of 0 indicates success. |
 | targetFullPath | For operations such as RENAME or SYMLINK, this is the destination path of the object. |
 | feOpId | The operation ID from the front-end container. |
 | requestedAccess | The type of access requested during an open operation, such as read or write. |
 | timeSent | The timestamp of when the audit message was sent from the telemetry gateway. |
 | wekaServer | The hostname of the server that serviced the audit event. |
 | key | The key of the extended attribute involved in an xattr operation. |
 | modeBits | The new POSIX mode bits of a file or directory after a permission change operation. |
 | outageStart | For LOST_AUDIT events, an estimate of when the audit message outage started. |
 | outageEnd | For LOST_AUDIT events, an estimate of when the audit message outage ended. |

<details>

<summary>Audit message example</summary>

```
{
  "header": {
    "clusterId": "0",
    "recordType": "AUDIT",
    "recordVersion": "1",
    "recordId": 8,
    "timestamp": "2024-02-08T14:06:46.413235500Z"
  },
  "auditInfo": {
    "operation": "FILEOPEN",
    "clientHostname": "0",
    "clientIp": "0",
    "uid": "4294967295",
    "ouid": "0",
    "ogid": "0",
    "gid": "4294967295",
    "fsId": "0",
    "fsName": "0",
    "snapshotId": "1",
    "snapshotName": "1",
    "inodeId": "5478414575613706240",
    "fullPath": "0",
    "targetInodeId": "0",
    "targetFullPath": "0",
    "errorCode": "SUCCESS",
    "feOpId": "5919",
    "nodeId": "1",
    "parentInodeId": "63447005790208",
    "parentInodeSnapViewId": "1"
  },
  "wekaServerInfo": {
      "wekaServerOrigin": "lf-0",
      "source_type": "file",
      "timeSent": "2024-02-08T14:06:46.4132355Z"
  }
}
```

</details>

<!-- ============================================ -->
<!-- File 161/259: operation-guide_audit-and-forwarding-management_manage-audit-and-forwarding-using-the-cli.md -->
<!-- ============================================ -->

# Manage audit and forwarding using the CLI

You can use the `weka audit` commands to configure and manage the audit and forwarding feature. The commands are organized into the following groups:

* `weka audit cluster`: Manage audit settings at the cluster level.
* `weka audit fs`: Manage audit settings for specific filesystems.

## Manage cluster-level auditing

Use the `weka audit cluster` commands to control the audit feature for the entire cluster.

### **Enable or disable cluster-wide auditing**

The auditing functionality is disabled by default. Enable it at the cluster level before you can configure auditing for specific filesystems. This initial enablement sets up the necessary infrastructure and components required for the audit system to function.

*   To enable:

    ```
    weka audit cluster enable
    ```
*   To disable:

    ```
    weka audit cluster disable
    ```

### **View cluster audit status**

View the current cluster-wide status of the audit feature.

```
weka audit cluster status
```

### **View cluster audit statistics**

View detailed statistics about the audit logs for the entire cluster.

```
weka audit cluster stats
```

### **Set the global audit operations**

Defines the global default policy for which categories of operations are audited across the cluster. This policy applies to all filesystems but can be overridden by settings on an individual filesystem. You can choose to audit `all` categories or specify a subset (for example, `read`, `delete`).

```
weka audit cluster set-global-operations [<operations>]...
```

**Parameter**

 | Name | Description |
 | --- | --- |
 | operations* | A space-separated list of operation categories to audit. Providing a new list replaces any previously set global operations.Values: all, none, open, create, read, modify, delete, rename, close, sessionmanagement. |

### **Manage full path resolution**

Use the following commands to control the asynchronous process that adds full file paths to audit events. Including full paths provides valuable context for each operation but may introduce a performance impact. Disabling this feature can increase event throughput if the path information is not required for your use case.

*   To enable resolution of full file paths in forwarded audit events:

    ```
    weka audit cluster resolve-paths enable
    ```
*   To disable the resolution of full file paths in forwarded audit events.:

    ```
    weka audit cluster resolve-paths disable
    ```

## Manage filesystem-level auditing

Use the `weka audit fs` commands to control audit settings for individual filesystems.

### **View filesystem audit status**

List the audit status for all filesystems or a specific filesystem.

```
weka audit fs status [--name name]
```

**Parameter**

 | Name | Description |
 | --- | --- |
 | name | The name of a specific filesystem to view. |

### **Enable or disable auditing for a filesystem**

Enable or disable auditing on a specific filesystem.

*   To enable:

    ```
    weka audit fs enable <name>
    ```
*   To disable:

    ```
    weka audit fs disable <name>
    ```

**Parameter**

 | Name | Description |
 | --- | --- |
 | name* | The name of the filesystem on which to enable or disable auditing. |

**Related topics**

### **Set audit operations for a specific filesystem**

Override the global audit settings and define a specific set of operations to audit for an individual filesystem.

```
weka audit fs set-operations <name> [<operations>]...
```

**Parameters**

 | Name | Description |
 | --- | --- |
 | name* | The name of the filesystem to configure. |
 | operations* | A space-separated list of operation categories to audit. This list replaces any previously set operations for this filesystem.Possible values: all, none, open, create, read, modify, delete, rename, close, sessionmanagement. |

<!-- ============================================ -->
<!-- File 162/259: operation-guide_audit-and-forwarding-management_manage-audit-and-forwarding-using-the-gui.md -->
<!-- ============================================ -->

---
description:
---

# Manage audit and forwarding using the GUI

The audit and forwarding functionality allows administrators to monitor and export system-level file operations for security, compliance, and operational insight. Using the GUI, you can enable or disable auditing, configure which types of operations to monitor, and manage the export of telemetry data to supported external systems like Splunk, Amazon S3, and Kafka.

## Configure global audit and forwarding settings

You can configure the global audit policy for the cluster. These settings define the default behavior for auditing across all filesystems.

**Before you begin**

The auditing functionality is disabled by default. Enabling it at the cluster level is the first step in the configuration process. This action activates the necessary telemetry containers (gateways) on the backend servers, preparing the system to forward audit events. Once auditing is enabled cluster-wide, you can configure it for individual filesystems.

To enable:

```
weka audit cluster enable
```

**Procedure**

1. From the top menu, select **Configure > Cluster Settings**.
2. From the left-hand menu, select **Audit & Forwarding**.\
   The **Configuration** tab is displayed.

### Disable audit control

If the audit and forwarding functionality is not required, you can disable it at the cluster level to stop all audit logging.

Note: **Important:**
Disabling this functionality from the GUI removes the Audit & Forwarding page. After it is disabled, you can only re-enable it by running the `weka audit cluster enable` CLI command.

**Before you begin**

* Navigate to the audit configuration page by selecting **Configure > Cluster Settings** from the top menu, and then selecting **Audit & Forwarding**.

**Procedure**

1. From the Configuration tab, locate the Audit Control widget.
2. Select **Disable** to deactivate the auditing functionality.
3. Confirm the action when prompted.

The audit functionality is disabled across the cluster, and the Audit & Forwarding page is no longer visible in the GUI.

### Edit audit telemetry operation types

Define the global default for which categories of filesystem operations are audited. This policy applies to all filesystems by default, but you can override it with a more specific policy at the individual filesystem level.

**Before you begin**

* Navigate to the audit configuration page by selecting **Configure > Cluster Settings** from the top menu, and then selecting **Audit & Forwarding**.

**Procedure**

1. In the Audit Telemetry Control widget, select **Edit**.
2. In the **Edit Audit Telemetry** dialog, configure the settings:
   * **Resolve Full File Path**: Controls the asynchronous process that adds full file paths to audit events. Including full paths provides valuable context but may introduce a performance impact, while disabling it can increase event throughput.
   * **Audit All Operation Types**: Select this to audit every category of operation.
   * **Audit Specific Operation Types**: Select this to choose which categories of operations to audit. Turn on the toggles for the specific categories you want to monitor.
3. Select **Save**.

**Related topic**

#audit-operation-types

## View and create telemetry exports

To forward audit data, you must configure one or more telemetry export destinations. The **Telemetry Exports** tab displays a list of all configured destinations and allows you to create new ones.

The list of telemetry exports provides the following information:

* **Type**: The type of the external system (for example, Kafka, S3, or Splunk).
* **Name**: The user-defined name for the export configuration.
* **Endpoint URL**: The destination URL for the external system.
* **Enabled**: The status of the export (enabled or disabled).

To add a new destination, select **+ Create**.

### Create telemetry export to Splunk

Use this procedure to configure the export of audit events to a Splunk HTTP Event Collector (HEC) endpoint. This enables centralized log analysis, real-time monitoring, and alerting using Splunk‚Äôs advanced search and visualization capabilities.

**Procedure**

1. From the **Telemetry Exports** tab, select **+ Create**.
2. In the **Create Telemetry Export** dialog, from the Type list, select **S3.**
3. Provide the required information:
   * **Name:** A unique name for the Splunk export configuration.
   * **Auth Token:** The authentication token for the Splunk HTTP Event Collector (HEC).
   * **Endpoint URL:** The URL of the Splunk HEC endpoint.
   * **Certificate Configuration:** The certificate to use for the connection. Select **Container Certificate** to use the default system certificate.
4. Select **Create**.

### Create telemetry export to Amazon S3

Use this procedure to configure the export of audit events to an Amazon S3 bucket. This allows secure, persistent storage of audit logs for long-term retention, compliance auditing, or offline analysis using AWS tools and services.

**Procedure**

1. From the **Telemetry Exports** tab, select **+ Create**.
2. In the **Create Telemetry Export** dialog, from the Type list, select **S3.**
3. Provide the required information:
   * **Name:** A unique name for the S3 export configuration.
   * **Endpoint URL:** The S3 endpoint URL. This can be an AWS S3 endpoint or an AWS-compliant object store endpoint.
   * **Region:** The AWS region of the S3 bucket.
   * **Bucket Name:** The name of the S3 bucket where audit events will be stored.
   * **Access Key ID:** The access key ID for authenticating with the S3 bucket.
   * **Secret Key:** The secret key for authenticating with the S3 bucket.
   * **Certificate Configuration:** The certificate to use for the connection. Select **Container Certificate** to use the default system certificate.
4. Select **Create**.

### Create telemetry export to Kafka

Use this procedure to configure the export of audit events to an Apache Kafka endpoint. This enables real-time streaming of file system audit data for integration with monitoring, analytics, or compliance systems that consume data from Kafka topics.

**Procedure**

1. From the **Telemetry Exports** tab, select **+ Create**.
2. In the **Create Telemetry Export** dialog, from the Type list, select **Kafka.**
3. Provide the required information:
   * **Name:** A unique name for the Kafka export configuration.
   * **Endpoint URL:** The URL of the Kafka endpoint.
   * **Topic:** The specific Kafka topic to which audit events will be sent.
   * **Enable SASL Authentication:** Turn on to use SASL for secure authentication with the Kafka cluster.
   * **SASL Mechanism:** The SASL mechanism to use for authentication.
   * **SASL Username:** The username for SASL authentication.
   * **SASL Password:** The password for SASL authentication.
   * **Key Field:** The field to use as the key for Kafka messages.
   * **Certificate Configuration:** The certificate to use for the connection. Select **Container Certificate** to use the default system certificate.
4. Select **Create**.

<!-- ============================================ -->
<!-- File 163/259: operation-guide_background-tasks.md -->
<!-- ============================================ -->

---
description:
---

# Background tasks

The WEKA system performs internal and external asynchronous operations and maintenance tasks in the background using minimal CPU resources, allowing no interference nor starving the WEKA system from serving high-performing IOs.‚Äå

Background tasks include, for example, checking metadata integrity, downloading and uploading snapshots, and detaching an object store.

Adhere to the following considerations:

* **CPU resource consumption:** The WEKA system limits these tasks‚Äô CPU resources to 5% of the overall CPU. When the CPU is idle, background tasks can use more than the configured resources but are immediately freed if needed to serve IOs.
* **Concurrent tasks:** The maximum number of concurrent tasks is 16, with restrictions such as:
  * Only a single local upload can exist concurrently inside a filesystem.
  * Only a single remote upload inside a filesystem can be done concurrently (but local and remote uploads can co-exist).
  * Only a single upload from any filesystem can exist in the same object store bucket to prevent slowing down each other uploads.
  * Object store snapshot download operation cannot be run simultaneously with other snapshot download or upload operations.
  * A paused or aborted task is counted as part of the maximum number of concurrent tasks.
* **Snapshot metadata prefetch:** When a snapshot is downloaded from the object store, the system automatically prefetches its metadata as the initial step.
* **Multiple background tasks can run in parallel:** For all tasks, except the QUOTA_COLORING, up to 32 background tasks can run in parallel. A paused or aborted task is also counted as a running background task. For QUOTA_COLORING, dedicated Data Services containers can run up to 16 tasks in parallel.

Note: More restrictions exist between different tasks and multiple tasks of the same type. If a background task does not run due to a restriction, the system provides a relevant message.

### Background tasks list <a href="#managing-background-tasks" id="managing-background-tasks"></a>

 | Taks name | Task description | Possible actions |
 | --- | --- | --- |
 | OBS_DETACH2 | Detaching Object Storage <OBS name> from filesystem <fs name>. | Pause, Resume, Abort |
 | STOW_UPLOAD | Uploading snapshot <snapshot name> from filesystem <fs name> to <OBS site> object-store bucket <OBS bucket name>. | Pause, Resume, Abort |
 | STOW_DOWNLOAD_FILESYSTEM | Downloading filesystem <fs name> from locator <snapshot locator> in object-store <OBS bucket name>. | Pause, Resume |
 | STOW_DOWNLOAD_SNAPSHOT | Downloading the snapshot <snapshot name> to <fs name> from locator <snapshot locator> in object-store <OBS bucket name>.This task includes two additional internal phases:Fetching the snapshot metadata.Squashing the filesystem. | Pause, Resume |
 | FSCK | Checking metadata integrity. | Pause, Resume, Abort |
 | DATA_REDUCTION | Compressing data. | Pause, Resume, Abort |
 | DATA_REDUCTION_GC | Garbage collection (GC). | Pause, Resume, Abort |
 | QUOTA_COLORING | Modifies every extent in each existing file included in the quota. | Pause, Resume, Abort |

<!-- ============================================ -->
<!-- File 164/259: operation-guide_background-tasks_set-up-a-data-services-container-for-background-tasks.md -->
<!-- ============================================ -->

---
description:
---

# Set up a Data Services container for background tasks

The Data Services container runs tasks in the background, particularly those that can be resource-intensive. At present, it runs the Quota Coloring task. In upcoming releases, it will handle additional tasks that consume significant resources.

Running these tasks in the background ensures your CLI remains accessible and responsive without consuming compute resources. This strategy enhances performance, efficiency, and scalability when managing quotas. If a task is interrupted, it automatically resumes, providing reliability.

Note: If the Data Services container is not operational, the quota coloring task reverts to the previous implementation and runs in a single process. This could result in the CLI hanging for an extended period. Therefore, ensuring the Data Services container runs is crucial to prevent this situation.

To improve data service performance, you can set up multiple Data Service containers, one per WEKA server.

After setting up the Data Service container, you can manage it like any other container within the cluster. If there‚Äôs a need to adjust its resources, use the `weka cluster container resources` or `weka local resources` commands. For more details, see .

**Before you begin**

1. Ensure the server where you‚Äôre adding this container has sufficient memory available:
   * 3.5 GB if no dedicated core is specified.
   * 5.5 GB if a dedicated core is specified.
2. The Data Service containers require a persistent 22 GB filesystem for intermediate global configuration data. Do one of the following:
   * If a configuration filesystem for the protocol containers exists (typically named `.config_fs`), use it and expand its size by 22 GB. See #dedicated-filesystem-requirement-for-cluster-wide-persistent-protocol-configurations
   * If a configuration filesystem does not exist, create a dedicated 22 GB configuration filesystem for the Data Service containers.
3. Set the Data Service global configuration. Run the following command:

```
weka dataservice global-config set --config-fs <configuration filesystem name>
```

Example:

```
weka dataservice global-config set --config-fs .config_fs
```

Note: By default, the Data Service containers share the core of the Management process. However, if you have enough resources, you can assign a separate core to it.

**Procedure**

1. **Set up the Data Services container:** Run the following command:

```

```bash
weka local setup container --name <container_name> --base-port <base-port> --join-ips <management-ip> --only-dataserv-cores --allow-mix-setting
```

```

Parameters:

 | Parameter | Description |
 | --- | --- |
 | name* | The Data Services container name. Setdataserv0 to avoid confusion. |
 | only-dataserv-cores* | Creates a Data Services container. This parameter is mandatory. |
 | base-port | If a base-port is not specified, the Data Services container may still initialize as it attempts to allocate an available port range and could succeed. However, for optimal operation, it is recommended to provide the base port externally. |
 | join-ips* | Specify the management IP of one of the servers in the cluster to join. |
 | management-ips | This is optional. If not provided, it automatically takes the management IP of the server. |
 | memory | Configure the container memory to be allocated for huge pages. It is recommended to set it to 1.5 GB. |
 | allow-mix-setting | This option enables using specified core IDs, even when containers with AUTO core ID allocation run on the same server. It is required if the core allocation is not explicitly specified. |

<details>

<summary>Example</summary>

```

```bash
$ weka local setup container --name dataserv0 --base-port 14400 --join-ips 10.108.234.164  --only-dataserv-cores --allow-mix-setting
Version 4.3.2 is already downloaded.
Created Weka container named dataserv0
Preparing version 4.3.2 of container dataserv0
No net parameter specified, configuring in UDP mode
Successfully set up container dataserv0
Starting container
Waiting for container to start up
Container "dataserv0" is ready (pid = 66904)
```

```

</details>

2. **Verify the Data Services container is up**: Run `weka local ps`.

<details>

<summary>Example</summary>

```bash
$ weka local ps
CONTAINER  STATE    DISABLED  UPTIME    MONITORING  PERSISTENT  PORT   PID    STATUS  VERSION  LAST FAILURE
compute0   Running  False     1:21:58h  True        True        14300  44600  Ready   4.3.2
dataserv0  Running  False     44.59s    True        True        14400  66904  Ready   4.3.2
drives0    Running  False     1:22:39h  True        True        14000  43448  Ready   4.3.2
frontend0  Running  False     1:21:15h  True        True        14200  45680  Ready   4.3.2
```

</details>

3. **Verify the Data Services container is visible in the cluster:** Run `weka cluster container`.

<details>

<summary>Example</summary>

See `dataserve0` in the last row (CONTAINER ID 15).

```bash
$ weka cluster container
CONTAINER ID  HOSTNAME        CONTAINER  IPS             STATUS  RELEASE  FAILURE DOMAIN  CORES  MEMORY   UPTIME    LAST FAILURE
0             DataSphere-0    drives0    10.108.249.241  UP      4.3.2    DOM-000         1      1.54 GB  1:29:38h
1             DataSphere-1    drives0    10.108.211.190  UP      4.3.2    DOM-001         1      1.54 GB  1:29:39h
2             DataSphere-2    drives0    10.108.47.134   UP      4.3.2    DOM-002         1      1.54 GB  1:29:39h
3             DataSphere-3    drives0    10.108.234.164  UP      4.3.2    DOM-003         1      1.54 GB  1:29:39h
4             DataSphere-4    drives0    10.108.166.243  UP      4.3.2    DOM-004         1      1.54 GB  1:29:38h
5             DataSphere-0    compute0   10.108.249.241  UP      4.3.2    DOM-000         1      1.50 GB  1:28:56h
6             DataSphere-1    compute0   10.108.211.190  UP      4.3.2    DOM-001         1      1.50 GB  1:28:57h
7             DataSphere-2    compute0   10.108.47.134   UP      4.3.2    DOM-002         1      1.50 GB  1:28:57h
8             DataSphere-3    compute0   10.108.234.164  UP      4.3.2    DOM-003         1      1.50 GB  1:28:57h
9             DataSphere-4    compute0   10.108.166.243  UP      4.3.2    DOM-004         1      1.50 GB  1:28:58h
10            DataSphere-0    frontend0  10.108.249.241  UP      4.3.2    DOM-000         1      1.47 GB  1:28:13h
11            DataSphere-1    frontend0  10.108.211.190  UP      4.3.2    DOM-001         1      1.47 GB  1:28:13h
12            DataSphere-2    frontend0  10.108.47.134   UP      4.3.2    DOM-002         1      1.47 GB  1:28:13h
13            DataSphere-3    frontend0  10.108.234.164  UP      4.3.2    DOM-003         1      1.47 GB  1:28:14h
14            DataSphere-4    frontend0  10.108.166.243  UP      4.3.2    DOM-004         1      1.47 GB  1:28:14h
15            DataSphere-0    dataserv0  10.108.249.241  UP      4.3.2                    1      1.47 GB  0:07:41h
```

</details>

4. **Verify the data services and management processes have joined the cluster:** Run `weka cluster process`.

<details>

<summary>Example</summary>

See PROCESS IDs 300 and 301.

```bash
$ weka cluster process
PROCESS ID  HOSTNAME      CONTAINER  IPS             STATUS  RELEASE  ROLES       NETWORK  CPU  MEMORY   UPTIME    LAST FAILURE
0           DataSphere-0  drives0    10.108.249.241  UP      4.3.2    MANAGEMENT  UDP           N/A      1:22:26h  Host joined a new cluster (1 hour ago)
1           DataSphere-0  drives0    10.108.6.1      UP      4.3.2    DRIVES      DPDK     2    1.54 GB  1:22:24h
20          DataSphere-1  drives0    10.108.211.190  UP      4.3.2    MANAGEMENT  UDP           N/A      1:22:28h  Host joined a new cluster (1 hour ago)
21          DataSphere-1  drives0    10.108.18.211   UP      4.3.2    DRIVES      DPDK     4    1.54 GB  1:22:24h
40          DataSphere-2  drives0    10.108.47.134   UP      4.3.2    MANAGEMENT  UDP           N/A      1:22:27h  Host joined a new cluster (1 hour ago)
41          DataSphere-2  drives0    10.108.0.189    UP      4.3.2    DRIVES      DPDK     4    1.54 GB  1:22:24h
60          DataSphere-3  drives0    10.108.234.164  UP      4.3.2    MANAGEMENT  UDP           N/A      1:22:29h
61          DataSphere-3  drives0    10.108.181.42   UP      4.3.2    DRIVES      DPDK     6    1.54 GB  1:22:24h
80          DataSphere-4  drives0    10.108.166.243  UP      4.3.2    MANAGEMENT  UDP           N/A      1:22:26h  Host joined a new cluster (1 hour ago)
81          DataSphere-4  drives0    10.108.32.208   UP      4.3.2    DRIVES      DPDK     2    1.54 GB  1:22:24h
100         DataSphere-0  compute0   10.108.249.241  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:52h  Configuration snapshot pulled (1 hour ago)
101         DataSphere-0  compute0   10.108.150.39   UP      4.3.2    COMPUTE     DPDK     6    1.50 GB  1:21:50h
120         DataSphere-1  compute0   10.108.211.190  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:52h  Configuration snapshot pulled (1 hour ago)
121         DataSphere-1  compute0   10.108.162.229  UP      4.3.2    COMPUTE     DPDK     2    1.50 GB  1:21:50h
140         DataSphere-2  compute0   10.108.47.134   UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:46h  Removed from cluster: Not reachable by the cluster (1 hour ago)
141         DataSphere-2  compute0   10.108.38.178   UP      4.3.2    COMPUTE     DPDK     2    1.50 GB  1:21:50h
160         DataSphere-3  compute0   10.108.234.164  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:52h  Configuration snapshot pulled (1 hour ago)
161         DataSphere-3  compute0   10.108.254.134  UP      4.3.2    COMPUTE     DPDK     4    1.50 GB  1:21:50h
180         DataSphere-4  compute0   10.108.166.243  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:46h  Removed from cluster: Not reachable by the cluster (1 hour ago)
181         DataSphere-4  compute0   10.108.0.100    UP      4.3.2    COMPUTE     DPDK     4    1.50 GB  1:21:50h
200         DataSphere-0  frontend0  10.108.249.241  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:01h  Removed from cluster: Not reachable by the cluster (1 hour ago)
201         DataSphere-0  frontend0  10.108.10.152   UP      4.3.2    FRONTEND    DPDK     4    1.47 GB  1:21:05h
220         DataSphere-1  frontend0  10.108.211.190  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:01h  Removed from cluster: Not reachable by the cluster (1 hour ago)
221         DataSphere-1  frontend0  10.108.201.178  UP      4.3.2    FRONTEND    DPDK     6    1.47 GB  1:21:05h
240         DataSphere-2  frontend0  10.108.47.134   UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:01h  Removed from cluster: Not reachable by the cluster (1 hour ago)
241         DataSphere-2  frontend0  10.108.172.186  UP      4.3.2    FRONTEND    DPDK     6    1.47 GB  1:21:05h
260         DataSphere-3  frontend0  10.108.234.164  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:08h  Configuration snapshot pulled (1 hour ago)
261         DataSphere-3  frontend0  10.108.145.253  UP      4.3.2    FRONTEND    DPDK     2    1.47 GB  1:21:05h
280         DataSphere-4  frontend0  10.108.166.243  UP      4.3.2    MANAGEMENT  UDP           N/A      1:21:08h  Configuration snapshot pulled (1 hour ago)
281         DataSphere-4  frontend0  10.108.219.191  UP      4.3.2    FRONTEND    DPDK     6    1.47 GB  1:21:05h
300         DataSphere-0  dataserv0  10.108.249.241  UP      4.3.2    MANAGEMENT  UDP           N/A      33.05s    Configuration snapshot pulled (40 seconds ago)
301         DataSphere-0  dataserv0  10.108.249.241  UP      4.3.2    DATASERV    UDP      1    1.47 GB  14.55s
```

</details>

    During the procedure of setting or unsetting a directory quota, the Data Services container creates a background task referred to as `QUOTA_COLORING`. This task scans the entire directory tree and assigns the quota ID to each file and directory within the tree.

<!-- ============================================ -->
<!-- File 165/259: operation-guide_background-tasks_manage-background-tasks-using-the-cli.md -->
<!-- ============================================ -->

# Manage background tasks using the CLI

Using the CLI, you can:

* View running background tasks
* Limit background task resources
* Pause/Resume/Abort a background task

## View active background tasks <a href="#viewing-running-background-tasks" id="viewing-running-background-tasks"></a>

You can view the active background tasks' status, progress, and description.‚Äå

‚Äå**Command:** `weka cluster task`‚Äå

This command is used for viewing all active background tasks.

Example:

```
# weka cluster task
| Type | State | Progress | Description |
------------+---------+----------+-----------------------------------------------------------
| OBS_DETACH2 | RUNNING | 94 | Detaching Object Storage `obs_1` from filesystem `default` |
```

## ‚ÄåLimit background task resources

It is possible to limit the resources being used by background tasks.

The configured limit affects external tasks and internal low-priority asynchronous operations.‚Äå

**Command:** `weka cluster task limits`

This command is used to view the defined limits.

**Command:** `weka cluster task limits set [--cpu-limit cpu-limit]`

This command is used to update the CPU limit.

## Pause/Resume/Abort a background task

If there are other background tasks or activities that are of higher priority, you can pause and later resume the background task, or abort it.

**Command:** `weka cluster task pause / resume / abort <task-id>`

This command is used to pause, resume, or abort a specific task process. The `abort` subcommand is not applicable when downloading a filesystem or a snapshot. Instead, delete them directly.

<!-- ============================================ -->
<!-- File 166/259: operation-guide_background-tasks_manage-background-tasks-using-the-gui.md -->
<!-- ============================================ -->

# Manage background tasks using the GUI

The GUI includes a **Background Tasks** page that displays both active and pending tasks, along with details such as their duration since initiation, state, phase, and progress percentage.

If other tasks have higher priority, you can pause and resume tasks as needed.

The **Abort** action is available for specific tasks, such as checking metadata integrity. However, it is not applicable to tasks like downloading a filesystem or snapshot, squashing a filesystem, or detaching object storage. To terminate these tasks, delete the associated entity.

**Procedure:**

1. From the **Monitor** tab, select **Background Tasks**.
2. To pause a task, select **Pause** (the button will toggle to **Resume**).
3. To resume a paused task, select **Resume**.
4. To abort a task, select **Abort**.
5. To view waiting tasks (pending), toggle the **Show Waiting Tasks** switch.

<!-- ============================================ -->
<!-- File 167/259: operation-guide_expanding-and-shrinking-cluster-resources.md -->
<!-- ============================================ -->

---
description: Expand and shrink a cluster in a homogeneous WEKA system configuration.
---

# Expand and shrink cluster resources

A WEKA cluster is a collection of backend servers configured with containers, SSDs, cores, memory, and network resources. You can expand or shrink your cluster resources to meet ongoing business changes.

* **Expansion:** To increase the performance and capacity of your cluster, you can expand your cluster by adding new servers to the cluster and configuring new resources anytime after installing and connecting them to the network on the same subnet as the cluster.
* **Shrinking:** If you want to reduce the cluster's costs and the performance degradation does not affect your business, you can shrink the cluster by removing SSDs and backend servers.

The expansion and shrinking procedures only apply to homogeneous WEKA clusters in which all the cluster servers are similar and have the same number of cores, memory, SSD capacity per server, and servers per failure domain (if any).

Note: * For heterogeneous WEKA cluster configurations and estimation of the performance change, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).
* For AWS deployments, use the CloudFormation for the initial deployment, not for expanding and shrinking cluster resources.

The expansion and shrinking procedures include:

* Add or delete backend servers and containers.
* Add or delete SSDs (drives).
* Modify the number of cores assigned to the WEKA cluster.
* Modify the memory size allocated to the WEKA cluster.
* Modify the network resources assigned to the WEKA cluster (not required frequently).

## Expansion considerations

The expansion procedures are similar to the _WEKA installation on bare metal_ procedures but require specific attention to the following considerations when planning the expansion:

* **Containers architecture:** The WEKA container architecture must be retained. Two procedures for adding a backend server are provided:
  * Add a backend server in a multiple-container architecture.
  * Add a backend server in a single container architecture.
* **Protection scheme:** The WEKA cluster protection scheme is retained. You cannot modify it.
* **Failure domains:** Adding or removing failure domains are done automatically.
* **Memory expansion:** When expanding memory resources, the new containers must have the same memory allocated as the existing containers.

Note: To calculate the capacity of the WEKA cluster after expansion, see #ssd-net-storage-capacity-calculation.

**Related topics**

## What happens after expansion or shrinking?

Once the WEKA cluster expansion or shrinking is completed, the system starts a redistribution process. This involves redistributing all the existing data, to be balanced between the original system SSDs and newly added SSDs.

The redistribution process time depends on the cluster capacity and the networking CPU resources. It can take between minutes and hours.

The capacity increase is instant. Therefore, it is possible to define more filesystems immediately without waiting to complete the redistribution process.

When more containers or cores are expanded, the added CPU resources are operational in less than a minute. The write performance improves almost immediately, while the read performance only improves upon completing the data redistribution.

<!-- ============================================ -->
<!-- File 168/259: operation-guide_expanding-and-shrinking-cluster-resources_add-a-backend-server.md -->
<!-- ============================================ -->

# Add a backend server

Expanding a cluster in a multi-container backend architecture with a new server is similar to the WEKA multi-container backend installation process.

Adding a server to the cluster includes discovering the existing cluster resources, generating the resource files, creating containers using the resource files, and adding the SSDs to the new server.

### Before you begin

1. Review the system dashboard and ensure that the system is operational and does not indicate any alarms.
2. Discover the number of cores for each container type in the cluster server.

```
| weka local resources -C drives0 | grep -c DRIVES |

| weka local resources -C compute0 | grep -c COMPUTE |

| weka local resources -C frontend0 | grep -c FRONTEND |

```

3. Discover the Management IPs of one of the containers. In a high-availability system, more than one IP exists.

```
| weka local resources -C drives0 | grep "Management IPs" |

```

4. Ensure that the new backend server meets the requirements and is available for installation.
5. Download from get.weka.io the same WEKA software version as in the existing WEKA cluster servers.

Note: To learn how about the options of the commands in the following procedure, see the related topics.

### **Procedure**

1. Install the WEKA software on the new backend server.
2. Remove the default container from the new backend server.

```
weka local stop default && weka local rm -f default

```

3. Download the WEKA tools from the GitHub repository.

```
cd ~
git clone https://github.com/weka/tools/
cd ~/tools/install/

```

4. Generate the resource files with the same network devices and options as the existing WEKA cluster servers.

```
./resources_generator.py --net <net-devices> [options]

```

<details>

<summary>Example of high-availability system with two network devices</summary>

```

```
./resources_generator.py --net ens4 ens5 --compute-dedicated-cores 3 --drive-dedicated-cores 2 --frontend-dedicated-cores 2

```

```

</details>

<details>

<summary>Example of a high-availability system with two network devices and a gateway</summary>

Add to the `--net` option the following for each network device:\
`<net device name>/<net device IP>/<net mask>/<gateway IP>`

```

```
./resources_generator.py --net enp197s0np0/172.25.5.132/16/172.25.5.2 enp129s0np0/172.25.6.132/16/172.25.5.2 --compute-dedicated-cores 12 --drive-dedicated-cores 12 --frontend-dedicated-cores 1

```

```

</details>

5.  Create the drive, compute, and frontend containers, and join them to the existing cluster. Use the following options to specify the required parameters:

    * `resources-path`: Specify the path to the resource file `(drives0.json`, `compute0.json`, or `frontend0.json`) created in Step 4 using the resource generator.
    * `management-ips`: Specify the management IP of the new server joining the cluster. For high availability, provide two or more comma-separated IPs.
    * `join-ips`: Specify the management IP of an existing cluster server.

    Run the following commands:

<pre data-overflow="wrap"><code><strong>weka local setup container --name drives0 --resources-path <path>/drives0.json --management-ips=<management IPs of the new server> --join-ips=<management IP of the existing server>
</strong>
weka local setup container --name compute0 --resources-path <path>/compute0.json --management-ips=<management IPs of the new server> --join-ips=<management IP of the existing server>

weka local setup container --name frontend0 --resources-path <path>/frontend0.json --management-ips=<management IPs of the new server> --join-ips=<management IP of the existing server>

```

6. Verify that the server is added to the cluster successfully.\
   Run `weka local ps`.

```
[root@weka8 ~]# weka local ps
CONTAINER  STATE    DISABLED  UPTIME    MONITORING  PERSISTENT  PORT   PID    STATUS  VERSION    LAST FAILURE
compute0   Running  False     0:09:08h  True        True        14300  26441  Ready   4.2.0.153
drives0    Running  False     0:09:41h  True        True        14000  25295  Ready   4.2.0.153
frontend0  Running  False     0:08:35h  True        True        14200  27911  Ready   4.2.0.153
```

7. Configure the SSD drives on the drive container.

```
weka cluster drive add <container-id> <device-paths>

```

**Related topics**

\
(see Path C: Manual installation and configuration)

<!-- ============================================ -->
<!-- File 169/259: operation-guide_expanding-and-shrinking-cluster-resources_expansion-of-specific-resources.md -->
<!-- ============================================ -->

---
description:
---

# Expand specific resources of a container

Expanding resources within a container involves dynamically adjusting the allocation of CPU, memory, storage, and other system resources to meet applications' changing demands. By effectively managing these resources, organizations can optimize performance, enhance scalability, and ensure the smooth operation of their containerized applications.

## Expansion guidelines

The following commands are available to expand the containers' resources:

* `weka cluster container`: Run actions on a remote container (or containers for specific sub-commands).
* `weka local resources`: Run actions locally.

Adhere to the following guidelines when expanding specific resources:

* **Specify the container:** Run the relevant `weka cluster container` command with the specific `container-id` you want to expand. Once you run the command, the container is staged to update in the cluster.
* **View existing resources:** To view the non-applied configuration, run the `weka cluster container resources <container-id>` command.
* **Apply changes on a specific container:** To apply changes on a specific container in the cluster, run the `weka cluster container apply <container-ids>` command.  It is possible to accumulate several changes on a container and apply only once on completion.
* **Apply changes on a local server:** To apply changes in the local container, run the `weka local resources apply` command.
* **The apply command saves the last configuration:** Once the apply command is complete, the last local configuration of the container that successfully joined the cluster is saved.\
  If a failure occurs with the new configuration, the container automatically remains with the existing stable configuration. \
  Run the `weka cluster container resources <container-id> --stable` command to view the existing configuration.
* **Expansion on active or deactivated containers:** Some resources can be expanded on active containers, such as adding CPU cores. Others require container deactivation, like setting failure domain. If deactivation is required, you can use the `--deactivation-check` option to check if the specified containers can be deactivated.

## weka cluster container command description

**Command:** `weka cluster container <sub-command> <container-id> [options]`

Some sub-commands accept `<container-ids>`. See details in the following table.

**Subcommands**

 | Sub-command | Description | Comment |
 | --- | --- | --- |
 | activate | Activate the containers. | Specify the list of containers with a space delimiter. |
 | add | Add a container to the cluster. |  |
 | apply | Apply changes to the resources on the containers. | Specify the list of containers with a space delimiter. |
 | auto-remove-timeout | Set the time to wait before removing the containers from clients if they disconnect from the cluster. The minimum value is 60. Use 0 to disable automatic removal. | This subcommand only applies to clients. |
 | bandwidth | Limit the bandwidth of the containers. |  |
 | clear-failure | Clear the last failure fields of the containers. | Specify the list of containers with a space delimiter. |
 | cores | Change the number of cores in the containers. | Increasing the number of cores does not require deactivating the container, whereas decreasing the core count requires deactivation. |
 | deactivate | Deactivate the containers. | Specify the list of containers with a space delimiter. |
 | deactivation-check | Check if the specified containers can be deactivated. |  |
 | dedicate | Set the containers to be dedicated to the WEKA cluster. |  |
 | failure-domain | Set the failure domain on the container. | Requires deactivating the container. |
 | info-hw | Show hardware information about the containers. |  |
 | join-secret | Set the secret this container uses when joining or validating other backends. |  |
 | management-ips | Set the management IPs of the container. To achieve high availability, set two IPs. |  |
 | memory | Set the RAM size dedicated to the container. |  |
 | net | List the WEKA-dedicated networking devices in the containers. | Specify the list of containers with a space delimiter. |
 | remove | Remove a container from the cluster. |  |
 | requested-action | Set the specified containers' requested action to stop, restart, or apply resources gracefully. |  |
 | resources | Get the resources of the containers. |  |
 | restore | Restore staged resources of the containers or all containers to their stable state. | Specify the list of containers with a space delimiter. |

**Options**

 | Option | Description |  |
 | --- | --- | --- |
 | -b | Only return backend containers. |  |
 | -c | Only return client containers. |  |
 | -l | Only return containers that are part of the cluster leadership. |  |
 | -L | Only return the cluster leader. |  |

## Expansion procedures on a remote container

### Modify the memory

Run the following command lines on the active container:

```
weka cluster container memory <container-id> <capacity-memory>
weka cluster container apply <container-ids>
```

<details>

<summary><strong>Example</strong></summary>

To change the memory of `container-id 0` to 1.5 GiB, run the following commands:

```
weka cluster container memory 0 1.5GiB
weka cluster container apply 0
```

</details>

After reducing the memory allocation for a container, follow these steps to release hugepages on each container:

1. Stop the container locally. Run `weka local stop`
2. Release hugepages. Run `weka local run release_hugepages`
3. Restart the container locally. Run `weka local start`

### Modify the network configuration

Run the following command lines on the active container:

```
weka cluster container net add <container-id> <device>
weka cluster container apply <container-ids>
```

<details>

<summary><strong>Example</strong></summary>

To add another network device to `container-id 0`, run the following commands:

```
weka cluster container net add 0 eth2
weka cluster container apply 0
```

</details>

### Modify the container IP addresses

Run the following command lines on the active container:

```
weka cluster container management-ips <container-id> <management-ips>
weka cluster container apply <container-ids>
```

<details>

<summary><strong>Example</strong></summary>

To change the management IPs on `container-id 0`, run the following commands:

```
weka cluster container management-ips 0 192.168.1.10 192.168.1.20
weka cluster container apply 0
```

</details>

The number of management IP addresses determines whether the container uses high-availability (HA) networking, causing each IO process to use both containers' NICs.

A container with two IP addresses uses HA networking. A container with only one IP does not use HA networking.

If the cluster uses InfiniBand and Ethernet network technologies, you can define up to four IP addresses.

### Add CPU cores to a container

You can add dedicated CPU cores to a container locally and on an active container.

For clarity, the following procedure exemplifies expansion on the container running the compute processes.

Note: Decreasing the number of cores, requires container deactivation before setting the number of cores, using the command: `weka cluster container deactivate <container-ids>`.
Reactivate the container after completing the procedure below.

**Procedure**

1. Run the following command line to set the number of dedicate cores to the compute container:\
   `weka cluster container cores <container-id> <number of total cores> --compute-dedicated-cores <number of total cores> --no-frontends`
2. Apply the changes. Run the following command:\
   `weka cluster container apply <container-ids>`
3. Check the number of cores dedicated to the compute processes. Run the following command: \
   `weka cluster container <container-ids>`

<details>

<summary>Example</summary>

The following example sets 10 cores to the `compute0` container. The container id is 1. It is important to add `--no-frontends` to allocate the cores dedicated to the compute processes.

```
weka cluster container cores 1 10 --compute-dedicated-cores 10 --no-frontends
weka cluster container apply 1
weka cluster container 1
//response
ROLES       NODE ID  CORE ID
MANAGEMENT  0        <auto>
COMPUTE     1        <auto>
COMPUTE     2        <auto>
COMPUTE     3        <auto>
COMPUTE     4        <auto>
COMPUTE     5        <auto>
COMPUTE     6        <auto>
COMPUTE     7        <auto>
COMPUTE     8        <auto>
COMPUTE     9        <auto>
COMPUTE     10       <auto>
```

</details>

### Add SSDs to a container

You can expand the cluster storage capacity by adding new SSD drives to a specific container.

When adding drives, you can assign them to specific drive pools, such as `iu4k` or `legacy`. This allows for the integration of diverse SSD types within a single cluster, providing greater hardware flexibility.

Adding SSD drives might alter the ratio between SSDs and drive cores, which can impact performance.

Note: Support for mixed drive pools is only available for clusters newly installed with this software version and is not supported for upgraded clusters.

**Before you begin**

* Ensure the cluster has a drive core available to allocate to the new SSD.
* Identify the container ID for the SSD addition.

**Procedure**

1. Identify the relevant container ID to which you want to add the SSD drive. Run the following command: `weka cluster container`
2. Scan for new drives. Run the following command: `weka cluster drive scan`
3.  To add the SSDs, run the following command: `weka cluster drive add <container-id> <device-paths> [--pool <pool>]`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | container-id* | The identifier of the drive container to which to add the local SSD drives. |
 | device-paths* | A list of block devices that identify local SSDs. It must be a valid Unix network device name. Format: Space-separated strings.Example: /dev/nvme0n1 /dev/nvme1n1 |
 | pool | Specifies the disk pool to which you add the drive. Disk pools help organize drives based on their indirection unit (IU) size to optimize performance and endurance.Possible values include:auto: Automatically selects the appropriate pool by detecting the drive's characteristics.iu4k: Adds the drive to the Indirection Unit 4K pool. This pool is for drives with a 4KiB indirection unit size.iubig: Adds the drive to the Indirection Unit "big" pool. This pool is for drives with large indirection units, such as 32KiB.legacy: Adds the drive to the legacy pool. Use this option for compatibility with systems that were set up before the introduction of indirection unit-based pooling. |

## weka local resources command description

You can also modify the resources on a local container by connecting to it and running the `weka local resources` command equivalent to its `weka cluster` remote counterpart command.

These local commands have the same semantics as their remote counterpart. You do not specify the `container-id` as the first parameter. All actions are done on the local container.

**Command**: `weka local resources`

**Subcommands**

 | Sub-command | Description | Comment |
 | --- | --- | --- |
 | apply | Apply changes to the resources locally. |  |
 | auto-remove-timeout | Set the time to wait before removing the containers from clients if they disconnect from the cluster. The minimum value is 60. Use 0 to disable automatic removal. | This subcommand only applies to clients. |
 | bandwidth | Limit the bandwidth of the container. |  |
 | base-port | Change the port range used by the container. WEKA containers require 100 ports to operate. |  |
 | cores | Change the number of cores in the container. | Increasing the number of cores does not require deactivating the container, whereas decreasing the core count requires deactivation. |
 | dedicate | Set the container to be dedicated to the WEKA cluster. |  |
 | export | Export stable resources to a file. |  |
 | failure-domain | Set the container failure-domain. | Requires deactivating the container. |
 | fqdn | Configure the FQDN for other containers for TLS hostname verification when interacting with the cluster. |  |
 | import | Import resources from a file. |  |
 | join-ips | Set the IPs and ports of all containers in the cluster. This enables the container to join the cluster using these IPs. |  |
 | join-secret | Configure the secret used when joining a cluster as a backend. |  |
 | management-ips | Set the container's management IPs. To achieve high-availability, set two IPs. |  |
 | memory | Set the RAM size dedicated to the container. |  |
 | net | List the WEKA-dedicated networking devices in a container. |  |
 | restore | Restore resources from stable resources. |  |

**Options**

 | Option | Description | Comment |
 | --- | --- | --- |
 | --stable | List the resources from the last successful container boot. |  |
 | -C | The container name. |  |

<details>

<summary>Example: Set dedicated cores for the compute processes locally</summary>

The following example sets 10 cores to the `compute0` container. The container id is 1. It is important to add `--no-frontends` to allocate the cores dedicated to the compute processes. Connect to the relevant server to run the following commands locally.

```
weka local resources cores 10 --compute-dedicated-cores 10 -C compute0 --no-frontends
weka local resources -C compute0

//response
ROLES       NODE ID  CORE ID
MANAGEMENT  0        <auto>
COMPUTE     1        <auto>
COMPUTE     2        <auto>
COMPUTE     3        <auto>
COMPUTE     4        <auto>
COMPUTE     5        <auto>
COMPUTE     6        <auto>
COMPUTE     7        <auto>
COMPUTE     8        <auto>
COMPUTE     9        <auto>
COMPUTE     10       <auto>
```

</details>

## Graceful container management: ensuring safe actions

The `weka local stop`, `restart`, and `apply resources` commands perform graceful stop operations by default, ensuring actions are executed safely to minimize the risk of unexpected issues or disruptions. The system automatically prioritizes safety during cluster maintenance without requiring the `--graceful` option. If non-graceful action is required, add the `--force` option.

Additionally, stopping and starting dependent containers is the default behavior for the `weka local stop/start` commands, providing seamless management of dependent services. To override this behavior, use the `--skip-start-and-enable-dependent` or `--skip-stop-and-enable-dependent` options.

**How the default graceful process works:**

* **Action Initiation:** Sends a request to the container for the specified action (STOP, RESTART, or APPLY_RESOURCES).
* **Safety check:** Evaluates feasibility based on current state and safety constraints (for example, ensuring sufficient resources post-action).
* **Draining and execution:** If safe, the container transitions to the DRAINING state to complete ongoing operations. Once DRAINED, the action is executed.

**Example: Prioritizing stability**

If stopping a container would violate minimum failure domain requirements, the graceful stop prevents the action to maintain system health.

The graceful process applies exclusively to cluster containers, not to protocol containers.

<pre class="language-bash" data-title="Example: prioritizing stability" data-full-width="true"><code class="lang-bash"><strong>CONTAINER ID  HOSTNAME  CONTAINER  IPS             STATUS          REQUESTED ACTION  REQUESTED ACTION FAILURE
</strong>0             Host-0    drives0    10.108.206.201  UP              STOP              Upon completion of this operation, there are 4 reliable containers available for cluster leadership, while the requirement is for 5.
6             Host-0    compute0   10.108.206.201  DRAINED (DOWN)  STOP
12            Host-0    frontend0  10.108.206.201  DRAINING        RESTART

```

<!-- ============================================ -->
<!-- File 170/259: operation-guide_expanding-and-shrinking-cluster-resources_shrinking-a-cluster.md -->
<!-- ============================================ -->

# Shrink a cluster

Shrinking a cluster may be required when you need to reduce the cluster's costs, and the performance degradation does not affect your business.

You can shrink the cluster by performing one of the following:

* Remove only some drives from the cluster.
* Remove containers with their allocated drives.

Removing cores, drives, or containers requires deactivating the drives you want to remove. But, if the deactivation leads to insufficient SSD capacity of the currently-provisioned filesystems, the WEKA system does not deactivate the drives, and shrinking the cluster is not allowed.

### Before you begin

Run the following command to display a list of all the drives in the cluster with their details, such as UUID and status:

`weka cluster drive`

<details>

<summary>Example</summary>

```
root@void-new-1:~# weka cluster drive
DISK ID  UUID                                  HOSTNAME     NODE ID  SIZE      STATUS    LIFETIME % USED  ATTACHMENT  DRIVE STATUS
37       84c4574d-5a46-4644-91aa-df1ceef27ff1  void-new-10  1921     1.09 TiB  ACTIVE    0                OK          OK
45       ecd05959-629c-4319-9d24-f69497c499e3  void-new-19  2401     1.09 TiB  ACTIVE    0                OK          OK
46       4c8af0fa-894b-4096-adb6-17fe98a3a690  void-new-17  2281     1.09 TiB  ACTIVE    0                OK          OK
47       49f684d0-9f2e-4b0a-9153-9aa3570067bd  void-new-18  2341     1.09 TiB  ACTIVE    0                OK          OK
57       7202db57-1f4e-4332-a132-33a47a729d46  void-new-0   1141     1.09 TiB  INACTIVE  0                OK
58       6c2ad35b-a1ff-4b30-9882-0ed3ec166747  void-new-1   1321     1.09 TiB  ACTIVE    0                OK          OK
59       ae8dd40a-9d3d-4154-a26d-3e9643f59e6f  void-new-2   1381     1.09 TiB  ACTIVE    0                OK          OK
60       b96e3c32-3a29-436a-ac35-2e8cf6808e9a  void-new-3   1441     1.09 TiB  ACTIVE    0                OK          OK
61       63ab4d5d-82ed-4248-9ce1-817ce5d7e106  void-new-4   1501     1.09 TiB  ACTIVE    0                OK          OK
62       0f303d2c-5fd0-47e6-9150-0da4afcc454b  void-new-5   1561     1.09 TiB  ACTIVE    0                OK          OK
63       d21f4b3b-1458-4402-8592-06e7ca426d9c  void-new-6   1621     1.09 TiB  ACTIVE    0                OK          OK
64       0c3de49c-b123-4b0b-bd64-e7a90454b41d  void-new-7   1681     1.09 TiB  ACTIVE    0                OK          OK
65       c519e608-ae1d-402e-9f10-da69b227d2c8  void-new-8   1741     1.09 TiB  ACTIVE    0                OK          OK
66       80d53c1d-206e-4021-848b-e52b47bf32fa  void-new-9   1801     1.09 TiB  ACTIVE    0                OK          OK
68       3d669d70-6db2-4a7d-a13b-47ad531f43dd  void-new-11  1861     1.09 TiB  ACTIVE    0                OK          OK
69       ded74ec1-d208-41a9-af2d-eb1c1e81e613  void-new-12  1981     1.09 TiB  ACTIVE    0                OK          OK
70       4451db18-8417-4d4f-b5d0-02bad359b9ff  void-new-13  2041     1.09 TiB  ACTIVE    0                OK          OK
71       019f2b88-c284-4cf4-b384-0a0fde6ea128  void-new-14  2101     1.09 TiB  ACTIVE    0                OK          OK
72       7a315ea8-9f12-4143-b67b-213f2f3f6748  void-new-15  2161     1.09 TiB  ACTIVE    0                OK          OK
73       dce3f522-5672-4964-8db8-383774c11569  void-new-16  2221     1.09 TiB  ACTIVE    0                OK          OK
```

</details>

## Remove only some drives from the cluster

Perform the following:

1. Deactivate drives.
2. Remove drives from the cluster.

### Deactivate drives

Drive deactivation starts an asynchronous process known as phasing out. It is a gradual redistribution of the data between the remaining drives in the system. On completion, the phased-out drives are in an inactive state. The WEKA cluster does not use inactive drives, but they still appear in the drives list.

To deactivate a drive, run the following command:

`weka cluster drive deactivate <uuids>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | uuids* | Comma-separated drive identifiers. |

Note: Running the `weka cluster drive` command is displayed whether the redistribution is still being performed.

### Remove drives from the cluster

Once you remove a drive from the cluster, the drive is not recoverable.

To remove a drive, run the following command:

`weka cluster drive remove <uuids>`

**Parameters**

 | Name | Value |
 | --- | --- |
 | Name | Value |
 | uuids* | Comma-separated drive identifiers. |

## Remove containers with their allocated drives

Perform the following:

1. Deactivate containers.
2. Remove containers from the cluster.

### Deactivate containers

To deactivate containers with their drives, run the following command:

`weka cluster container deactivate <container-ids> [--allow-unavailable]`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | container-ids* | Space-separated container identifiers |  |
 | allow-unavailable | Allow deactivation of an unavailable container.If the container-id value returns, it joins the cluster in an active state. | No |

### Remove containers from the cluster

Removing containers from the cluster switches them to a stem mode (not part of a cluster), so they can be reallocated to another cluster or purpose.

To remove the container from the cluster, run the following command:

`weka cluster container remove <container-id>`

**Parameters**

 | Name | Value | Default |
 | --- | --- | --- |
 | container-id* | Comma-separated container identifiers. |  |

<!-- ============================================ -->
<!-- File 171/259: operation-guide_system-congestion.md -->
<!-- ============================================ -->

---
description: This page describes possible congestion issues in the WEKA system.
---

# System congestion

## Overview

The WEKA system is designed for efficiency, delivering maximum performance and fully utilizing network links. However, in certain situations, the system may slow down I/O operations or even block new I/Os if specific limits are reached, until the congested resource is alleviated.

While these situations are often temporary and may resolve themselves quickly, persistent congestion can indicate an underlying issue, such as a workload overwhelming the cluster's resources. In such cases, expanding the cluster's resources, as detailed in . For further assistance, contact the [Customer Success Team](../support/getting-support-for-your-weka-system).

## System congestion events and alerts

The WEKA system can issue several types of congestion events and alerts:

 | Type | Description | Actions |
 | --- | --- | --- |
 | FIBERS | Extreme load of concurrent system operations on a process. | This is typically a transient condition caused by system load. If the load is persistent, consider adding more resources, such as servers or cores. See add-a-backend-server or #add-cpu-cores-to-a-container for guidance. |
 | DESTAGER | Excessive pending I/O operations waiting to be written for a specific process. | This situation is often temporary due to system load. If the condition persists, increase the number of servers in the cluster. See add-a-backend-server or expansion-of-specific-resources for guidance. |
 | SSD | An excessive number of pending I/O operations to the SSD. | If there is only one SSD, it may be faulty and require replacement. If multiple SSDs are involved, the system load is too high. To manage the load, add more SSDs to the system. See expansion-of-specific-resources for guidance. |
 | RAID_NOT_OK | I/O failures exceed the system's handling capacity, and I/Os cannot be processed. | Ensure all servers are operational. If any server is down, bring it up. If all servers are active and the issue persists, contact the Customer Success Team. |
 | XDESTAGE | Auxiliary cluster resources are low | This is usually a temporary condition due to system load. If the problem persists, add more servers to the cluster.See add-a-backend-server for guidance or contact the Customer Success Team. |

<!-- ============================================ -->
<!-- File 172/259: operation-guide_upgrading-weka-versions.md -->
<!-- ============================================ -->

---
description: Upgrade your WEKA system with the latest version.
---

# Upgrade WEKA versions

## Upgrade overview

The WEKA upgrade process supports non-disruptive upgrades (NDUs) to ensure minimal impact on system operations. For a complete list of supported source and target versions, refer to the official compatibility information at get.weka.io.

### Upgrade guidelines

**Version requirements**

* Upgrades must progress from older versions to newer versions.
* Version compatibility is based on release dates relative to LTS releases.
* Major version upgrades must be to the next consecutive version only.
* Confirm specific version compatibility at  get.weka.io.
* Make sure the client's version is compatible with the backend upgrade version you intend to deploy. The `client-target-version` parameter must be consistently defined and identical across all clusters within a Single Client Multiple Clusters (SCMC). See .

**Version compatibility rules**

When upgrading from version 4.4.X to 5.0.Y:

* Version 5.0.Y must have been released after the 4.4.X LTS release.
* All intermediate versions must be supported versions.

### **Upgrade example**

#### Supported upgrades

```
4.4.6.122 -> 5.0.1.101    Maximum supported version
                         (released: May 15, 2025 -> June 16, 2025)
4.4.6.114 -> 5.0.1.101    Supported intermediate version
4.4.6     -> 5.0.1.101    Minimum supported version
```

#### Unsupported upgrades

```
4.4.8.53  -> 5.0.1.101     Version not in supported range
4.4.7.89  -> 5.0.1.101     Version not in supported range (released after 5.0.1 code freeze)
4.4.4     -> 5.0.1.101     Version not in supported range
4.4.3     -> 5.0.1.101     Version not in supported range
...
```

Always review release notes for version-specific upgrade requirements.

Note: The source system must be set up in MCB architecture. If not, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team) to convert the cluster architecture to MCB. See .\
This workflow is only intended for professional services.

Note: Customers running WEKA clusters on AWS with **auto-scaling groups** must contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team) before converting the cluster to MCB.

## What is a non-disruptive upgrade (NDU)

In MCB architecture, each container serves a single type of process, drive, frontend, or compute function. Therefore, upgrading one container at a time (rolling upgrade) is possible while the remaining containers continue serving the clients.

Note: Some background tasks, such as snapshot uploads or downloads, must be postponed or aborted. See the prerequisites in the upgrade workflow for details.

#### **Internal upgrade process**

Once you run the upgrade command in `ndu` mode, the following occurs:

1. Downloading the version and preparing all backend servers.
2. Rolling upgrade of the **drive** containers.
3. Rolling upgrade of the **compute** containers.
4. Rolling upgrade of the **frontend** and **protocol** containers and the protocol gateways.

**Related topics**

## Upgrade workflow

1. Verify system upgrade prerequisites
2. Prepare the cluster for upgrade
3. Prepare the backend servers for upgrade (optional)
4. Upgrade the backend servers
5. Enable LLQ and WC in AWS
6. Upgrade the clients
7. Check the status after the upgrade

Note: Adhere to the following considerations:
* **Protocol separation**: Upgrading a WEKA cluster with a server used for more than one of the following protocols, NFS, SMB, or S3, is not permitted. If such a case arises, the upgrade process does not initiate and indicates the servers that require protocol separation. Contact the Customer Success Team to ensure only one additional protocol is installed on each server.
* **Legacy NFS protocol**: If a legacy NFS protocol is implemented, contact the Customer Success Team. In this case, the upgrade is blocked.
* **NFS file-locking prerequisite before upgrade:** Ensure the `rpc.statd` and `rpc-statd-notifiy` services are stopped on the WEKA servers. If not, run the following commands:\
`systemctl disable rpc-statd.service`\
`systemctl disable srpc-statd-notify-service`
* **S3 Cluster Creation**: If you plan to create an S3 cluster, it‚Äôs crucial to ensure the upgrade process is complete and all containers are up before initiating the creation.

### 1. Verify system upgrade prerequisites

Ensure the environment meets the necessary prerequisites before proceeding with any system upgrade. The **WEKA Upgrade Checker Tool** automates these essential checks, comprehensively assessing the system‚Äôs readiness. Whether performing a single-version upgrade or a multi-hop upgrade, following this procedure is mandatory.

#### Summary of the WEKA Upgrade Checker Tool results:

1. **Passed checks (Green)**: The system meets all prerequisites for the upgrade.
2. **Warnings (Yellow)**: Address promptly to resolve potential issues.
3. **Failures (Red)**: Do not proceed; they may lead to data loss.

<details>

<summary>Sample list of the verification steps performed by the WEKA Upgrade Checker Tool </summary>

* [x] **Backend server Prerequisites and compatibility**:
  * Confirm that all backend servers meet the [prerequisites and compatibility](../planning-and-installation/prerequisites-and-compatibility) requirements of the target version. Address any discrepancies promptly.
  * **Contact the Customer Success Team** if there are compatibility issues or missing prerequisites.
* [x] **Source version architecture**:
  * Verify that the source version is configured in an **MCB (Multi-Cluster Backend)** architecture.
  * If the source version still uses the legacy architecture, take the necessary steps to **convert it to MCB**.
  * **Contact the Customer Success Team** for assistance during this conversion process.
* [x] **S3 protocol configuration and target version 4.2.4**:
  * If the S3 protocol is configured and the target version is **4.2.4**, the tool performs additional checks.
  * **Contact the Customer Success Team** to confirm that the internal key-value store (**ETCD**) has been successfully upgraded to **KWAS** (Key-Value WEKA Accelerated Store).
* [x] **Backend server availability**:
  * Ensure that all backend servers are **online and operational**.
  * Address any server availability issues promptly.
* [x] **User role**:
  * Log in with a user role that has **Cluster Admin privileges**.
  * If necessary, adjust user roles to meet this requirement.
* [x] **Rebuild completion**:
  * Verify that any ongoing rebuild processes have been successfully completed.
  * Do not proceed with the upgrade until the rebuilds are finished.
* [x] **Alerts and outstanding issues**:
  * Check for any outstanding alerts or unresolved issues.
  * Resolve any pending alerts before proceeding.
* [x] **Free space in /opt/weka directory**:
  * Ensure that there is **at least 4 GB of free space** in the `/opt/weka` directory.
  * If space is insufficient, address it promptly.
* [x] **Non-Disruptive Upgrade (NDU) process tasks**:
  * Before initiating the NDU process, **stop the following tasks** (if applicable):
    * **Upload a snapshot**:
      * If applicable, wait for the snapshot upload to complete.
      * Alternatively, abort the upload process if needed.
      * Task Name: **STOW_UPLOAD**
    * **Create a filesystem from an uploaded snapshot**:
      * Wait for the download to complete.
      * If necessary, abort the process by deleting the downloaded filesystem or snapshot.
      * If the task is in the snapshot prefetch stage of the metadata phase, wait for the prefetch to complete or abort it. Resuming snapshot prefetch after the upgrade is not possible.
      * Task Names: STOW_DOWNLOAD_SNAPSHOT, STOW_DOWNLOAD_FILESYSTEM, FILESYSTEM_SQUASH, and SNAPSHOT_PREFETCH
    * **Sync a Filesystem from a Snapshot**:
      * **Wait for the download to complete**.
      * If needed, abort the process by deleting the downloaded filesystem or snapshot.
      * Task Name: STOW_DOWNLOAD_SNAPSHOT
    * **Detach Object Store Bucket from a Filesystem**:
      * During the upgrade, detaching an object store is blocked.
      * If the task is currently running, **ignore it**.
      * Task Name: OBS_DETACH
  * **Postpone planned tasks or address running tasks**:
    * If any planned tasks are scheduled during the upgrade, postpone them until after the NDU process.
    * If tasks are currently running, take necessary actions based on their status.
    * Consult the [**Background tasks**](background-tasks) topic for comprehensive guidance.

</details>

Note: **Multi-hop version upgrades:**
After completing an upgrade, a background process initiates the conversion of metadata to a new format (in specific versions). This conversion may take several minutes before another upgrade can commence. To monitor the progress, use the `weka status` CLI command and check if a data upgrade task is RUNNING.

By diligently following this system readiness validation procedure, you can confidently proceed with system upgrades, minimizing risks and ensuring a smooth upgrade.

Demo: WEKA Upgrade Checker
{% endembed %}

Note: * Prioritize running the WEKA Upgrade Checker **24 hours** before any scheduled upgrades. This step is critical to identify and address any potential issues proactively.
* Ensure **passwordless SSH access** is set up on all backend servers. This is crucial for the seamless execution of the Python script while running the WEKA Upgrade Checker.

#### **Procedure**

1. **Log in to one of the backend servers as a root user:**
   * Access the server using the appropriate credentials.
2. **Obtain the WEKA Upgrade Checker:** \
   Choose one of the following methods:
   * **Method A:** Direct download
     * Clone the WEKA Upgrade Checker GIT repository with the command: \
       `git clone https://github.com/weka/tools.git`
   * **Method B:** Update from existing tools repository
     * If you have previously downloaded the tools repository, navigate to the **tools** directory.
     * Run `git pull` to update the tools repository with the latest enhancements. (The WEKA tools, including the WEKA Upgrade Checker, continuously evolve.)
3.  **Run the WEKA Upgrade Checker:** Navigate to the `weka_upgrade_checker` directory. It includes a binary version and a Python script of the tool. A minimum of Python 3.8 is required if you run the Python script.

    *   Run the Python script:

        `python3.8 ./weka_upgrade_checker.py --target-version <version>`

    Or

    * Run the Python precompiled script:\
       `./weka_upgrade_checker --target-version <version>`

    Replace `<version>` with your target version. For example `4.4.4`.\
    The tool scans the backend servers and verifies the upgrade prerequisites.
4. **Review the results:**
   * Pay attention to the following indicators:
     * **Green:** Passed checks. Ensure the tool's version is the latest.
     * **Yellow**: Warnings that require attention and remedy.
     * **Red**: Failed checks. If any exist, **do not proceed**. Contact the Customer Success Team.
5. **Send the log file to the Customer Success Team:**
   * The `weka_upgrade_checker.log` is located in the same directory where you ran the tool. Share the latest log file with the Customer Success Team for further analysis.

### 2. Prepare the cluster for upgrade

Download the new WEKA version to one of the backend servers using one of the following methods depending on the cluster deployment:

* Method A: Using a distribution server
* Method B: Direct download and install from get.weka.io
* Method C: If the connectivity to get.weka.io is limited

For details, select the relevant tab.

Use this method if the cluster environment includes a distribution server from which the target WEKA version can be downloaded.

If the distribution server contains the target WEKA version, run the following commands from the cluster backend server:

```
weka version get <version>
weka version prepare <version>
```

Where: \<version> is the target WEKA version, for example: `4.4.0`.

If the distribution server does not contain the target WEKA version, add the option `--from` to the command, and specify the get.weka.io distribution site, along with the token.

Example:

```
weka version get <version> --from https://[GET.WEKA.IO-TOKEN]@get.weka.io
weka version prepare <version>
```

Use this method if the cluster environment has connectivity to get.weka.io.

1. From the Public Releases on the get.weka.io, select the required release.
2. Select the **Install** tab.
3. From the backend server, run the `curl` command line as shown in the following example.

Use this method if the cluster environment does not have connectivity to get.weka.io, such as with private networks or dark sites.

1. Download the new version tar file to a location from which you copy it to a dedicated directory in the cluster backend server, and untar the file.
2. From the dedicated directory in the cluster backend server, run the `install.sh` command.

### 3. Prepare the backend servers for upgrade (optional)

When working with many backend servers, preparing them separately from the upgrade process in advance is possible to minimize the total upgrade time. For a small number of backend servers, this step is not required.

The preparation phase prepares all the connected backend servers for the upgrade, which includes downloading the new version and getting it ready to be applied.

Once the new version is downloaded to one of the backend servers, run the following CLI command:

`weka local run --container drives0 --in <new-version> upgrade --prepare-only`

Where:

`<new-version>`: Specify the new version. For example,`4.4.1`.

### 4. Upgrade the backend servers

Once a new software version is installed on one of the backend servers, upgrade the entire cluster backend servers to the new version by running the following command on the backend server.

If you already ran the preparation step, the upgrade command skips the download and preparation operations.

`weka local run --container drives0 --in <new-version> upgrade`

**Consider the following guidelines:**

* Before switching the cluster to the new software release, the upgrade command distributes the new release to all cluster servers. It makes the necessary preparations, such as compiling the new `wekafs` driver.
* If a failure occurs during the preparation, such as a disconnection of a server or failure to build a driver, the upgrade process stops, and a summary message indicates the problematic server.
*   If cleanup issues occur during a specific upgrade phase, rerun it with the relevant option:

    ```bash
    --ndu-drives-phase
    --ndu-frontends-phase
    --ndu-computes-phase
    ```
* If the container running the upgrade process uses a port other than the default (14000), include the option `--mgmt-port <existing-port>` to the command.

### 5. Verify LLQ and WC are enabled in AWS

Enabling the Low Latency Queue (LLQ) improves data processing efficiency in AWS by reducing I/O operation delays. LLQ is enabled by default after an upgrade, but if Write Combining (WC) is not activated in the `igb_uio` driver, the LLQ driver option does not function. After upgrading the backends, verify that WC is enabled.

**Procedure**

1. **Check for upgrade events:**
   * Review the upgrade events on the backends.
   *   If `NetDevDriverReloadFailed` appears, restart the WEKA service by running the following commands on each backend server:

       ```
       weka local stop
       weka local start
       ```
2. **Verify WC activation:**
   *   Check if WC is activated by running:

       ```
       cat /sys/module/igb_uio/parameters/wc_activate
       ```
   * If the output is `#1`, WC is activated, which enables the LLQ driver option.

### 6. Upgrade the clients

After all backend components have been upgraded, clients continue operating with their existing version and can interact with the upgraded backends. Typically, a client version is supported only if it is no more than one major version behind the backend version. Therefore, clients must be upgraded before the subsequent cluster software version upgrade to maintain compatibility.

An exception to this rule is that clients running version 4.2.1 or later are compatible with clusters running version 4.4.6.

#### Stateless client upgrade options

* If a stateless client is mounted on a single cluster, it is automatically upgraded to the backend version after rebooting, or a complete `umount` and `mount` is performed.
* If a stateless client is mounted on multiple clusters, the client container version is the same as the `client-target-version` in the cluster (see [Mount filesystems from multiple clusters on a single client](../weka-filesystems-and-object-stores/mounting-filesystems/mount-fs-from-scmc)).
* Stateless clients can also be upgraded manually.
* Use the `--client-only` flag in the `weka version get` command to ensure that only the essential components relevant to the stateless client operation are downloaded, excluding non-relevant packages.
* To limit the display of versions unless the complete set of components is present, use the `--full` flag with the `weka version` command  This provides you with finer control over version information visibility.
* You can manually upgrade the clients locally (one by one) or remotely (in batches), usually during a maintenance window.

#### Persistent client upgrade options

* Clients can be upgraded manually. This can be done either locally on each client individually or remotely in batches. This process typically occurs during a scheduled maintenance window.
* An upgrade is performed on a gateway, which is a persistent client that runs a specific protocol. This gateway is associated with containers with the `allow_protocols` parameter set to true. The upgrade process involves interaction with backend servers.

#### Client upgrade procedures

To upgrade a stateless or persistent client locally, connect to the client and run the following command line:

1. Run: `weka version get <target-version> --from <backend name or IP>:<port>`
2. Upgrade the agent by running the following (replace the x with the latest minor version):\
   `weka version set --agent-only 4.4.x`
3. Upgrade the client containers. Do one the following following:
   * For clients connected to a single cluster, run `weka local upgrade`
   * For clients connected to a multiple  clusters, upgrade all containers simultaneously by running  `weka local upgrade --all`

An alert is raised if there is a mismatch between the clients' and the cluster versions.

Add the `--from <backend name or IP>` option to download the client package only from the backend, thus avoiding downloading from get.weka.io. The default port is 14000.

To upgrade stateless or persistent clients remotely in batches, add the following options to the  `weka local upgrade` command:

* `--mode=clients-upgrade`: This option activates the remote upgrade.
* `--client-rolling-batch-size`: This option determines the number of clients to upgrade in each batch.  For example, if there are 100 clients, you can set this option to 10 and the upgrade will run 10 batches of 10 clients each.

If you need upgrade specific clients, add the `--clients-to-upgrade` and the clients' ids to upgrade. For example, `--clients-to-upgrade 33,34,34`.

If you need to skip upgrade of specific clients, add the `--drop-host`  and the clients' ids to skip. For example, `--drop-host 22,23`.

If an upgrade of a client part of a batch fails, it stops the following batch upgrade. The current running batch continues the upgrade.

**Command syntax**

`weka local run -C <backend name> --in <target release> upgrade --mode=clients-upgrade --client-rolling-batch-size <number of clients in a batch> --clients-to-upgrade <comma separated clients' ids> --drop-host <comma separated clients' ids> --from backends`

**Example**

The following command line upgrade two clients in two batches (each batch has one client):

`weka local run -C drives0 --in 4.3.0.78 upgrade --mode=clients-upgrade --client-rolling-batch-size 1`

**Output example:**

### 7. Check the status after the upgrade

Once the upgrade is complete, verify that the cluster is in the new version by running the `weka status` command.

**Example:** The following is returned when the system is upgraded to version 5.0.1:

```
# weka status
Weka v5.0.1
...
```

<!-- ============================================ -->
<!-- File 173/259: operation-guide_manage-weka-drivers.md -->
<!-- ============================================ -->

---
description:
---

# Manage WEKA drivers

## Overview

Updating WEKA drivers ensures compatibility with new kernels, improves performance, resolves bugs, applies security patches, and introduces new features, helping maintain system stability and security. WEKA's driver management process simplifies the distribution of driver packages across environments. You can sign, build, or import drivers on to the backend and use it as a distribution server for other cluster members, streamlining the management and deployment of drivers.

**Key features**

* **Simplification**: The WEKA agent manages downloading, signing, and handling archives, while driver scripts focus on core tasks.
* **Support for requirements**: Includes local builds, driver signing, and archive distribution for different kernels and architectures.
* **Consistency and flexibility**: Ensures consistent archives and kernel signatures across systems, streamlining version management.

#### WEKA drivers directory structure

The WEKA driver directory structure is organized to help you manage driver packages and files efficiently. Here's an overview of the key directories and their contents:

* `/opt/weka/dist/drivers`: Stores driver packages by name, version, and kernel signature.
* `/opt/weka/data/drivers`: Parent directory for all driver-related files.
* `/opt/weka/data/drivers/*driver-package*/stage`: Contains staged drivers and kernel object files (`.ko` files ).

### weka driver subcommands

* `weka driver build`:  Builds drivers without installing.
* `weka driver install`: Builds and installs drivers.
* `weka driver sign`: Signs drivers for installation or archiving.
* `weka driver pack`: Builds and packages drivers for distribution.
* `weka driver export`: Exports driver packages into an archive.
* `weka driver import`: Imports driver archives.
* `weka driver kernel`: Shows the kernel signature of the system.
* `weka driver ready`: Checks if drivers are loaded.

## Manage WEKA drivers

Follow these steps to build and manage WEKA drivers for a specific OS and WEKA kernel version on a system running the **target OS kernel version**. This system, which can be a virtual machine (VM), **must** have the exact kernel version that you intend to use for distribution across the WEKA cluster.

Note: The `weka driver` commands described in this section provide a high-level overview. For a complete list of arguments and options, refer to the CLI reference guide in the  section.

1.  **Install the WEKA agent:**\
    Run the following command to install the WEKA agent on the VM:

    ```bash
| curl http://<backend IP>:14000/dist/v1/install | sh |
    ```
2.  **Retrieve the agent version and driver source code:**\
    Use the following command to download only the source code packages needed to build the drivers:

    ```bash
    weka version get <version> --driver-only
    ```

    This allows you to compile the required drivers without installing the full WEKA software.
3.  **Create a driver package:**\
    Build the necessary drivers by running:

    ```bash
    weka driver pack --version <version>
    ```

    This command packages the drivers for the specified OS and WEKA kernel versions into an archive, which can be deployed across the cluster.

    The `--version` option is required only if a full WEKA installation is not present or the version is not set. If both are available, the command defaults to the version reported by the installed WEKA software.
4.  **Sign drivers (optional):**\
    To enable secure boot, sign the drivers by running:

    ```bash
    weka driver sign <key> --pack
    ```

    This signs the drivers or the entire distribution package, replacing any existing signatures, making the package secure for deployment.
5.  **Export the driver archive:**\
    Run the following command to export the driver package:

    ```bash
    weka driver export <path>
    ```

    Copy the exported archive to all backends in the WEKA cluster.
6.  **Import the driver package:**\
    On each backend, import the driver package from the location where it was copied by running:

    ```bash
    weka driver import <path>
    ```
7.  **Install the imported driver**:

    Install the driver on each backend using the following command:

    ```bash
    weka driver install
    ```

    This command builds and installs the driver for the current kernel, ensuring it is ready for use by the system.

<details>

<summary>Example: Import and installation flow on a client</summary>

1. Copy the exported driver archive to the client: \
   `scp <driver-package>.tar.gz user@<client>:/tmp/`
2. SSH into the client: `ssh user@<client>`
3. Import the driver archive: `weka driver import /tmp/<driver-package>.tar.gz`
4. Install the imported driver: `weka driver install`
5. After running weka driver install, the driver is built and loaded for the kernel running on the client. Use the following command to verify the driver status: \
   `weka driver ready`

</details>

<!-- ============================================ -->
<!-- File 174/259: drivers-distribution-service.md -->
<!-- ============================================ -->

---
description:
---

# Drivers distribution service

## Overview

The Drivers Distribution Service (DDS) streamlines WEKA deployment by providing pre-built kernel drivers for supported Linux distributions. It automatically delivers appropriate drivers during installation of WEKA clusters and clients, eliminating the need for manual compilation or configuration.

By ensuring consistent driver availability across supported environments, DDS enables cluster and client pods to initialize and operate correctly with minimal administrative overhead.

### Specifications

* **Available drivers:**
  * weka-driver
  * driver-igb-uio
  * driver-mpin-user
* **Supported operating systems**:
  * Amazon Linux 2 (for EKS)
  * Amazon Linux 2023
  * Oracle Linux 8
  * Rocky Linux 8
  * Red Hat Enterprise Linux 9 (for OpenShift)

### Accessing the service

Configure your WEKA clusters and clients to automatically download drivers from: https://drivers.weka.io.

For a configuration example, see .

<!-- ============================================ -->
<!-- File 175/259: monitor-the-weka-cluster.md -->
<!-- ============================================ -->

# Monitor the WEKA Cluster

## Topics in this section

### Deploy monitoring tools using the WEKA Management Station (WMS)

Deploy the monitoring tools, LWH, WEKAmon, and SnapTool, using the WEKA Management Station (WMS) in an on-premises environment.

### WEKA Home - The WEKA support cloud

Improve the WEKA support process with WEKA Home.

### Set up WEKAmon for external monitoring

Configure WEKAmon to integrate Grafana and Prometheus for centralized monitoring of WEKA cluster metrics, logs, alerts, and statistics.

### Set up the SnapTool external snapshots manager

The SnapTool is an external snapshots manager that enables scheduled snapshots and automatic operations

<!-- ============================================ -->
<!-- File 176/259: monitor-the-weka-cluster_snapshot-management.md -->
<!-- ============================================ -->

---
description:
---

# Set up the SnapTool external snapshots manager

WEKA provides an external snapshots manager called SnapTool, allowing you to efficiently schedule and manage filesystem snapshots for your WEKA cluster.

Key features of SnapTool include:

* Schedule snapshots at monthly, daily, or minute-level intervals throughout the day.
* Set the number of snapshot copies to retain for each schedule.
* Automatically delete expired snapshots.
* Automatically upload snapshots of a filesystem to its configured object store.
* Perform uploads and deletions in the background.
* Access a Web Status GUI to view snapshot schedules, upload/download queues, locator IDs for uploaded snapshots, and logs. The default URL is http://\<snaptool server hostname/IP>:8090.

Configuration of SnapTool is managed by editing the `snaptool.yml` file before installation. To adjust SnapTool‚Äôs behavior after installation, update this file. SnapTool automatically monitors the configuration file and reloads it approximately every minute when changes are detected.

Note: The Snapshot Policies feature replaces the SnapTool external snapshot manager. Snapshot Policies offer greater flexibility and improved control for creating and managing snapshots. See .

## **SnapTool installation and communication with WEKA cluster**

The SnapTool runs on any Linux-based physical server or VM, communicating with the WEKA cluster via an IP connection to a WEKA host using the WEKA REST API. It can be installed as a systemd service or within a Docker container.

Note: If you have deployed the WMS, follow the procedure in:. Otherwise, continue with this workflow.

### Before you begin

If a previous SnapTool version exists in the physical server, make a copy of your existing `snaptool.yml` file.

If the `snaptool.yml` file is from releases before 1.0.0, it is incompatible with 1.0.0 or higher. You need to modify the file to use the new syntax.

Setting up a dedicated physical server (or VM) for the installation is recommended.

#### Server minimum requirements

* 2 cores
* 8 GB RAM
* 5 GB /opt/ partition (for the SnapTool installation)
* Network access to the WEKA cluster
*   To use Docker, the following must be installed on the dedicated physical server (or VM):

    * `docker-ce`
    * `docker-compose` or `docker-compose-plugin`, depending on the existing operating system.

    For instructions on the Docker installation, see the Docker website.

### SnapTool authentication

For the SnapTool host to communicate with the WEKA cluster, a security token is necessary. However, the SnapTool host is not required to have the WEKA client installed.

#### Prepare SnapTool user and token

Perform the following steps on an **existing host with access to the WEKA CLI**, for example, on a WEKA backend server.

1. **Create a dedicated user:** Create a unique local username (for example, `snaptool`) for SnapTool. The unique username is displayed in the event logs, making the identification and troubleshooting of issues easier. Then, assign the ClusterAdmin or OrgAdmin role.\
   Example: `weka user add snaptool clusteradmin`
2. **Generate an authentication token for the user:** Run the following command:\
   `weka user login snaptool --path snaptool-authtoken.json`
3. **Transfer the token:** Copy the `snaptool-authtoken.json` file to the SnapTool management server. It will later be placed in a specific directory on that host.
4. **Remove the token file:** Delete the `snaptool-authtoken.json` locally.\
   Example: `rm snaptool-authtoken.json`

#### Configure SnapTool host with authentication token

Perform the following steps on the **SnapTool host**.

1.  **Create a directory for the authentication token:** Run the following command:

    `mkdir /root/.weka`
2. **Move the previously-created authentication token into the new directory: :** Run the following command: `mv ~/snaptool-authtoken.json /root/.weka/auth-token.json`
3. **Ensure appropriate ownership and permissions are set:** Run the following commands:\
   `chown root:root /root/.weka/auth-token.json`\
   `chmod 400 /root/.weka/auth-token.json`

**Related topics**

#create-a-local-user

### Option 1: Install the SnapTool package with the systemd service

1. Download the latest `snaptool.tar` file from this link and extract it to the physical server.
2. Edit the `snaptool.yml` configuration file (default location: /opt/weka/snaptool).\
   See Edit the configuration in snaptool.yml.\
   This is a mandatory step before running the installer. Otherwise, the installation fails.
3. Install the _unit_ file into the `systemd` and start the service. Run the following command:\
   `./install.sh`\
   The installer validates the connection to the cluster by the hosts specified in the `snaptool.yml` file.

Note: If the systemd service is already running locally, the installer stops it and preserves the existing `snaptool.yml` file before restarting it.

### Option 2: Install the SnapTool package in Docker

The `snaptool` container runs similarly to other WEKA Docker containers.

1. Download the docker image from the docker hub. Run the following command:\
   `docker pull wekasolutions/snaptool:latest`
2. Download the following files from GitHub https://github.com/weka/snaptool/releases to a dedicated directory in the physical server:
   * `snaptool.yml`
   * `docker_run.sh`
3. Edit the `snaptool.yml` configuration file (default location: /opt/weka/snaptool).\
   See Edit the configuration in snaptool.yml.\
   This is a mandatory step before running the installer. Otherwise, the installation fails.
4. Edit the `time_zone` field in the `docker_run.sh` file.
5. Run the following command:\
   `./docker_run.sh`
6. Verify that the SnapTool container is running using the following command:\
   `docker ps`

Example:

```
oot@weka142:~# docker ps
CONTAINER ID   IMAGE                   COMMAND                 CREATED      STATUS     PORTS   NAMES
718486e75b38   wekasolutions/snaptool  "/wekabin/snaptool -‚Ä¶"  30 hours ago Up 5 hours         weka_snaptool
```

Note: A `logs` directory is created during the installation in the current working directory for logs and snapshot journaling files.

## Edit the configuration in the snaptool.yml file

The SnapTool configuration is defined in the `snaptool.yml` file.

1. Go to the `snaptool` directory and open the `snaptool.yml` file.
2. In the **cluster** section under the **hosts** list, replace the hostnames with the actual hostnames/IP addresses of the Weka containers (up to three would be sufficient).

Syntax:

```
cluster:
    auth_token_file: auth-token.json
    hosts: vweka01,vweka02,vweka03
```

Example:

```
cluster:
    auth_token_file: auth-token.json
    hosts: hostname1,hostname2,hostname3
```

3\. In the **snaptool** section, the default network port to access the Web Status GUI is 8090. If required, you can modify it. To disable the Web Status GUI, set the port to 0.

Syntax:

```
snaptool:
    port: 8090
```

4\. In the **filesystems** section, specify the filesystems and their schedule names to run snapshots.

Syntax:

```
<fs_name1>:  <schedule1>,<schedule2>...
<fs_name2>:  <schedule1>,<schedule2>...
```

Example:

```
filesystems:
   fs01: default
   fs02: Weekdays-6pm, Weekends-noon
```

5\. Optional. Customize the snapshot schedules.

Adhere to the following rules when customizing the schedules:

* Schedules within a schedules group, such as `default`, cannot be assigned separately from the group. Use only the group name.
* To set a specific schedule within a schedules group, such as monthly and weekly, **not** to run on a filesystem, remove it from the filesystem's schedule list.
* When deleting snapshots automatically, based on the `retain:` value, snapshots for a schedule and filesystem are sorted by the creation time. The oldest snapshots are deleted until the number of snapshots to retain (the value specified in the `retain:` section) remains.
* The SnapTool checks if the `snaptool.yml` file has changed about every minute and reloads it if it is changed. Snapshot schedules are then recalculated before creating new snapshots.

Note: For details about the syntax of the `schedules` section, see the comments in the `snaptool.yml` file.

Example:

```yaml
schedules:
   default:
      monthly:
         every: month
         retain: 6
         # day: 1   (this is default)
         # at: 0000 (this is default)
      weekly:
         every: Sunday
         retain: 8
         # at: 0000 (this is default)
      daily:
         every: Mon,Tue,Wed,Thu,Fri,Sat
         retain: 14
         # at: 0000 (this is default)
      hourly:
         every: Mon,Tue,Wed,Thu,Fri
         retain: 8
         interval: 60
         at: 9:00am
         until: 5pm
   workhoursHourlyUp:
      every: Mon,Tue,Wed,Thu,Fri
      retain: 7
      at: 0900
      until: 5pm
      interval: 60
      upload: True
   workhoursEvery20:
      every: Mon,Tue,Wed,Thu,Fri
      retain: 7
      at: 0900
      until: 5pm
      interval: 20
   weekendsNoon:
      every: Sat,Sun
      retain: 4
      at: 1200
   fridayUpload:
      every: Friday
      retain: 3
      at: 7pm
      upload: True

```

### Snapshot naming conventions

The format of the snapshot names is `<schedulename>.YYMMDDHHMM`, with the access point `@GMT-YYYY.MM.DD-HH.MM.SS`.

**Example:** For a snapshot name `Weekends-noon.2103101200` and access point `@GMT-2021.03.10-12.00.00`, the snapshot name is in the local timezone, the access point is in GMT, and the server timezone is GMT.

The name for a group of snapshots is`<schedulegroupname>_<schedulename>.YYMMDDHHMM`. The length of the full name before the '.' is a maximum of 18 characters.

**Example:** The `default` schedule group with an `hourly` schedule can be named `default_hourly.YYMMDDHHMM`.

Note: The SnapTool distinguishes between user-created snapshots and scheduled snapshots only by their name.
When creating user-created snapshots, avoid name collisions with scheduled snapshot names. The SnapTool might automatically select the user-created snapshots for deletion if the same naming format is used.

## Modify SnapTool schedules

To change, add, or remove schedules in the SnapTool configuration after installation, follow these steps:

1. **Edit the configuration file**: Open the `snaptool.yml` file, typically located at `/opt/weka/snaptool/snaptool.yml`, and make the necessary changes, such as adjusting schedules or adding filesystems. See #edit-the-configuration-in-the-snaptool.yml-file.
2. **Save changes**: Save the modified `snaptool.yml` file after making the updates.
3. **Automatic reload**: SnapTool checks for changes in the configuration file approximately every minute and automatically reloads it if any changes are detected.
4.  **Manually apply changes (optional)**: If you want to apply the changes immediately, restart the SnapTool service with the following command:

    ```
    sudo systemctl restart weka-snaptool.service
    ```
5. **Verify changes**: Review the SnapTool logs in the `logs` directory or monitor snapshot schedules to ensure the new configuration is applied. (The logs directory is created during installation in the current working directory for logs and snapshot journaling files.)

## **Check snapshot status and rotation**

To check if snapshots are running and rotating in a WEKA system, use the following commands:

* **View snapshots**: Run `weka fs snapshot` to list all available snapshots, including their creation time and status.
* **View snapshot details**: For details on a specific snapshot, use `weka fs snapshot --name <snapshot-name>`.
* **Check snapshot rotation**: Regularly review the list and timestamps to ensure old snapshots are being deleted and new ones are created as per the snapshot policy.

For more information on checking snapshot status and rotation, see .

<!-- ============================================ -->
<!-- File 177/259: monitor-the-weka-cluster_external-monitoring.md -->
<!-- ============================================ -->

---
description:
---

# Set up WEKAmon for external monitoring

WEKAmon is an external monitoring package integrating Grafana and Prometheus to provide a centralized metrics, logs, alerts, and statistics dashboard.

WEKAmon includes the following components:

* **Exporter**: Collects data from the WEKA cluster and sends it to Prometheus.
* **Quota Export**: Manages storage quotas and exports quota data to Prometheus.
* **Alert Manager**: Sends alerts via SMTP when users approach soft quota limits.

You can set up WEKAmon independently of the WEKA GUI's built-in monitoring.

If you already use Grafana and Prometheus for other products, you can integrate WEKAmon to visualize all monitoring data on a unified dashboard.

Note: If you have deployed the WMS, follow the procedure in:. Otherwise, continue with this workflow.

## Before you begin

Setting up a dedicated physical server (or VM) for the installation is recommended.

### Server minimum requirements

* 4 cores
* 16 GB RAM
* 50 GB / partition (for the root)
* 50 GB /opt/ partition (for WEKAmon installation)
* 1 Gbps network
*   _Docker_ is the recommended container for the WEKAmon setup.\
    To use Docker, the following must be installed on the dedicated physical server (or VM):

    * `docker-ce`
    * `docker-compose` or `docker-compose-plugin`, depending on the existing operating system.

    For instructions on the Docker installation, see the Docker website.

## Workflow: Install the WEKAmon package

1. Obtain the WEKAmon package: Obtain the WEKAmon package from the GitHub repository by downloading or cloning.
2. Set the WEKAmon authentication: Prepare WEKAmon user and token and configure WEKAmon host with authentication token.
3. Run the install.sh script: The script creates a few directories and sets their permissions.
4. Edit the export.yml file: The `export.yml` file contains the WEKAmon and the exporter configuration. Customize the file according to your actual WEKA deployment.
5. Edit the quota-export.yml file: The `quota-export.yml` file contains the configuration of the quota-export container. Customize the file according to your actual WEKA deployment.
6. Start the docker-compose containers: Once done, you can connect to Grafana on port 3000 of the physical server running the docker containers.

### 1. Obtain the WEKAmon package

The WEKAmon package resides on the GitHub repository. Obtain the WEKAmon package using one of the following methods:

* Download the WEKAmon source code
* Clone the repository

#### Download the WEKAmon source code

It is recommended installing weka-mon in the `/opt` partition of the host server. If you choose a different location, make a note of the location and adjust the instructions accordingly.

1. Go to https://github.com/weka/weka-mon/releases.
2. On the **latest** release section, select the **Source Code** link to download.
3. Copy the downloaded source code to the host server and unpack it into `/opt`.

#### Clone the repository

1. Run the following commands to clone the WEKAmon package from GitHub:

```
cd /opt
git clone https://github.com/weka/weka-mon
cd /opt/weka-mon
```

### 2.  Set the WEKAmon authentication

For the WEKAmon host to communicate with the WEKA cluster, a security token is necessary. However, the WEKAmon host is not required to have the WEKA client installed.

#### Prepare WEKAmon user and token

Perform the following steps on an **existing host with access to the WEKA CLI**, for example, on a WEKA backend server.

1. **Create a dedicated user:** Create a unique local username (for example, `wekamon`) for WEKAmon. The unique username is displayed in the event logs, making the identification and troubleshooting of issues easier. Then, assign the ClusterAdmin or OrgAdmin role.\
   Example: `weka user add wekamon clusteradmin`
2. **Generate an authentication token for the user:** Run the following command:\
   `weka user login wekamon --path wekamon-authtoken.json`
3. **Transfer the token:** Copy the `wekamon-authtoken.json` file to the WEKAmon management server. It will later be placed in a specific directory on that host.
4. **Remove the token file:** Delete the `wekamon-authtoken.json` locally.\
   Example: `rm wekamon-authtoken.json`

#### Configure WEKAmon host with authentication token

Perform the following steps on the **WEKAmon host**.

* **Prerequisite**: Ensure the authentication token file (`/weka/.weka/auth-token.json`) is readable by the user running the WEKAmon container. If the container operates with restricted permissions, adjust the file permissions accordingly. Typically, you can determine the container‚Äôs user using `docker inspect`.
*   **Create a directory for the authentication token:** Run the following command:

    `mkdir /opt/weka-mon/.weka`
* **Move the previously-created authentication token into the new directory: :** Run the following command: `mv ~/wekamon-authtoken.json /opt/weka-mon/.weka/auth-token.json`
* **Ensure appropriate ownership and permissions are set:** Run the following commands:\
  `chown root:root /opt/weka-mon/.weka/auth-token.json`\
  `chmod 400 /opt/weka-mon/.weka/auth-token.json`

**Related topics**

#create-a-local-user

### 3. Run the install.sh script

The `install.sh` script creates a few directories and sets their permissions.

Run the following command:

```
./install.sh
```

### 4. Edit the export.yml file

The WEKAmon and exporter configuration are defined in the `export.yml` file.

1. Change directory to `/opt/weka-mon` and open the `export.yml` file.
2. In the **cluster** section under the **hosts** list, replace the hostnames with the actual hostnames/IP addresses of the Weka containers (up to three would be sufficient). Ensure the hostnames are mapped to the IP addresses in /etc/hosts.

```
hosts:
 - hostname01
 - hostname02
 - hostname03
```

3. Optional. In the **exporter** section, customize the values according to your preferences. For details, see the Exporter configuration options topic below.
4. Optional. Add custom panels to Grafana containing other metrics.

All other settings in the `export.yml` file have pre-defined defaults that do not need modification to work with WEKAmon. All the configurable items are defined but marked as comments by an asterisk (#).

To add custom panels to Grafana containing other metrics from the cluster, you can remove the asterisk from the required metrics (uncomment).

**Example:** In the following snippet of the `export.yml`, to enable getting the FILEATOMICOPEN_OPS statistic, remove the `#` character at the beginning of the line.

If the statistic you want to get is in a Category that is commented out, also uncomment the Category line (the first line in the example). Conversely, insert the # character at the beginning of the line to stop getting a statistic.

```
 'ops_driver':     # Category
   'DIRECT_READ_SIZES':  'sizes'
   'DIRECT_WRITE_SIZES':  'sizes'
#   'FILEATOMICOPEN_LATENCY':  'microsecs'
#   'FILEATOMICOPEN_OPS':  'ops'
```

### 5. Edit the quota-export.yml file

The WEKAmon deployment includes a dedicated container named **quota-export**. The container includes an Alert Manager that emails users when they reach their soft quota.

The configuration of the quota-export container is defined in the `quota-export.yml` file.

1. Go to the `weka-mon` directory and open the `quota-export.yml` file.
2. Specify the same **hosts** as you specified in the `export.yml file` (see above).

Note: The configuration of the Alert Manager is defined in the `alertmanager.yml` file found in the `etc_alertmanager` directory. It contains details about the SMTP server, user email addresses, quotas, and alert rules. To set this file, contact the [Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).

### 6. Start the docker-compose containers

1. Run the following command:

```
docker compose up -d
```

Note: Some older docker versions require `docker-compose up -d` (note the dash between `docker` and `compose).`

2. Verify that the containers are running using the following command:

```
docker ps
```

Example:

```
[root@av0412CL-3 weka-mon] 2022-12-05 17:30:37 $ docker ps
CONTAINER ID   IMAGE                               COMMAND                  CREATED          STATUS            PORTS                                       NAMES
ec1d2584acab   grafana/loki:2.3.0                  "/usr/bin/loki -conf‚Ä¶"   20 minutes ago   Up 20 minutes     0.0.0.0:3100->3100/tcp, :::3100->3100/tcp   weka-mon_loki_1
4645533501f0   grafana/grafana:latest              "/run.sh"                20 minutes ago   Up 20 minutes     0.0.0.0:3000->3000/tcp, :::3000->3000/tcp   weka-mon_grafana_1
d930e903b74e   wekasolutions/export:latest         "/weka/export -v"        20 minutes ago   Up 7 minutes      0.0.0.0:8001->8001/tcp, :::8001->8001/tcp   weka-mon_export_1
dc5f9f710997   wekasolutions/quota-export:latest   "/weka/quota-export"     20 minutes ago   Up 7 minutes      0.0.0.0:8101->8101/tcp, :::8101->8101/tcp   weka-mon_quota-export_1
17689ac9377d   prom/prometheus:latest              "/bin/prometheus --s‚Ä¶"   20 minutes ago   Up 20 minutes     0.0.0.0:9090->9090/tcp, :::9090->9090/tcp   weka-mon_prometheus_1
[root@av0412CL-3 weka-mon] 2022-12-05 17:35:46 $
```

If the status of the containers is not up, check the logs and troubleshoot accordingly. To check the logs, run the following command:

```
docker logs <container id>
```

Once all containers run, you can connect to Grafana on port 3000 of the physical server running the docker containers. The default credentials for Grafana are `admin/admin`.

## Integrate with an existing Grafana/Prometheus environment

If you already have Grafana and Prometheus running in your environment, you only need to run the exporter and add it to the Prometheus configuration.

### 1. Obtain the WEKAmon package

Follow the steps in the 1. Obtain the WEKAmon package section.

### 2. Import the dashboard JSON files

In the Grafana application, import the dashboard `JSON` files from the directory `weka-mon/var_lib_grafana/dashboards`. For instructions, see the Import dashboard topic in Grafana documentation.

### 3. Edit the export.yml and quota-export.yml files

Perform the steps in the following sections above:

#id-4.-edit-the-export.yml-file

#id-5.-edit-the-quota-export.yml-file

### 4. Run the exporter

Do one of the following:

* Run the exporter in the docker container (if you have a docker, this is the simple method).
* Run the exporter as a compiled binary (if you do not have a docker, use this option)
* Run the exporter as a Python script (requires installing a few Python Modules from PyPi).

#### Run the exporter in the docker container

Get and run the container (the `export.yml` configuration file is already edited).

The following example maps the `export.yml` configuration file in several volumes in the container:

*  `~/.weka directory` to enable the container to read the authentication file.
* `/dev/log` to enable entries in the Syslog.
* `/etc/hosts` to enable the hostname resolution (a DNS can also be used, if exists in the docker environment).

There are more options; you can run the command with`-help` or `-h` for a full description.

```
# get the container from dockerhub:
docker pull wekasolutions/export

# example of how to run the container
docker run -d --network=host \
  --mount type=bind,source=/root/.weka/,target=/weka/.weka/ \
  --mount type=bind,source=/dev/log,target=/dev/log \
  --mount type=bind,source=/etc/hosts,target=/etc/hosts \
  --mount type=bind,source=$PWD/export.yml,target=/weka/export.yml \
  wekasolutions/export -v
```

#### Run the exporter as a compiled binary

1. Go to https://github.com/weka/export/releases and download the tarball from the latest release.
2. Copy this file to the physical server (or VM).
3. Run the exporter as follows (for the description of the command-line parameters, see the Exporter section parameters):

```
tar xvf export-1.3.0.tar
cd export
./export -v
```

#### Run the exporter as a Python script

1. Do one of the following:
   * Run `git clone https://github.com/weka/export`
   * Go to https://github.com/weka/export/releases and download the source tarball.
2. Install the required python modules by running the following command:\
   `pip3 install -r requirements.txt`
3. Run the exporter (for the description of the command-line parameters, see the Exporter section parameters):

```
./export -v
```

### Exporter configuration options in the export.yml file

The **exporter** section defines the program behavior.

```
# exporter section
exporter:
  listen_port: 8001
  loki_host: loki
  loki_port: 3100
  timeout: 10.0
  max_procs: 8
  max_threads_per_proc: 100
  backends_only: True
```

#### **Exporter section parameters**

 | Parameter | Description |
 | --- | --- |
 | listen_port | The Prometheus listening port. Do not modify this port unless you modify the Prometheus configuration. |
 | loki_host | If using the Weka-mon setup, do not modify the hostname. Leave blank to disable sending events to Loki. |
 | loki_port | If using the Weka-mon setup, do not modify the port. |
 | timeout | The max time in seconds to wait for an API call to return. The default value is sufficient for most purposes. |
 | max_procs and max_threads_per_proc | Define the scaling behavior. If the number of hosts (servers and clients) exceeds max_threads_per_proc, the exporter runs more processes accordingly.Example: a cluster with 80 Weka servers and 200 compute nodes (aka clients) has 280 hosts. With the default max_threads_per_proc of 100, it runs 3 processes (280 / 100 ~ 3).It's recommended to have 1 available core per process. In this cluster example, deploy at least 4 available cores on the server/VM. |
 | backends_only | Run only on the Weka backend hosts |

The exporter always tries to allocate one host per thread but does not exceed the maximum processes specified in the `max_procs` parameter. In a cluster with 1000 hosts, it doubles or triples up the hosts on the threads.

Note: **Example:**
In a cluster with 3000 hosts, `max_procs` = 8,  and `max_threads_per_proc`= 100, only 8 processes running. Each process with 100 threads, but there are close to 4 hosts serviced per thread instead of the default 1 host.

<!-- ============================================ -->
<!-- File 178/259: monitor-the-weka-cluster_deploy-monitoring-tools-using-the-weka-management-station-wms.md -->
<!-- ============================================ -->

---
description:
---

# Deploy monitoring tools using the WEKA Management Station (WMS)

The WEKA Management Station (WMS) is an installation kit similar to an OS installation disk that simplifies the installation and configuration of the Local WEKA Home (LWH), WEKAmon, and SnapTool in an on-premises environment. The WMS installs the WEKA OS, drivers, and WEKA software automatically and unattended.

The WMS can also install a WEKA cluster by deploying the WEKA Software Appliance (WSA) package on bare metal servers.

See the related topics to learn about the tools installed with the WMS.

**Related topics**

## WMS deployment prerequisites

The server or VM must meet the following requirements:

* **Boot drives:** One or two identical boot drives as an installation target.
  * A system with two identical boot drives has the OS installed on mirrored partitions (LVM).
  * A system with one drive has a simple partition.
* **Minimum boot drive capacity:**
  * If not configuring LWH: SSD 141 GB (131 GiB).
  * If configuring LWH: See the SSD-backed storage requirements section in #1.-verify-prerequisites.
* **Boot type:** UEFI boot (BIOS boot is also supported but deprecated).
* **Cores and RAM:**
  * If not configuring LWH: minimum 4 cores and 16 GiB.
  * If configuring LWH, see the Server minimum CPU and RAM requirements section in #1.-verify-prerequisites.
* **Network interface:** 1 Gbps.
* **Firewall rules:** The WMS listens on multiple ports depending on which service you are accessing. See #required-ports

Note: This workflow only applies to installation on a server or VM. It does not apply to installation on AWS. To install on AWS, contact the Customer Success Team.

## Before you begin

Before deploying the WMS, adhere to the following:

* The root password is `WekaService`
* The weka user password is `weka.io123`
* If errors occur during installation and the installation halts (no error messages appear), use the system console to review the logs in `/tmp`. The primary log is `/tmp/ks-pre.log`.
* To get a command prompt from the Installation GUI, do one of the following:
  * On macOS, type **ctrl+option+f2**
  * On Windows, type **ctrl+alt+f2**.
* Creating a unique local username dedicated to WMS with a ClusterAdmin or OrgAdmin role is highly recommended. The unique username is displayed in the event logs, making identifying and troubleshooting issues easier.\
  To create a local user, see the [Create local users](../../operation-guide/user-management/user-management#create-a-local-user) topic.

## WMS deployment workflow

1. Install the WMS.
2. Configure the WMS.
3. Change password.
4. Configure the email notifications.
5. Install and configure the LWH.
6. Configure the WEKAmon.
7. Edit the hosts file.
8. Configure the Snaptool.
9. Download logs.

### Install the WMS

**Procedure**

1. Download the latest WMS image from get.weka.io (requires sign-in).
2. Boot the server from the WMS image.\
   The following are some options to do that:

Copy the WEKA Management Station ISO image to an appropriate location so the server‚Äôs BMC (Baseboard Management Controller) can mount it or be served through a PXE (Preboot Execution Environment).

Depending on the server manufacturer, consult the documentation for the server‚Äôs BMC (for example, iLO, iDRAC, and IPMI) for detailed instructions on mounting and booting from a bootable ISO image, such as:

* A workstation or laptop sent to the BMC through the web browser.
* An SMB share in a Windows server or a Samba server.
* An NFS share.

To use PXE boot, use the WEKA Management Station as any other Operating System ISO image and configure accordingly.

Burn the WMS image to a DVD and boot it from the physical DVD. However, most modern servers do not have DVD readers anymore.

A bootable USB drive should work (follow online directions for creating a bootable USB drive) but has not been tested yet.

Once you boot the server, the WEKA Management Station installs the WEKA OS (Rocky Linux), drivers, and WEKA software automatically and unattended (no human interaction required).

Depending on network speed, this can take about 10-60 mins (or more) per server.

### Configure the WMS

Once the WMS installation is complete and rebooted, configure the WMS.

**Procedure**

1. Run the OS using one of the following options:

Run the OS through the BMC‚Äôs Console. See the specific manufacturer‚Äôs BMC documentation.

Run the OS through the Cockpit Web Interface on port 9090 of the OS management network.

If you don‚Äôt know the WMS hostname or IP address, go to the console and press the **Return** key a couple of times until it prompts the URL of the WMS OS Web Console (Cockpit) on port 9090.

Change the port from 9090 to 8501, which is the WMS Admin port.

2. Browse to the WMS Admin UI using the following URL:\
   `http://<WMS-hostname-or-ip>:8501`

3. If you have created a local username dedicated to WMS, as recommended, enter its credentials. Otherwise, enter the default username and password _admin_/_admin_. Then, select **Login**.\
   The Landing Page appears.

### Change password

The default password is _admin_. It is recommended to change it for security reasons.

**Procedure**

1. From the left pane, select **Change Password**.
2. Provide your current password, choose a new password, confirm the new password, and click **Reset**.

### Configure the email notifications

Set up email notifications by configuring the SMTP Relay to enable WMS for sending notifications related to:

* **LWH alerts and events:** The LWH sends email alerts and events notifications.
* **WEKAmon quota notifications:** The WEKAmon Alert Manager sends email alerts when a user reaches the soft quota limit.

**Procedure**

1. From the left pane, select **Email Notification Settings**.
2. Set the required details:
   * **Email From Name:** The designated name for the sender of WMS emails.
   * **Email From Address:** The email address used for outgoing emails from WMS.
   * **Email Relay Host:** The smart host or upstream SMTP Relay address WMS uses for sending emails.
   * **Email Relay Port:** The port number used on the SMTP Relay. Typically set to 25, 465, 587, or 2525.
   * **SMTP Relay allows/requires TLS:** Select if the SMTP Relay uses TLS encryption.
   * **Email Relay Username:** Enter the username for logging into the SMTP Relay, if necessary.
   * **Email Relay Password:** Enter the password for logging into the SMTP Relay, if necessary.
   * **Allow Insecure TLS with SMTP Relay:** Enable TLS for an SMTP Relay with a self-signed certificate.
3. Select **Save**.

### Install and configure the LWH

**Procedure**

1. From the left pane, select **Configure Local WEKA Home**.
2. Set the required details:
   * **Listen Address/Domain:** Specify the address or hostname on which LWH will listen. Leave it blank or use 0.0.0.0 to listen on all interfaces. Alternatively, input an IP address, hostname, or FQDN as the TLS certificate requires.
   * **Email Alert Domain Name (REQUIRED):** Enter a domain name (or IP address) for Alert Email URL links. For instance, if you input _sample.com_, the links appear as _https://sample.com/something_. Typically, this is the domain you use to access WMS (this server's name).
   * **Enable Ingress TLS:** Toggle to enable TLS for all connections.
   * **TLS Cert:** Specify the TLS certificate to be used.
   * **TLS Key:** Enter the TLS key corresponding to the specified certificate above.
   * **Enable email notifications (configure in the Email Notification Setting page):** Activate email notifications and set up your email server configurations in the Email Notification Settings page.
   * **Enable forwarding data to Cloud WEKA Home:** Activate this feature to send data to Cloud WEKA Home. Internet connectivity to api.home.weka.io is required for this functionality. The default setting is activated.

3. Select **Save**.\
   The WMS saves the configuration and installs the Minikube and LWH. This process can take several minutes. When the process completes, the following appears:

4. Retrieve the LWH Admin and Grafana passwords as follows:
   * Select **Get Admin Password**. The password appears below the button.
   * Select **Get Grafana Password**. The password appears below the button. This password only applies to the Grafana instance within LWH (not related to the Grafana instance of the WEKAmon).

4. Register the cluster with the LWH.
   * Choose one of the backend servers in your cluster to run the command.
   * Run the appropriate command based on your TLS configuration. If TLS is configured, use the following command with the WMS server IP or hostname:

```

````
   ```bash
   weka cloud enable --cloud-url https://<WMS server IP or hostname>
   ```

````

```

```
   If TLS is not configured, use the following command with the WMS server IP or hostname:

```

```

````
   ```bash
   weka cloud enable --cloud-url http://<WMS server IP or hostname>
   ```

````

```

```
   Ensure that the provided WMS server IP or hostname matches the WMS instance information.
```

Note: The WMS can have multiple IP interfaces, such as when installed as a jump host with distinct interfaces for the corporate network and the cluster network. In scenarios where the cluster is isolated from the corporate network, it is essential to specify the IP address of the WMS associated with the cluster network.

5. Log in to the LWH.\
   On the Landing Page, select **Open Local WEKA Home**.\
   If the tab does not appear, check that the browser pop-up blocker does not block it.\
   When prompted for a password, enter the Admin password retrieved in the previous steps.\
   The **LWH Cluster Overview** page opens on a new tab.

Note: **Reconfiguring LWH:** If required, return to the LWH configuration page, update the configuration, and select **Save** again. The LWH configuration will be updated and restarted.

### Configure the WEKAmon

**Procedure**

1. From the left pane, select **Configure WEKAmon**.

2. Select the services you want to enable. Possible options:
   * **Enable Metrics Exporter & Grafana:** Select to activate metrics exporter and Grafana integration to visualize and analyze performance metrics seamlessly.
   * **Enable Quota Exporter & Notifications:** Select to enable the WEKAmon to send notifications for soft storage quotas.
   * **Enable Snaptool:** Select to activate the snapshots manager, facilitating scheduled snapshots and automated operations.
   * **Enable WEKAmon Log Storage:** Select this option to enable long-term event storage within WEKAmon.

Note: To set the quota limits, see .

3. Set the hostname or public IP address, username, and password of the cluster to monitor (it can be a backend server of the cluster). Then, select **Save**.

Note: If DNS does not have the hostnames of the cluster, do one of the following:
* Edit the /etc/hosts file before trying again. See Edit the hosts file.
* Use the Cockpit Web Interface on port 9090 to change the DNS settings. See Cockpit Web Interface in Configure the WMS.

Once the WMS successfully logs in to the cluster, the WEKAmon installation begins. When the WEKAmon installation is completed, you can open it from the WMS Landing Page.

4. Log in to **Grafana**.\
   On the Landing Page, select **Grafana**.\
   If the tab does not appear, check that the browser pop-up blocker does not block it.\
   When prompted for a username and password, enter the enter `admin/admin` (not the username/password that was retrieved for LWH).\
   The **Grafana** page opens on a new tab.

### Edit the hosts file

If the WEKA cluster servers are not resolvable with a DNS, resolve the server names and associate them with the relevant IP addresses accessible to the WMS in the `/etc/hosts` file.

The WMS provides a simple text editor to facilitate editing the `/etc/hosts` file.

**Procedure**

1. From the left pane, select **Edit Hosts File**.
2. Add the IP addresses of the cluster servers. Type, copy, and paste as in any other simple text editor.
3. Select **Save**.

### Configure the Snaptool

Snaptool is pre-installed in the `/opt/snaptool` directory and includes all the containers, so there is no need to download anything. Only configuration is required.

**Procedure**

1. From the left pane, select **Snaptool Configuration**.
2. In the Snaptool Configuration Editor, if required, you can update the configuration. For details, see #edit-the-configuration-in-the-snaptool.yml-file.\
   Snaptool shares the same cluster login information as WEKAmon and automatically detects and re-loads its configuration when any changes are made.
3. Select **Save**.

### Download diagnostics logs

If errors occur during the WMS installation, download the diagnostics logs and send them to the Customer Success Team.

**Procedure**

1. From the left pane, select **Download Logs**.
2. Select **Gather Logs** to collect the logs.
3. Once finished, select **Download Logs** to save a local copy of the diagnostics tarball on your workstation.

4. Share the downloaded files with the Customer Success Team.

<!-- ============================================ -->
<!-- File 179/259: monitor-the-weka-cluster_the-wekaio-support-cloud.md -->
<!-- ============================================ -->

---
description: Improve the WEKA support process with WEKA Home.
---

# WEKA Home - The WEKA support cloud

WEKA Home is a central cloud location that collects telemetry data, monitors, and keeps track of WEKA clusters in the field. This information is uploaded from customers' WEKA clusters and clients and is primarily used to improve the support process.

WEKA Home is intended for the Customer Success Team and is not accessible to customers directly. WEKA Home enables the Customer Success Team to provide proactive support when recognizing cluster irregularities, improving incident response time, and streamlining the troubleshooting process.

It is intended to be the first source of information to investigate a critical event or an issue in the field. Also, it provides insights into customer usage and behaviors to improve the WEKA product further.

Only licensed WEKA clusters are monitored through WEKA Home, with all telemetry data sent in an encrypted format to ensure security.

WEKA Home provides the following main features:

* Receive and store alerts, events, usage, analytics, statistics, and support diagnostics.
* Query cluster-wide events and statistics.
* Trigger events and alerts for a 24x7 support response.

In the WEKA Home portal, the Customer Success Team can view the cluster‚Äôs statistics, state of health, consolidated view of events, and diagnostics for various triaging activities. The team can offer a comprehensive 24x7x365 support view of all customer systems sending telemetry data.

## Which information is uploaded to WEKA Home?

The WEKA cluster periodically and on-demand uploads various information types to Cloud WEKA Home. The retention period for all the following is 14 days.

**Periodic uploads:**

* **Alerts:** Alerts indicate problematic ongoing states that are impacting the cluster. Alerts are uploaded immediately when a cluster container (host) creates an alert.
* **Events:** Events contain relevant information for the WEKA cluster and customer environment. Triggered by a customer or an environmental change, events can be informational, indicate an issue with the cluster, or a previously resolved issue. Events are uploaded immediately when a cluster container creates an event.
* **Statistics:** Statistics help analyze the WEKA system performance and determine the source of any issue. The uploaded statistics information includes a subset of the complete list available  from the cluster, such as IOPS, throughput, latency, metadata, and block size. Statistics are uploaded every minute from each container.
* **Usage reports:**  Usage reports provide metrics for interface groups, containers, processes (nodes), drives, status, version, and filesystems. Usage reports are uploaded every minute.
* **Analytics:** Analytics provide metrics for the cluster configuration, including drives, filesystem settings, containers, network devices, nodes, protocols, and more. Analytics are uploaded every 30 minutes.

**On-demand uploads:**

* **Diagnostics (support files):** These are uploaded on-demand from the container that collected the diagnostics.

Note: **WEKA Home data privacy notice:** WEKA Home is committed to safeguarding your data privacy. To ensure the confidentiality and security of your information, the WEKA Home support cloud explicitly excludes the following from upload and collection:
* File and directory names
* File contents
* User passwords

## Upload information from the WEKA cluster to the WEKA Home

Uploading information to WEKA Home from the WEKA cluster backend servers and clients is essential for the Customer Success Team to provide practical assistance. If client connectivity cannot be configured, enabling upload information from the backend servers is still beneficial.

**Before you begin**

* Ensure the Cloud WEKA Home and Customer Success Team remote access ports are open. For details, see #required-ports.
* If the connection to the Cloud WEKA Home is through a proxy, set the proxy by running the command: `weka cloud proxy --set <proxy_url>`.
* Ensure that the proxy allow list includes the following two endpoints:
  * `api.home.weka.io`
  * `get.weka.io`

**Procedure**

1. To enable cloud notifications, run the `weka cloud enable` command (during the WEKA cluster installation, it is an optional step, which may be already done).
2. To upload diagnostics collected by the cluster, run the `weka diags upload` command.

**Related topics**

#upload-diagnostics-data-to-weka-home

<!-- ============================================ -->
<!-- File 180/259: monitor-the-weka-cluster_the-wekaio-support-cloud_local-weka-home-deployment.md -->
<!-- ============================================ -->

---
description:
---

# Deploy Local WEKA Home v3.0 or higher

This Local WEKA Home v3.0 (or higher) runs on K3s, a lightweight Kubernetes installed on a single node cluster. Customize the deployment by specifying configuration parameters in the `config.json` file.

## Workflow: Local WEKA Home deployment

If you have deployed the WMS and do not require IPv6 networking, follow the procedure:. Otherwise, perform the following workflow:

1. Verify prerequisites.
2. Prepare the physical server (or VM).
3. Configure IPv6 (optional).
4. Download the Local WEKA Home bundle.
5. Install and configure the Local WEKA Home.
6. Access the Local WEKA Home portal and Grafana.
7. Enable the WEKA cluster to send data to the Local WEKA Home.
8. Test the deployment.

### 1.  Verify prerequisites

Verify that the following requirements are met:

* A dedicated physical server (or VM) for the installation with `systemd`.
* The user account for installing the LWH must have root privileges.
* Server minimum CPU core and RAM requirements:
  * Minimum 8 CPU cores and 20 GiB RAM for up to 1000 total processes.
    * Total processes are equal to the cores used on the cluster **backends** for Management/Frontend/Compute/Drives roles and the cores used on **clients** for Management/Frontend roles.
  *   Sizing for additional processes:

      * The total number of processes determines the number of CPU cores and RAM required.
      * For every additional 1000 processes or less, add 1 CPU core and 8 GiB RAM.

      Example: 20 backends with 10 processes each = 200 processes; 500 clients with 2 processes each = 1000 processes. The total is 1200 processes. This deployment requires 9 CPU cores and 28 GiB.
* SSD-backed storage requirements:
  * Minimum 500 GiB for locally collected data in `/opt/wekahome/data`
* 1 Gbps network

Note: For using other operating systems, contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team).

### 2. Prepare the physical server (or VM)

1. If you added an extra disk to hold the  `/opt/wekahome` data, make sure to format and mount it.  Then ensure it is remounted on reboot.
2. It's recommended to disable the _SELinux_.
3.  If enabled, it is required to disable nm-cloud-setup and reboot the node:

    ```
    systemctl disable nm-cloud-setup.service nm-cloud-setup.timer
    reboot
    ```
4.  Ensure the following ports are open and not used by any other process. Each port is used for the process specified in the brackets. `homecli` adds firewall rules automatically during installation for supported systems `(firewalld`, `ufw)`. For any other setup, check the following ports:

    `6443`   (kube-apiserver)

    `10259` (kube-scheduler)

    `10257` (kube-controller-manager)

    `10250` (kubelet)

    `80`        (Local WEKA Home, WEKA cluster, and web browser)

    `443`      (Local WEKA Home, WEKA cluster, and web browser)
5.  Ensure the following networks are trusted:

    1. `10.42.0.0/16` (pods)
    2. `10.43.0.0/16` (service)

    If these networks are not available, you can customize them in the configuration file before running the installation.

Note: If you forward data from the Local WEKA Home to the Cloud WEKA Home, ensure the outbound traffic on port 443 is open.

### 3. Configure IPv6 (optional)

Local WEKA Home (LWH) supports dual-stack networking, allowing communication over both IPv4 and IPv6 protocols. Enabling IPv6 ensures LWH can function in modern network environments that require IPv6 compliance and connectivity. Both IPv4 and IPv6 can operate simultaneously, providing flexibility and compatibility.

**Benefits of IPv6 configuration**

* Support IPv6-only and dual-stack environments for seamless communication.
* Enable network segmentation to meet enterprise deployment requirements.
* Ensure compatibility with IPv6-enabled client applications.
* Future-proof LWH deployments for evolving IPv6 adoption.

#### Prerequisites

Before configuring IPv6 support, ensure the following requirements are met:

* The host system has IPv6 networking enabled.
* Client machines have IPv6 connectivity to access LWH over IPv6.
* Required IPv6 network ranges are available. If unavailable, customize them (refer to the relevant step in the procedure):
  * Pod network: `2001:cafe:42::/56`
  * Service network: `2001:cafe:43::/112`
* LWH installation has not been completed.

Note: Dual-stack networking must be configured during cluster creation. It cannot be enabled later if the cluster was initially deployed with IPv4-only.

#### Procedure

1. Verify IPv6 readiness:

```sh
cat /proc/sys/net/ipv6/conf/all/disable_ipv6
```

Ensure the output is `0` to confirm IPv6 is enabled.

2. Verify network interface IPv6 assignment:

```shell
ip -6 addr show
```

Confirm the presence of a **global** IPv6 address (`scopeid 0x0<global>`) on your intended network interface.

Example:

```sh
ifconfig ens5
ens5: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 9001
       inet 10.0.4.73  netmask 255.255.0.0  broadcast 10.0.255.255
       inet6 fe80::8b1:29ff:fea7:a707  prefixlen 64  scopeid 0x20<link>
       inet6 2a05:d018:d69:4ef6:65c9:843e:ef26:a5b3  prefixlen 128  scopeid 0x0<global>
       ether 0a:b1:29:a7:a7:07  txqueuelen 1000  (Ethernet)
```

3. If using **custom** network ranges for dual-stack networking, modify the k3s arguments in your LWH configuration:

```shell
"k3sArgs": [
  "--cluster-cidr=10.142.0.0/16,2001:cafe:46::/56",
  "--service-cidr=10.143.0.0/16,2001:cafe:48::/112"
]
```

Replace the `cluster-cidr` and `service-cidr` values with your available networks.

4. Validate the IPv6 configuration **after** the LWH deployment (see #ipv6-validation).

### 4. Download the Local WEKA Home bundle

Download the latest Local WEKA Home bundle (v3.0 or above) to the dedicated server or VM.

### 5. Install and configure Local WEKA Home

1. Run the Local WEKA Home setup bundle as a root user (where `*` is wekahome version):\
   `bash wekahome-*.bundle`
2. To customize the configuration, create a `config.json` file from the following examples and the template located in `/opt/wekahome/current/config.json.sample`.

<details>

<summary>GitHub SSO integration</summary>

Centralize and secure organizational access through GitHub Single Sign-On (SSO), simplifying authentication and user management.

**Prerequisites:** Prepare GitHub OAuth App

1. Go to GitHub Developer Settings.
2. Create a new OAuth application.
3. Set the Authorization Callback URL the same as your Homepage URL.
4. Note the Client ID and Client Secret.

**Implement GitHub-based login**

Add the following lines to the configuration file.

```
"githubSSO": {
    "enabled": true,
    "clientId": "your_github_oauth_client_id",
    "clientSecret": "your_github_oauth_client_secret",
    "emailDomain": "weka.io",
}
```

</details>

<details>

<summary>Trusted network for pods and services (optional)</summary>

If the networks for the pods (cluster) and service (`10.42.0.0/16` and `10.43.0.0/16`, are not available, set your available networks as shown in the following example:

```
"k3sArgs": ["--cluster-cidr=10.142.0.0/16", "--service-cidr=10.143.0.0/16"]
```

Replace the `cluster-cidr` and `service-cidr` values with your available networks.

</details>

<details>

<summary>Domain</summary>

Set the domain for URL accessing the Local WEKA Home portal either by the organization domain FQDN (DNS-based) or IP address (IP-based).

The URL to access the Local WEKA Home does not accept aliases of the DNS name.

Only the name configured in the `config.json` or passed via CLI argument `--host some.domain.com` during setup can be used for accessing the Local WEKA Home.

DNS-based domain setting:\
In the **host** section at the top of the file, set the domain FQDN as shown in the following example:

```json
{
  "host": "some.domain.com"
}
```

IP-based domain setting:\
In the **host** section (at the top of the file) set the IP address of the domain as shown in the following example:

```json
{
  "host": "52.20.26.14"
}
```

If the host section is not set - the first IP address from the provided network interface will be used.

</details>

<details>

<summary>SMTP</summary>

To enable the Local WEKA Home to send emails, set the SMTP details in the **smtp** section as shown in the following example:

```json
{
  "smtp":{
    "sender": "Weka Home",
    "host": "smtp.gmail.com",
    "port": 587,
    "user": "username@your-domain.com",
    "password": "your_password",
    "senderEmail": "weka-home-noreply@your-domain.com",
    "insecure": false,
   }
}
```

If your SMTP server uses a self-signed certificate, set the line `"insecure":` to `true`.

Ensure to enable the SMTP relay service in your SMTP service.

The Google SMTP server requires an app password.

Once the Local WEKA Home is deployed, you can set it to send alerts by email, SNMP, or PagerDuty. See .

</details>

<details>

<summary>TLS certificates</summary>

To enforce an HTTPS connection, you can pass the TLS certificate and private key to config.json or use CLI arguments with certificate and key filenames `--tls-cert cert.pem --tls-key key.pem` during the next setup step.

You can generate a self-signed certificate and a private key using the following example:\
`openssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days <days> -nodes`

Example of passing TLS settings in JSON:

```json
{
  "tls":{
    "cert":"-----BEGIN CERTIFICATE-----\
-----END CERTIFICATE-----\
",
    "key":"-----BEGIN PRIVATE KEY-----\
-----END PRIVATE KEY-----\
"
  }
}
```

To use an IP address as a hostname and to have a valid certificate, make sure you pass SAN during creation. A SAN or subject alternative name is a structured way to indicate all domain names and IP addresses secured by the certificate.

**Related topic**

</details>

<details>

<summary>Events retention period</summary>

The default number of days to keep events in the Local WEKA Home is 30. To reduce the consumption of disk space, specify the max number of days in the **retentionDays** section, as shown in the following example:

```json
{
  "retentionDays": {
   "diagnostics": 10,
   "events": 20,
   "stats": 30
  }
}
```

</details>

<details>

<summary>Forward data from the Local WEKA Home to the Cloud WEKA Home</summary>

Set the forwarding parameters to `true`, as demonstrated in the sample below. Note that this is the default setting commencing from Local WEKA Home v2.10.

To turn off the forwarding of metrics from Local WEKA Home to Cloud WEKA Home, set `enabled: false`.

```
{
  "forwarding": {
    "enabled": true,
    "url": "https://api.home.weka.io",
    "enableEvents": true,
    "enableUsageReports": true,
    "enableAnalytics": true,
    "enableDiagnostics": true,
    "enableStats": true,
    "enableClusterRegistration": true
  }
}

```

For networks that require a **proxy** to forward requests, incorporate the following lines as shown in the example below:

```json
{
  "proxy": {
    "url": "http://user:pass@server.com:3128",
    "noProxy": ["server.com"]
  }
}
```

The supported proxy types include:

* http
* https
* socks5

**Note:** You don‚Äôt need to set the proxy if you are just debugging.

</details>

4. To reload your path variable do one of the following:
   * Re-login to the server.
   * Run the following command: `source /etc/profile`
5.  To initialize the setup, run the following command from the root user: \
    `homecli local setup -c config.json`\
    \
    For a fresh installation, expect approximately 5 minutes for completion.\

Note: If you get a "command not found" error, make sure you did not skip step 4 above.

    \
    **Options:**

    * You can use the default configuration by running: \
      `homecli local setup`
    * Specify the network interface or bind address for the cluster using:\
      `homecli local setup --iface <interface> --ip <IP address>`
    * Set your domain name or external IP as the host with:\
      `homecli local setup --host <host.domain.com>`
    * Enable HTTPS by providing a certificate and key directly to the command instead of using the  `config.json`:\
      `homecli local setup --iface <interface> --tls-cert <cert.pem> --tls-key <key.pem>`

<details>

<summary>Response example of a successful Local WEKA Home installation</summary>

```
helm status wekahome -n home-weka-io
NAME: wekahome
LAST DEPLOYED: Thu Jan  5 09:30:42 2023
NAMESPACE: home-weka-io
STATUS: deployed
REVISION: 3
TEST SUITE: None
NOTES:
Thank you for installing home-weka-io.
Your release is named homewekaio
To learn more about the release, try:

  $ helm status homewekaio -n home-weka-io
  $ helm get all homewekaio -n home-weka-io

------------------------------------------------------------------------
Weka Home Frontend:
------------------------------------------------------------------------
URL:
https://172.31.46.11
Username:
admin
To obtain password, run:
| kubectl get secret -n home-weka-io weka-home-admin-credentials -o jsonpath='{.data.admin_password}' | base64 -d |

------------------------------------------------------------------------
Weka Home REST API:
------------------------------------------------------------------------
URL:
https://172.31.46.11/api/

------------------------------------------------------------------------
Weka Home Statistics (Grafana):
------------------------------------------------------------------------
URL:
https://172.31.46.11/stats/
Username:
admin
To obtain password, run:
| kubectl get secret -n home-weka-io weka-home-grafana-credentials -o jsonpath='{.data.password}' | base64 -d |

------------------------------------------------------------------------
Weka Home Encryption Secret Key
------------------------------------------------------------------------
To obtain secretkey, run:
| kubectl get secret -n home-weka-io weka-home-encryption -o jsonpath='{.data.encryption_secret_key}' | base64 -d |

------------------------------------------------------------------------
Technical information
------------------------------------------------------------------------
Number of event store databases: 1
Easy wekahoming!

```

</details>

### 6. Access the Local WEKA Home portal and Grafana

* **URLs:**
  * **LWH portal:** `https://<your_domain>`
  * **Grafana:** `https://<your_domain>/stats/`
  * **WEKA Home REST API:** `https://<your_domain>/api/`
* **Username:** `admin` (for accessing all portals).
* **Passwords to access the URLs:**
  * **Obtain the LWH portal password:** Run the command:\
| `kubectl get secret -n home-weka-io wekahome-admin-credentials -o jsonpath='{.data.adminPassword}' | base64 -d` |
  * **Obtain the Grafana portal password:** Run the command:\
| `kubectl get secret -n home-weka-io wekahome-grafana-credentials -o jsonpath='{.data.password}' | base64 -d` |
  * **Obtain the WEKA Home secret key:** Run the command:\
| `kubectl get secret -n home-weka-io wekahome-encryption-key -o jsonpath='{.data.encryptionKey}' | base64 -d` |

### 7. Enable the WEKA cluster to send information to the Local WEKA Home

By default, the WEKA cluster is set to send information to the public instance of WEKA Home. To get the information in the Local WEKA Home, connect to the WEKA cluster and run one of the following commands depending on the configuration:

* Standard configuration:

```
weka cloud enable --cloud-url http://<ip or hostname of the Local WEKA Home server>
```

* Secure configuration with valid TLS certificates:

```
weka cloud enable --cloud-url https://<ip or hostname of the Local WEKA Home server>
```

### 8. Test the deployment

The WEKA cluster periodically and on-demand uploads data to the Local WEKA Home according to its information type (see #which-information-is-uploaded-to-weka-home).

1. Access the WEKA Home portal and verify that the test data appears.
2. To trigger a test event, run `weka events trigger-event test` and verify the test event is received in the Local WEKA Home portal under the **Events** section.
3. If required, go to `/var/log/pods` and review the relevant log according to the timestamp (for example, `wekahome-install-03-08-2023_16-29.log`).

#### **IPv6 validation**

1. **Confirm IPv4 and IPv6 networking**:\
   Run the following and confirm both IPv4 and IPv6 addresses are listed in the output.

```shell
kubectl get nodes wekahome.local -o jsonpath='{.status.addresses}'
```

Example output:

```json
[
  {
    "address": "10.0.4.73",
    "type": "InternalIP"
  },
  {
    "address": "2a05:d018:d69:4ef6:65c9:843e:ef26:a5b3",
    "type": "InternalIP"
  },
  {
    "address": "wekahome.local",
    "type": "Hostname"
  }
]
```

2. **Access LWH using IPv4:**\
   Replace `<IPv4_address>` with the actual IPv4 address of the LWH and run the following command:

```shell
curl -4 http://<IPv4_address>/api/v3/auth/login/sso/github
```

Example:

```shell
curl -4 http://10.0.4.73/api/v3/auth/login/sso/github
```

Expected response:

```json
{"enabled":false}
```

3. **Access LWH using IPv6:**\
   Replace `<IPv6_address>` with the actual IPv6 address of the LWH and run the following command:

```shell
curl -6 "http://[<IPv6_address>]/api/v3/auth/login/sso/github"
```

Example:

```shell
curl -6 "http://[2a05:d018:d69:4ef6:65c9:843e:ef26:a5b3]/api/v3/auth/login/sso/github"
```

Expected respons&#x65;**:**

```json
{"enabled":false}
```

#### Troubleshoot IPv6 issues

* IPv6 address not assigned:
  * Verify network interface configuration.
  * Check network connectivity with IPv6 ping.
  * Ensure router advertisements are enabled if using SLAAC.
* Connection failures:
  * Verify firewall rules allow IPv6 traffic.
  * Confirm client machine has global IPv6 connectivity.
  * Check network security group configurations.

## Upgrade the Local WEKA Home

The upgrade process takes up to 5 minutes. It is recommended to perform the upgrade during a maintenance window.

Certain upgrades require a fresh installation, as direct in-place upgrades are not supported in some cases.

* Upgrading from **minikube or WMS** to the **Local WEKA Home 3.0 bundle** (based on K3s) is not supported. To upgrade, install the new Local WEKA Home bundle on a new server and configure API forwarding from the minikube cluster to the new K3s cluster.
* **IPv6 support requires a fresh installation.** It is not possible to upgrade an existing LWH deployment with IPv4 to include IPv6. If IPv6 is needed, install a new LWH instance with dual-stack networking configured during cluster creation.

**Procedure**

1. Download the latest Local WEKA Home bundle to the dedicated physical server (or VM).
2. Run `bash wekahome-*.bundle`
3. To modify the existing configuration, open the `/opt/wekahome/config/config.json` file and modify the settings. See #id-4.-install-and-configure-local-weka-home.
4. Run `homecli local upgrade`. For an upgrade, it takes about 2 minutes.
5. Run `kubectl get pods -n home-weka-io` and verify in the results that all pods have the status **Running** or **Completed**. \
   To wait for the pods' statuses, run `watch kubectl get pods -n home-weka-io`.
6. Verify the Local WEKA Home is upgraded successfully. Run the following command line:\
   `helm status wekahome -n home-weka-io`

## Modify the Local WEKA Home configuration

If there is a change in the TLS certificates, SMTP server in your environment, or any other settings in the Local WEKA Home configuration, you can modify the existing `config.json` with your new settings and apply them.

**Procedure**

1. Open the `/opt/wekahome/config/config.json` file and modify the settings. See #id-4.-install-and-configure-local-weka-home.
2. Run `homecli local upgrade`
3. Run `kubectl get pods -n home-weka-io` and verify in the results that all pods have the status **Running** or **Completed**.\
   To wait for the pods' statuses, run `watch kubectl get pods -n home-weka-io`.
4. Verify the Local WEKA Home is updated successfully. Run the following command line:\
   `helm status wekahome -n home-weka-io`

## Change the Local WEKA Home listening ports

The LWH uses ports 80 for HTTP and 443 for HTTPS by default. You can reconfigure these ports if they are already in use on the management host. Changing the ports allows LWH to run without conflicting with existing applications.

**Procedure**

1.  Run the following command to edit the `traefik` service configuration that manages ingress traffic for LWH:

    ```
    kubectl -n kube-system edit svc traefik
    ```
2. In the configuration file that opens, locate the entries for `port: 80` and `port: 443`.
3. Update the port numbers to the alternative ports you require.
4. Save the file and exit the text editor. For example, in the `vi` editor, type `:wq` and press **Enter**.

The changes take effect immediately.

## Check Local WEKA Home health

After deploying LWH, it is essential to verify its health to ensure all components are functioning correctly. A healthy LWH means that all pods are running without issues, CPU and memory usage are within acceptable limits, and there are no critical low disk space.

If some pods are restarting frequently, producing errors, or failing to start, LWH is considered unhealthy and may require intervention. The following steps guide you in checking the health status of LWH.

#### Procedure

1.  **Check the status of all pods**\
    Run the following command to get an overview of the pod statuses:

    ```sh
    homecli local status pods
    ```

    * If all pods are running, the system is **healthy**.
    * If any pods are faulty, proceed with a detailed check.
2.  **Get detailed pod status**\
    To identify specific faulty pods and their issues, use the following command:

    ```sh
    homecli local status pods --detailed
    ```

    (For more information on the pod statuses and reasons, see the **kubectl** documentation.)\
    Example output for a faulty pod:

    ```
    Faulty pods:

      Pod Name  Status   Reason
      pod-1     Pending  ErrImagePull
    ```
3.  **Check pod status in JSON format (optional)**\
    For automated processing, output can be formatted in JSON:

    ```sh
    homecli local status pods -o json
    ```

    * A response of `{ "healthy": true }` indicates that all pods are running correctly.
    *   If any pod has issues, to get detailed JSON output, use the following command:

        ```sh
        homecli local status pods -o json --detailed
        ```

        Example output showing a faulty pod:

        ```json
        {
          "faultyPods": [
            {
              "name": "pod-1",
              "reason": "ImagePullBackOff",
              "status": "Pending"
            }
          ],
          "healthy": false
        }
        ```
4.  **Verify ingress address response**\
    The ingress address in the `home-weka-io` namespace should return HTTP 200, indicating that the service is reachable:

    ```sh
    homecli local status wekahome
    ```

    * If this check fails, LWH might be experiencing connectivity or service-related issues.

## Troubleshoot the Local WEKA Home deployment

### Symptom: browsing to the Local WEKA Home returns an error

The probable cause can be, for example, a communication problem.

#### Resolution

1. Check the firewall and node IP settings. If you didn't set up a firewall (see #id-2.-prepare-the-management-server), set valid rules and run:

```
k3s-killall.sh
homecli local upgrade
```

2. Retrieve the ingress pod (controller) of the Local WEKA Home.

```
kubectl get pods -n kube-system -o name -l app.kubernetes.io/name=traefik
```

3. Retrieve the logs and look for the error.

```
kubectl logs <pod name from previous command> -n kube-system > traefik.out
```

### Symptom: when executing any command on the Local WEKA Home, the error ‚Äúno space left‚Äù is displayed

The probable cause for this issue is that the containers dir (`/var/lib/rancher/k3s`) consumes disk space.

#### Resolution

Do one of the following:

* Try to clean unused images with `homecli local cleanup images`
* Resize the disk and reinstall the Local WEKA Home.
* Relocate the K3s root directory path to a new path on a larger device (if it exists) and copy the content from the old path to the new path.

### Symptom: when testing the integration, the email is not received

The probable cause can be issues related to the SMTP server, such as wrong credentials or recipient email address.

#### Resolution

1. On the **Integration** page, select **Test Integration**.\
   Wait until an error appears.
2. Retrieve the logs and search for the error. On the Local WEKA Home terminal, run the following command:

```

```bash
for dep in `kubectl get deployment -n home-weka-io -o name`; do echo -----$dep-----; kubectl logs $dep -n home-weka-io --all-containers=true --timestamps=true --since=5m ; done
```

```

## Collect LWH deployment diagnostics

The LWH provides a script that collects various resource details from the LWH deployed on the Kubernetes cluster and generates an archive. This information helps the Customer Success Team and R\&D to analyze and provide support when troubleshooting is needed.

The LWH deployment diagnostics provide the following information:

* Pods status
* Logs of all pods
* K3s settings
* Free disk space
* CPU and memory usage
* Name resolutions
* LWH version
* Syslogs

**Procedure**

1. Run the following command:

```

```bash
homecli local collect-debug-info [archive] [--include-sensitive] [--full-disk-scan] [--verbose]
```

```

2. Once you generate the LWH deployment diagnostics archive file, send it to the [Customer Success Team](../../support/getting-support-for-your-weka-system) for analysis.

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | archive* | The path and output archive file name. For example: /path/diag/lwh_diagnostics.tar.gz |
 | include-sensitive | Include sensitive data in the archive. For example, value overrides.Use this parameter only if required by the Customer Success Team. |
 | full-disk-scan | Perform a higher level of disk scan.Use this parameter only if required by the Customer Success Team. |
 | verbose | Provide a higher verbosity level of the debug information. |

<!-- ============================================ -->
<!-- File 181/259: monitor-the-weka-cluster_the-wekaio-support-cloud_local-weka-home-overview.md -->
<!-- ============================================ -->

---
description:
---

# Local WEKA Home overview

In scenarios where a customer lacks connectivity to the public instance of WEKA Home, such as when the WEKA cluster is deployed in a dark site or VPC, WEKA offers the option to deploy a Local WEKA Home. This is a private instance of WEKA Home, hosted on a management server or VM, designed to meet your specific needs.

## **Key functions of Local WEKA Home**

The Local WEKA Home serves the following key functions:

* **Event management:** It receives events from the WEKA cluster, stores them locally, and allows event querying and filtering.
* **Multi-cluster monitoring:** It allows you to monitor multiple clusters within your organization.
* **Cluster overview:** The Local WEKA Home displays a comprehensive overview of your clusters and allows you to drill down into cluster telemetry data.
* **Alerting:** It triggers specific alerts based on predefined rules and supports integrated delivery methods, including Email (SMTP), SNMP, and PagerDuty.
* **Diagnostics support:** The Local WEKA Home receives support files (diagnostics) from the WEKA cluster, stores them, and makes them accessible for remote viewing by the Customer Success Team.
* **Usage and performance insights:** It receives usage, analytics, and performance statistics from the WEKA cluster, stores, displays, and enables querying and filtering this data.

## **Key features and capabilities**

Local WEKA Home offers the following features and capabilities, categorized as follows:

#### **Monitoring and insight features**

* **Cluster Insights:** Monitor and report on multiple clusters within your organization.
* **Statistics:** Display various cluster-wide statistics and health status information.
* **Event Data:** Access offline event data and associated details.
* **Diagnostics:** Access diagnostics, including event logs, syslog files, trace files, and container information.
* **Usage reports:** Download JSON-formatted Usage Reports and Analytics for analysis and support, including anonymized versions for data security.

#### **Alerting and integrations**

* **Custom rules:** Create custom rules for specific events and alerts and route them to predefined integrations.
* **Custom integrations:** Create destinations where you want alerts and events defined in the Rules page to be sent. These can be Email, PagerDuty, and SNMP Traps.

**Security and compliance controls**

* **Audit information:** View a list of audited activities.
* **Admin privileges:** Apply admin privileges for cluster management and maintenance.
* **User management:** Manage users, groups, and access permissions.

#### **Supportability and data management**

* **Data forwarding:** Forward data from the Local WEKA Home to the cloud WEKA Home for enhanced support and monitoring by the Customer Success Team.
* **REST API:** Use the RESTful API for automation and integration with your workflows and monitoring systems.

## Sign in to Local WEKA Home

Authentication methods:

* Local users
* GitHub SSO

### Local user sign-in flow

1. Initiate sign-in
   * Enter email and password
   * Click **Login**
2. User validation
   * Check email and password in internal LWH database
   * Existing user: Direct sign-in
   * Non-existent user: Return `401` error
3. Access granted
   * Successful authentication
   * Enter application

### GitHub SSO sign-in flow

GitHub SSO provides secure, streamlined authentication by:

* Centralizing user access management
* Simplifying login process
* Enforcing organizational access controls
* Reducing manual user provisioning

Sign-in steps:

1. Initiate sign-in
   * Click **Sign in with GitHub**
   * Redirect to GitHub authentication
2. GitHub authentication
   * Grant access
   * Retrieve public email
   * No public email: Return `400` error
3. User validation
   * Check email in internal LWH database
   * Existing user: Direct sign-in
   * New user:
     * Validate email domain matches `emailDomain`
     * Matching domain: Auto-create account
     * Non-matching domain: Return `401` error
4. Access granted
   * Successful authentication
   * Enter application

**Related topics**

<!-- ============================================ -->
<!-- File 182/259: monitor-the-weka-cluster_the-wekaio-support-cloud_deploy-local-weka-home-v2.x.md -->
<!-- ============================================ -->

---
description:
---

# Deploy Local WEKA Home v2.x

This Local WEKA Home v2.x runs on Minikube (a lightweight Kubernetes implementation) installed on a single Docker container. You specify the configuration parameters in the config.yaml file as part of the deployment workflow.

## Workflow: Local WEKA Home deployment

If you have deployed the WMS, follow the procedure:. Otherwise, perform the following workflow:

1. Verify prerequisites.
2. Prepare the physical server (or VM).
3. Download the Local WEKA Home and Minikube packages.
4. Install the Minikube.
5. Install and configure the Local WEKA Home.
6. Access the Local WEKA Home portal and Grafana.
7. Enable the Local WEKA cluster to send data to the Cloud WEKA Home.
8. Test the deployment.

### 1. Verify prerequisites

Verify that the following requirements are met:

* A dedicated physical server (or VM) for the installation.
* The user account for installing the LWH must have root privileges.
* Server minimum CPU core and RAM requirements:
  * Minimum 8 CPU cores and 20 GiB RAM for up to 1000 total processes.
    * Total processes are equal to the cores used on the cluster **backends** for Management/Frontend/Compute/Drives roles and the cores used on **clients** for Management/Frontend roles.
  *   Sizing for additional processes:

      * The total number of processes determines the number of CPU and RAM required.
      * For every additional 1000 processes or less, add 1 CPU core and 8 GiB RAM.

      Example: 20 backends with 10 processes each = 200 processes; 500 clients with 2 processes each = 1000 processes. The total is 1200 processes. This deployment requires 9 CPU cores and 28 GiB.
* SSD-backed storage requirements:
  * Minimum 500 GiB for locally collected data in `/opt/local-path-provisioner`
  * Minimum 10 GiB for `/tmp/host-path-provisioner`.
* Docker version 20 or higher.
* 1 Gbps network

Note: For using other operating systems, contact the Customer Success Team.

### 2. Prepare the physical server (or VM)

1. Disable the _SELinux_.
2. Disable the _iptables_, _UFW_, or _firewalld_.
3.  Ensure the following ports are open and not used by any other process. Each port is used for the process specified in the brackets.

    `6443` (kube-apiserver)

    `10259` (kube-scheduler)

    `10257` (kube-controller-manager)

    `10250` (kubelet)

    `2379` (etcd)

    `2380` (etcd)

    `80` (Local WEKA Home, WEKA cluster, and web browser)

    `443` (Local WEKA Home, WEKA cluster, and web browser)

Note: If you forward data from the Local WEKA Home to the Cloud WEKA Home, ensure the outbound traffic on port 443 is open.

4. Install the Docker Engine version 20 or higher on the physical server according to the Docker documentation.\
   To install the Docker on RHEL, see Install Docker Engine on Centos (the instructions in _Install Docker Engine on RHEL_ do not work).
5. Run the following to verify that the required docker version is installed:\
   `docker --version.`
6. Run the following to start the docker and enable it:\
   `systemctl start docker && systemctl enable docker`
7. Run the following to set the iptables and pre-load it:\
   `echo net.bridge.bridge-nf-call-iptables=1 >> /etc/sysctl.conf; sysctl -p`
8. Run the following to install the rule tables manager, connection tracking, and multi-purpose relay tool:\
   `yum install -y ebtables conntrack socat`
9.  Run the following to install the Traffic Control tool (`tc`):\
    `yum install -y tc`

    (Depending on the Linux distribution, `tc` may already be installed. Or it is called `iproute-tc`. If it is, run: `yum install -y iproute-tc`.)
10. Verify that the HugePages is disabled (`HugePages_Total: 0`).\
    Run the following command:\
    `grep HugePages_Total /proc/meminfo`\
    If the returned value of the HugePages_Total is higher than 0, run the following to disable the HugePages:\
    `echo 0 > /proc/sys/vm/nr_hugepages`

### 3. Download the Local WEKA Home and Minikube packages

Download the latest packages of the following to the dedicated physical server (or VM):

* **Local WEKA Home v2.x:** https://get.weka.io/ui/lwh/download.
* **Minikube for Local WEKA Home:** `curl -OL https://home-weka-io-offline-packages-dev.s3.eu-west-1.amazonaws.com/weka_minikube.tar.gz`

### 4. Install the Minikube

1. Unpack the Minikube package:\
   `tar xvf <file name>`
2. From the `minikube_offline` directory, run the install script:\
   `./minikube-offline_install.sh`\
   The installation takes about 3 minutes.
3. Verify the minikube is installed successfully:\
   `minikube status`

<details>

<summary>Response example of a successful minikube installation</summary>

```
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
```

</details>

Note: If the minikube installation fails, do one of the following:
* Go to `/var/log/wekahome` and review the relevant log according to the timestamp (for example, `minikube-install-03-08-2023_16-29.log`).
* Run the command `minikube logs`. A log file is created in `/tmp` directory. Open the log file and search for the reason.

### 5. Install and configure the Local WEKA Home

1. Unpack the Local WEKA Home package:\
   `tar xvf <file name>`
2. From the `wekahome_offline` directory, run:\
   `./update_config.sh`
3. Open the `/root/.config/wekahome/config.yaml` file and set the following:

<details>

<summary>Domain</summary>

Set the domain for URL accessing the Local Weka Home portal either by the organization domain FQDN (DNS-based) or IP address (IP-based).

The URL to access the Local Weka Home does not accept aliases of the DNS name. Only the name configured in the `config.yaml` can be used for accessing the Local Weka Home.

DNS-based domain setting:\
In the **domain** section at the top of the file, set the domain FQDN after **@DOMAIN** as shown in the following example:

```
# TOP of file
domain: &DOMAIN "some.domain.com"
```

IP-based domain setting:\
In the **domain** section (at the top of the file) and the **alertdispatcher** section (at the end of the file), set the IP address of the domain as shown in the following example:

```
# TOP of file
domain: &DOMAIN "52.20.26.14"

# End of file
alertdispatcher:
  email_link_domain_name: "52.20.26.14"
```

</details>

<details>

<summary>SMTP</summary>

To enable the Local Weka Home to send emails, set the SMTP details in the **smtp_user_data** section as shown in the following example:

<pre><code>smtp_user_data:
  sender_email_name: "Weka Home"
  sender_email: "weka-home-noreply@your-domain.com"
  smtp_host: "smtp.gmail.com"
  smtp_port: "587"
  smtp_username: "username@your-domain.com"
  smtp_password: "heslbgtrjhzfpdci"
  smtp_insecure_tls: false
<strong>  # false is the default. Change to true if a non-trusted SSL certificate is used
</strong>
```

Ensure to enable the SMTP relay service in your SMTP service.

Once the Local Weka Home is deployed, you can set it to send alerts by email, SNMP, or PagerDuty. See the Set the Local Weka Home to send alerts topic.

</details>

<details>

<summary>TLS certificates</summary>

To enforce HTTPS connection, change the value of `enabled:` to `true`, set the common name (CN, also known as FQDN), certificate data, and private key in the **tls** section (under the **nginx** section) as shown in the following section:

```
nginx:
  tls:
     enabled: true
     # Must set to the CN of the certificate or wildcard
     cn: "server.example.com"
cert: |
     -----BEGIN CERTIFICATE-----
     KJDDLJDLjdkm1718dljkdsljdh92edkjdjdjdkjddjsgsglgLQKSJDKDSKLKSf
        .... Example of a truncated PEM encoded certificate   .....
     DDSHJkadsjkjask7U782CHDF8HD0ihjx8iwciw8wJHDSKDHIO
     -----END CERTIFICATE-----
key: |
     -----BEGIN PRIVATE KEY-----
     MIIBOgIBAAJBAKj34GkxFhD90vcNLYLInFEX6Ppy1tPf9Cnzj4p4WGeKLs1Pt8
          ..... Example of a truncated private key  .....
     n5OiPgoTdSy7bcF9IGpSE8ZgGKzgYQVZeN97YE00
     -----END PRIVATE KEY-----

```

You can generate a self-signed certificate using the following example:\
`openssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days <days> -nodes`

**Related topic**

</details>

<details>

<summary>Events retention period</summary>

The default number of days to keep events in the Local Weka Home is 30 days. To reduce the consumption of disk space, you can specify the max_age in the **events** section (under the **garbage collection** section), as shown in the following example:

```
garbage_collection:
 support_files:
  # max 365 days
  max_age: 365d
 events:
  # max 30 days
  max_age: 30d
```

</details>

<details>

<summary>Listen on all interfaces</summary>

If the Local Weka Home requires listening to all interfaces (for example, 0.0.0.0), in the **global** section, under **ingress**, leave the domain empty as shown in the following example:

```yaml
# Domain name for notifications and alerts
domain: &DOMAIN "local.weka.home"

global:
  ingress:
    # The interface address the LWH listens to. Leave empty to listen to all
    domain: ""
```

</details>

<details>

<summary>Forward data from the Local WEKA Home to the Cloud WEKA Home</summary>

In the **apiforwarding** section, set the forwarding parameters to **true** (except the forwarding_bulk_size), as shown in the following sample. This is the default setting starting in Local WEKA Home v2.10. To disable forwarding metrics to Cloud WEKA Home from Local WEKA Home, set `enabled: false` below.

```
apiforwarding:
  enabled: true
  api_forwarding_url: "https://api.home.weka.io"
  replica_count: 1
  resources:
    requests:
      memory: "100Mi"
      cpu: 10m
    limits:
      memory: "200Mi"
      cpu: 300m
  nodeSelector: { }
  priorityClassName: ""
  forwarding_cluster_registration_enabled: true
  forwarding_events_enabled: true
  forwarding_usage_report_enabled: true
  forwarding_analytics_enabled: true
  forwarding_diagnostics_enabled: true
  forwarding_stats_enabled: true
  forwarding_bulk_size: 50
```

</details>

4. Run `./wekahome-install.sh`.\
   For new installation, it takes about 5 minutes.
5. Run `kubectl get pods -n home-weka-io` and verify in the results that all pods have the status **Running** or **Completed**. \
   To wait for the pods statuses, run `watch kubectl get pods -n home-weka-io`.
6. Verify the Local WEKA Home is installed successfully. Run the following command line:\
   `helm status homewekaio -n home-weka-io`

<details>

<summary>Response example of a successful Local WEKA Home installation</summary>

```
helm status homewekaio -n home-weka-io
NAME: homewekaio
LAST DEPLOYED: Thu Jan  5 09:30:42 2023
NAMESPACE: home-weka-io
STATUS: deployed
REVISION: 3
TEST SUITE: None
NOTES:
Thank you for installing home-weka-io.
Your release is named homewekaio
To learn more about the release, try:

  $ helm status homewekaio -n home-weka-io
  $ helm get all homewekaio -n home-weka-io

------------------------------------------------------------------------
Weka Home Frontend:
------------------------------------------------------------------------
URL:
https://172.31.46.11
Username:
admin
To obtain password, run:
| kubectl get secret -n home-weka-io weka-home-admin-credentials -o jsonpath='{.data.admin_password}' | base64 -d |

------------------------------------------------------------------------
Weka Home REST API:
------------------------------------------------------------------------
URL:
https://172.31.46.11/api/

------------------------------------------------------------------------
Weka Home Statistics (Grafana):
------------------------------------------------------------------------
URL:
https://172.31.46.11/stats/
Username:
admin
To obtain password, run:
| kubectl get secret -n home-weka-io weka-home-grafana-credentials -o jsonpath='{.data.password}' | base64 -d |

------------------------------------------------------------------------
Weka Home Encryption Secret Key
------------------------------------------------------------------------
To obtain secretkey, run:
| kubectl get secret -n home-weka-io weka-home-encryption -o jsonpath='{.data.encryption_secret_key}' | base64 -d |

------------------------------------------------------------------------
Technical information
------------------------------------------------------------------------
Number of event store databases: 1
Easy wekahoming!

```

</details>

### 6. Access the Local WEKA Home portal and Grafana

* The Local WEKA Home URL is `https://<your_domain>`
* The Grafana URL of the Local WEKA Home is `https://<your_domain>/stats/`
* The WEKA Home REST API URL is `https://<your_domain>/api/`
* The user name for accessing the portals is `admin`.
* To obtain the password for accessing the Local WEKA Home portal, run the following command:\
| `kubectl get secret -n home-weka-io weka-home-admin-credentials -o jsonpath='{.data.admin_password}' | base64 -d` |
* To obtain the password for accessing the Local Weka Home grafana portal, run the following command:\
| `kubectl get secret -n home-weka-io weka-home-grafana-credentials -o jsonpath='{.data.password}' | base64 -d` |
* To obtain the secret key of the Local WEKA Home portal, run the following command:\
| `kubectl get secret -n home-weka-io weka-home-encryption -o jsonpath='{.data.encryption_secret_key}' | base64 -d` |

### 7. Enable the Local WEKA cluster to send data to the Cloud WEKA Home

By default, the WEKA cluster is set to send information to the public instance of WEKA Home. To get the information in the Local WEKA Home, set the URL of the Local WEKA Home in the WEKA cluster.

Connect to the WEKA cluster and run the following command:\
`weka cloud enable --cloud-url https://<ip or hostname of the Local Weka Home server>`

### 8. Test the deployment

The WEKA cluster uploads data to the Local WEKA Home periodically and on-demand according to its information type (see #which-information-is-uploaded-to-weka-home).

Access the WEKA Home portal and verify that the test data appears.

To trigger a test event, run `weka events trigger-event test` and verify the test event is received in the Local WEKA Home portal under the **Events** section.

If required, go to `/var/log/wekahome` and review the relevant log according to the timestamp (for example, `wekahome-install-03-08-2023_16-29.log`).

## Set local Certificate Authorities

To support dark site customers and other users with local Certificate Authorities (CAs) for configuring secure connectivity to the Local WEKA Home.

**Procedure**

1.  Create a secret using the same method as described in the nginx documentation. For example, you can use the following command:

    ```
    kubectl create secret generic ca-secret --from-file=ca.crt=ca.crt
    ```

    Optionally, you can specify a name for the secret (in this command, it's named `ca-secret`) and a namespace for it using the `-n` option.
2.  Add an Ingress annotation that references this CA certificate by extending the `global.ingress.annotations` with the following field:

    ```
    nginx.ingress.kubernetes.io/auth-tls-secret: "default/ca-secret"
    ```

    In this annotation, replace `default` with the namespace you've specified (or leave it as "default"), and replace `ca-secret` with the actual name of the secret.
3. After making these changes, proceed to the general upgrade instructions.

## Upgrade the Local WEKA Home

The Local WEKA Home upgrade workflow is similar to the deployment workflow (without reinstalling the Minikube). The upgrade process takes about 5 minutes, and the LWH is unavailable during this time. It is recommended to perform the upgrade during a maintenance window.

**Before you begin**

1. If your initial Local WEKA Home (LWH) installation was done using the WMS, change the directory to `/opt/local-weka-home`. Otherwise, change the directory to the location where LWH was initially installed.

Here's an example of how to change the directory to `/opt/local-weka-home` when the WMS was used:

* Log into the WMS:
  *   As root user:

      ```
      ssh root@my_wms
      # Enter password (default: WekaService)
      ```
  *   Or as the weka user:

      ```
      ssh weka@my_wms
      # Enter password (default: weka.io123)
      ```
*   Switch to root user (if not already):

    ```
    sudo bash
    ```
*   Change to the local-weka-home directory:

    ```
    cd /opt/local-weka-home
    ```

2. Check the version.

```
curl http://localhost/api/v3/status
```

3. If the source LWH version is lower than 2.11.0, run the following commands to prepare the source LWH for upgrade:

```
kubectl delete statefulset -l app.kubernetes.io/name=nats -n home-weka-io
kubectl delete pvc -l app.kubernetes.io/name=nats -n home-weka-io
```

**Procedure**

1. Download the latest Local WEKA Home package (_wekahome-vm-docker-images_). See the location in Download the Local Weka Home and Minikube package&#x73;_._
2. Unpack the Local Weka Home package to the same directory used for installing the LWH. `tar xvf <file name> -C <path>`
3. From the `wekahome_offline` sub-directory, run `./update_config.sh`
4. If you want to modify the existing configuration, open the `/root/.config/wekahome/config.yaml` file and do the following:
   * Modify the settings. See Install and configure Local WEKA Home.
   * If you update the following sections: **TLS certificates**, **admin credentials**, and **Grafana**, add the line `force_update: true` to the end of the updated section in the `config.yaml` file. For example:

<details>

<summary>Update the TLS certificates</summary>

```
nginx:
  tls:
     enabled: true
     # Must set to the CN of the certificate or wildcard
     cn: "server.example.com"
cert: |
     -----BEGIN CERTIFICATE-----
     KJDDLJDLjdkm1718dljkdsljdh92edkjdjdjdkjddjsgsglgLQKSJDKDSKLKSf
        .... Example of a truncated PEM encoded certificate   .....
     DDSHJkadsjkjask7U782CHDF8HD0ihjx8iwciw8wJHDSKDHIO
     -----END CERTIFICATE-----
key: |
     -----BEGIN PRIVATE KEY-----
     MIIBOgIBAAJBAKj34GkxFhD90vcNLYLInFEX6Ppy1tPf9Cnzj4p4WGeKLs1Pt8
          ..... Example of a truncated private key  .....
     n5OiPgoTdSy7bcF9IGpSE8ZgGKzgYQVZeN97YE00
     -----END PRIVATE KEY-----
     force_update: true
```

</details>

5. Run `./wekahome-install.sh`. For an upgrade, it takes about 2 minutes.
6. Run `kubectl get pods -n home-weka-io` and verify in the results that all pods have the status **Running** or **Completed**. \
   To wait for the pods statuses, run `watch kubectl get pods -n home-weka-io`.
7. Verify the Local WEKA Home is upgraded successfully. Run the following command line:\
   `helm status homewekaio -n home-weka-io`
8. If any changes made to the _config.yaml_ required setting `force_update: true`, change it back to `false`.

## Modify the Local WEKA Home configuration

Suppose there is a change in the TLS certificates, SMTP server in your environment, or any other settings in the Local WEKA Home configuration, you can modify the existing `config.yaml` with your new settings and apply them.

**Procedure**

1. Open the `/root/.config/wekahome/config.yaml` file and do the following:
   * Modify the settings. See Install and configure Local WEKA Home.
   * If you update the following sections: **TLS certificates**, **admin credentials**, and **Grafana**, add the line `force_update: true` to the end of the updated section in the `config.yaml` file. For example:

<details>

<summary>Update the TLS certificates</summary>

```
nginx:
  tls:
     enabled: true
     # Must set to the CN of the certificate or wildcard
     cn: "server.example.com"
cert: |
     -----BEGIN CERTIFICATE-----
     KJDDLJDLjdkm1718dljkdsljdh92edkjdjdjdkjddjsgsglgLQKSJDKDSKLKSf
        .... Example of a truncated PEM encoded certificate   .....
     DDSHJkadsjkjask7U782CHDF8HD0ihjx8iwciw8wJHDSKDHIO
     -----END CERTIFICATE-----
key: |
     -----BEGIN PRIVATE KEY-----
     MIIBOgIBAAJBAKj34GkxFhD90vcNLYLInFEX6Ppy1tPf9Cnzj4p4WGeKLs1Pt8
          ..... Example of a truncated private key  .....
     n5OiPgoTdSy7bcF9IGpSE8ZgGKzgYQVZeN97YE00
     -----END PRIVATE KEY-----
     force_update: true
```

</details>

2. Run `./wekahome-install.sh`
3. Run `kubectl get pods -n home-weka-io` and verify in the results that all pods have the status **Running** or **Completed**. \
   To wait for the pods statuses, run `watch kubectl get pods -n home-weka-io`.
4. Verify the Local WEKA Home is updated successfully. Run the following command line:\
   `helm status homewekaio -n home-weka-io`
5. If any changes made to the _config.yaml_ required setting `force_update: true`, change it back to `false`.

## Troubleshoot the Local WEKA Home deployment

### Symptom: browsing to the Local WEKA Home returns an error

The probable cause can be, for example, a communication problem.

#### Resolution

1. Retrieve the ingress pod (controller) of the Local WEKA Home.\
| `kubectl get pods -n ingress-nginx -o name | grep controller` |
2. Retrieve the logs and look for the error.\
   `kubectl logs <pod name from previous command> -n ingress-nginx > nginx.out`

### Symptom: when executing any command on the Local WEKA Home, the error ‚Äúno space left‚Äù is displayed

The probable cause for this issue is that the docker root dir (/var/lib/docker) consumes disk space.

#### Resolution

Do one of the following:

* Resize the disk and reinstall the Local WEKA Home.
* Relocate the docker root directory path to a new path on a larger device (if it exists) and copy the content from the old path to the new path.

### Symptom: when testing the integration, the email is not received

The probable cause can be issues related to the SMTP server, such as wrong credentials or recipient email address.

#### Resolution

1. On the **Integration** page, select **Test Integration**.\
   Wait until an error appears.
2. Retrieve the logs and search for the error. On the Local WEKA Home terminal, run the following command:\
   ``for dep in `kubectl get deployment -n home-weka-io -o name`; do echo -----$dep-----; kubectl logs $dep --all-containers=true --timestamps=true --since=5m ; done``

## Collect LWH deployment diagnostics

The LWH provides a script that collects various resource details from the LWH deployed on the Kubernetes cluster and generates an archive. This information helps the Customer Success Team and R\&D to analyze and provide support when troubleshooting is needed.

The LWH deployment diagnostics provide the following information:

* Pods status
* Logs of all pods
* Minikube settings
* Free disk space
* CPU and memory usage
* Name resolutions
* LWH version
* Syslogs

**Procedure**

1. Obtain the `dump.sh` script from the LWH installation package.
2. Run the following command:

```
dump.sh [archive] [--include-sensitive] [--full-disk-scan] [--verbose]
```

Once you generate the LWH deployment diagnostics archive file, send it to the Customer Success Team for analysis.

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | archive* | The path and output archive file name.For example: /path/diag/lwh_diagnostics.tar.gz |
 | include-sensitive | Include sensitive data in the archive. For example, value overrides.Use this parameter only if required by the Customer Success Team. |
 | full-disk-scan | Perform a higher level of disk scan.Use this parameter only if required by the Customer Success Team. |
 | verbose | Provide a higher verbosity level of the debug information. |

<!-- ============================================ -->
<!-- File 183/259: monitor-the-weka-cluster_the-wekaio-support-cloud_explore-cluster-insights-and-statistics.md -->
<!-- ============================================ -->

# Explore cluster insights

## Clusters

### View clusters list

The Local WEKA Home interface provides comprehensive multi-cluster monitoring across your WEKA environment. The initial dashboard displays all registered WEKA clusters that report telemetry data to your Local WEKA Home instance.

To view the cluster overview, select it from the list, or select the 3-dot icon on the right and select **Open in new tab**.

**Clusters graphs view**

Toggle between List and Graph views using the Graphs View selector. The Graph view presents analytical visualizations of cluster distribution by version, protocol usage, installation location, network link layer (IPv4/IPv6), OBS bucket type, and reporting status (WEKA Home-connected vs. dark site installations).

### Filter clusters list

You can filter and sort the clusters based on a range of criteria, including their last seen status, licensed or unlicensed status, and whether the clusters have silenced alerts.

For more advanced filtering options, select the **Advanced filters** icon.

### Download clusters report

To download a report of all your clusters, select the **Download** icon.

## Overview

The Overview page provides a consolidated dashboard of critical cluster-wide metrics and operational status. This interface displays multiple monitoring panels that facilitate rapid assessment of cluster health, configuration parameters, active alerts, system events, and performance metrics.

#### Navigation

Access specialized view pages by either:

* Selecting the corresponding option in the left navigation menu.
* Clicking directly on panel titles within the Overview dashboard.

#### Panel functionality

Each panel in the Overview dashboard serves as both a summary display and an entry point to more detailed information:

**Alerts panel**

The Alerts panel displays severity-categorized cluster alerts with interactive drill-down capabilities. Click any alert to access complete diagnostic details including severity classification, timestamp, condition description, and prescribed remediation steps.

**Hardware Panel** (example of drill-down functionality)

The Hardware panel provides visibility into physical and logical cluster components through the Backends and Clients tabs, displaying servers, containers, processes, drives, OBS, and link layer with relevant operational metrics. Each component supports drill-down functionality for detailed diagnostics.

Additional panels provide similar drill-down functionality for their respective domains, following the same interaction pattern demonstrated in the Alerts and Hardware panels.

## Events

The Events page displays the offline event data for the cluster and associated detail with each event. You can use filters to refine your search according the event severity, category, process ID, and more.

## Filesystems

The Filesystems page provides comprehensive information about the configured filesystems in the cluster.

You have the flexibility to reorganize the display by selecting any column in the tables, and you can use filters to refine your search. For more in-depth information in JSON format, select a specific row.

## Topology

The Topology page presents an array of cluster configurations categorized into sections: Servers, Containers, Processes, Drives, and Network Devices.

You have the flexibility to reorganize the display by selecting on any column in the tables, and you can use filters to refine your search. For more in-depth information in JSON format, select a specific row.

Select each tab to explore the topology screens.

## Protocols

The Protocols page provides comprehensive information about the configured protocols in the cluster. Each tab corresponds to a specific protocol, including NFS, S3, and SMB.

You can customize the display order of information in the tables by selecting the column title.

Select each tab to explore the protocol screens.

## Settings

The Settings page displays a variety of panels containing information such as traces, container uptimes, process uptimes, overrides, and more.

## Diagnostics

The Diagnostics page displays the content of all diagnostic files uploaded from the cluster. Diagnostics are neatly organized in a file system tree structure for convenient navigation.

To access the file content, select the respective file.

Additionally, you have the option to directly upload a diagnostics tar file. You can effortlessly locate specific information by using the search feature and its additional options.

## Admin

See .

## Analytics

The Analytics page displays the cluster configuration (similar to the Topology view) in JSON format. To locate specific information, use the search feature and its options. If you need to copy the content, click the copy icon located next to the first row.

## Tasks

The Tasks page presents a list of background tasks with various statuses that are currently active in your cluster.

You can further refine your task list and customize the display order of information in the tables by selecting the column title.

To access detailed information about a specific task, select its row to view it in JSON format.

## Snapshot Locators

The Snapshot Locators page displays a list of snapshot locators, which serve as unique references to snapshots stored in an object store. These locators are essential for restoring filesystems, managing snapshots, and recovering data. Each locator enables the system to access and manipulate the corresponding snapshot data.

The page presents the following details in a table format for each locator:

* Locator: The unique identifier of the snapshot in the object store.
* Snapshot: The name or ID of the associated snapshot.
* Filesystem: The filesystem from which the snapshot was created.
* OBS Site: The object store site where the snapshot is stored.
* Event Time: The timestamp of when the snapshot was created or modified.

## Download Usage Report and Analytics

Local Weka Home allows you to download JSON-formatted reports to your local workstation for viewing and forwarding to the Customer Success Team for case resolution. You can also add these reports to Cloud Weka Home for offline monitoring.

If your data includes sensitive values that you want to protect, you can choose to download anonymized reports. In anonymized reports, sensitive values are handled as follows: The cluster name is replaced with `weka-[first part of cluster GUID]` and other sensitive values are removed.

<details>

<summary>Usage Report default anonymized data</summary>

The following properties are affected by anonymization:

* name
* alerts
* status/name
* status/cloud

</details>

<details>

<summary>Analytics default anonymized data</summary>

The following properties are affected by anonymization:

* cluster/name
* For all hosts:
  * host/hosts/ips
  * host/hosts/host_ip
  * host/hosts/hostname
  * host/hosts/instance_region
  * host/hosts/aws_instance_region
* misc/kms/type
* misc/cloud/url
* nfs/interface_groups/name
* nfs/interface_groups/gateway
* nfs/interface_groups/subnet_mask
* s3/domain
* s3/filesystem
* smb/smb_trusted_domains

</details>

Note: The anonymization option is supported with WEKA clusters of version 3.14.2 and above.

<!-- ============================================ -->
<!-- File 184/259: monitor-the-weka-cluster_the-wekaio-support-cloud_explore-performance-statistics-in-grafana.md -->
<!-- ============================================ -->

# Explore performance statistics in Grafana

## Performance Statistics

You can open the Grafana application to view some of the various performance visualizations directly from the Local WEKA Home.

From the **Cluster Configuration** menu, select **Performance Statistics**.

<!-- ============================================ -->
<!-- File 185/259: monitor-the-weka-cluster_the-wekaio-support-cloud_manage-alerts-and-integrations.md -->
<!-- ============================================ -->

# Manage alerts and integrations

After deploying the Local WEKA Home, you can configure it to send specific alerts or events through Email (SMTP), PagerDuty, or SNMP Traps (v1/v2c/v3). For example, you can set it to email alerts to a specific address when the cluster's data protection level falls below the configured threshold.

## Set the Local WEKA Home to send events and alerts

Setting the Local WEKA Home to send events and alerts includes the following procedures:

1. **Create an integration:** Set the destination on the **Integration** page.
2. **Create rules:** On the **Rules** page, select the rule conditions to trigger specific alerts or events and assign the rule to the integration.

### Create an integration

1. Access the Local WEKA Home portal with an admin account and the password (obtained. during the LWH deployment. See #id-5.-access-the-local-weka-home-portal-and-grafana).
2. From the menu, select **Manage** > **Integrations**.
3. On the **Integration** page, select **New**.

3. On the **Create Integration** page, select one of the destinations and set the relevant values as follows:

1) In **Name**, enter a meaningful destination name for the integration.
2) In **Type**, select **PageDuty**.
3) In **Routing Key**, set the routing key of your pager duty.
4) Verify that the integration is enabled (the arrow is green).
5) Select **Save Integration**.

1. In **Name**, enter a meaningful destination name for the integration.
2. In **Type**, select **Email**.
3. In **Destination**, set the destination email address.
4. Verify that the integration is enabled (the arrow is green).
5. Select **Save Integration**.

1. In **Name**, enter a meaningful destination name for the integration.
2. In **Type**, select **SNMP Trap**.
3. In the **Version**, select the required SNMP version to use with your SNMP-based tool.
4. Set the values of the properties required according to the selected version:
   * **v1:** SNMP version 1, which only requires the SNMP server hostname or IP address and a plaintext community.
   * **v2c:** SNMP version 2c, similar to SNMP v1, but adds support for 64-bit counters.
   * **v3_NoAuthNoPriv:** SNMP version 3 with security of a user name and EngineID, but without authentication and privileges.
   * **v3AuthNoPriv:** SNMP version 3 with security of a user name, EngineID, and authentication but without privileges.
   * **v3AuthPriv:** SNMP version 3 with security of a user name, EngineID, authentication, and privileges.
5. Verify that the integration is enabled (the arrow is green).
6. Download the WEKA_HOME-MIB.txt file and apply it in your SNMP system.
7. Select **Save Integration**.

### Create a rule

1. From the menu, select **Manage** > **Rules**.
2. On the **Rules** page, select **New**.

5.  On the **Create Rule** page, do the following:

    1. Enter a meaningful name for the rule.
    2. Select the event or alert type from **Rule Type** and set the entity, operator, and condition, for the selected rule type.
    3. Select **View integrations** and select the required integration (destination) from the list.
    4. Select **Save Rule**.

    A green confirmation message appears for a successful setting.

### Examples

#### Create an event rule that sends all critical events to a predefined email

#### Create an alert rule that sends all tiering connectivity alerts to a predefined email

<!-- ============================================ -->
<!-- File 186/259: monitor-the-weka-cluster_the-wekaio-support-cloud_export-cluster-metrics-to-prometheus.md -->
<!-- ============================================ -->

---
description:
---

# Export cluster metrics to Prometheus

## Overview

Cloud WEKA Home (CWH) allows exporting key cluster metrics directly to Prometheus using a custom application that follows the Prometheus Data Model.

This feature is designed for customers who do not use the [WEKAmon](../external-monitoring) monitoring utility but still need to integrate essential cluster statistics into their Prometheus environments.

Key exported metrics include:

* Throughput metrics
* IO performance metrics
* Cluster resource metrics
* Cluster health and alerts
* Cluster capacity metrics

<details>

<summary>Cluster metrics in Prometheus format</summary>

```

```
Name: weka_throughput_bytes_per_second
Help: Weka throughput in bytes per second
Labels with Values:
type: read, write

Name: weka_iops
Help: Weka cluster IOPS
Labels with Values:
type: read, write, metadata

Name: weka_cluster_drives_count
Help: Number of drives in the cluster
Labels with Values:
status: active, created

Name: weka_cluster_processes_count
Help: Number of processes in the cluster
Labels with Values:
status: active, created
type: compute, drive

Name: weka_cluster_containers_count
Help: Number of containers in the cluster
Labels with Values:
status: active, created
type: compute, drive

Name: weka_cluster_alerts_count
Help: Number of alerts in the cluster
Labels with Values:
status: active

Name: weka_cluster_status
Help: Weka cluster status
Labels with Values:
status: (Dynamic values based on cluster status, e.g., OK, Rebuilding, etc.)

Name: weka_cluster_protection_level
Help: Weka cluster protection level
Labels with Values:
num_failures: (Dynamic values based on the number of failures, e.g., 0, 1, 2, etc.)

Name: weka_cluster_capacity_bytes
Help: Weka cluster capacity in bytes
Labels with Values:
type: total, unprovisioned, unavailable, hotSpare
```

```

</details>

## Export Prometheus metrics from CWH

To export Prometheus metrics from CWH, implement the Prometheus API call within a custom application to retrieve metrics at the required intervals. This procedure demonstrates the operation using the `curl` command to extract the metrics.

**Procedure**

1. **Obtain the CWH cluster API key:**
   * Contact the Customer Success Team to obtain the API key.
2.  **Export metrics in Prometheus format:**

    *   Use the following API call, replacing `<cluster_id>` with your cluster ID and `<cluster_api_key>` with the API key from Step 1:

        ```

        ```bash
        curl --location \
        'https://api.home.weka.io/api/v3/clusters/<cluster_id>/stats/prometheus' \
        --header 'Authorization: token <cluster_api_key>'
        ```

```

    The command returns Prometheus-formatted metrics cluster information.
3. **Import the metrics into Prometheus:**
   1. Configure Prometheus to import the exported metrics.
   2. Use the Prometheus UI to visualize and monitor the metrics.

Note: If accessing the backend, prepend `api.` to the URL for legacy ingress support.

**Related information**

Prometheus documentation

<!-- ============================================ -->
<!-- File 187/259: monitor-the-weka-cluster_the-wekaio-support-cloud_optimize-support-and-data-management.md -->
<!-- ============================================ -->

# Optimize support and data management

## Cloud forwarding

This feature is aimed at customers who want to send events, usage, and analytics data from the Local WEKA Home to the cloud WEKA Home for supportability and monitoring by the Customer Success Team. The forwarded data is not anonymized.

**Related topic**

 [Forward data from the Local WEKA Home to the Cloud WEKA Home](../local-weka-home-deployment#forward-data-from-the-local-weka-home-to-the-cloud-weka-home)

## API Documentation

The Local WEKA Home supports a RESTful API. This is useful when automating the interaction with the Local WEKA Home and integrating it into your workflows or monitoring systems.

To access the API documentation:

1. Select the profile icon, and from the menu that appears, select User Management and create an API token.

2. Select **API Docs** and apply the API token.

A static API documentation is also found in: https://home.weka.io/help/api.

<!-- ============================================ -->
<!-- File 188/259: monitor-the-weka-cluster_the-wekaio-support-cloud_enforce-security-and-compliance.md -->
<!-- ============================================ -->

# Enforce security and compliance

## Cluster administration and audit

The Admin page provides a comprehensive set of administrative and audit features, all conveniently located in one place:

**CONFIGURATION:**

* **Mute Cluster:** Temporarily disable alert and event notifications from the cluster to Local WEKA Home. This feature is useful for reducing alert noise during maintenance activities.
* **Delete Cluster:** Remove a legacy WEKA cluster from the Local WEKA Home without affecting the cluster itself.
* **Maintenance Window:** Define a specific time period during which alert and event notifications‚Äîsuch as email, SNMP, and PagerDuty alerts‚Äîare suppressed.
* **Freeze Cluster Retention:** Temporarily halt the data retention policy for a specified period.

**AUDIT:**

* **Cluster Audit:** Monitor and maintain a detailed record of audited activities, including actions such as muting or unmuting clusters and modifying maintenance windows. Access a comprehensive history to track when these actions were performed.

**CLUSTER API KEYS:**

* **Cluster API Keys**: Enable the creation of cluster-specific API keys to facilitate the use of the statistics export API for a designated cluster.

## Users management

The Users page presents the current list of users who have login access to Local WEKA Home and provides the option to add new users. Authentication for access is done using a local username and password.

To open the Users page, from the menu, select **Manage > Users**.

### Modify a user password

Only the administrator can modify the passwords for users.

**Procedure**

1. From the menu, select **Manage > Users**.
2. Select the user to modify.
3. Select **Edit User** and modify the password.

### Delete a user

Only the administrator can delete the users.

**Procedure**

1. From the menu, select **Manage > Users**.
2. Select the user to delete.
3. Select **Delete User**.

## Groups management

The Group Management page offers a comprehensive view of all groups, their respective members, and the scopes (roles) associated with each group's access or visibility within the Local WEKA Home.

A user can belong to multiple groups, and in such cases, the highest level of privileges from all groups is granted.

To open the Groups page, from the menu, select **Manage > Groups**.

### Create a new group

You can create new groups as required and customize role-based access control (RBAC) scoping for each group.

<!-- ============================================ -->
<!-- File 189/259: security.md -->
<!-- ============================================ -->

# Security

## Topics in this section

### WEKA security overview

WEKA implements a comprehensive security framework with multiple controls to ensure secure communication and protect user data in the WEKA cluster.

### Obtain authentication tokens

### Manage token expiration

Learn how to configure and manage token expiration settings to maintain a secure authentication environment aligned with best practices.

### Manage account lockout threshold policy

### Manage KMS

Efficiently manage and safeguard WEKA system keys through strategic KMS configurations and best practices. Optimize security and operational resilience.

### Manage TLS certificates

Manage TLS certificates to encrypt data in transit and verify system identity, ensuring secure communication in the WEKA cluster.

### Manage Cross-Origin Resource Sharing

### Manage CIDR-based security policies

Manage CIDR-based security policies to control access to WEKA clusters based on client IP address ranges, enhancing security and simplifying administration.

### Manage login banner

This page describes how to set a login banner displayed on the sign-in page.

<!-- ============================================ -->
<!-- File 190/259: security_security.md -->
<!-- ============================================ -->

---
description:
---

# WEKA security overview

## Introduction

As data security becomes increasingly critical, WEKA provides a comprehensive security framework to ensure the confidentiality, integrity, and availability of your data.

Learn about the supported security features and recommended configurations to protect sensitive data, comply with industry regulations, and reduce the risk of unauthorized access. Implementing these measures makes secure deployment and operation both straightforward and effective.

***

## Authentication

Authentication is the process of verifying the identity of a user, system, or device before granting access to resources. It ensures that the entity attempting to gain access is who they claim to be. Authentication is a critical component and is often paired with authorization, which determines what authenticated users are allowed to do.

### **User authentication**

#### Kerberos

NFS with Kerberos enhances security by allowing mounts to clients with valid Kerberos tickets. It supports three Kerberos modes:

* **`krb5`**: Authenticates users through Kerberos V5 instead of local UNIX UIDs and GIDs.
* **`krb5i`**: Adds integrity checking using secure checksums to prevent data tampering.
* **`krb5p`**: Encrypts NFS traffic for maximum security, though with higher performance overhead.

Encryption support varies by NFS version. NFSv3 has no native encryption, while NFSv4 (with Kerberos configured) can encrypt all RPC communication, including metadata and payload.

To enable these modes, list `krb5`, `krb5i`, or `krb5p` in the `--enable-auth-types` parameter within the global NFS configuration or when creating an export.

**Related topic**

#nfs-integration-with-kerberos-service

#### Active Directory

WEKA integrates with Active Directory to assign roles to users based on their Active Directory group memberships. These roles determine whether users can manage the cluster, mount filesystems, or more.

To join a WEKA cluster to Active Directory, use the `weka user ldap setup-ad` command.

Active Directory also serves as an identity source for SMB users. Use the `weka smb cluster create` command during SMB setup to set the Active Directory domain as an identity source for SMB users.

**Related topic**

#join-the-smb-cluster-in-the-active-directory

#### LDAP

LDAP functions similarly to Active Directory but offers additional features, such as retrieving user and group IDs from extended attributes.

To connect the WEKA cluster to LDAP, use the `weka user ldap setup` command.

**Related topic**

#### Local

The WEKA system manages user access and roles locally or through directories like LDAP or AD. Local users can have roles just like LDAP or AD users, but you assign them through direct mapping.

To manage local users, use the `weka user add/update/delete` commands.

**Related topic**

#authentication-and-login-process

#### Security token service (STS)

The WEKA Security Token Service (STS) provides temporary credentials for S3 API access. Using temporary credentials enhances security by limiting the exposure of long-term access keys and enforcing the principle of least privilege for specific tasks.

To retrieve temporary credentials, run the `weka s3 sts assume-role` command. The API response provides an access key, a secret key, and a session token. Access is granted based on the permissions defined in the IAM policy attached to the user's role. You can also specify a more restrictive IAM policy when requesting the credentials to further limit the token's permissions.

Before you can assume a role, perform the following actions:

* Create a local S3 user.
* Assign the S3 role to the user.
* Attach an IAM policy to the user's role.

**Related topic**

#generate-a-temporary-security-token

#### S3 service accounts

An S3 service account provides persistent, long-term credentials for programmatic access to manage object store buckets and S3 APIs. Unlike the temporary credentials from STS, service account credentials do not expire, making them suitable for trusted applications and automated services.

Service accounts operate as child identities of a parent S3 user. They inherit their base privileges from the IAM policies attached to the parent user. You can also apply an additional, more restrictive IAM policy directly to the service account to limit its access to specific actions and resources.

To create a service account, run the `weka s3 service-account add` command.

**Related topic**

### **Cluster entity authentication**

Securing the cluster infrastructure involves authenticating the cluster entities, such as servers, containers, processes, and clients. This process ensures that only authorized entities can join the cluster, which is critical for maintaining data integrity and preventing security breaches.

Cluster entity authentication prevents servers in multi-cluster environments from mistakenly joining a neighboring cluster and blocks malicious resources from infiltrating your system. The primary mechanism for this is the join secret.

#### Join secret

The join secret ensures that backends can only join a cluster if they share the same secret key. This feature mitigates two risks:

* In multi-cluster environments, backends might mistakenly rejoin a neighboring cluster instead of their original one.
* During a security breach, a malicious resource could infiltrate a cluster and access sensitive information.

Generate a join token by running `weka cluster join-token generate` on a backend server.

To add the join secret to a backend container, run `weka local resources join-secret <secret>`.

**Related topic**

***

## Access control

Access control models define how users and systems interact with resources in the WEKA cluster. The WEKA platform uses two fundamental models to provide robust and flexible security:

* Discretionary Access Control (DAC)
* Mandatory Access Control (MAC)

### Discretionary access control (DAC)

In the DAC model, an owner of an object (like a file or directory) can grant or revoke permissions for other users at their discretion.

#### Access control lists (ACLs)

Access Control List (ACL) is a list of entries that specifies which users or groups have permission to access an object, such as a file or a directory. By managing ACLs, users can enforce granular control over who can perform what actions on their resources, ensuring data confidentiality and integrity.

The WEKA platform supports ACLs across SMB, POSIX, and NFS, offering protocol-specific permission models and a hybrid flavor for versatile multi-protocol access.

**SMB ACL flavors**

WEKA SMB supports three ACL flavors to manage permissions in different environments.

* **Windows:** When you set an ACL through an SMB client, the system stores it exclusively in Windows format within an extended attribute metadata file (`xattr`). Consequently:
  * SMB clients see the Windows ACL.
  * POSIX and NFS clients do not see any ACLs on the object.
* **POSIX:** When you select POSIX as the preferred ACL flavor, the system stores ACLs set through SMB in POSIX format in the POSIX metadata. This makes the ACL visible for all clients (SMB, POSIX, and NFS), ensuring consistent permissions.
*   **Hybrid:** When you set an ACL through an SMB client, the system stores it in both the `xattr` (in Windows format) and the POSIX metadata (translated to the closest POSIX format). As a result:

    * SMB clients see the original Windows ACL from the `xattr`.
    * POSIX and NFS clients see the translated POSIX ACL.

    However, in Hybrid mode, a direct update to the POSIX ACL invalidates the corresponding ACL in the `xattr`. This, in turn, invalidates the ACL of the opposing protocol. For example, an ACL update from an SMB client overrides the POSIX ACL and invalidates a previously set NFSv4 ACL.

**NFS ACL flavors**

The WEKA NFS service provides a similar model with four distinct flavors to manage permissions.

* **NFSv4:** When you set an ACL through an NFSv4 client, the system stores it in the `xattr`. This enforces native NFSv4 ACLs, which offer fine-grained control. However, this limits interoperability, as SMB and POSIX clients will not see this ACL.
* **POSIX:** When you set an ACL through an NFS or POSIX client, the system stores it in the POSIX metadata, ensuring compatibility across different protocols. All clients (NFS, POSIX, and SMB) will see the same POSIX ACL.
*   **Hybrid:** When you set an ACL through an NFSv4 client, the system combines both POSIX and NFSv4 ACLs to ensure consistency. As a result:

    * NFSv4 clients see the native NFSv4 ACL.
    * POSIX and SMB clients see a translated POSIX ACL.

    However, similar to the SMB Hybrid flavor, any direct update to the POSIX ACL invalidates the corresponding ACL stored in the `xattr`. This action invalidates the ACL of the opposing protocol. For instance, an ACL update from an NFS client overrides the POSIX ACL and invalidates a previously set Windows ACL.
* **None:** This flavor disables all ACL enforcement and updates for the share, even if POSIX ACLs exist on an object.

**ACL best practices**

To ensure predictable and stable permissions in a multi-protocol environment, follow these best practices:

* **Avoid using Hybrid mode on both protocols simultaneously:** When you set both SMB and NFS to the Hybrid flavor, they continuously override each other's POSIX ACL updates. This conflict leads to unpredictable permissions and is not a recommended configuration.
* **Use the POSIX flavor for consistent multi-protocol access:** If both NFS and SMB access are required, using the POSIX flavor for both is the recommended approach. This method standardizes permission granularity and provides the most predictable permission translation between protocols.
* **Use native flavors for isolated permissions:** If you require simultaneous protocol access but do not need permissions to be consistent, you can use native security flavors (Windows for SMB and NFSv4 for NFS). This configuration stores each protocol's ACLs in the extended attribute metadata file (`xattr`) without translation, which can result in different access permissions depending on the protocol you use.
* **Prioritize the dominant protocol:** If one access protocol is used significantly more than others, set its preferred security flavor and configure the minor protocol to use either POSIX or None, depending on your specific use case.

***

### Mandatory access control (MAC)

The MAC model enforces access policies from a central authority. Users cannot change the access rights, as the system strictly enforces security labels defined by an administrator.

#### Roles

The WEKA system uses roles to manage user access permissions. You can assign roles to local users or map them to users from a directory service like LDAP or AD. Each user account has a unique display name and is authenticated with a secret. The system supports up to 1,152 local users.

When you create a WEKA cluster, the system generates a default Cluster Admin user (`admin`) with a default password. It is critical to reset this password upon first login, as the `admin` user has full administrative privileges across the cluster.

**Password policy**

User passwords must meet the following requirements:

* Minimum of 8 characters
* At least one uppercase letter
* At least one lowercase letter
* At least one number or special character

#### Authentication and authorization

During an authentication attempt, the system first searches for the user in the local accounts. If the user is not found locally, the system then checks the configured directory service. WEKA supports a single directory service (either AD or LDAP) per organization.

The system provides automatic user mapping from LDAP or AD group memberships to WEKA roles. If a user is a member of more than one mapped group, they receive a union of all associated roles, granting them aggregated permissions.

**Special roles**

In addition to standard administrative and user roles, WEKA provides special roles for specific services. You can assign these roles only to local users.

* **CSI:** The CSI role facilitates Kubernetes interaction with the WEKA cluster through the WEKA CSI Plugin. It grants permissions limited to filesystem provisioning and management through the CLI and API.
*   **S3:** The S3 user role provides access to the WEKA system exclusively through the S3 protocol.

    This role allows a user to execute S3 commands and API calls. All actions are governed by the permissions defined in the user's attached S3 IAM policy.

    A user with the S3 role can also create S3 service accounts and assign specific IAM policies to them. The role is strictly limited to S3 protocol operations and does not grant any management permissions on the WEKA cluster.

#### Account lockout

The WEKA platform provides an account lockout policy to prevent brute-force login attacks. The system temporarily locks a user's account after multiple consecutive incorrect login attempts.

The default policy settings are:

* Failed attempts threshold: 5
* Lockout duration: 2 minutes

This feature discourages automated password guessing while minimizing disruption to legitimate users.

To manage the account lockout policy use the `weka security lockout-config set` command.

**Related topic**

#### CIDR-based policies

CIDR-based policies allow administrators to regulate access to WEKA cluster management and POSIX data services by creating rules that permit or block connections from specific client IP ranges. These policies strengthen security by limiting network access without requiring user authentication. You manage them centrally at the organization and filesystem levels to ensure only authorized clients can connect.

Policy management and considerations:

* **Administrator control:** Only administrators can manage CIDR-based security policies. Cluster Admins manage policies in the root organization, while Organization Admins manage them in their respective non-root organizations.
* **Live sessions:** Policy updates do not affect active connections. A mounted client session remains active until it is disconnected.
* **Rule order:** The order in which you attach policies determines the filtering sequence.
* **Implementation models:** The WEKA platform supports both whitelist and blacklist approaches.
  * To implement an allowlist, place an **any-deny** rule at the end of the policy list and add specific **allow** rules above it.
  * To implement a denylist, add only the specific **deny** rules for the clients you wish to block.

**Related topic**

**S3 IAM policies**

An S3 IAM policy is a JSON-based document that defines the specific actions a user, group, or role can perform on WEKA S3 resources, such as reading or writing objects in buckets. These policies are crucial for securely managing access and enforcing the principle of least privilege.

After a Cluster Admin creates an S3 user, they must attach an IAM policy to grant the necessary permissions. Without an attached policy, the user cannot execute any S3 commands or API calls.

The Cluster Admin can attach either a pre-defined WEKA policy or a custom-created policy. You can use a tool like the AWS Policy Generator to create custom policies in the required JSON format.

**Related topic**

**Bucket policies**

A bucket policy is a JSON document that you attach directly to an S3 bucket. It defines what actions are allowed or denied on that specific bucket and its objects for various principals, regardless of the user, group, or role making the request.

**Related topic**

Note: **S3 IAM and bucket policies notes:**
* When a request is made to an S3 resource, WEKA evaluates both the bucket policy and the relevant IAM policies. If either policy explicitly denies access, the request is denied, even if the other policy grants permission.
* As a security best practice, grant permissions according to the principle of least privilege, ensuring users have only the access they require.

#### SELinux

The WEKA system supports the use of SELinux (Security-Enhanced Linux) on its clients. SELinux is a security architecture in the Linux kernel that enforces mandatory access control (MAC) policies defined by a system administrator.

You can enable SELinux on a client server using one of the following methods:

* **For the current session**: To enable SELinux in `Enforcing` mode temporarily, run the following command on the client: `sudo setenforce Enforcing`
*   **For persistent configuration:** To enable SELinux persistently across system reboots, edit the

    `/etc/selinux/config` file. A system reboot is required for this change to take effect.

#### Login banner

The login banner displays a customizable legal or security message on the GUI sign-in page. It serves to warn against unauthorized access and to inform authorized users of their responsibilities and acceptable use policies.

To configure the login banner, use the `weka security login-banner set` command.

**Related topic**

***

## Encryption

The WEKA platform uses encryption to protect data confidentiality by scrambling readable data (plaintext) into an unreadable format (ciphertext). This fundamental security technique prevents unauthorized parties from viewing or tampering with your data.

WEKA applies encryption in two primary contexts:

* **Encryption at rest:** Secures data stored on the physical media within the cluster.
* **Encryption in motion:** Secures data as it travels across the network between clients and the cluster.

### **Encryption at rest**

Encryption at rest secures data that is stored on physical media, such as the SSDs within the WEKA cluster. This practice ensures that data remains unreadable and protected even if the physical storage devices are stolen or accessed by unauthorized individuals. WEKA encrypts all data before it is written to the storage tier and leverages an external Key Management Service (KMS) for secure and robust management of the encryption keys.

#### **Key management service (KMS)**

A Key Management Service (KMS) provides a centralized system to securely manage the entire lifecycle of cryptographic keys, including their generation, storage, distribution, rotation, and destruction. It offers administrators a secure and simplified method for controlling access to the essential keys used for data encryption and decryption.

In the WEKA system, the KMS plays a crucial role during startup by encrypting and decrypting filesystem keys. For ongoing data operations, WEKA relies on efficient in-memory functions to maintain high performance for encryption and decryption. To enhance security, the WEKA system never stores any information that could potentially reconstruct the master encryption keys managed by the KMS.

Using a KMS is vital for security when you use the Snap-to-object feature. Without KMS integration, the system protects an encrypted filesystem snapshot with a generic key, meaning the snapshot can potentially be restored on any WEKA cluster. The generic key uses the XTS-AES 256 library.

**Related topic**

### **Encryption in motion**

Encryption in motion protects data as it travels across networks, such as between clients and the WEKA cluster. This process is essential for preventing unauthorized interception, eavesdropping, or tampering with data during transmission. The WEKA platform provides distinct encryption methods for its various data protocols to ensure the confidentiality and integrity of your data in transit.

**POSIX**

The WEKA system provides transparent encryption for data accessed by POSIX clients using the XTS-AES-256 algorithm. An automatic layer encrypts data when it is written to disk and decrypts it when it is read, making the process invisible to POSIX programs.

The key features of this encryption method include:

* **512-bit key:** The system uses a 512-bit key that is split into two parts. One part performs the encryption, while the other ensures that each block of data is unique.
* **Block-level encryption:** Each 512-byte (or larger) block is encrypted separately. This secures the data even if an application accesses parts of the disk out of order.

#### **NFS**

The WEKA system supports both NFSv3 and NFSv4. NFSv3 does not have a native data encryption mechanism. In contrast, NFSv4 supports encryption with Kerberos when you configure it to use the `krb5p` security mode. WEKA supports NFSv4 only with the NFS-W service.

#### **SMB**

You can encrypt SMB share access to secure data in motion. When a supported client sends an encrypted request, the WEKA SMB-W service replies with an encrypted message. You can also configure the cluster to always encrypt outgoing SMB traffic, regardless of the client‚Äôs settings.

By default, WEKA SMB-W follows the client‚Äôs message signing preference; the server signs its response only if the client signs its request. The message signing algorithm depends on the SMB version:

* SMB2: Uses HMAC-SHA256
* SMB3: Uses AES-CMAC

#### **S3**

The WEKA S3 service uses Transport Layer Security (TLS) to secure data in motion, ensuring confidentiality, integrity, and authentication between clients and the S3 service. It supports both TLS 1.2 and 1.3 and leverages modern cipher suites, including AES-256-GCM and ChaCha20-Poly1305. The implementation, built on OpenSSL 3.3, provides enhanced performance for HTTPS connections and adheres to strong cryptographic standards.

<!-- ============================================ -->
<!-- File 191/259: secure-cluster-membership-with-join-secret-authentication.md -->
<!-- ============================================ -->

# Secure cluster membership with join secret authentication

To enhance security in multi-cluster environments, WEKA supports join secret authentication in on-premises deployments, which ensures that only authorized backends with the correct secret can join a cluster. This mechanism prevents accidental cross-cluster joins and unauthorized access, maintaining a secure and isolated cluster environment.

Note: Join secret authentication is not supported on cloud deployments.

## Enable join secret authentication

Join secret authentication is enabled during cluster creation by specifying a secret:

```
weka cluster add --join-secret <join-secret>
```

The `<join-secret>` acts as a shared credential required for all backend containers that need to join the cluster.

## Set join secret for existing clusters

If the cluster was initially created without a join secret, you can assign it post-deployment to each container using:

```
weka cluster container join-secret <container-id> <secret>
```

Replace `<container-id>` with the container‚Äôs identifier and with the desired join secret value.

## Add resources to a cluster with join secret

When expanding a cluster with join secret authentication enabled, the following commands must include the correct secret:

* Set up a new container:

```
weka local setup container --join-secret <join-secret>
```

* Join using local resources:

```
weka local resources join-secret <secret>
```

**Related topic**

<!-- ============================================ -->
<!-- File 192/259: security_obtain-authentication-tokens.md -->
<!-- ============================================ -->

# Obtain authentication tokens

There are two types of authentication tokens: an access token and a refresh token.

* **Access token:** A short-lived token (valid for five minutes) used to access the WEKA system API and enable secure filesystem mounting.
* **Refresh token:** A long-lived token (one month by default, but customizable) used to obtain new access tokens as needed.

**Procedure**

Do one of the following:

*   **Using the CLI**: To obtain the refresh token and access token through the **CLI**, log in to the system using the command: `weka user login`. For details, see #log-in-to-the-weka-cluster.

    The system creates an authentication token file and saves it in `~/.weka/auth-token.json`. The token file contains both the access token and the refresh token.

* **Using the REST API**: To obtain the refresh token and access token through the **REST API,** use the `POST /login`. The API returns the token in the response body.

## Manage long-lived tokens for REST API usage

When working with the REST API, local users may use a long-lived token (a token that doesn't require a refresh every 5 minutes).

As a local user, you can generate a long-lived token using the GUI or the CLI.

### Generate a long-live access token using the GUI

**Procedure**

1. From the signed-in user menu, select **API Token**.
2. In the Manage API Token dialog, select Generate token and set the expiration time. Then, select **Generate**.

Note: If you want to revoke all existing login tokens of the local user and refresh them, select **Revoke Tokens**.

3\. Copy the generated token and paste it to the REST API authorization dialog.

The following demonstrates how to generate the API token and authorize it in the REST API.

### Generate a long-lived access token using the CLI

**Command:** `weka user generate-token [--access-token-timeout timeout]`

The default timeout is 30 days.

To revoke the access and refresh tokens, use the CLI command: `weka user revoke-tokens`.

<!-- ============================================ -->
<!-- File 193/259: security_manage-token-expiration.md -->
<!-- ============================================ -->

---
description:
---

# Manage token expiration

Token expiration ensures authentication credentials remain valid for a limited time, reducing risks like unauthorized access and token misuse. Use `weka security token-expiry` commands to configure token lifetimes and maintain a secure, policy-aligned authentication environment.

### View existing token expiration settings <a href="#view-existing-token-expiration-settings" id="view-existing-token-expiration-settings"></a>

**Command:** `weka security token-expiry show`

This command displays the default and maximum expiration times for access and refresh tokens.

### Set token expiration <a href="#set-token-expiration" id="set-token-expiration"></a>

**Command:** `weka security token-expiry set`

This command allows you to define the default and maximum expiration times for both access and refresh tokens.

```

```
weka security token-expiry set [--access-token access-token] [--refresh-token refresh-token] [--access-token-max access-token-max] [--refresh-token-max refresh-token-max]
```

```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | access-token | Default lifetime of an access token.Possible values: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited |
 | refresh-token | Default lifetime of a refresh token.Possible values: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited |
 | access-token-max | Maximum allowable lifetime for an access token.Possible values: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited |
 | refresh-token-max | Maximum allowable lifetime for a refresh token.Possible values: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited |

**Examples:**

* To set the default access token lifetime to 5 minutes and refresh token lifetime to 2 weeks:

```
weka security token-expiry set --access-token 5m --refresh-token 2w
```

* To enforce stricter maximum values for token lifetimes:

```
weka security token-expiry set --access-token-max 5m --refresh-token-max 2w
```

### Recommendations for token expiration <a href="#recommendations-for-token-expiration" id="recommendations-for-token-expiration"></a>

#### **Access tokens**

* **Default lifetime**: Set to 5 minutes.
* **Maximum lifetime**: Enforce a maximum of 5 minutes.
* **Reason**: Shorter lifetimes reduce exposure to risks from stale tokens and ensure permissions are frequently reevaluated.

#### **Refresh tokens**

* **Default Lifetime**: Set to 2 weeks.
* **Maximum Lifetime**: Enforce a maximum of 2 weeks.
* **Reason**: This balance minimizes reauthentication burdens while ensuring periodic user validation.

<!-- ============================================ -->
<!-- File 194/259: security_account-lockout-threshold-policy-management.md -->
<!-- ============================================ -->

# Manage account lockout threshold policy

To prevent brute force attacks, if several sign-in attempts fail (default: 5), the user account is locked for several minutes (default: 2 minutes).

You can control these default values using the GUI or the CLI.

## Manage account lockout threshold policy using GUI

Using the GUI, you can set the number of failed attempts until the account is locked and the lockout duration. You can also reset the account lockout threshold policy properties to the default values.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.
3. In the Account Lockout Threshold Policy section, select **Set Account Lockout Policy**.
4. In the Set Lockout Policy dialog, do the following:
   * **Failed Attempts Until Lockout:** Set the number of sign-in attempts to lockout between 2 to 50.
   * **Lockout Duration:** Set the lockout duration between 30 seconds to 60 minutes.
5. Select **Save**.

6. To reset the account lockout threshold policy properties to the default values, select **Reset account lockout policy**. In the confirmation message, select **Yes**.

## Manage account lockout threshold policy using CLI

To control the default values, use the following CLI commands:

| `weka security lockout-config set | show | reset` |

**Commands options:**

`set`: Sets the number of failed attempts until the account is locked (`--failed-attempts`) and the lockout duration (`--lockout-duration`).

`reset`:  Resets the number of failed attempts until the account is locked and the lockout duration to their default values.

`show`: Shows the number of failed attempts until the account is locked and the lockout duration.

<!-- ============================================ -->
<!-- File 195/259: security_manage-the-login-banner.md -->
<!-- ============================================ -->

---
description: This page describes how to set a login banner displayed on the sign-in page.
---

# Manage login banner

The login banner provides a security statement or a legal message displayed on the sign-in page displayed on the GUI. The statement can be a definitive warning to any possible intruders that may want to access your system that certain types of activity are illegal, but at the same time, it also advises the authorized and legitimate users of their obligations relating to acceptable use of the system.

## Manage login banner using GUI

You can set a login banner containing a security statement or a legal message displayed on the sign-in page. You can also disable, edit, or reset the login banner.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the Cluster Settings pane, select **Security**.
3. On the Security page, select **Login Banner**.

4. Select **Edit Banner**.

5. In the Edit Login Banner, write your organization statement in the banner text box.
6. Select **Save**.
7. To prevent displaying the login banner, select **Disable Login Banner**.
8. To clear the banner text, select **Clear Login Banner Message.**

## Manage login banner using CLI

To manage the login banner, use the following CLI commands:

| `weka security login-banner set | show | reset | enable | disable` |

**Command options:**

`set:` Sets the login banner text.

`show`: Shows the login banner text.

`reset`: Clears the login banner text.

`enable`: Displays the login banner when accessing the cluster.

`disable:` Prevents displaying the login banner when accessing the cluster.

<!-- ============================================ -->
<!-- File 196/259: security_manage-cidr-based-security-policies.md -->
<!-- ============================================ -->

---
description:
---

# Manage CIDR-based security policies

## Overview

CIDR-based policies allow administrators to control access to WEKA cluster management and filesystems over POSIX clients by specifying permitted and restricted IP address ranges. This network-level security measure complements traditional user authentication, providing organizations with finer control over cluster access.

**Key benefits:**

* **Enhanced security**: Restrict access to the cluster by controlling which clients can connect based on their IP addresses.
* **No authentication required**: Secure access through network-level restrictions, simplifying management for trusted environments.
* **Simplified management**: Centralized control over client access without needing user credentials.

## Guidelines and considerations

When implementing CIDR-based security policies in WEKA, consider the following:

* **Role requirement:** Only users with the **ClusterAdmin** role can manage security policies for the root organization. For non-root organizations, only the **OrgAdmin** can manage security policies.
* **Active mounts remain unaffected**: Client revocation is disabled, meaning any changes to policies do not impact active mounts. This ensures ongoing connections remain stable until they are manually disconnected.
* **Policy order matters**: The order in which policies are attached determines the filtering sequence. For example, if the first policy denies access from IP1 and IP2, and the second policy allows IP1, the first policy takes precedence, overriding subsequent policies. Always review the order to ensure the desired access control.
* **Default access behavior**: Clients without a related policy are allowed by default. To secure your organization or filesystem, always include a final policy that denies access to all other IPs after attaching the necessary policies.
* **Policy capacity:**
  * 16 policies can be assigned per organization.
  * 16 policies can be assigned per filesystem.
  * 8 policies are allowed per client or backend join.
  * Each policy supports up to 32 IP address ranges.
  * A total of 5,120 policies can be defined system-wide.

## Manage security policies using the CLI

Add and manage security policies so that you can apply them on the organization or filesystem. You can perform the following:

* List security policies defined in the WEKA cluster.
* Display information about a specific security policy.
* Add a new security policy.
* Remove a security policy.
* Duplicate an existing security policy, creating a new one.
* Update the settings of an existing security policy.
* Simulate the effect of one or more security policies.
* List security policies applied when joining containers.
* Set security policies for joining cluster, replacing the existing set of policies.
* Attach a security policy when joining cluster.
* Detach a security policy when joining cluster.
* Remove all security policies applied when joining cluster

### List security policies

**Command:** `weka security policy list`

Use the following command line to list security policies defined in the WEKA cluster.

```sh
weka security policy list [--action action] [--roles roles]...[--ips ips]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | action | Lists security policies that match a specific action. (format: allow or deny) |
 | roles... | Lists security policies that include specific roles. (format: clusteradmin, orgadmin, regular, readonly or s3, may be repeated or comma-separated) |
 | ips... | Lists security policies that include specific IP address ranges. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |

### Display information of a security policy

**Command:** `weka security policy show`

Displays information about a specific security policy.

```sh
weka security policy show <policy>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policy* | Name or ID of security policy. |

### Add a new security policy

**Command:** `weka security policy add`

Use the following command line to add a new security policy.

```

```sh
weka security policy add <name> [--description description] [--action action]
[--ips ips]...[--roles roles]...
```

```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | name* | Name of the new security policy. (up to 64 alphanumeric characters, hyphens (-), underscores (_), and periods (.), starting with a letter) |
 | description | Description of the security policy. (up to 256 characters) |
 | action | Whether access is granted or denied when the security policy matches. (format: allow or 'deny) |
 | ips... | IP address ranges to which the security policy applies. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | roles... | User roles to which the security policy applies. (format: clusteradmin, orgadmin, regular, readonly or s3, may be repeated or comma-separated) |

In this example a policy is created that allows access by users with the clusteradmin role from two specific subnets:

```

```
weka security policy create admin_network --action allow --ips 10.1.0.0/16,10.2.1.0/24 --roles clusteradmin
```

```

### Remove a security policy

**Command:** `weka security policy remove`

Use the following command line to delete a security policy.

```sh
weka security policy remove <policy>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policy* | Name or ID of security policy. |

### Duplicate an existing security policy

**Command:** `weka security policy duplicate`

Use the following command line to duplicate an existing security policy, creating a new one.

```sh
weka security policy duplicate <policy> <name>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policy* | Name or ID of the security policy to duplicate. |
 | name* | Name of the new security policy. (up to 64 alphanumeric characters, hyphens (-), underscores (_), and periods (.), starting with a letter) |

Example:

```
weka security policy duplicate sourcePolicy newPolicyName
```

### Update security policy settings

**Command:** `weka security policy update`

Use the following command line to update the settings of an existing security policy.

```

```sh
weka security policy update <policy> [--description description] [--action action] [--new-name new-name] [--roles roles]... [--add-roles add-roles]... [--remove-roles remove-roles]... [--ips ips]... [--add-ips add-ips]... [--remove-ips remove-ips]...
```

```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policy* | Name or ID of security policy. |
 | --description | Updates the description of the security policy. (up to 256 characters) |
 | --action | Changes whether access is granted when the security policy matches. (format: allow or deny) |
 | --new-name | New name of the security policy. (up to 64 alphanumeric characters, hyphens (-), underscores (_), and periods (.), starting with a letter) |
 | --roles... | User roles to which the security policy applies. (format: clusteradmin, orgadmin, regular, readonly or s3, may be repeated or comma-separated) |
 | --add-roles... | User roles to append to the security policy. (format: clusteradmin, orgadmin, regular, readonly or s3, may be repeated or comma-separated) |
 | --remove-roles... | User roles to remove from the security policy. (format: clusteradmin, orgadmin, regular, readonly or s3, may be repeated or comma-separated) |
 | ips | IP address ranges to which the security policy applies. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | add-ips | IP address ranges to append to the security policy. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |
 | remove-ips | IP address ranges to remove from the security policy. (format: IP or IP/CIDR or IP1-IP2 or A.B.C.D-E, may be repeated or comma-separated) |

In this example the <kbd>readonly</kbd> role is added to an existing security policy called <kbd>admin_network</kbd>:

```

```
weka security policy update admin_network --add-roles readonly --description "Limit Cluster Admin Access to HQ Network"
```

```

### Simulate the effect of one or more security policies

**Command:** `weka security policy test`

Use the following command line to simulates the effect of one or more security policies.

```sh
weka security policy test [--role role] [--ip ip] [--join] [<policy>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policy... | Policies to evaluate, with access verified in the order listed. |
 | role | Simulate effect of policies on API access from the given user role. (format: clusteradmin, orgadmin, regular, readonly or s3) |
 | ip | IP address to evaluate as the source address. |
 | join | Simulate effect of policies when joining the cluster. |

Example:

```
weka security policy test policy1 policy2 policy3 --ip 10.2.1.0 --role clusteradmin
```

### List security policies applied when joining containers

**Command:** `weka security policy join list`

Use the following command line to list security policies applied when joining containers.

```sh
weka security policy join list [--client] [--backend]
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | client | List policies for clients. |
 | backend | List policies for backends. |

### Set security policies for joining cluster

**Command:** `weka security policy join set`

Use the following command line to set security policies for joining cluster, replacing the existing set of policies.

```
weka security policy join set [--client] [--backend] [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policies... | Security policy names or IDs applied to cluster join process. |
 | client | Apply policies to clients. |
 | backend | Apply policies to backends. |

### Attach a security policy when joining cluster

**Command:** `weka security policy join attach`

Use the following command line to attach security policies applied when joining cluster, adding them to the existing policies.

```sh
weka security policy join attach [--client] [--backend] [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policies... | Security policy names or IDs to attach to cluster join process. |
 | client | Apply policies to clients. |
 | backend | Apply policies to backends. |

### Detach a security policy when joining cluster

**Command:** `weka security policy join detach`

Use the following command line to remove security policies applied when joining cluster.

```sh
weka security policy join detach [--client] [--backend] [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | policies... | Security policy names or IDs to remove from cluster join proces |
 | client | Apply policies to clients. |
 | backend | Apply policies to backends. |

### Remove all security policies applied when joining cluster

**Command:** `weka security policy join reset`

Use the following command line to remove all security policies applied when joining cluster.

```sh
weka security policy join reset [--client] [--backend]
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | client | Apply policies to clients. |
 | backend | Apply policies to backends. |

## Manage organization security policies using the CLI

Once security policies are defined, you can perform the following tasks at the organization level:

* List security policies for a specified organization.
* Set security policies for a specified organization.
* Remove all security policies from a specified organization.
* Attach new security policies to a specified organization.
* Detach security policies from a specified organization.

### List the organization security policies

Command: `weka org security policy list`

Use the following command to list the security policies of a specified organization.

```sh
weka org security policy list <org>
```

Note: The command `weka org` also displays the attached policies for each organization.

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | org* | Organization name or ID. |

### Set security policies for an organization

Command: `weka org security policy set`

Use the following command to set security policies for an organization, replacing the existing list of policies. If setting multiple policies, separate each with a space.

```
weka org security policy set <org> [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | org* | Organization name or ID. |
 | policies... | Security policy names or IDs to assign to the organization, space separated. |

### Remove all security policies from an organization

**Command:** `weka org security policy reset`

Use the following command to removes all security policies from an organization.

```sh
weka org security policy reset <org>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | org* | Organization name or ID. |

### Attach new security policies to an organization

**Command:** `weka org security policy attach`

Use the following command to attach new security policies to an organization, adding them to the existing policies. If attaching multiple policies, separate each with a space.

```sh
weka org security policy attach <org> [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | org* | Organization name or ID. |
 | policies... | Security policy names or IDs to attach to the organization, space separated. |

### Detach security policies from an organization

**Command:** `weka org security policy detach`

Use the following command to detach (remove) security policies from an organization. If detaching multiple policies, separate each with a space.

```sh
weka org security policy detach <org>[<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | org* | Organization name or ID. |
 | policies... | Security policy names or IDs to remove from the organization, space separated. |

## Manage filesystem security policies using the CLI

Once security policies are defined, you can perform the following tasks at the filesystem level:

* List security policies for a specified filesystem.
* Set security policies for a specified filesystem.
* Remove all security policies from a specified filesystem.
* Attach new security policies to a specified filesystem.
* Detach security policies from a specified filesystem.

### List security policies for a filesystem

**Command:** `weka fs security policy list`

Use the following command to list security policies for a specified filesystem.

```
weka fs security policy list <fs-name>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | fs-name* | Filesystem name. |

### Set security policies for a filesystem

**Command:** `weka fs security policy set`

Use the following command to set security policies for a specified filesystem, replacing the existing list of policies. If setting multiple policies, separate each with a space.

```
weka fs security policy set <fs-name> [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | fs-name* | Filesystem name. |
 | policies... | Security policy names or IDs to set for a filesystem, space separated. |

Example to apply two security policies to a filesystem named <kbd>fs0</kbd>:

```
weka fs security policy set fs0 fs0allow denyall
```

### Remove all security policies from a filesystem

**Command:** `weka fs security policy reset`

Use the following command to remove all security policies from a specified filesystem.

```
weka fs security policy reset <fs-name>
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | fs-name* | Filesystem name. |

### Attach new security policies to a filesystem

**Command:** `weka fs security policy attach`

Use the following command to attach additional security policies to the specified filesystem. If attaching multiple policies, separate each with a space.

```
weka fs security policy attach <fs-name> [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | fs-name* | Filesystem name. |
 | policies... | Security policy names or IDs to attach new security policies to the specified filesystem, space separated. |

### Detach security policies from a filesystem

**Command:** `weka fs security policy detach`

Use the following command to detach (remove) security policies from a filesystem. If detaching multiple policies, separate each with a space.

```
weka fs security policy detach <fs-name> [<policies>]...
```

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | fs-name* | Filesystem name. |
 | policies... | Security policy names or IDs to remove from the specified filesystem, space separated. |

## Examples: Implementing CIDR-based security policies

This section provides practical examples of implementing CIDR-based security policies for common use cases.

### Example 1: Restrict cluster admin access to backend network

This example demonstrates how to allow only `clusteradmin` access from a specific backend subnet while denying all other IP addresses.

**Scenario:** Allow cluster administrators to access the cluster only from the backend network (10.10.0.0/16) and implicitly deny access from all other IP addresses.

1. **Create the security policy:** Create an allow policy for cluster administrators from the backend subnet:

```

```bash
weka security policy add allow-cluster-admins-backend \
  --action allow \
  --ips 10.10.0.0/16 \
  --roles clusteradmin
```

```

2. **Attach policy to the root organization:** Attach the policy to the root organization to enforce it cluster-wide:

```bash
weka security policy attach root allow-cluster-admins-backend
```

**Result:** Only users with the `clusteradmin` role connecting from IP addresses within the 10.10.0.0/16 range can access the cluster. All other connections are implicitly denied.

### Example 2: Multi-network access control

This example shows how to allow access from multiple networks, such as backend infrastructure and administrative workstations.

**Scenario:** Allow `clusteradmin` access from both the backend network (10.10.0.0/16) and administrative workstations (192.168.100.0/24).

1. **Create a policy for backend access:**

```

```bash
weka security policy add allow-backend-admins \
  --action allow \
  --ips 10.10.0.0/16 \
  --roles clusteradmin
```

```

2. **Create a policy for admin workstation access:**

```

```bash
weka security policy add allow-admin-workstations \
  --action allow \
  --ips 192.168.100.0/24 \
  --roles clusteradmin
```

```

3. **Attach both policies:** Attach both policies to the root organization:

```

```bash
weka org security policy attach root allow-backend-admins allow-admin-workstations
```

```

**Result:** Cluster administrators can access the cluster from either the backend network or designated admin workstations.

### Example 3: Read-only filesystem access

This example demonstrates how to provide read-only access to filesystems from specific networks.

**Scenario:** Allow read-only access to a filesystem from a data analysis network (172.16.0.0/12) while maintaining full access control for other roles.

1. **Create read-only access policy:** Create a policy allowing readonly role access from the analysis network:

```

```bash
weka security policy add readonly-analysis-network \
   --action allow \
   --ips 172.16.0.0/12 \
   --roles readonly
```

```

2. Apply policy to specific filesystem: Apply the policy to a specific filesystem (for example, `data-warehouse`):

```

```bash
weka fs security policy attach data-warehouse readonly-analysis-network
```

```

**Result:** Users with the `readonly` role can access the "data-warehouse" filesystem from the analysis network for read-only operations.

<!-- ============================================ -->
<!-- File 197/259: security_manage-cross-origin-resource-sharing.md -->
<!-- ============================================ -->

# Manage Cross-Origin Resource Sharing

Cross-Origin Resource Sharing (CORS) is a security mechanism implemented by web browsers that restricts how a web page from one domain can request resources from a different domain. This essentially defines the rules for communication between a web application and the server that hosts the resources it needs.

**Why do we need CORS?**

Imagine a scenario where a website (yourbank.com) relies on functionalities from another website (images.com) to display product images. If browsers didn't have CORS, images.com could potentially steal your login credentials from yourbank.com through a malicious script.

CORS prevents such unauthorized access by enforcing a set of rules. The server hosting the resources (images.com) can specify which websites (origins) can access them through special HTTP headers. This ensures that your sensitive data on yourbank.com remains secure.

In summary, CORS helps maintain web security by:

* **Preventing unauthorized access:** It restricts malicious websites from accessing resources on other domains.
* **Enforcing communication protocols:** It defines a clear communication channel between browsers and servers for cross-origin requests.

## **CORS CLI commands**

### List the CORS trusted sites

**Command:** `weka security cors-trusted-sites list`

Use the following command line to display the list of trusted sites with configured CORS settings:

`weka security cors-trusted-sites list`

Example:

```
$ weka security cors-trusted-sites list
http://site_1.com
http://Site_2.com
```

### Add a CORS trusted site

**Command:** `weka security cors-trusted-sites add`

Use the following command line to add a trusted site to the CORS list.

`weka security cors-trusted-sites add <site>`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | site* | Trusted site to add. Include the URL with the http or https prefix, and specify the port number if it‚Äôs not the default. |

Example:

```
$ weka security cors-trusted-sites add http://site_3.com
```

### Remove a CORS trusted site

**Command:** `weka security cors-trusted-sites remove`

Use the following command line to remove a specified trusted site from the CORS list.

`weka security cors-trusted-sites remove`

 | Parameter | Description |
 | --- | --- |
 | site* | Trusted site to remove from the CORS list. |

### Remove all trusted sites from the CORS list

**Command:** `weka security cors-trusted-sites remove-all`

Use the following command line to remove all trusted sites from the CORS list.

`weka security cors-trusted-sites remove`

<!-- ============================================ -->
<!-- File 198/259: security_tls-certificate-management.md -->
<!-- ============================================ -->

---
description:
---

# Manage TLS certificates

The TLS implementation in a WEKA cluster secures communication between clients and the system, and between WEKA servers and clients. TLS certificates serve two key purposes:

* **Data Protection:** Encrypting data in transit to ensure confidentiality and integrity.
* **Authentication:** Verifying the system‚Äôs identity to prevent unauthorized access.

By default, the WEKA system deploys a self-signed TLS certificate to secure access to the Web UI, CLI, and API over HTTPS (port 14000) . Users can enhance security by deploying their own TLS certificates, which requires providing an unencrypted private key and certificate in PEM format.

The system supports TLS 1.2 and higher, enforcing encrypted communication with a minimum of 128-bit ciphers.

To establish trust with external services, such as a Key Management System (KMS), the system relies on well-known CA certificates. If a custom CA certificate is required for WEKA servers to authenticate external services, manually configure the custom CA certificate on the WEKA servers.

**Related topics**

<!-- ============================================ -->
<!-- File 199/259: security_tls-certificate-management_manage-the-tls-certificate-using-the-cli.md -->
<!-- ============================================ -->

# Manage TLS certificates using CLI

## Set the TLS certificate

**Command:** `weka security tls set`

Use the following command line to use TLS when accessing UI. If TLS is already set, this command updates the key and certificate.

`weka security tls set [--private-key private-key] [--certificate certificate]`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | private-key | Path to TLS private unencrypted key pem file. |
 | certificate | Path to TLS certificate pem file. |

Note: **Example:**
This command is similar to the WEKA's OpenSSL command to generate the self-signed certificate: `openssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days <days> -nodes`

## Replace the TLS certificate

To replace the TLS certificate with a new one, use the CLI command: `weka security tls set`.

Once you issue a TLS certificate, it is used for connecting to the cluster (for the time it is issued), while the revocation is handled by the CA and propagating its revocation lists into the various clients.

## Unset the TLS certificate

You can unset your TLS certificates using the CLI command: `weka security tls unset`.

## Download the TLS certificate

To download the TLS certificate, use the CLI command: `weka security tls download`.

## View the TLS certificate status

To view the cluster TLS status and certificate, use the CLI command: `weka security tls status`.

## Set CA certificate

The system uses well-known CA certificates to establish trust with external services. For example, when using a KMS. If a different CA certificate is required for WEKA servers to establish trust, set this custom CA certificate on the WEKA servers.

Use the CLI command:

`weka security ca-cert set [--cert-file cert-file]`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | cert-file | Path to the certificate file. |

<!-- ============================================ -->
<!-- File 200/259: security_tls-certificate-management_manage-the-tls-certificate-using-the-gui.md -->
<!-- ============================================ -->

# Manage TLS certificates using GUI

## Set and download TLS certificate

Upon system installation, the cluster's TLS certificate is activated with an auto-generated self-signed certificate, enabling access to the GUI, CLI, and API via HTTPS. If you have a custom TLS certificate, you may replace the auto-generated self-signed certificate with your own. Additionally, you can download the existing TLS certificate for integration with other applications that require communication with the cluster, such as Local WEKA Home.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.
3. In the TLS Certificate section, select **Set TLS certificate**.
4. In the Set Custom TLS Certificate dialog, do one of the following:
   * Select **Upload TLS certificate files**, and upload the TLS certificate and private key files.
   * Select **Paste the custom certificate content**, and paste the content of the TLS certificate and private key.

5. To download the existing TLS certificate, select **Download TLS certificate**. \
    In the dialog, set a name for the certificate and select **Download**.

## Set custom CA certificate <a href="#set-custom-ca-certificate" id="set-custom-ca-certificate"></a>

The system uses well-known CA certificates to establish trust with external services. For example, when using a KMS. If a different CA certificate is required for Weka servers to establish trust, set this custom CA certificate on the Weka servers.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.
3. In the TLS Certificate section, select **Set custom CA certificate**.
4. In the Set Custom CA Certificate dialog, do one of the following:
   * Select **Upload CA certificate file**, and upload the custom CA certificate file.
   * Select **Paste the custom certificate content**, and paste the content of the custom CA certificate.
5. Select **Save**.

## Manage the custom CA certificate <a href="#manage-the-custom-ca-certificate" id="manage-the-custom-ca-certificate"></a>

Once a CA certificate is set, you can:

* Replace the CA certificate with a new one according to the deployment needs.
* Remove (reset) the custom CA certificate settings.
* Download the existing CA certificate for later use.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.
3. In the TLS Certificate section, select **Replace custom CA certificate**.
4. In the Set Custom CA Certificate dialog, do one of the following:
   * Select **Upload CA certificate file**, and upload the custom CA certificate file.
   * Select **Paste the custom certificate content**, and paste the content of the custom CA certificate.
5. Select **Save**.
6. If required to remove the custom CA certificate, select **Reset custom CA certificate settings**. In the confirmation message, select **Yes**.
7. To download the existing CA certificate, select **Download custom CA certificate**. In the dialog, set a name for the certificate and select **Download**.

**Related topic**

<!-- ============================================ -->
<!-- File 201/259: security_kms-management.md -->
<!-- ============================================ -->

---
description:
---

# Manage KMS

## Overview

When creating an encrypted filesystem in the WEKA system, a Key Management System (KMS) is essential for securely managing encryption keys. WEKA relies on the KMS to encrypt filesystem keys and decrypt them during system startup, using in-memory capabilities for ongoing data encryption and decryption.

The Snap-To-Object feature stores the encrypted filesystem key alongside the encrypted data when snapshots are taken. During snapshot promotion to a different filesystem or disaster recovery within the WEKA cluster, the KMS decrypts the filesystem key. Therefore, ensuring access to the same KMS configuration is crucial for these operations.

To enhance security, WEKA does not store any data that could reconstruct the KMS encryption keys. As a result, careful consideration of the following factors is required:

* **Disaster recovery strategy:** Loss of the KMS configuration can result in the loss of encrypted data. A robust Disaster Recovery (DR) plan is essential in production environments.
* **High availability:** The KMS must be available during system startup, filesystem creation, and key rotations. Ensuring high availability for the KMS is highly recommended.

**Supported KMS types:**

* **KMIP-compliant KMS****:** Supports protocol versions 1.2+ and 2.x. Only TTLV is supported as the messaging protocol. Supports commercial solutions such as Thales CipherTrust Manager.
* **HashiCorp Vault****:** Supports versions 1.1.5 to 1.14.x.

### **KMS encryption models**

WEKA supports two primary models for KMS integration.

#### **Cluster encryption key**

In this model, a single encryption key from the KMS serves as the master key for the entire WEKA cluster. All individual filesystem keys are encrypted (wrapped) by this cluster key.

For HashiCorp Vault integration, this model supports two authentication methods:

* **Token-based authentication:** Uses a Vault token for authentication.
* **AppRole authentication:** Uses a **Role ID** and **Secret ID** for more structured, automated authentication.

#### **Per-filesystem encryption keys**

This model, available exclusively with HashiCorp Vault, allows each filesystem to have its own unique encryption key sourced from the KMS. This approach provides enhanced data isolation, which is ideal for multi-tenant environments where each tenant requires a distinct, customer-controlled encryption key.

This configuration requires using **AppRole authentication**. Filesystems can share authentication credentials if they are in the same KMS namespace, or you can use cluster-level credentials for key access.

## **KMS integration best practices**

To ensure seamless operations and safeguard your data, adhere to the following best practices:

* **Disaster recovery setup:** Establish a robust backup or replication mechanism for the KMS to minimize the risk of data loss.
* **High availability:** Ensure that the KMS remains highly available, as the WEKA system identifies it through a single address. Downtime in KMS availability could disrupt critical operations.
* **KMS access:** Ensure that all WEKA backend servers can access the KMS to maintain consistent encryption and decryption functionality.
* **KMS method verification:** Familiarize yourself with the specific methods your KMS uses for key security, unsealing, and recovery. Different systems have distinct processes; for example, HashiCorp Vault can enable auto-unsealing using a trusted service. Understanding these mechanisms is essential for efficient recovery and key management.
* **Snapshot backup:** When using Snap-To-Object, ensure that encrypted filesystem keys are backed up to an object store. This provides an additional layer of protection in case the WEKA system configuration is compromised.

For further guidance on securing HashiCorp Vault in production environments, refer to the Production Hardening documentation.

## KMS integration: cluster encryption keys

The diagram illustrates WEKA's cluster encryption process, which supports both HashiCorp Vault KMS and KMIP (Key Management Interoperability Protocol). Here, we focus on HashiCorp Vault KMS for key management.

The following steps outline the process for managing encryption keys across the WEKA cluster:

1. **Initial setup:** Each encrypted filesystem (FS) in the WEKA cluster requires a unique encryption key. FS keys are encrypted using a cluster key and stored securely in the configuration table. The KMS configuration table stores the namespace and key-identifier for streamlined key management and secure access to encryption keys within the WEKA cluster.
2. **Encryption process:** During normal operation, encrypted FS keys are stored in the configuration table. FS keys in-memory are used for real-time encryption/decryption. After a restart, encrypted FS keys are retrieved and decrypted using the cluster key.
3. **Rewrap operation:** The rewrap process involves decrypting the FS key, retrieving it, and then re-encrypting the FS key with a new version of the cluster key. This ensures that the FS keys remain protected with updated encryption, enhancing security based on KMS policies.

## KMS integration: per-filesystem encryption keys

In multi-tenant environments, such as clusters offering POSIX or S3 services to multiple customers, strong data isolation is essential. To meet this requirement, WEKA extends its KMS integration to support per-filesystem encryption keys using HashiCorp Vault. This enables each tenant to use a distinct, customer-controlled encryption key.

#### Why use per-filesystem encryption keys?

In multi-tenant deployments, isolating encryption keys by tenant ensures that no single key compromises data confidentiality across the environment. Assigning a unique encryption key to each filesystem, managed externally by a KMS, eliminates shared cluster-wide keys and enhances security.

#### **Key features**

* **Independent encryption keys per filesystem:** Enables tenant-level data isolation by assigning a unique external key to each filesystem.
* **Shared credentials option:** Filesystems within the same namespace can share authentication credentials if appropriate.
* **Cluster-level authentication support:** Optionally allows use of cluster-wide credentials for key access.
* **Role-based access control (RBAC):** Supports role-based authentication to enforce secure and auditable key access.

#### Authentication method

WEKA integrates with HashiCorp Vault using the **AppRole** authentication method, enabling each filesystem to use a distinct, revokable identity for secure key access. This design is critical for ensuring tenant-level isolation and key management flexibility.

#### Architecture overview

The following diagram illustrates how WEKA integrates with HashiCorp Vault to manage per-filesystem encryption keys:

#### **Configuration process**

Administrators must configure multiple keys within the KMS, one for each filesystem. During the creation of a filesystem, specific parameters are required, including the namespace, KMS key identifier, role ID, and secret ID. This configuration ensures that the filesystem operates with its dedicated encryption key.

The `weka fs create` command supports this process. For details, see #create-a-filesystem.

#### **Key update and fallback**

Filesystem keys can be updated when necessary, such as when transitioning from Key1 to Key2. Administrators also have the option to revert to the cluster key, which involves removing the individual filesystem keys and returning to the previous setup.

The `weka fs update` command supports these updates. For details, see #edit-a-filesystem.

**Rewrap KMS security key for a specific filesystem**

Rewrap operations can be performed per filesystem, enabling each key to be re-encrypted with a new version if there are concerns about key compromise.

The `weka fs kms-rewrap` command supports this operation. For details, see #rewrap-the-filesystem-encryption-key.

#### Create a filesystem from an encrypted snapshot

To create a filesystem from an uploaded snapshot originating from an encrypted source, use the `weka fs download` command. For details, see #create-a-filesystem-from-an-uploaded-snapshot.

#### **Security configuration**

Before configuring per-filesystem encryption, you must first establish a connection between the WEKA cluster and the configured HashiCorp Vault KMS by setting up the `key-identifier` (Weka_cluster_key in the illustration above). Use the `weka security kms set` command to specify the `role-id` and `secret-id` for this connection. For details, see #configure-the-kms.

If there are concerns about key compromise, administrators can rewrap all filesystem keys using the  `weka security kms rewrap --all` command. \
When the `--all` option is omitted, the system rewraps only the filesystems that use the cluster encryption key.   For details, see #rewrap-filesystem-keys.

Note: The configuration for per-filesystem encryption keys in KMS integration is available exclusively in the CLI.

**Related topics**

<!-- ============================================ -->
<!-- File 202/259: security_kms-management_kms-management.md -->
<!-- ============================================ -->

---
description:
---

# Manage KMS using GUI

Using the GUI, you can:

* Configure a KMS
* View the KMS configuration
* Update the KMS configuration
* Remove the KMS configuration

## Configure a KMS

Configure the KMS of either HashiCorp Vault or KMIP within the WEKA system to encrypt filesystem keys securely.

**Before you begin**

Ensure the KMS is preconfigured, and the key and a valid token are readily available.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.
3. On the **Security** page, select **Configure KMS**.
4. On the **Configure KMS** dialog, select the KMS type to deploy: **HashiCorp Vault** or **KMIP**.
5. Set the connection properties according to the selected KMS type. Select the relevant tab for details:

To configure the HashiCorp Vault connection from the GUI, set the following properties.

* **Address:** The KMS server address.
* **Key Identifier:** The key name used to secure the filesystem keys.
* **Role ID:** The Role ID for AppRole authentication, provided by the Vault administrator.
* **Secret ID:** The Secret ID for AppRole authentication, provided by the Vault administrator.
* **Namespace:** The namespace in Vault that identifies the logical partition for organizing data and policies. Namespace names must not end with `/`, avoid spaces, and refrain from using reserved names like `root`, `sys`, `audit`, `auth`, `cubbyhole`, and `identity`.

Note: The GUI procedure configures HashiCorp Vault for **per-filesystem encryption**, which uses the AppRole authentication method. To configure cluster-wide encryption (using either a token or AppRole), use the CLI. See #configure-the-kms.

<div align="left"></div>

To configure the KMIP connection, set the following properties:

* **Address:** The hostname and port of the KMS, in `hostname:port` format. The hostname can be a fully qualified domain name (FQDN) or an IP address. Do not include protocol prefixes such as `https://`. The default port for KMIP is 5696, but this can vary based on the server configuration.
* **KMS Identifier:** The key UID used to secure the filesystem keys.
* **Client Certificate:** The content of the client certificate PEM file.
* **Client Key:** The content of the client key PEM file.
* **CA Certificate:** (Optional) The content of the CA certificate PEM file.

6. Select **Save**.

**Related topics**

[Obtain an API token from the vault](../kms-management-1#obtain-an-api-token-from-the-vault)

[Obtain a certificate for a KMIP-based KMS](../kms-management-1#obtain-a-certificate-for-a-kmip-based-kms)

## View the KMS configuration

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.\
   The **Security** page displays the configured KMS.

## Update the KMS configuration

Update the KMS configuration in the WEKA system when changes occur in the KMS server details or cryptographic keys, ensuring seamless integration and continued secure filesystem key encryption.

Note: If your system is upgraded to version 4.4.2 or higher, the **Update KMS Configuration** screen displays a configuration with the Token parameter. Reset the KMS configuration and configure it using the new **Role ID** and **Secret ID** parameters.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.
3. The **Security** page displays the configured KMS.
4. Select **Update KMS**, and update the settings. For the parameter descriptions, see #configure-a-kms.
5. Select **Save**.

## Reset the KMS configuration

Reseting a KMS configuration is possible only if no encrypted filesystems exist.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Security**.
3. The **Security** page displays the configured KMS.
4. Select **Reset KMS.**
5. In the message that appears, select **Yes** to confirm the KMS configuration reset.

<!-- ============================================ -->
<!-- File 203/259: security_kms-management_kms-management-1.md -->
<!-- ============================================ -->

---
description:
---

# Manage KMS using CLI

Using the CLI, you can:

* Configure the KMS
* View the KMS configuration
* Remove the KMS configuration
* Rewrap filesystem keys
* Set up vault configuration
* Obtain a certificate for a KMIP-based KMS

## Configure the KMS

**Command:** `weka security kms set`

Use this command to add or update the KMS configuration. For HashiCorp Vault, you can configure a single cluster-wide encryption key or enable per-filesystem encryption keys.

Per-filesystem encryption requires using Vault's AppRole authentication method, which you configure using the `--role-id` and `--secret-id` parameters.

Run the following command to establish a connection between the WEKA system and the configured Vault KMS.

`weka security kms set <type> <address> <key-identifier> [--token token] [--namespace namespace] [--client-cert client-cert] [--client-key client-key] [--ca-cert ca-cert] [--role-id role-id] [--secret-id secret-id] [--convert-to-cluster-key-on-fs]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | type* | Type of the KMS.Values: vault or kmip |
 | address* | KMS server address.Values:URL for vaulthostname:port for kmip |
 | key-identifier* | Key name for vault UID for kmip to secure filesystem keys. |
 | token | The API token for authenticating with a HashiCorp Vault KMS.This parameter applies only to Vault and is used for cluster-wide encryption. It cannot be used with the role-id or secret-id parameters, which are used for AppRole authentication.The access token must have the following permissions in Vault:Read access totransit/keys/<master-key-name>Write access totransit/encrypt/<master-key-name> and transit/decrypt/<master-key-name>Permissions for/transit/rewrap and auth/token/lookup |
 | namespace | The namespace name in HashiCorp Vault.Namespace names must not end with "/", avoid spaces, and refrain from using reserved names like root, sys, audit, auth, cubbyhole, and identity. |
 | client-cert | Path to the client certificate PEM file.Must permit encrypt and decrypt permissions.Mandatory for kmip .Prohibited for vault. |
 | client-key | Path to the client key PEM file.Mandatory for kmip .Prohibited for vault. |
 | ca-cert | Path to the CA certificate PEM file.Optional for kmip.Prohibited for vault. |
 | role-id | Role ID for HashiCorp Vault's AppRole authentication method, which is provided by the Vault administrator. This parameter must be used with secret-id and cannot be used with token. |
 | secret-id | Secret ID for HashiCorp Vault's AppRole authentication, which is provided by the Vault administrator. This parameter must be used with role-id. Alternatively, you can set this value using theWEKA_KMS_SECRET_ID environment variable. |
 | convert-to-cluster-key-on-fs | Convert all encrypted filesystems to use cluster key. |

### Obtain `role-id` and `secret-id` from HashiCorp Vault

Use this procedure when configuring HashiCorp Vault with AppRole authentication. This method is required for per-filesystem encryption and is an option for cluster-wide encryption. The Vault administrator provides the `role-id` and `secret-id` needed for access.

#### **Recommendation: use batch tokens for per-filesystem encryption**

For per-filesystem encryption, it is recommended to configure the AppRole with **batch tokens** to ensure optimal performance and scalability in your HashiCorp Vault environment.

Unlike standard service tokens, which generate a new lease for each authentication request, batch tokens are designed for high-volume, automated workflows. They do not create new leases upon use, which is critical for maintaining efficiency in environments with many filesystems.

While batch tokens can be used multiple times, their lifetime is limited by a Time-to-Live (TTL). It is recommended to set a short TTL (for example, 20 minutes) to align with security best practices.

Batch tokens are not single-use but are limited by their TTL, and they do not create new leases upon use, which prevents this scalability problem. To implement this, create your AppRole with the `token_type` set to `batch`.

For more information, refer to the official HashiCorp Vault documentation.

Note: **Disclaimer**: The following example is provided as a courtesy to illustrate possible integration with **HashiCorp Vault** and is not part of our product.

#### Set up roles for cluster access

Enable AppRole authentication:

```
$ vault auth enable approle
```

Role for cluster:

```shell
$ vault write -f auth/approle/role/weka-role-cluster
Success! Data written to: auth/approle/role/weka-role-cluster

$ vault write -f auth/approle/role/weka-role-cluster token_policies="weka_cluster_role_key_policy"
Success! Data written to: auth/approle/role/weka-role-cluster
```

Retrieve the **role-id**:

```shell
$ vault read auth/approle/role/weka-role-cluster/role-id
```

Role for **Key1**:

```shell
$ vault write -f auth/approle/role/weka-role-1
Success! Data written to: auth/approle/role/weka-role-1

$ vault write -f auth/approle/role/weka-role-1 token_policies="weka_fs_role_key1_policy"
Success! Data written to: auth/approle/role/weka-role-1
```

Retrieve the **role-id** and generate a **secret-id**:

```
$ vault read auth/approle/role/weka-role-1/role-id
Key        Value
---        -----
role_id    5a574437-72b8-17b0-dbce-f36731d77663

$ vault write -f auth/approle/role/weka-role-1/secret-id
Key                   Value
---                   -----
secret_id             69c26538-27cb-bcce-1ac2-27d4de590d5b
secret_id_accessor    a3b885ff-ba25-560d-cc56-58df99962b2d
secret_id_num_uses    0
secret_id_ttl         0s
```

### **Examples**

**Setting the WEKA system with a HashiCorp Vault KMS for cluster-wide encryption:**

```

```
weka security kms set vault https://vault-dns:8200 weka_cluster_key --token s.nRucA9Gtb3yNVmLUK221234
```

```

**Setting the WEKA system with a HashiCorp Vault KMS for per-filesystem encryption:**

```

```
weka security kms set  vault  https://vault-dns:8200 weka_cluster_key --role-id 26e2576f-cb9d-b48a-057d-e37d8956b00c --secret-id 44797329-e729-6j80-m9d4-b1825037cha6
```

```

**Setting the WEKA system with a KMIP complaint KMS (SmartKey example):**

```

```
weka security kms set kmip amer.smartkey.io:5996 b2f81634-c0f6-4y63-b5b3-84a82e231634 --client-cert smartkey_cert.pem --client-key smartkey_key.pem
```

```

## View the KMS configuration

**Command:** `weka security kms`

Use this command to show the details of the configured KMS.

## Remove the KMS configuration

**Command:** `weka security kms unset`

Use this command to remove the KMS from the WEKA system. It is only possible to remove a KMS configuration if no encrypted filesystems exist.

Note: To force remove a KMS even if encrypted filesystems exist, use the `--allow-downgrade` attribute. In such cases, the encrypted filesystem keys are re-encrypted with local encryption and may be compromised.

## **Rewrap filesystem keys**

**Command:** `weka security kms rewrap`

If the KMS key is compromised or requires rotation, the KMS administrator can rotate the key in the KMS. In such cases, this command is used to re-encrypt the encrypted filesystem keys with the new KMS cluster key.

`weka security kms rewrap [--new-key-uid new-key-uid] [--all] [--convert-to-cluster-key-on-fs]`

**Parameters**

 | Name | Value |
 | --- | --- |
 | new-key-uid* | Unique identifier for the new key to be used to wrap filesystem keys.Mandatory for kmip only.Do not specify any value for vault. |
 | all | Rewrap all the filesystem encryption keys. Applicable when using HashiCorp Vault for per-filesystem encryption keys.Without the --all option, the command re-encrypts only the keys of filesystems that use the cluster key for encryption. |
 | convert-to-cluster-key-on-fs | Convert all encrypted filesystems to use the KMS cluster key. |

Note: WEKA does not automatically re-encrypt existing filesystem keys with the new KMS key for snapshots that were previously uploaded with the old encrypted keys.

Note: Unlike HashiCorp Vault KMS, re-wrapping a KMIP-based KMS necessitates generating a new key within the KMS rather than rotating the existing one. Therefore, it is essential to retain the old key in the KMS to ensure the decryption of older Snap-to-Object snapshots.

## Set up vault configuration

### Enable 'Transit' secret engine in vault

The WEKA system uses encryption-as-a-service capabilities of the KMS to encrypt/decrypt the filesystem keys. This requires the configuration of Vault with the `transit` secret engine with this command:

```
vault secrets enable transit
```

The expected output is:

```
Success! Enabled the transit secrets engine at: transit/

```

### Set up a master key for the WEKA system

Once the `transit` secret engine is set up, a master key for use with the WEKA system must be created with this command:

```
vault write -f transit/keys/weka-key
```

The expected output is:

```bash
Success! Data written to: transit/keys/weka-key
```

Note: It is possible to either create a different key for each WEKA cluster or to share the key between different WEKA clusters.

**Related information:**

Vault transit secret-engine documentation

### Create a policy for master key permissions

* Create a `weka_policy.hcl` file with the following content:

```bash
path "transit/+/weka-key" {
  capabilities = ["read", "create", "update"]
}
path "transit/keys/weka-key" {
  capabilities = ["read"]
}
```

This limits the capabilities so there is no permission to destroy the key, using this policy. This protection is important when creating an API token.

* Create the policy using the following command:

```bash
vault policy write weka weka_policy.hcl
```

### Obtain an API token from the vault

Authentication from the WEKA system to Vault relies on an API token. Since the WEKA system must always be able to communicate with the KMS, a periodic service token must be used.

* Verify that the`token` authentication method in Vault is enabled. This can be performed using the following command:

```
vault auth list
```

The expected output is:

```bash
vault auth list

Path         Type        Description
----         ----        -----------
token/       token       token based credentials
```

* To enable the token authentication method use the following command:

```
vault auth enable token
```

* Log into the KMS system using any of the identity methods Vault supports. The identity must have permission to use the previously set master key.
* Create a token role for the identity using the following command:

```

```bash
vault write auth/token/roles/weka allowed_policies="weka" period="768h"
```

```

Note: The `period` is the designated timeframe for a renewal request. If a renewal is not requested within this period, the token is revoked, necessitating the retrieval of a new token from the Vault and its configuration in the WEKA system.

* Generate a token for the logged-in identity using the following command:

```
vault token create -role=weka
```

The expected output is:

```bash
vault token create -role=weka

Key                  Value
---                  -----
token                s.nRucA9Gtb3yNVmLUK221234
token_accessor       4Nm9BvIVS4HWCgLATc3r1234
token_duration       768h
token_renewable      true
token_policies       ["default"]
identity_policies    []
policies             ["default"]
```

For more information on obtaining an API token, refer to Vault Tokens documentation.

Note: The WEKA system does not automatically renew the API token lease. It can be renewed using the Vault CLI/API. It is also possible to define a higher maximum token value (`max_lease_ttl)`by changing the Vault Configuration file.

## Obtain a certificate for a KMIP-based KMS

Each KMS employs a unique process for obtaining a client certificate and key and configuring it through the KMS. The certificate is generated using OpenSSL and utilizes a UID obtained from the KMS.

**Example**:

```

```bash
openssl req -x509 -newkey rsa:4096 -keyout client-key.pem -out client-cert.pem -days 365 -nodes -subj '/CN=f283c99b-f173-4371-babc-572961161234'
```

```

Refer to the specific KMS documentation to create a certificate and associate it with the WEKA cluster within the KMS, ensuring it has the necessary privileges for encryption and decryption.

<!-- ============================================ -->
<!-- File 204/259: kubernetes.md -->
<!-- ============================================ -->

# Kubernetes

## Topics in this section

### Composable clusters for multi-tenancy in Kubernetes

Explore WEKA‚Äôs multi-tenancy with composable clusters, ensuring full resource isolation and optimal performance through Kubernetes-driven deployment.

### WEKA Operator deployments

Discover how the WEKA Operator streamlines deploying, scaling, and managing the WEKA Data Platform on Kubernetes, delivering high-performance storage for compute-intensive workloads like AI and HPC.

### WEKA Operator day-2 operationsWEKA Operator:CLI

WEKA Operator day-2 operations involve managing hardware, scaling clusters, and optimizing resources to ensure system stability and performance.

<!-- ============================================ -->
<!-- File 205/259: kubernetes_composable-clusters-for-multi-tenancy-in-kubernetes.md -->
<!-- ============================================ -->

---
description:
---

# Composable clusters for multi-tenancy in Kubernetes

## Overview

WEKA enables multi-tenancy by allowing multiple cluster deployments to share the same hardware while maintaining full resource isolation. This is achieved through a Kubernetes Operator that facilitates the composition of resource sets‚Äîincluding hosts, drives, cores, and memory‚Äîinto independent clusters.

The process of creating these composable clusters is efficient, taking only a few minutes. Each cluster is allocated dedicated resources, ensuring consistent performance without interference from other tenants.

### Key benefits

* **Full resource isolation:** WEKA‚Äôs composable cluster model ensures complete isolation across drives, processors, and memory, providing a higher level of security than traditional multi-tenant models that rely on process isolation while sharing hardware resources. This architecture strengthens security boundaries, mitigates risks associated with legacy designs, and ensures both isolation and performance requirements are met.
* **Scalability and performance:** WEKA‚Äôs multi-tenant architecture is aligned with NCP guidelines, including the common network reference architecture. Service providers can benefit from all of WEKA's scalability and performance in a multi-tenant deployment without compromise, while also simplifying network management and optimizing hardware utilization.
* **Enhanced security:** WEKA clusters leverage multiple layers of security. It employs a comprehensive encryption model, with both cluster-wide and per-filesystem customizations, which are fully supported in multi-tenant environments. Tenant management uses least-privileges role definitions to integrate with modernities like Kubernetes and CSI. Multi-tenancy encryption protect file and object data, and sensitive metadata such as file names and timestamps, with only the file size remaining unencrypted. Features such as Organizations add additional delegations within tenant clusters and help protect end user data from administrators while clients maintain full control over encryption management. This approach provides a higher level of protection than traditional models, ensuring data security and compliance.
* **Simplified management:** The WEKA Operator simplifies deployment and management of the WEKA Data Platform. It automates routine storage operations and enhances cluster resilience using custom resources. Real-time monitoring tools provide insights into usage and performance, enabling proactive management and seamless resource expansion.
* **Cost optimization and efficiency:** WEKA maximizes hardware utilization, reduces infrastructure costs, and streamlines configuration to minimize administrative overhead. This approach optimizes resource efficiency while lowering operational expenses.

## Composable clusters for multi-tenancy deployment at a glance

1. The WEKA Operator monitors the current state of the Kubernetes cluster.
2. When there are changes to WEKA custom resource definitions, the Operator applies the changes to the running Kubernetes cluster.
3. The changes are composed sets of resources, which bring about the creation of a new cluster, perform an expansion or contraction of an existing cluster, decommission a cluster, or perform an upgrade.

## Summary

WEKA‚Äôs multi-tenant architecture delivers a highly secure, scalable, and cost-efficient solution for service providers and enterprises. Composable clusters deliver full resource isolation and automated management to meet the ever-evolving demands of modern AI, ML, and high-performance computing environments.

For detailed deployment procedures and day-2 operations see:

*
*

<!-- ============================================ -->
<!-- File 206/259: kubernetes_weka-operator-deployments.md -->
<!-- ============================================ -->

---
description:
---

# WEKA Operator deployments

## Overview

The WEKA Operator simplifies deploying, managing, and scaling the WEKA Data Platform within a Kubernetes cluster. It provides custom Kubernetes resources that define and manage WEKA components effectively.

By integrating WEKA's high-performance storage into Kubernetes, the Operator supports compute-intensive applications like AI, ML, and HPC. This enhances data access speed and boosts overall performance.

The WEKA Operator automates tasks, enables periodic maintenance, and ensures robust cluster management. This setup provides resilience and scalability across the cluster. With its persistent, high-performance data layer, the WEKA Operator enables efficient management of large datasets, ensuring scalability and efficiency.

Note: **Target audience:** This guide is intended exclusively for experienced Kubernetes cluster administrators. It provides detailed procedures for deploying the WEKA Operator on a Kubernetes cluster that meets the specified requirements in the #id-2.-prepare-kubernetes-environment section.

### WEKA Operator backend deployment overview

The WEKA Operator backend deployment integrates various components within a Kubernetes cluster to deploy, manage, and scale the WEKA Data Platform effectively.

#### How it works

* **Local Server Setup**: This setup integrates Kubernetes with the WekaCluster custom resources (CRDs) and facilitates WEKA Operator installation through Helm. Configuring Helm registry authentication provides access to the necessary CRDs and initiates the operator installation.
* **WekaCluster CR**: The WekaCluster CR defines the WEKA cluster‚Äôs configuration, including storage, memory, and resource limits, while optimizing memory and CPU settings to prevent out-of-memory errors. Cluster and container management also support operational tasks through on-demand executions (through WekaManualOperation) and scheduled tasks (through WekaPolicy).
* **WEKA Operator**:
  * The WEKA Operator retrieves Kubernetes configurations from WekaCluster CRs, grouping multiple WEKA containers to organize WEKA nodes into a single unified cluster.
  * To enable access to WEKA container images, the Operator retrieves credentials from Kubernetes secrets in each namespace that requires WEKA resources.
  * Using templates, it calculates the required number of containers and deploys the WEKA cluster on Kubernetes backends through a CRD.
  * Each node requires specific Kubelet configurations‚Äîsuch as kernel headers, storage allocations, and huge page settings‚Äîto optimize memory management for the WEKA containers. Data is stored in the `/opt/k8s-weka` directory on each node, with CPU and memory allocations determined by the number of WEKA containers and available CPU cores per node.
* **Driver Distribution Model**: This model ensures efficient kernel module loading and compatibility across nodes, supporting scalable deployment for both clients and backends. It operates through three primary roles:
  * **Distribution Service**: A central repository storing and serving WEKA drivers for seamless access across nodes.
  * **Drivers Builder**: Compiles drivers for specific WEKA versions and kernel targets, uploading them to the Distribution Service. Multiple builders can run concurrently to support the same repository.
  * **Drivers Loader**: Automatically detects missing drivers, retrieves them from the Distribution Service, and loads them using `modprobe`.

### WEKA Operator client deployment overview

The WEKA Operator client deployment uses the WekaClient custom resource to manage WEKA containers across a set of designated nodes, similar to a DaemonSet. Each WekaClient instance provisions WEKA containers as individual pods, creating a persistent layer that supports high availability by allowing safe pod recreation when necessary.

#### How it works

* **Deployment initiation**: The user starts the deployment from a local server, which triggers the process.
* **Custom resource retrieval**: The WEKA Operator retrieves the WekaClient custom resource (CR) configuration. This CR defines which nodes in the Kubernetes cluster run WEKA containers.
* **WEKA containers deployment**: Based on the WekaClient CR, the Operator deploys WEKA containers across the specified Kubernetes client nodes. Each WEKA container instance runs as a single pod, similar to a DaemonSet.
*   **Persistent storage setup**: The WEKA Operator automates the deployment of the WEKA Container Storage Interface (CSI) plugin, which is the standard way to provide persistent storage for applications within Kubernetes. This plugin enables pods (clients) to dynamically provision and mount Persistent Volumes (PVs) from the WEKA system.

    Starting with Operator version 1.7.0, the deployment process has been streamlined:

    * **Embedded CSI plugin:** The CSI plugin is now embedded directly within the WekaClient CR, simplifying its management.
    * **Co-located cluster requirement:** This integrated CSI deployment is only supported when the WEKA cluster and the WEKA clients reside within the same Kubernetes cluster. This is configured by referencing the WEKA cluster in the `targetCluster` field of the WekaClient CR.
* **High availability**: The WEKA containers act as a persistent layer, enabling each pod to be safely recreated as needed. This supports high availability by ensuring continuous service even if individual pods are restarted or moved.

#### WEKA Operator client-only deployment

If the WEKA cluster is outside the Kubernetes cluster but you have workloads inside Kubernetes, you can deploy a WEKA client within the Kubernetes cluster to connect to the external WEKA cluster.

## Deployment workflow

1. Obtain setup information.
2. Prepare Kubernetes environment.
3. Set up driver distribution.
4. Discover drives for WEKA cluster provisioning.
5. Install the WEKA Operator.
6. Install the WekaCluster and WekaClient custom resources.

### 1. Obtain setup information

Before deploying the WEKA Operator in your Kubernetes environment, contact the WEKA Customer Success Team to obtain the necessary setup information.

You need the following credentials to proceed with the deployment:

* Container repository (quay.io)
  * Image pull secrets and Docker:
    * `QUAY_USERNAME`: `example_user`
    * `QUAY_PASSWORD`: `example_password`
    * `QUAY_SECRET_KEY`: `quay-io-robot-secret`
* WEKA Operator version and image version tag
  * For the most current operator and image versions, refer to the WEKA Operator page at https://get.weka.io/ui/operator. From there, you can obtain the latest `WEKA_OPERATOR_VERSION` and `WEKA_IMAGE_VERSION_TAG`.

Gathering this information in advance provides all the required values to complete the deployment workflow efficiently. Use these values to replace the placeholders in the setup files.

### 2. Prepare Kubernetes environment

Ensure the following requirements are met:

* Local server requirements
* Kubernetes cluster and node requirements
* Kubernetes port requirements
* Kubelet requirements
* Image pull secrets requirements

#### **Local server requirements**

1. Ensure access to a server for manual `helm install`, unless a higher-level tool (for example, Argo CD) is used.

```

```bash
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 && chmod 700 get_helm.sh &&./get_helm.sh
```

```

#### **Kubernetes cluster and node requirements**

Ensure that Kubernetes is correctly set up and configured to handle WEKA workloads.

* Minimum Kubernetes version: 1.25
* Minimum OpenShift version: 4.17

1. **Kernel headers**: Ensure kernel headers on each node match the kernel version.
2. **Storage**: Allocate storage on `/opt/k8s-weka` for WEKA containers.\
   Estimate: \~20 GiB per WEKA container + 10 GiB per CPU core in use.
3. **Huge pages configuration**:
   * **Compute core**: 3 GiB of huge pages
   * **Drive core**: 1.5 GiB of huge pages
   * **Client core**: 1.5 GiB of huge pages\
     Check current huge pages with command:\
     `grep Huge /proc/meminfo`
   * Add the appropriate number of huge pages:\
     `sudo sysctl -w vm.nr_hugepages=3000`
   * Set huge pages to persist through reboots:\
     `sudo sh -c 'echo "vm.nr_hugepages = 3000" >> /etc/sysctl.conf'`

#### **Kubernetes port requirements**

Ensure ports availability according to the following table:

 | Purpose | Source | Target | Target Ports | Protocol | Comments |
 | --- | --- | --- | --- | --- | --- |
 | Client connection | Client | Backend | 45000-65000 | TCP/UDP | Clients find free ports dynamically within this range. Not mandatory to define explicitly. |
 | Cluster allocation | WEKA Operator | Cluster Nodes | 35000-35499 (default) | TCP/UDP | Default port range for cluster allocation. Each WEKA cluster requires a unique range of 500 ports (Baseport to Baseport+499). You can override this range, but you must ensure it does not conflict with other clusters. |
 | Backend communication | Backend | Backend | 35000-35499 (default) | TCP/UDP | Default port range for internal backend communication. Each WEKA cluster requires a unique range of 500 ports (Baseport to Baseport+499). Ensure the selected port range is available across all servers. |
 | Port override | Operator API | WekaCluster CR | User-defined | TCP/UDP | Overrides allow specifying ports manually, mainly useful for migrating non-K8s clusters. |

#### **Kubelet requirements**

1. Configure Kubelet with static CPU management to enable exclusive CPU allocation:\
   `reservedSystemCPUs: "0"`\
   `cpuManagerPolicy: static`
2. Check which configmap holds the kubelet config.\
| `kubectl get cm -A | grep kubelet`\ |
   If there are more than one kubelet config, modify the config for worker nodes.
3. Edit the kubelet config map to add the CPU settings.\
   `kubectl edit cm -n kube-system kubelet-config`

#### **Image pull secrets requirements**

* Set up Kubernetes secrets for secure image pulling across namespaces. Apply the secret in all namespaces where WEKA resources are deployed.
* Verify that namespaces are defined and do not overlap to avoid configuration conflicts.

**Example:**

The following example creates a secret for quay.io authentication for both the `weka-operator-system` namespace and the `default` namespace. Repeat as necessary for namespaces. Replace the placeholders with the actual values.

```bash
export QUAY_USERNAME='QUAY_USERNAME' # Replace with the actual value
export QUAY_PASSWORD='QUAY_PASSWORD' # Replace with the actual value

kubectl create ns weka-operator-system
kubectl create secret docker-registry QUAY_SECRET_KEY \ # Replace with the actual value
  --docker-server=quay.io \
  --docker-username=$QUAY_USERNAME \
  --docker-password=$QUAY_PASSWORD \
  --docker-email=$QUAY_USERNAME \
  --namespace=weka-operator-system

kubectl create secret docker-registry QUAY_SECRET_KEY \ # Replace with the actual value
  --docker-server=quay.io \
  --docker-username=$QUAY_USERNAME \
  --docker-password=$QUAY_PASSWORD \
  --docker-email=$QUAY_USERNAME \
  --namespace=default
```

### 3. Install the WEKA Operator

1. **Apply WEKA Custom Resource Definitions (CRDs):** Download and apply the WEKA Operator CRDs to define WEKA-specific resources in Kubernetes. Replace the version placeholder (WEKA_OPERATOR_VERSION) with the actual value.

```

```bash
helm pull oci://quay.io/weka.io/helm/weka-operator --untar --version <WEKA_OPERATOR_VERSION>
kubectl apply -f weka-operator/crds
```

```

2.  **Install the WEKA Operator:** Deploy the WEKA Operator to the Kubernetes cluster. Specify the namespace, image version, and pull secret to enable WEKA‚Äôs resources. Replace the version placeholder (WEKA_OPERATOR_VERSION) with the actual value.

    To install WEKA Operator with the CSI plugin (from `v1.7.0`) run the following command:

```bash
helm upgrade --create-namespace \
    --install weka-operator oci://quay.io/weka.io/helm/weka-operator \
    --namespace weka-operator-system \
    --version <WEKA_OPERATOR_VERSION> \
    --set csi.installationEnabled=true
```

For earlier versions of the WEKA Operator, omit the `--set csi.installationEnabled=true` from the command.

3. **Verify the installation:** Run the following: `kubectl -n weka-operator-system get pod`\
   The returned results should look similar to this:

```
NAME                                               READY  STATUS  RESTARTS   AGE
weka-operator-controller-manager-564bfd6b49-p6k7d   2/2   Running     0      13s
```

### 4. Set up driver distribution

Driver distribution applies to client and backend entities.

1. **Verify driver distribution prerequisites**:
   1. Ensure a WEKA-compatible image (`weka-in-container`) is accessible through the registry and has the necessary credentials (`imagePullSecret`).
   2. Define node selection criteria, especially for the Driver Builder role, to match the kernel requirements of target nodes.
2.  **Set up the driver distribution service and driver builder:** Driver distribution is typically included as part of the operator installation process. Therefore, it is not necessary to install drivers separately. To build and distribute WEKA drivers, the standard approach involves deploying the following components:

    * **drivers-builder container:** One container per combination of Weka version, kernel version, and architecture.
    * **drivers-dist container:** A single container responsible for serving the compiled drivers.
    * **Service:** Exposes the drivers-dist container.

    This setup supports scenarios such as handling multiple kernel versions and executing custom pre-run scripts.Important notes:

    * Deploy multiple drivers-builder containers only if you need to support multiple kernel versions or multiple WEKA versions.
    * Replace placeholder versions with your target WekaClient and WekaCluster versions.
    * The image versions used in the builder containers must match the corresponding WEKA versions.

<details>

<summary>Driver distribution service for WEKA Operator using WekaPolicy, starting from version 1.6.0</summary>

The WEKA operator supports driver distribution deployment using the WEKA policy. When a valid policy is applied, the operator automatically creates the required resources as shown in the examples.

**Requirements:** When configuring driver distribution, the following elements must be preserved exactly as shown in the provided configuration snippets:

* Ports
* Network modes
* Core configurations
* Container name (spec.name)

#### Example 1: Manual deployment of WEKA driver distribution and builder containers

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  name: weka-drivers-dist
  namespace: weka-operator-system
  labels:
    app: weka-drivers-dist
spec:
  agentPort: 60001
  image: quay.io/weka.io/weka-in-container:4.4.2.144-k8s
  imagePullSecret: "quay-io-robot-secret"
  mode: "drivers-dist"
  name: dist
  numCores: 1
  port: 60002
---
apiVersion: v1
kind: Service
metadata:
  name: weka-drivers-dist
  namespace: weka-operator-system
spec:
  type: ClusterIP
  ports:
    - name: weka-drivers-dist
      port: 60002
      targetPort: 60002
  selector:
    app: weka-drivers-dist
---
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  name: weka-drivers-builder-157
  namespace: weka-operator-system
spec:
  agentPort: 60001
  image: quay.io/weka.io/weka-in-container:4.4.2.157-k8s
  imagePullSecret: "quay-io-robot-secret"
  mode: "drivers-builder"
  name: dist # WEKA container name
  numCores: 1
  uploadResultsTo: "weka-drivers-dist"
  port: 60002
  nodeSelector:
    weka.io/supports-backends: "true"
---
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  name: weka-drivers-builder-157-ubuntu-1
  namespace: weka-operator-system
spec:
  agentPort: 60001
  image: quay.io/weka.io/weka-in-container:4.4.2.157-k8s
  imagePullSecret: "quay-io-robot-secret"
  mode: "drivers-builder"
  name: dist # WEKA container name
  numCores: 1
  uploadResultsTo: "weka-drivers-dist"
  port: 60002
  nodeSelector:
    weka.io/supports-backends: "true"
    weka.io/kernel: "6.5.0-45-generic"
  overrides:
    preRunScript: "apt-get update && apt-get install -y gcc-12"
```

#### Example 2: Example: WekaPolicy for enabling local driver distribution

```

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaPolicy
metadata:
  name: weka-drivers
  namespace: weka-operator-system # Specify the namespace where the Weka operator is deployed
spec:
  type: "enable-local-drivers-distribution"
  # Base image used for the drivers-dist container; also used as the default for driver builders
  image: "quay.io/weka.io/weka-in-container:4.4.5.118-k8s.4" # Replace with the target Weka image version
  imagePullSecret: "quay-io-robot-secret" # Replace with your image pull secret for accessing the image registry
  tolerations:
  - key: "example-key"
    operator: "Exists"
    effect: "NoSchedule"
  payload:
    interval: "1m" # Reconciliation interval for the policy
    driverDistPayload: # Required: configuration for driver distribution
      # List of additional Weka images for which drivers should be prebuilt
      # These are in addition to any images detected from existing WekaCluster/WekaClient resources
      ensureImages:
        - "quay.io/weka.io/weka-in-container:4.4.2.157-k8s.2" # Example image for proactive driver build
        - "quay.io/weka.io/weka-in-container:4.4.5.118-k8s.4" # Another example
      # Node selectors defining where builder containers can be scheduled
      # Builders run on nodes matching both these selectors and the discovered kernel/architecture
      nodeSelectors:
        - role: "worker-nodes"
          environment: "production"
        - custom-label: "drivers-build-pool"
      # Optional: Override default label keys for kernel and architecture detection
      # Defaults: weka.io/kernel and weka.io/architecture
      # kernelLabelKey: "custom.io/kernel-version"
      # architectureLabelKey: "custom.io/arch"
      # Optional: Node selector for the driver distribution container
      # Leave empty to allow scheduling on any node
      # distNodeSelector: {}
      # Optional: Script to run in builder containers after kernel validation and before the build process
builderPreRunScript: |
        #!/bin/sh
        apt-get update && apt-get install -y gcc-12
```

```

#### Example 3: Minimal policy for drivers distribution

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaPolicy
metadata:
  name: weka-drivers
  namespace: weka-operator-system
spec:
  image: quay.io/weka.io/weka-in-container:4.4.5.118-k8s.4
  payload:
    driverDistPayload: {}
    interval: 1m
  type: enable-local-drivers-distributio
```

</details>

<details>

<summary>Driver distribution service for WEKA Operator version 1.4.x</summary>

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  name: weka-drivers-dist
  namespace: default
  labels:
    app: weka-drivers-dist
spec:
  agentPort: 60001
  image: quay.io/weka.io/weka-in-container:<WEKA_IMAGE_VERSION_TAG> # Replace with the actual value
  imagePullSecret: "<QUAY_SECRET_KEY>" # Replace with the actual value
  mode: "drivers-dist"
  name: dist
  numCores: 1
  port: 60002
---
apiVersion: v1
kind: Service
metadata:
  name: weka-drivers-dist
  namespace: default
spec:
  type: ClusterIP
  ports:
    - name: weka-drivers-dist
      port: 60002
      targetPort: 60002
  selector:
    app: weka-drivers-dist
---
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  name: weka-drivers-builder
  namespace: default
spec:
  agentPort: 60001
  image: quay.io/weka.io/weka-in-container:<WEKA_IMAGE_VERSION_TAG> # Replace with the actual value
  imagePullSecret: "<QUAY_SECRET_KEY>" # Replace with the actual value
  mode: "drivers-loader"
  name: dist
  numCores: 1
  port: 60002
```

</details>

Note: Ensure that `nodeSelector` or `nodeAffinity` aligns with the kernel requirements of the build nodes.

3. Save the manifest above to `weka-driver.yaml` , and apply it:\
   `kubectl apply -f weka-driver.yaml`

### 5. Discover drives for WEKA cluster provisioning

To provision drives for a WEKA cluster, each drive must go through a discovery process. This process ensures that all drives are correctly identified, accessible, and ready for use within the cluster.

The discovery process involves the following key actions:

* **Node updates during discovery**
  * Each node is annotated with a list of known serial IDs for all drives accessible to the operator, providing a unique identifier for each drive.
  * An extended resource, `weka.io/drives`, is created to indicate the number of drives that are ready and available on each node.
* **Available drives**
  * Only healthy, unblocked drives are marked as available. Drives that are manually flagged due to issues such as corruption or other unrecoverable errors are excluded from the available pool to ensure cluster stability.

**Drive discovery steps**

1. **Sign drives**\
   Each drive receives a WEKA-specific signature, marking it as ready for discovery and integration into the cluster.
2. **Discover drives**\
   The signed drives are detected and prepared for cluster operations. If drives already have the WEKA signature, only the discovery step is required to verify and track them in the cluster.

**Drive discovery methods**

The WEKA system supports two primary methods for drive discovery:

* **WekaManualOperation**\
  A one-time operation that performs both drive signing and discovery, suitable for manual provisioning.
* **WekaPolicy**\
  An automated, policy-driven approach that performs periodic discovery across all matching nodes. The `WekaPolicy` method operates on an event-driven model, initiating discovery immediately when relevant changes (such as node updates or drive additions) are detected.

**Operations examples**

<details>

<summary>Sign drivers using the WekaPolicy starting from WEKA Operator 1.6.x</summary>

Drive containers will be scheduled on nodes with available signed drives.

To identify drives that can be used by WEKA and sign them, apply the following policy:

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaPolicy
metadata:
  name: sign-drives
  namespace: weka-operator-system # Replace with your namespace
spec:
  type: sign-drives
  payload:
    signDrivesPayload:
      type: "all-not-root"
```

**Drive selection types:**

* `all-not-root`: Signs all block devices except the root device.
* `aws-all`: AWS-specific, detects NVMe devices by AWS PCI identifiers.
* `device-paths`: Lists specific device paths. Each node presents its subset of this list.

</details>

<details>

<summary>Sign specific drivers manually in WEKA Operator 1.4.x</summary>

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaManualOperation
metadata:
  name: sign-specific-drives
  namespace: weka-operator-system
spec:
  action: "sign-drives"
  image: quay.io/weka.io/weka-in-container:WEKA_IMAGE_VERSION_TAG # Replace with the actual value
  imagePullSecret: "QUAY_SECRET_KEY"  \ # Replace with the actual value
  payload:
    signDrivesPayload:
      type: device-paths
      nodeSelector:
\t      weka.io/supports-backends: "true"
      devicePaths:
        - /dev/nvme0n1
        - /dev/nvme1n1
        - /dev/nvme2n1
        - /dev/nvme3n1
        - /dev/nvme4n1
        - /dev/nvme5n1
        - /dev/nvme6n1
        - /dev/nvme7n1
```

</details>

<details>

<summary>Discover drives</summary>

The following example initiates a drive discovery operation:

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaManualOperation
metadata:
  name: discover-drives
  namespace: weka-operator-system
spec:
  action: "discover-drives"
  image: quay.io/weka.io/weka-in-container:WEKA_IMAGE_VERSION_TAG # Replace with the actual value
  imagePullSecret: "QUAY_SECRET_KEY" # Replace with the actual value
  payload:
    discoverDrivesPayload:
      nodeSelector:
\t      weka.io/supports-backends: "true"
```

Key fields:

* `nodeSelector` (payload): Limits the operation to specific nodes.
* `tolerations` (spec): Supports Kubernetes tolerations for high-level objects like WekaCluster and WekaClient. Only `tolerations` are supported for WekaManualOperation, WekaContainer, and WekaPolicy.

</details>

### 6. Install the WekaCluster and WekaClient custom resources

This procedure provides step-by-step instructions for deploying the WekaCluster and WekaClient Custom Resources (CRs) in a Kubernetes cluster. Follow these procedures in sequence if both components are required. Begin with the **WekaCluster CR**, then create the necessary **client secret**, and finally deploy the **WekaClient CR**.

#### Step 1: Install the WekaCluster CR

To deploy a WEKA cluster backend using the WekaCluster CR, perform the following:

1. Prerequisites:
   1. Ensure the **driver distribution service** is configured. This is the same service used by WEKA clients. See #id-4.-set-up-driver-distribution.
   2. Use either the `WekaManualOperation` (recommended for initial deployments) or `WekaPolicy` to sign and discover drives. See #id-5.-discover-drives-for-weka-cluster-provisioning.
2.  Create a manifest file (for example, weka-cluster.yaml) with the required configuration:

    ```yaml
    apiVersion: weka.weka.io/v1alpha1
    kind: WekaCluster
    metadata:
      name: cluster-dev
      namespace: default
    spec:
      template: dynamic
      dynamicTemplate:
        computeContainers: 6
        driveContainers: 6
        numDrives: 1
      image: quay.io/weka.io/weka-in-container:WEKA_IMAGE_VERSION_TAG # Replace with actual image tag
      nodeSelector:
        weka.io/supports-backends: "true"
      driversDistService: "https://weka-drivers-dist.weka-operator-system.svc.cluster.local:60002"
      imagePullSecret: "QUAY_SECRET_KEY" # Replace with the actual secret
      network:
        udpMode: true
        ethDevice: br-ex
    ```

<details>

<summary>WekaCluster key parameters and configurations</summary>

* **template**: Only `dynamic` is currently supported. Future templates will include `capacity` and `performance`.
*   **dynamicTemplate**: Configure dynamic settings for compute and drive containers within this template.

    ```yaml
    dynamicTemplate:
      computeContainers: <number>
      driveContainers: <number>
      numDrives: <number>
    ```
* **image**, **imagePullSecret**, **driversDistService**, **nodeSelector**, **tolerations**, **rawTolerations**, and **network** are configured similarly to the WekaClient CR.
* **roleNodeSelector**: Defines scheduling by role (compute, drive, s3) through a map of node selectors.
*   **WekaHome Configuration**: Sets the WekaHome endpoint and certificate.

    ```yaml
    wekaHome:
      endpoint: "https://custom-domain.lan:30443"
      cacertSecret: "weka-home-cacert"
    ```
* **ipv6**: Enables IPv6 (default is false).
* **additionalMemory**: Adds memory per role beyond default allocations.
* **ports**: Override default port assignments if needed, such as for cluster migration.
* **operatorSecretRef** and **expandEndpoints**: Parameters used exclusively for migration, supporting migration-by-healing from a non-K8s environment to K8s.
* **Hugepages Offsets**: Specifies offsets for hugepage allocations for drives, compute, and S3 (for example, `driveHugepagesOffset`).

</details>

3. Apply the WekaCluster CR:

```
kubectl apply -f weka-cluster.yaml
```

#### Step 2: Create WEKA cluster client secret

Before deploying a WekaClient CR, create a Kubernetes Secret with credentials required to join the WEKA cluster.

1. Prepare the secret YAML: Create a file named secret.yaml:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: weka-cluster-dev  # The wekaSecretRef in the WekaClient CR must much this secret name
  namespace: weka-operator-system
type: Opaque
data:
  org: <base64-encoded-org>
  join-secret: <base64-encoded-join-secret>
  password: <base64-encoded-password>
  username: <base64-encoded-username>
```

Note: Replace all placeholder with base64-encoded values provided by your WEKA backend.

2. Apply the secret:

```
kubectl apply -f secret.yaml
```

#### Step 3: Install the WekaClient CR

The WekaClient CR deploys WekaContainers across designated Kubernetes nodes, similar to a DaemonSet but without automatic pod cleanup.

**WekaClient specification (reference)**\
Key configurable fields in the `WekaClientSpec`:

```go
type WekaClientSpec struct {
    Image               string            `json:"image"`                   // Image to be used for WekaContainer
    ImagePullSecret     string            `json:"imagePullSecret,omitempty"` // Secret for pulling the image
    Port                int               `json:"port,omitempty"`           // If unset (0), WEKA selects a free port from PortRange
    AgentPort           int               `json:"agentPort,omitempty"`      // If unset (0), WEKA selects a free port from PortRange
    PortRange           *PortRange        `json:"portRange,omitempty"`      // Used for dynamic port allocation
    NodeSelector        map[string]string `json:"nodeSelector,omitempty"`   // Specifies nodes for deployment
    WekaSecretRef       string            `json:"wekaSecretRef,omitempty"`  // Reference to Weka secret
    NetworkSelector     NetworkSelector   `json:"network,omitempty"`        // Defines network configuration
    DriversDistService  string            `json:"driversDistService,omitempty"` // URL for driver distribution service
    DriversLoaderImage  string            `json:"driversLoaderImage,omitempty"` // Image for drivers loader
    JoinIps             []string          `json:"joinIpPorts,omitempty"`    // IPs to join for cluster setup
    TargetCluster       ObjectReference   `json:"targetCluster,omitempty"`  // Reference to target cluster
    CpuPolicy           CpuPolicy         `json:"cpuPolicy,omitempty"`      // CPU policy, e.g., "auto," "shared," "dedicated," etc.
    CoresNumber         int               `json:"coresNum,omitempty"`       // Number of cores to use
    CoreIds             []int             `json:"coreIds,omitempty"`        // Specific core IDs to use
    TracesConfiguration *TracesConfiguration `json:"tracesConfiguration,omitempty"` // Trace settings
    Tolerations         []string          `json:"tolerations,omitempty"`    // Tolerations for nodes
    RawTolerations      []v1.Toleration   `json:"rawTolerations,omitempty"` // Detailed toleration settings
    AdditionalMemory    int               `json:"additionalMemory,omitempty"` // Additional memory allocation
    WekaHomeConfig      WekahomeClientConfig  `json:"wekaHomeConfig,omitempty"` // Deprecated field
    WekaHome            *WekahomeClientConfig `json:"wekaHome,omitempty"`       // Deprecated field
    UpgradePolicy       UpgradePolicy     `json:"upgradePolicy,omitempty"`   // Policy for handling upgrades
}
```

<details>

<summary>WekaClient key parameters and configurations</summary>

* **image**: Specifies the image to use for the container.
* **imagePullSecret**: Defines the secret to use for pulling the image, which is propagated into the pod.
* **port** and **agentPort**:
  * **agentPort**: A single port used by the agent.
  * **port**: Represents a range of 100 ports. This range may be reduced in the future, as it is not fully utilized by clients and is shared on the WEKA side.
*   **portRange**:\
    Instead of specifying individual ports, a range can be defined. The operator automatically finds an available port instead of using the same one across all servers.

    ```yaml
    portRange:
      basePort: 45000
    ```
* **nodeSelector**: Selects the node where the WekaContainer will be scheduled.
*   **network**: Defines the network device for WEKA to use. By default, WEKA runs in UDP mode if no network device is specified. If using an Ethernet device, specify the device name (for example, `mlnx0`).

    ```yaml
    network:
      ethDevice: mlnx0
    ```
* **driversDistService**:\
  A reference to the distribution service for drivers.
*   **joinIpPorts**: Used when the WEKA cluster and WEKA clients are not in the same Kubernetes cluster.

    ```yaml
    joinIpPorts: ["10.0.1.168:16101"]
    ```
*   **targetCluster**: Used when the WEKA cluster and WEKA clients are in the same Kubernetes cluster.

    ```yaml
    targetCluster:
      name: cluster-dev
      namespace: default
    ```
* **coresNum**: Specifies the number of full cores to use for each WekaContainer.
* **cpuPolicy**:\
  Default value is `auto`, which automatically detects whether nodes are running with hyperthreading and allocates cores accordingly.
  * **Example**: 2 WEKA cores = 2 full cores, reserving 5 hyperthreads for a pod.
  * **coreIds**: Used in combination with `cpuPolicy: manual` for manual core allocation.\
    **Note**: Unless advised by WEKA support, avoid using any policy other than `auto`.
* **tracesConfiguration**: Configures trace capacity allocations.
*   **tolerations** and **rawTolerations**:

    * **tolerations**: A list of strings that expand to `NoSchedule` and `NoExecution` tolerations for existing keys.
    * **rawTolerations**: A list of Kubernetes toleration objects.

    ```yaml
    tolerations:
      - simple-toleration
      - another-one
    rawTolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "weka-cluster"
        effect: "NoSchedule"
    ```
* **additionalMemory**: Specifies additional memory in megabytes for cases when default memory allocation is insufficient.\
  **Note**: Default memory allocations are typically set for 90%+ utilization.
*   **wekaHome**: Configures the Weka home directory to use. Defaults to the Weka cloud home.\
    The primary configuration of Weka home is in the `WekaCluster` CR, but WekaClient can also specify a `cacert` for the client. This certificate is placed on client pods to connect to Weka Home.

    ```yaml
    wekaHome:
      cacertSecret: "weka-home-cacert"
    ```
* **upgradePolicy**:\
  Defines how the WekaContainers are upgraded.
  * **rolling** (default): WekaContainers are updated one by one.
  * **manual**: WekaContainers are set to a new version, but the pod is not deleted until manually triggered. This gives the user control over when to update.
  * **all-at-once**: All WekaContainers are upgraded simultaneously after the image is changed.
*   **gracefulDestroyDuration**:\
    Specifies the duration for which the cluster remains in a paused state, keeping local data and drive allocations while deleting all pods.

    * **Default**: 24 hours.
    * **Note**: In case of accidental cluster deletion, override this duration with a larger value and contact Weka support for recovery procedures. This is a safety measure, not a pause/unpause feature.

    To override the graceful destroy duration:

```

```
kubectl patch WekaCluster cluster-dev -n weka-operator-system --type='merge' -p='{"status":{"overrideGracefulDestroyDuration": "10000h"}}' --subresource=status
```

```

To release the cluster (allow full deletion):

```

```
kubectl patch WekaCluster cluster-dev -n weka-operator-system --type='merge' -p='{"status":{"overrideGracefulDestroyDuration": "0"}}' --subresource=status
```

```

</details>

Note: **Label propagation behavior:** All labels are automatically propagated from parent objects to the child objects they create. The propagation behavior is as follows:
* WekaContainer propagates labels to the corresponding Pods.
* WekaCluster propagates labels to the WekaContainer objects it creates.
* WekaPolicy propagates labels to the WekaContainer objects it creates.
* WekaClient propagates labels to the WekaContainer objects it creates.

<details>

<summary>Example: Connect to an internal WEKA cluster</summary>

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaClient
metadata:
  name: cluster-dev-clients
spec:
  image: quay.io/weka.io/weka-in-container:WEKA_IMAGE_PLACEHOLDER
  imagePullSecret: "QUAY_SECRET_KEY" # Replace with the actual value
  driversDistService: "https://weka-drivers-dist.weka-operator-system.svc.cluster.local:60002"
  portRange:
    basePort: 46000
  nodeSelector:
    weka.io/supports-clients: "true"
  wekaSecretRef: weka-cluster-dev # Must match secret name created using secret yaml
  targetCluster:
    name: cluster-dev
    namespace: default
  network:
    ethDevice: mlnx0
```

</details>

<details>

<summary>Example: Connect to an external WEKA cluster</summary>

```
apiVersion: weka.weka.io/v1alpha1
kind: WekaClient
metadata:
  name: cluster-dev-clients
spec:
  image: quay.io/weka.io/weka-in-container:WEKA_IMAGE_PLACEHOLDER
  imagePullSecret: "QUAY_SECRET_KEY" # Replace with the actual value
  driversDistService: "https://weka-drivers-dist.weka-operator-system.svc.cluster.local:60002"
  portRange:
    basePort: 46000
  nodeSelector:
    weka.io/supports-clients: "true"
  wekaSecretRef: weka-cluster-dev # Must match secret name created using secret yaml
  joinIpPorts: ["10.0.2.137:16101"] # Replace with backend or LB IP:port
  network:
    ethDevice: mlnx0

```

</details>

Apply the manifest:

```
kubectl apply -f weka-client.yaml
```

#### Step 4: Next steps

After deploying the WekaCluster and WekaClient CRs:

1. Monitor their status using `kubectl get wekaClusters` and `kubectl get wekaClients`.
2. After deploying the `WekaCluster` and `WekaClient` Custom Resources (CRs), perform one of the following steps based on your WEKA Operator version.
   *   **For WEKA Operator v1.7.0 and newer:**

       With newer WEKA Operator versions, the CSI plugin, necessary secrets, and a default `StorageClass` are configured automatically.

       * **StorageClass naming:** A `StorageClass` is automatically created using the pattern `weka-<groupName>-<fsName>`. Any non-standard mount options will be reflected in the name (e.g., `weka-<groupName>-<fsName>-forcedirect`).
       * **Disabling auto-creation:** To prevent the automatic creation of a `StorageClass`, you can set `csi.storageClassCreationDisabled: true` in your Helm values or operator configuration.
       * **Next steps:** You can now proceed to create a Persistent Volume Claim (PVC) or define additional `StorageClass` objects. For instructions, see the  topic.
   *   **For WEKA Operator v1.6.2 and older:**

       If you are using an older WEKA Operator (v1.6.2 and below) or are not using the `targetCluster` parameter, you must install the CSI plugin manually.

       * **Next steps:** Proceed to the  for complete installation instructions.

## Upgrade the WEKA Operator

Upgrading the WEKA Operator involves updating the Operator and managing `wekaClient` configurations to ensure all client pods operate on the latest version. Additionally, each WEKA version requires a new builder instance with a unique `wekaContainer` metadata name, ensuring compatibility and streamlined management of version-specific resources.

**Procedure:**

1. **Upgrade the WEKA Operator**\
   Follow the steps in Install the WEKA Operator using the latest version. Re-running the installation process with the updated version upgrades the WEKA Operator without requiring additional setup.
2.  **Configure upgrade policies for `wekaClient`**\
    The `upgradePolicy` parameter in the `wekaClient` Custom Resource (CR) specification controls how client pods are updated when the WEKA version changes. Options include:

    * **rolling**: The operator automatically updates each client pod sequentially, replacing one pod at a time to maintain availability.
    * **manual**: No automatic pod replacements are performed by the operator. Manual deletion of each client pod is required, after which the pod restarts with the updated version. Use `kubectl delete pod <pod-name>` to delete each pod manually.
    * **all-at-once**: The operator updates all client pods simultaneously, applying the new version cluster-wide in a single step.

    To apply the upgrade, update the `weka-in-container` version by:

    * Directly editing the version with `kubectl edit` on the `wekaClient` CR.
    * Modifying the client configuration manifest, then reapplying it with `kubectl apply -f <manifest-file>`.
3. **Create a new builder Instance for each WEKA version**\
   Rather than updating existing builder instances, create a new instance of the builder with each WEKA kernel version. Each builder must have a unique `wekaContainer` metadata name to support version-specific compatibility.
   * **Create a new builder**: For each WEKA version, create a new builder instance with an updated `wekaContainer` meta name that corresponds to the new version. This ensures that clients and resources linked to specific kernel versions can continue to operate without conflicts.
   * **Cleanup outdated builders**: Once the upgrade is validated and previous versions are no longer needed, you can delete outdated builder instances associated with those older versions. This cleanup step optimizes resources but allows you to maintain multiple builder instances if supporting different kernel versions is required.

## Delete a WekaCluster

When you delete a WekaCluster, the system enforces a 24-hour grace period before completing the removal. To expedite this process and delete the cluster immediately, you can set the graceful destroy duration to zero before initiating the deletion.

**Procedure**

1.  Run the following command to set the graceful destroy duration to zero:

    ```

    ```bash
    kubectl patch WekaCluster <cluster name> --type='merge' -p='{"spec":{"gracefulDestroyDuration": "0"}}'
    ```

```

    **Where:**

    * `<cluster name>`: Specifies the name of your WekaCluster.
2.  Run the following command to delete the WekaCluster:

    ```bash
    kubectl delete WekaCluster <cluster name> --namespace <cluster namespace>
    ```

    **Where:**

    * `<cluster name>`: Specifies the name of the WekaCluster you want to delete.
    * `<cluster namespace>`: Specifies the namespace where the cluster is located.

## Best practices

### Preloading images

To optimize runtime and minimize delays, preloading images during the reading or preparation phase can significantly reduce waiting time in subsequent steps. Without preloading, some servers may sit idle while images download, leading to further delays when all servers advance to the next step.

<details>

<summary>Sample DaemonSet configuration for preloading images</summary>

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: weka-preload
  namespace: default
spec:
  selector:
    matchLabels:
      app: weka-preload
  template:
    metadata:
      labels:
        app: weka-preload
    spec:
      imagePullSecrets:
        - name: quay-secret
        - name: QUAY_SECRET_KEY" # Replace with the actual value
      nodeSelector:
        weka.io/supports-backends: "true"
      tolerations:
        - key: "key1"
          operator: "Equal"
          value: "value1"
          effect: "NoSchedule"
        - key: "key2"
          operator: "Exists"
          effect: "NoExecute"
      containers:
        - name: weka-preload
          image: quay.io/weka.io/weka-in-container:WEKA_IMAGE_VERSION_TAG # Replace with the actual value
          command: ["sleep", "infinity"]
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
```

</details>

### Display custom fields

WEKA Custom Resources enable enhanced observability by marking certain display fields. While `kubectl get` displays only a limited set of fields by default, using the `-o wide` option or exploring through `k9s` allows you to view all fields.

**Example command to quickly assess WekaContainer status:**

```bash
kubectl get wekacontainer -o wide --all-namespaces
```

**Example output:**

```bash
NAMESPACE              NAME                                                       STATUS          MODE              AGE     DRIVES COUNT   WEKA CID
weka-operator-system   cluster-dev-clients-34.242.2.16                            Running         client            64s
weka-operator-system   cluster-dev-clients-52.51.10.75                            Running         client            64s                    12
weka-operator-system   cluster-dev-compute-16fd029f-8aad-487c-be32-c74d70350f69   Running         compute           6m49s                  9
weka-operator-system   cluster-dev-compute-33f54d4b-302d-4d85-9765-f6d9a7a31d02   Running         compute           6m50s                  8

... (additional rows)

weka-operator-system   weka-dsc-34.242.2.16                                       PodNotRunning   discovery         64s
```

This view provides a quick status overview, showing progress and resource allocation at a glance.

## Troubleshooting

This section provides guidance for resolving common deployment issues with WEKA Operator.

### Pod stuck in pending state

Describe the pod to identify the scheduling issue (using Kubernetes native reporting).

If the pod is blocked on `weka.io/drives`, it indicates that the operator was unable to allocate the required drives for the corresponding WekaContainer. This issue may occur if the user has requested more drives than are available on the node or if there are too many `driveContainers` already running.

Ensure the  drives are signed and the number of drives corresponds to the requested in the spec of the WekaCluster.

If there‚Äôs an image pull failure, verify your `imagePullSecret`. Each customer must have a unique robot secret for quay.io.

### Pod in ‚Äúwekafsio driver not found‚Äù loop

Check the logs for this message and see  for further steps.

### CSI not functioning

Ensure the `nodeSelector` configurations on both the CSI installation and the WekaClient match.

## Appendix: Kubernetes Glossary

<details>

<summary>Kubernetes Glossary</summary>

Learning Kubernetes is outside the scope of this document. This glossary covers essential Kubernetes components and concepts to support understanding of the environment. It is provided for convenience only and does not replace the requirement for Kubernetes knowledge and experience.

**Pod**

A Pod is the smallest, most basic deployable unit in Kubernetes. It represents a single instance of a running process in a cluster, typically containing one or more containers that share storage, network, and a single IP address. Pods are usually ephemeral; when they fail, a new Pod is created to replace them.

**Node**

A Node is a physical or virtual machine that serves as a worker in a Kubernetes cluster, running Pods and providing the necessary compute resources. Each Node is managed by the Kubernetes control plane and runs components like kubelet, kube-proxy, and a container runtime.

**Namespace**

A Namespace is a Kubernetes resource that divides a cluster into virtual sub-clusters, allowing for isolated environments within a single physical cluster. Namespaces help organize resources, manage permissions, and enable resource quotas within a cluster.

**Label**

Labels are key-value pairs attached to Kubernetes objects, like Pods and Nodes, used for identification and grouping. Labels facilitate organizing, selecting, and operating on resources, such as scheduling workloads based on specific node labels.

**Taint**

Taints are properties applied to nodes to restrict the schedule of pods. A taint on a Node prevents Pods without a matching toleration from being scheduled there. Taints often prevent certain workloads from running on specific Nodes unless explicitly permitted.

**Toleration**

A Toleration is a property of Pods that enables them to be scheduled on Nodes with matching taints. Tolerations work with taints to control, which workloads can run on specific Nodes in the cluster.

**Affinity and Anti-Affinity**

Affinity rules allow administrators to specify which Nodes or other Pods a given Pod should run nearby. Anti-affinity rules define the opposite: which Pods should not be scheduled near each other. These rules help with optimal resource allocation and reliability.

**Selector**

Selectors are expressions that enable filtering and selecting specific resources within the Kubernetes API. Node selectors, for example, specify the Nodes on which a Pod can run by matching their labels.

**Deployment**

A Deployment is a higher-level object for managing and scaling applications in Kubernetes. It defines the desired state for Pods and ensures they are created, updated, and scaled to maintain that state.

**DaemonSet**

A DaemonSet ensures that a specific Pod runs on all (or some) Nodes in the cluster, often used for tasks like logging, monitoring, or networking, where each Node requires the same component.

**ReplicaSet**

A ReplicaSet ensures a specified number of replicas of a Pod are running at any given time, allowing for redundancy and high availability. It is often managed by a Deployment, which abstracts the ReplicaSet management.

**Service**

A Service is an abstraction that defines a logical set of Pods and provides a stable network endpoint for access. It enables reliable communication between different Pods or external services, regardless of the individual Pods‚Äô IP addresses.

**ConfigMap**

A ConfigMap is a Kubernetes resource used to store application configuration data. It separates configuration from application code, enabling easy updates without redeploying the entire application.

**Secret**

A Secret is a Kubernetes object used to store sensitive information, such as passwords, tokens, or keys. Like ConfigMaps, secrets are designed for confidential data, and Kubernetes provides mechanisms for securely managing and accessing them.

**Persistent Volume (PV)**

A Persistent Volume is a storage resource in Kubernetes that exists independently of any particular Pod. PVs provide long-term storage that persists beyond the lifecycle of individual Pods.

**Persistent Volume Claim (PVC)**

A Persistent Volume Claim is a request for storage made by a Pod. PVCs allow Pods to use persistent storage resources, which are dynamically or statically provisioned in the cluster.

**Ingress**

Ingress is a Kubernetes resource that manages external access to services within a cluster, typically via HTTP/HTTPS. Ingress enables load balancing, SSL termination, and routing to various services based on the request path.

**Container Runtime**

The container runtime is the underlying software that runs containers on a Node. Kubernetes supports multiple container runtimes, such as Docker, containers, and CRI-O.

**Operator**

An Operator is a method of packaging, deploying, and managing a Kubernetes application or service. It often provides automated management and monitoring for complex applications in Kubernetes clusters.

</details>

<!-- ============================================ -->
<!-- File 207/259: kubernetes_weka-operator-deployments_deploy-the-weka-client-on-amazon-eks.md -->
<!-- ============================================ -->

---
description:
---

# Deploy the WEKA client on Amazon EKS

The WEKA client enables Kubernetes workloads on Amazon EKS to connect to and access a WEKA cluster deployed in AWS. Client pods are managed using Kubernetes custom resources and require coordination with the WEKA Operator for installation and lifecycle management.

#### Prerequisites

1. **Verify network access to the WEKA driver distribution service:** Ensure that the deployment environment has network access to `https://drivers.weka.io`. The WEKA client pods automatically download the required driver components from this public distribution service to interface with the WEKA filesystem. For more information, see .
2. **Verify security groups and** **NACL** **configuration:** The WEKA client requires the ports specified in the following topics:
   * #required-ports (in the **WEKA Prerequisites and compatibility** topic).
   * #kubernetes-port-requirements (in the **WEKA Operator deployment** topic).
3. **Obtain setup information:** Contact the [WEKA Customer Success Team](../../support/getting-support-for-your-weka-system) to obtain the necessary setup information. You need the following credentials to proceed with the deployment:
   * Container repository (quay.io)
     * Image pull secrets and Docker:
       * `QUAY_USERNAME`: `example_user`
       * `QUAY_PASSWORD`: `example_password`
   *   WEKA Operator version and image version tag

       * For the most current operator and image versions, refer to the WEKA Operator page at https://get.weka.io/ui/operator. From there, you can obtain the latest `WEKA_OPERATOR_VERSION` and `WEKA_IMAGE_VERSION_TAG`.

       Gathering this information in advance provides all the required values to complete the deployment workflow efficiently. Use these values to replace the placeholders in the setup files.
4. **A deployed WEKA cluster is required:** Use the same subnets and security groups from the WEKA cluster when configuring the EKS environment for client deployment. For guidance on deploying the WEKA cluster with Terraform, see .
5. **EKS deployment prerequisites:** Configure the EKS cluster with the following global settings:
   * **IAM Role and policy configuration:** Configure the appropriate IAM roles and policies for both the EKS cluster and its worker nodes:
   *   **Cluster IAM role**

       Attach the IAM role associated with the EKS cluster:

       * AmazonEKSBlockStoragePolicy
       * AmazonEKSClusterPolicy
       * AmazonEKSComputePolicy
       * AmazonEKSLoadBalancingPolicy
       * AmazonEKSNetworkingPolicy

       (These policies are included when using the recommended IAM role: **EKS - Auto Cluster**.)
   *   **Node group IAM role**\
       Attach the IAM role for the managed node groups, which host the worker nodes running the WEKA operator:

       * AmazonEC2ContainerRegistryPullOnly
       * AmazonEKSWorkerNodeMinimalPolicy

       Additionally, to ensure proper networking functionality, attach the following policy:

       * AmazonEKS_CNI_Policy

       (These policies are included in the recommended IAM role: **EKS - Auto Node**.)
   * **CPU allocation**
     * Enable the Static CPU allocation.
     * Reserve Core 0.
   * **Hugepages allocation**
     * Reserve 1.5 GiB for the client core.

<details>

<summary>Example: How to set up CPU allocation and hugepages </summary>

Add to the worker nodes launch template the following sections:

```

```bash
#Set up core alloaction
CONFIG_PATH="/etc/kubernetes/kubelet/kubelet-config.json"
cat <<< $(jq '.systemReserved.cpu = "1"' "$CONFIG_PATH") > "$CONFIG_PATH"
cat <<< $(jq '.cpuManagerPolicy = "static"' "$CONFIG_PATH") > "$CONFIG_PATH"

#Set up hupepages
if [ $(cat /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages) == "` + hugepagesStr + `" ]; then
 echo hugepages already set
else
cat <<EOF > /etc/systemd/system/hugepages.service

[Unit]
Description=Hugepages

[Service]
Type=oneshot
ExecStart=/bin/sh -c 'echo "` + hugepagesStr + `" > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages;'
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF
systemctl daemon-reload
systemctl enable hugepages
systemctl restart hugepages
fi
```

```

</details>

#### Procedure

1. **Label the EKS nodes:** Label EKS worker nodes intended for WEKA client deployment. Apply the label to each node designated to host WEKA client pods.

```bash
kubectl label nodes <node-name> weka.io/supports-clients=true
```

2. **Create namespaces and configure Quay authentication**: Set up the required Kubernetes namespaces (`weka-operator-system` and `default`), and create a Docker registry secret to authenticate access to WEKA container images hosted on Quay:

```bash
kubectl create namespace weka-operator-system

kubectl create secret docker-registry quay-io-robot-secret \
   --docker-server=quay.io \
   --docker-username=$QUAY_USERNAME \
   --docker-password=$QUAY_PASSWORD \
   --docker-email=$QUAY_USERNAME \
   --namespace=weka-operator-system

kubectl create secret docker-registry quay-io-robot-secret \
   --docker-server=quay.io \
   --docker-username=$QUAY_USERNAME \
   --docker-password=$QUAY_PASSWORD \
   --docker-email=$QUAY_USERNAME \
   --namespace=default
```

3. **Install the WEKA Operator:** Install the WEKA Operator using the official Helm chart:

```bash
helm upgrade \
  --install weka-operator oci://quay.io/weka.io/helm/weka-operator \
  --namespace weka-operator-system \
  --version v1.6.2 \
  --set imagePullSecret=quay-io-robot-secret
```

4. **Configure NICs:** Create the `ensure-nics.yaml` manifest to enable multi-NIC support on selected nodes:

```

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaPolicy
metadata:
  name: ensure-nics-policy
  namespace: weka-operator-system
spec:
  type: "ensure-nics"
  image: quay.io/weka.io/weka-in-container:4.4.5.118-k8s.4
  imagePullSecret: "quay-io-robot-secret"
  payload:
    ensureNICsPayload:
      type: aws
      nodeSelector:
        support-client: "true"
      dataNICsNumber: 2
```

```

Apply the manifest:

```
kubectl apply -f ensure-nics.yaml
```

5. **Deploy the WEKA client resource:**  Define the WEKA client custom resource. Replace the `joinIpPorts` value with a valid IP or ALB DNS of the deployed WEKA cluster:

```

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaClient
metadata:
  name: cluster-dev-clients
  namespace: default
spec:
  image: quay.io/weka.io/weka-in-container:4.4.5.118-k8s.4
  imagePullSecret: "quay-io-robot-secret"
  driversDistService: "https://drivers.weka.io"
  portRange:
    basePort: 46000
  nodeSelector:
    weka.io/supports-clients: "true"
  joinIpPorts: ["10.0.76.143:14000"]
  coresNum: 4
```

```

Apply the manifest:

```
kubectl apply -f weka-client.yaml
```

6. **Install a CSI plugin:** Follow the procedures in .

<!-- ============================================ -->
<!-- File 208/259: kubernetes_weka-operator-day-2-operationsweka-operator-cli.md -->
<!-- ============================================ -->

---
description:
---

# WEKA Operator day-2 operationsWEKA Operator:CLI

WEKA Operator day-2 operations maintain and optimize WEKA environments in Kubernetes clusters by focusing in the core areas:

* **Hardware maintenance**
  * Component replacement and node management
  * Hardware failure remediation
* **Cluster scaling**
  * Resource allocation optimization
  * Controlled cluster expansion
* **Cluster maintenance**
  * Configuration updates
  * Pod rotation
  * Token secret management
* **WekaContainer lifecycle management**

Administrators execute both planned maintenance and emergency responses while following standardized procedures to ensure high availability and minimize service disruption.

***

## **Hardware maintenance**

Hardware maintenance operations ensure cluster reliability and performance through systematic component management and failure response procedures. These operations span from routine preventive maintenance to critical component replacements.

**Key operations:**

* **Node management**
  * Graceful and forced node reboots.
  * Node replacement and removal.
  * Complete rack decommissioning procedures.
* **Container operations**
  * Container migration from failed nodes.
  * Container replacement on active nodes.
  * Container management on denylisted nodes.
* **Storage management**
  * Drive replacement in converged setups.
  * Storage integrity verification.
  * Component failure recovery.

Each procedure follows established protocols to maintain system stability and minimize service disruption during maintenance activities. The documented procedures enable administrators to execute both planned maintenance and emergency responses while preserving data integrity and system performance.

### **Before you begin**

Before performing any hardware maintenance or replacement tasks, ensure you have:

* Administrative access to your Kubernetes cluster.
* SSH access to the cluster nodes.
* `kubectl` command-line tool installed and configured.
* Proper backup of any critical data on the affected components.
* Required replacement hardware (if applicable).
* Maintenance window scheduled (if required).

***

### Perform standard verification steps

This procedure describes the standard verification steps for checking WEKA cluster health. Multiple procedures in this documentation refer to these verification steps to confirm successful completion of their respective tasks.

**Procedure**

1. Log in to the wekacontainer:

```
kubectl exec -it <container-pod-name> -n weka-operator-system -- /bin/bash
```

2. Check the WEKA cluster status:

```
weka status
```

<details>

<summary>Example</summary>

```bash
WekaIO v4.4.1 (CLI build 4.4.1)

       cluster: cluster-dev (f2dca61b-f7ca-4b41-8cc3-89dd475e9ff2)
        status: OK (14 backend containers UP, 6 drives UP)
    protection: 3+2 (Fully protected)
     hot spare: 0 failure domains
 drive storage: 22.09 TiB total, 21.86 TiB unprovisioned
         cloud: connected
       license: Unlicensed

     io status: STARTED 2 hours ago (14 io-nodes UP, 138 Buckets UP)
    link layer: Ethernet
       clients: 6 connected
         reads: 10.38 MiB/s (2659 IO/s)
        writes: 3.48 MiB/s (892 IO/s)
    operations: 3551 ops/s
        alerts: 35 active alerts, use `weka alerts` to list them
```

</details>

3. Check cluster containers.

```
weka cluster container
```

<details>

<summary>Example</summary>

```
root@ip-10-0-78-157:/# weka cluster container
HOST ID  HOSTNAME        CONTAINER                                     IPS          STATUS  REQUESTED ACTION  RELEASE  FAILURE DOMAIN  CORES  MEMORY   UPTIME    LAST FAILURE                   REQUESTED ACTION FAILURE
0        ip-10-0-64-189  drivex8d9a7bcex5994x4566x975fx3c3fbc7bf017    10.0.64.189  UP      NONE              4.4.1    AUTO            1      1.54 GB  1:50:17h  Action requested (1 hour ago)
1        ip-10-0-119-18  drivexffc61f84x840ax4b4cx9944xabdf5d50ffac    10.0.119.18  UP      NONE              4.4.1    AUTO            1      1.54 GB  2:03:14h
2        ip-10-0-96-125  drivexddded148xa18cx4d31xa785xfd19e87ae858    10.0.96.125  UP      NONE              4.4.1    AUTO            1      1.54 GB  1:48:09h  Action requested (1 hour ago)
3        ip-10-0-119-18  computex62e21720xd6efx4797xa1cbx2c134c316c95  10.0.119.18  UP      NONE              4.4.1    AUTO            1      2.94 GB  2:03:16h
4        ip-10-0-68-19   drivex89230786x4d11x4364x9eb8x9d061baad12b    10.0.68.19   UP      NONE              4.4.1    AUTO            1      1.54 GB  2:03:14h
5        ip-10-0-64-189  computexcc3cb8d2x4a7dx4f38x9087xa99c021dc51b  10.0.64.189  UP      NONE              4.4.1    AUTO            1      2.94 GB  1:50:17h  Action requested (1 hour ago)
6        ip-10-0-78-157  s3x86193fa8xd55ax4bb1xb8c7xc77f2841c76b       10.0.78.157  UP      NONE              4.4.1    AUTO            1      1.26 GB  2:03:13h
7        ip-10-0-96-125  s3x1783ddc3x91a5x4fa9xa7aax83a88515418d       10.0.96.125  UP      NONE              4.4.1    AUTO            1      1.26 GB  1:53:18h  Action requested (1 hour ago)
8        ip-10-0-85-230  computexe4edde12xf27dx4745xafacx907dd046d202  10.0.85.230  UP      NONE              4.4.1    AUTO            1      2.94 GB  2:03:17h
9        ip-10-0-68-19   computexb1c3fad3xe566x4cd0xa7e9xe1cd885e4f43  10.0.68.19   UP      NONE              4.4.1    AUTO            1      2.94 GB  2:03:17h
10       ip-10-0-78-157  drivex34be6929x2711x42e9xa4afx532411fe3290    10.0.78.157  UP      NONE              4.4.1    AUTO            1      1.54 GB  2:03:15h
11       ip-10-0-85-230  drivexa994b565x00c7x4d12x887ax611926a9f6cc    10.0.85.230  UP      NONE              4.4.1    AUTO            1      1.54 GB  2:03:16h
12       ip-10-0-78-157  computex27909856x161ax4d29x9b86x233d94ff5c21  10.0.78.157  UP      NONE              4.4.1    AUTO            1      2.94 GB  2:03:15h
13       ip-10-0-96-125  computex4769a665x1c29x4fcax899axc8f55b7207f8  10.0.96.125  UP      NONE              4.4.1    AUTO            1      2.94 GB  1:49:37h  Action requested (1 hour ago)
14       ip-10-0-74-235  22b88fbd24c8client                            10.0.74.235  UP      NONE              4.4.1                    1      1.36 GB  1:18:27h
15       ip-10-0-94-134  22b88fbd24c8client                            10.0.94.134  UP      NONE              4.4.1                    1      1.36 GB  0:37:55h
16       ip-10-0-85-177  22b88fbd24c8client                            10.0.85.177  UP      NONE              4.4.1                    1      1.36 GB  1:18:24h
17       ip-10-0-71-46   22b88fbd24c8client                            10.0.71.46   UP      NONE              4.4.1                    1      1.36 GB  1:18:17h
18       ip-10-0-71-140  22b88fbd24c8client                            10.0.71.140  UP      NONE              4.4.1                    1      1.36 GB  0:37:22h
19       ip-10-0-92-127  22b88fbd24c8client                            10.0.92.127  UP      NONE              4.4.1                    1      1.36 GB  1:18:09h
```

</details>

4. Check the WEKA filesystem status.

```
weka fs
```

<details>

<summary>Example</summary>

```objectivec
root@ip-10-0-78-157:/# weka fs
FILESYSTEM ID  FILESYSTEM NAME  USED SSD   AVAILABLE SSD  USED TOTAL  AVAILABLE TOTAL  THIN PROVISIONED  THIN PROVISIONED MINIMUM SSD  THIN PROVISIONED MAXIMUM SSD
0              .config_fs       172.03 KB  107.37 GB      172.03 KB   107.37 GB        True              10.73 GB                      107.37 GB
1              default          2.72 GB    2.42 TB        2.72 GB     2.42 TB          True              242.94 GB                     2.42 TB
```

</details>

5. Verify the status of the WEKA cluster processes is UP.

```
weka cluster process
```

<details>

<summary>Example</summary>

```
PROCESS ID  CONTAINER ID  SLOT IN HOST  HOSTNAME         CONTAINER           IPS           STATUS  RELEASE  ROLES       NETWORK  CPU  MEMORY   UPTIME    LAST FAILURE
0           0             0             ip-10-0-78-96    computexxxxx        10.0.78.96    UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
1           0             1             ip-10-0-78-96    computexxxxx        10.0.78.96    UP      4.4.1    COMPUTE     UDP      3    2.94 GB  0:08:04h
20          1             0             ip-10-0-107-120  computexxxxx        10.0.107.120  UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
21          1             1             ip-10-0-107-120  computexxxxx        10.0.107.120  UP      4.4.1    COMPUTE     UDP      3    2.94 GB  0:08:03h
40          2             0             ip-10-0-78-96    drivexxxxx          10.0.78.96    UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:12h
41          2             1             ip-10-0-78-96    drivexxxxx          10.0.78.96    UP      4.4.1    DRIVES      UDP      1    1.54 GB  0:08:05h
60          3             0             ip-10-0-126-172  drivexxxxx          10.0.126.172  UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
61          3             1             ip-10-0-126-172  drivexxxxx          10.0.126.172  UP      4.4.1    DRIVES      UDP      1    1.54 GB  0:08:06h
80          4             0             ip-10-0-76-148   drivexxxxx          10.0.76.148   UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
81          4             1             ip-10-0-76-148   drivexxxxx          10.0.76.148   UP      4.4.1    DRIVES      UDP      1    1.54 GB  0:08:06h
100         5             0             ip-10-0-66-46    computexxxxx        10.0.66.46    UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
101         5             1             ip-10-0-66-46    computexxxxx        10.0.66.46    UP      4.4.1    COMPUTE     UDP      1    2.94 GB  0:08:06h
120         6             0             ip-10-0-64-105   computexxxxx        10.0.64.105   UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
121         6             1             ip-10-0-64-105   computexxxxx        10.0.64.105   UP      4.4.1    COMPUTE     UDP      3    2.94 GB  0:08:04h
140         7             0             ip-10-0-126-172  computexxxxx        10.0.126.172  UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
141         7             1             ip-10-0-126-172  computexxxxx        10.0.126.172  UP      4.4.1    COMPUTE     UDP      3    2.94 GB  0:08:04h
160         8             0             ip-10-0-107-120  drivexxxxx          10.0.107.120  UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
161         8             1             ip-10-0-107-120  drivexxxxx          10.0.107.120  UP      4.4.1    DRIVES      UDP      1    1.54 GB  0:08:05h
180         9             0             ip-10-0-64-105   drivexxxxx          10.0.64.105   UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
181         9             1             ip-10-0-64-105   drivexxxxx          10.0.64.105   UP      4.4.1    DRIVES      UDP      1    1.54 GB  0:08:05h
200         10            0             ip-10-0-82-61    drivexxxxx          10.0.82.61    UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
201         10            1             ip-10-0-82-61    drivexxxxx          10.0.82.61    UP      4.4.1    DRIVES      UDP      1    1.54 GB  0:08:05h
220         11            0             ip-10-0-82-61    computexxxxx        10.0.82.61    UP      4.4.1    MANAGEMENT  UDP           N/A      0:08:11h  Host joined a new cluster (8 minutes ago)
221         11            1             ip-10-0-82-61    computexxxxx        10.0.82.61    UP      4.4.1    COMPUTE     UDP      3    2.94 GB  0:08:03h
240         12            0             ip-10-0-76-148   computexxxxx        10.0.76.148   UP      4.4.1    MANAGEMENT  UDP           N/A      0:03:36h  Configuration snapshot pulled (3 minutes ago)
241         12            1             ip-10-0-76-148   computexxxxx        10.0.76.148   UP      4.4.1    COMPUTE     UDP      3    2.94 GB  0:03:32h
260         13            0             ip-10-0-66-46    drivexxxxx          10.0.66.46    UP      4.4.1    MANAGEMENT  UDP           N/A      0:03:36h  Configuration snapshot pulled (3 minutes ago)
261         13            1             ip-10-0-66-46    drivexxxxx          10.0.66.46    UP      4.4.1    DRIVES      UDP      3    1.54 GB  0:03:32h
```

</details>

6. Check all pods are up and running.

```
kubectl get pods --all-namespaces -o wide
```

<details>

<summary>Example</summary>

```
$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                               READY   STATUS    RESTARTS         AGE     IP            NODE             NOMINATED NODE   READINESS GATES
csi-wekafs             csi-wekafs-controller-5b7cd75846-csrch             6/6     Running   26 (3m38s ago)   14m     10.42.2.17    3.252.130.226    <none>           <none>
csi-wekafs             csi-wekafs-controller-5b7cd75846-hb6z2             6/6     Running   26 (3m37s ago)   14m     10.42.2.15    3.252.130.226    <none>           <none>
csi-wekafs             csi-wekafs-node-2tfjs                              3/3     Running   8 (3m15s ago)    82m     10.42.4.6     54.194.172.141   <none>           <none>
csi-wekafs             csi-wekafs-node-5dgcf                              3/3     Running   15 (3m41s ago)   82m     10.42.5.8     3.253.198.136    <none>           <none>
csi-wekafs             csi-wekafs-node-h562s                              3/3     Running   8 (3m4s ago)     82m     10.42.0.15    54.229.216.116   <none>           <none>
csi-wekafs             csi-wekafs-node-jsh8h                              3/3     Running   14 (4m10s ago)   82m     10.42.2.10    3.252.130.226    <none>           <none>
csi-wekafs             csi-wekafs-node-qsjrv                              3/3     Running   8 (3m39s ago)    82m     10.42.1.6     52.214.4.90      <none>           <none>
csi-wekafs             csi-wekafs-node-sd9tb                              3/3     Running   8 (3m16s ago)    82m     10.42.3.7     54.229.178.93    <none>           <none>
kube-system            coredns-7b98449c4-29ph6                            1/1     Running   0                14m     10.42.2.14    3.252.130.226    <none>           <none>
kube-system            local-path-provisioner-595dcfc56f-ffsxr            1/1     Running   0                14m     10.42.2.12    3.252.130.226    <none>           <none>
kube-system            metrics-server-cdcc87586-xzkt8                     1/1     Running   0                14m     10.42.2.11    3.252.130.226    <none>           <none>
kube-system            node-shell-605368a3-dcc4-4680-9063-2c9a8e7635ed    0/1     Unknown   0                7m5s    10.0.71.46    54.229.216.116   <none>           <none>
kube-system            traefik-d7c9c5778-jx7q5                            1/1     Running   0                14m     10.42.2.13    3.252.130.226    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-3.252.130.226               1/1     Running   0                14m     10.0.94.134   3.252.130.226    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-3.253.198.136               1/1     Running   0                14m     10.0.71.140   3.253.198.136    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-52.214.4.90                 1/1     Running   0                4m52s   10.0.74.235   52.214.4.90      <none>           <none>
weka-operator-system   cluster-dev-clientsnew-54.194.172.141              1/1     Running   0                5m38s   10.0.85.177   54.194.172.141   <none>           <none>
weka-operator-system   cluster-dev-clientsnew-54.229.178.93               1/1     Running   0                4m43s   10.0.92.127   54.229.178.93    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-54.229.216.116              1/1     Running   0                4m40s   10.0.71.46    54.229.216.116   <none>           <none>
weka-operator-system   weka-driver-builder                                1/1     Running   0                5m40s   10.42.2.19    3.252.130.226    <none>           <none>
weka-operator-system   weka-operator-controller-manager-bcf48df44-lk6cx   2/2     Running   0                14m     10.42.2.16    3.252.130.226    <none>           <none>
```

</details>

***

### Force reboot a machine

A force reboot may be necessary when a machine becomes unresponsive or encounters a critical error that cannot be resolved through standard troubleshooting. This task ensures the machine restarts and resumes normal operation.

#### **Procedure**

**Phase 1:** #perform-standard-verification-steps.

**Phase 2:** **Cordon** **and evict backend k8s nodes.**

To cordon and evict a node, run the following commands. Replace `<k8s_node_IP>` with the target k8s node's IP address.

1. Cordon the backend k8s node:

```
kubectl cordon <k8s_node_ip>
```

Example:

```
kubectl cordon 18.201.176.181
node/18.201.176.181 cordoned
```

2. Evict the running pods ensuring data is removed. For example, drain the backend k8s node:

```
kubectl drain <k8s_node_ip> --delete-emptydir-data --ignore-daemonsets --force
```

<details>

<summary>Example</summary>

```bash
$kubectl drain 18.201.176.181 --delete-local-data --ignore-daemonsets --force
Flag --delete-local-data has been deprecated, This option is deprecated and will be deleted. Use --delete-emptydir-data.
node/18.201.176.181 already cordoned
evicting pod weka-operator-system/cluster-dev-drive-8d9a7bce-5994-4566-975f-3c3fbc7bf017
evicting pod weka-operator-system/cluster-dev-compute-cc3cb8d2-4a7d-4f38-9087-a99c021dc51b
pod/cluster-dev-compute-cc3cb8d2-4a7d-4f38-9087-a99c021dc51b evicted
pod/cluster-dev-drive-8d9a7bce-5994-4566-975f-3c3fbc7bf017 evicted
node/18.201.176.181 drained
```

</details>

3. Validate node status:

```
kubectl get nodes
```

<details>

<summary>Example</summary>

```
NAME             STATUS                     ROLES                       AGE     VERSION
18.201.138.175   Ready                      control-plane,etcd,master   3h24m   v1.30.6+k3s1
18.201.176.181   Ready,SchedulingDisabled   control-plane,etcd,master   3h24m   v1.30.6+k3s1
34.240.186.72    Ready                      control-plane,etcd,master   3h25m   v1.30.6+k3s1
34.243.146.18    Ready                      control-plane,etcd,master   3h24m   v1.30.6+k3s1
34.245.228.117   Ready                      control-plane,etcd,master   3h25m   v1.30.6+k3s1
34.255.190.208   Ready                      control-plane,etcd,master   3h24m   v1.30.6+k3s1
```

</details>

4. Verify pod statuses across namespaces:

```
kubectl get pods --all-namespaces -o wide
```

<details>

<summary>Example</summary>

```
NAMESPACE              NAME                                                       READY   STATUS      RESTARTS   AGE
kube-system            coredns-7b98449c4-stzbf                                    1/1     Running     0          3h19m
kube-system            helm-install-traefik-65gnb                                 0/1     Completed   1          3h19m
kube-system            helm-install-traefik-crd-nnstz                             0/1     Completed   0          3h19m
kube-system            local-path-provisioner-595dcfc56f-ln82j                    1/1     Running     0          3h19m
kube-system            metrics-server-cdcc87586-ll9lf                             1/1     Running     0          3h19m
kube-system            traefik-d7c9c5778-qtbgz                                    1/1     Running     0          3h19m
weka-operator-system   cluster-dev-compute-27909856-161a-4d29-9b86-233d94ff5c21   1/1     Running     0          122m
weka-operator-system   cluster-dev-compute-4769a665-1c29-4fca-899a-c8f55b7207f8   1/1     Running     0          108m
weka-operator-system   cluster-dev-compute-62e21720-d6ef-4797-a1cb-2c134c316c95   1/1     Running     0          121m
weka-operator-system   cluster-dev-compute-b1c3fad3-e566-4cd0-a7e9-e1cd885e4f43   1/1     Running     0          121m
weka-operator-system   cluster-dev-compute-cc3cb8d2-4a7d-4f38-9087-a99c021dc51b   1/1     Running     0          113m
weka-operator-system   cluster-dev-compute-e4edde12-f27d-4745-afac-907dd046d202   1/1     Running     0          121m
weka-operator-system   cluster-dev-drive-34be6929-2711-42e9-a4af-532411fe3290     1/1     Running     0          122m
weka-operator-system   cluster-dev-drive-89230786-4d11-4364-9eb8-9d061baad12b     1/1     Running     0          122m
weka-operator-system   cluster-dev-drive-8d9a7bce-5994-4566-975f-3c3fbc7bf017     1/1     Running     0          113m
weka-operator-system   cluster-dev-drive-a994b565-00c7-4d12-887a-611926a9f6cc     1/1     Running     0          122m
weka-operator-system   cluster-dev-drive-ddded148-a18c-4d31-a785-fd19e87ae858     1/1     Running     0          106m
weka-operator-system   cluster-dev-drive-ffc61f84-840a-4b4c-9944-abdf5d50ffac     1/1     Running     0          122m
weka-operator-system   cluster-dev-envoy-d8942922-b183-4392-bf9d-92e6a3c32a1e     1/1     Running     0          112m
weka-operator-system   cluster-dev-envoy-ed3f917a-f4e5-42a7-93b2-b2a933f45c7c     1/1     Running     0          121m
weka-operator-system   cluster-dev-s3-1783ddc3-91a5-4fa9-a7aa-83a88515418d        1/1     Running     0          112m
weka-operator-system   cluster-dev-s3-86193fa8-d55a-4bb1-b8c7-c77f2841c76b        1/1     Running     0          121m
weka-operator-system   weka-driver-dist                                           1/1     Running     0          178m
weka-operator-system   weka-operator-controller-manager-bcf48df44-phb75           2/2     Running     0          3h17m
```

</details>

**Phase 3: Ensure the WEKA containers are marked as drained.**

1.  List the cluster backend containers.\
    Run the following command to display the current status of all WEKA containers in the k8s nodes:

    <pre><code><strong>weka cluster container
    </strong>
```
2. Check the status of the WEKA containers.\
   In the command output, locate the `STATUS` column for the relevant containers. Verify that it displays `DRAINED` for the host and backend container.

<details>

<summary>Example</summary>

```
HOST ID  HOSTNAME        CONTAINER                                     IPS          STATUS          REQUESTED ACTION  RELEASE  FAILURE DOMAIN  CORES  MEMORY   UPTIME    LAST FAILURE                   REQUESTED ACTION FAILURE
0        ip-10-0-64-189  drivex8d9a7bcex5994x4566x975fx3c3fbc7bf017    10.0.64.189  DRAINED (DOWN)  STOP              4.4.1    AUTO            1      1.54 GB            Action requested (1 hour ago)
1        ip-10-0-119-18  drivexffc61f84x840ax4b4cx9944xabdf5d50ffac    10.0.119.18  UP              NONE              4.4.1    AUTO            1      1.54 GB  2:06:45h
2        ip-10-0-96-125  drivexddded148xa18cx4d31xa785xfd19e87ae858    10.0.96.125  UP              NONE              4.4.1    AUTO            1      1.54 GB  1:51:40h  Action requested (1 hour ago)
3        ip-10-0-119-18  computex62e21720xd6efx4797xa1cbx2c134c316c95  10.0.119.18  UP              NONE              4.4.1    AUTO            1      2.94 GB  2:06:47h
4        ip-10-0-68-19   drivex89230786x4d11x4364x9eb8x9d061baad12b    10.0.68.19   UP              NONE              4.4.1    AUTO            1      1.54 GB  2:06:45h
5        ip-10-0-64-189  computexcc3cb8d2x4a7dx4f38x9087xa99c021dc51b  10.0.64.189  DRAINED (DOWN)  STOP              4.4.1    AUTO            1      2.94 GB            Action requested (1 hour ago)
6        ip-10-0-78-157  s3x86193fa8xd55ax4bb1xb8c7xc77f2841c76b       10.0.78.157  UP              NONE              4.4.1    AUTO            1      1.26 GB  2:06:44h
7        ip-10-0-96-125  s3x1783ddc3x91a5x4fa9xa7aax83a88515418d       10.0.96.125  UP              NONE              4.4.1    AUTO            1      1.26 GB  1:56:49h  Action requested (1 hour ago)
8        ip-10-0-85-230  computexe4edde12xf27dx4745xafacx907dd046d202  10.0.85.230  UP              NONE              4.4.1    AUTO            1      2.94 GB  2:06:48h
9        ip-10-0-68-19   computexb1c3fad3xe566x4cd0xa7e9xe1cd885e4f43  10.0.68.19   UP              NONE              4.4.1    AUTO            1      2.94 GB  2:06:48h
10       ip-10-0-78-157  drivex34be6929x2711x42e9xa4afx532411fe3290    10.0.78.157  UP              NONE              4.4.1    AUTO            1      1.54 GB  2:06:46h
11       ip-10-0-85-230  drivexa994b565x00c7x4d12x887ax611926a9f6cc    10.0.85.230  UP              NONE              4.4.1    AUTO            1      1.54 GB  2:06:47h
12       ip-10-0-78-157  computex27909856x161ax4d29x9b86x233d94ff5c21  10.0.78.157  UP              NONE              4.4.1    AUTO            1      2.94 GB  2:06:46h
13       ip-10-0-96-125  computex4769a665x1c29x4fcax899axc8f55b7207f8  10.0.96.125  UP              NONE              4.4.1    AUTO            1      2.94 GB  1:53:08h  Action requested (1 hour ago)
14       ip-10-0-74-235  22b88fbd24c8client                            10.0.74.235  UP              NONE              4.4.1                    1      1.36 GB  1:21:58h
15       ip-10-0-94-134  22b88fbd24c8client                            10.0.94.134  UP              NONE              4.4.1                    1      1.36 GB  0:41:26h
16       ip-10-0-85-177  22b88fbd24c8client                            10.0.85.177  UP              NONE              4.4.1                    1      1.36 GB  1:21:55h
17       ip-10-0-71-46   22b88fbd24c8client                            10.0.71.46   UP              NONE              4.4.1                    1      1.36 GB  1:21:48h
18       ip-10-0-71-140  22b88fbd24c8client                            10.0.71.140  UP              NONE              4.4.1                    1      1.36 GB  0:40:53h
19       ip-10-0-92-127  22b88fbd24c8client                            10.0.92.127  UP              NONE              4.4.1                    1      1.36 GB  1:21:40h
```

</details>

**Phase 4: Force a reboot on all backend k8s nodes.**\
Use the `reboot -f` command to force a reboot on each backend k8s node.

Example:

```
sudo reboot -f
Rebooting.
```

After running this command, the container restarts immediately. Repeat for all k8's nodes one by one in your environment.

**Phase 5: Uncordon the backend k8s node and verify WEKA cluster status.**

1. Uncordon the backend k8s node:

```
kubectl uncordon <k8s_node_ip>
```

Example:

```
kubectl uncordon 18.201.176.181
node/18.201.176.181 uncordoned
```

2. Access the WEKA Operator in the backend k8s node:

```
kubectl exec -it <weka_container_pod_name> -n weka-operator-system -- /bin/bash
```

3. Verify the weka drives status:

```
weka cluster drive
```

<details>

<summary>Example</summary>

```
DISK ID  UUID                                  HOSTNAME        NODE ID  SIZE      STATUS      LIFETIME % USED  ATTACHMENT  DRIVE STATUS
0        8406a082-cc8d-40a1-87b4-90f7053dc3f2  ip-10-0-119-18  21       6.82 TiB  ACTIVE      0                OK          OK
1        01738d49-f2eb-49aa-8e39-6b5ac7f12727  ip-10-0-68-19   81       6.82 TiB  ACTIVE      0                OK          OK
2        8684695a-9a7f-4a39-801c-7019bd5fd4ea  ip-10-0-64-189  1        6.82 TiB  PHASING_IN  0                OK          OK
3        ca4cacf6-4355-420d-a790-e0e58670e0ec  ip-10-0-96-125  41       6.82 TiB  ACTIVE      0                OK          OK
4        b410544f-9fef-42a9-a24d-c73ffd33cefc  ip-10-0-78-157  201      6.82 TiB  ACTIVE      0                OK          OK
5        16c55940-cf2c-4bf9-8d4b-d05f61be7264  ip-10-0-85-230  221      6.82 TiB  ACTIVE      0                OK          OK
```

</details>

4. #perform-the-standard-verification-steps.

Ensure all the pods, weka containers and the cluster is in a healthy state (`Fully Protected`) and IO operations are running (`STARTED`). Monitor the redistribution progress and alerts.

**Phase 6: Cordon and drain all client k8s nodes.**

To cordon and drain a node, run the following commands. Replace `<k8s_node_IP>` with the target k8s node's IP address.

1. Cordon the client k8s node to mark it as unschedulable:

```
kubectl cordon <k8s_node_ip>
```

Example:

```
kubectl cordon 3.252.130.226
node/3.252.130.226 cordoned
```

2. Evict the the workload.\
   For example:  Drain the client k8s node to evict running pods, ensuring data is removed:

```
kubectl drain <k8s_node_ip> --delete-emptydir-data --ignore-daemonsets --force
```

<details>

<summary>Example </summary>

```
$kubectl drain 3.252.130.226 --delete-local-data --ignore-daemonsets --force
Flag --delete-local-data has been deprecated, This option is deprecated and will be deleted. Use --delete-emptydir-data.
node/3.252.130.226 already cordoned
Warning: ignoring DaemonSet-managed Pods: csi-wekafs/csi-wekafs-node-jsh8h; deleting Pods that declare no controller: default/csi-app-on-dir-api2
evicting pod weka-operator-system/cluster-dev-clientsnew-3.252.130.226
evicting pod default/csi-app-on-dir-api2
pod/csi-app-on-dir-api2 evicted
pod/cluster-dev-clientsnew-3.252.130.226 evicted
node/3.252.130.226 drainedTBD

$ kubectl get nodes
NAME             STATUS                     ROLES                       AGE     VERSION
3.252.130.226    Ready,SchedulingDisabled   control-plane,etcd,master   3h13m   v1.30.6+k3s1
3.253.198.136    Ready,SchedulingDisabled   control-plane,etcd,master   3h13m   v1.30.6+k3s1
52.214.4.90      Ready,SchedulingDisabled   control-plane,etcd,master   3h14m   v1.30.6+k3s1
54.194.172.141   Ready,SchedulingDisabled   control-plane,etcd,master   3h13m   v1.30.6+k3s1
54.229.178.93    Ready,SchedulingDisabled   control-plane,etcd,master   3h13m   v1.30.6+k3s1
54.229.216.116   Ready,SchedulingDisabled   control-plane,etcd,master   3h14m   v1.30.6+k3s1

$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                               READY   STATUS    RESTARTS      AGE     IP           NODE             NOMINATED NODE   READINESS GATES
csi-wekafs             csi-wekafs-controller-5b7cd75846-csrch             0/6     Pending   0             2m41s   <none>       <none>           <none>           <none>
csi-wekafs             csi-wekafs-controller-5b7cd75846-hb6z2             0/6     Pending   0             2m41s   <none>       <none>           <none>           <none>
csi-wekafs             csi-wekafs-node-2tfjs                              3/3     Running   0             71m     10.42.4.5    54.194.172.141   <none>           <none>
csi-wekafs             csi-wekafs-node-5dgcf                              3/3     Running   7 (55m ago)   71m     10.42.5.5    3.253.198.136    <none>           <none>
csi-wekafs             csi-wekafs-node-h562s                              3/3     Running   0             71m     10.42.0.11   54.229.216.116   <none>           <none>
csi-wekafs             csi-wekafs-node-jsh8h                              3/3     Running   6 (56m ago)   71m     10.42.2.6    3.252.130.226    <none>           <none>
csi-wekafs             csi-wekafs-node-qsjrv                              3/3     Running   0             71m     10.42.1.4    52.214.4.90      <none>           <none>
csi-wekafs             csi-wekafs-node-sd9tb                              3/3     Running   0             71m     10.42.3.4    54.229.178.93    <none>           <none>
kube-system            coredns-7b98449c4-29ph6                            0/1     Pending   0             2m41s   <none>       <none>           <none>           <none>
kube-system            local-path-provisioner-595dcfc56f-ffsxr            0/1     Pending   0             2m41s   <none>       <none>           <none>           <none>
kube-system            metrics-server-cdcc87586-xzkt8                     0/1     Pending   0             2m41s   <none>       <none>           <none>           <none>
kube-system            traefik-d7c9c5778-jx7q5                            0/1     Pending   0             2m41s   <none>       <none>           <none>           <none>
weka-operator-system   cluster-dev-clientsnew-3.252.130.226               0/1     Pending   0             2m51s   <none>       <none>           <none>           <none>
weka-operator-system   cluster-dev-clientsnew-3.253.198.136               0/1     Pending   0             2m50s   <none>       <none>           <none>           <none>
weka-operator-system   weka-operator-controller-manager-bcf48df44-lk6cx   0/2     Pending   0             2m41s   <none>       <none>           <none>           <none>
```

</details>

3. Force reboot all client nodes. \
   Example for one client k8s node:

```
sudo reboot -f
Rebooting.
```

4. After the client k8s nodes are up, uncordon the client k8s node.\
   Example for one client k8s node:

```
kubectl uncordon <k8s_node_ip>
```

Example:

```
kubectl uncordon 3.252.130.226
node/3.252.130.226 uncordoned
```

5. Verify that after uncordoning all client Kubernetes nodes:

* All regular pods remain scheduled and running on those nodes.
* All client containers within the cluster are joined and operational.
* Only pods designated for data I/O operations are evicted.

```bash
kubectl get pods --all-namespaces -o wide
```

<details>

<summary>Example</summary>

```
$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                               READY   STATUS    RESTARTS         AGE     IP            NODE             NOMINATED NODE   READINESS GATES
csi-wekafs             csi-wekafs-controller-5b7cd75846-csrch             6/6     Running   26 (3m38s ago)   14m     10.42.2.17    3.252.130.226    <none>           <none>
csi-wekafs             csi-wekafs-controller-5b7cd75846-hb6z2             6/6     Running   26 (3m37s ago)   14m     10.42.2.15    3.252.130.226    <none>           <none>
csi-wekafs             csi-wekafs-node-2tfjs                              3/3     Running   8 (3m15s ago)    82m     10.42.4.6     54.194.172.141   <none>           <none>
csi-wekafs             csi-wekafs-node-5dgcf                              3/3     Running   15 (3m41s ago)   82m     10.42.5.8     3.253.198.136    <none>           <none>
csi-wekafs             csi-wekafs-node-h562s                              3/3     Running   8 (3m4s ago)     82m     10.42.0.15    54.229.216.116   <none>           <none>
csi-wekafs             csi-wekafs-node-jsh8h                              3/3     Running   14 (4m10s ago)   82m     10.42.2.10    3.252.130.226    <none>           <none>
csi-wekafs             csi-wekafs-node-qsjrv                              3/3     Running   8 (3m39s ago)    82m     10.42.1.6     52.214.4.90      <none>           <none>
csi-wekafs             csi-wekafs-node-sd9tb                              3/3     Running   8 (3m16s ago)    82m     10.42.3.7     54.229.178.93    <none>           <none>
kube-system            coredns-7b98449c4-29ph6                            1/1     Running   0                14m     10.42.2.14    3.252.130.226    <none>           <none>
kube-system            local-path-provisioner-595dcfc56f-ffsxr            1/1     Running   0                14m     10.42.2.12    3.252.130.226    <none>           <none>
kube-system            metrics-server-cdcc87586-xzkt8                     1/1     Running   0                14m     10.42.2.11    3.252.130.226    <none>           <none>
kube-system            node-shell-605368a3-dcc4-4680-9063-2c9a8e7635ed    0/1     Unknown   0                7m5s    10.0.71.46    54.229.216.116   <none>           <none>
kube-system            traefik-d7c9c5778-jx7q5                            1/1     Running   0                14m     10.42.2.13    3.252.130.226    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-3.252.130.226               1/1     Running   0                14m     10.0.94.134   3.252.130.226    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-3.253.198.136               1/1     Running   0                14m     10.0.71.140   3.253.198.136    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-52.214.4.90                 1/1     Running   0                4m52s   10.0.74.235   52.214.4.90      <none>           <none>
weka-operator-system   cluster-dev-clientsnew-54.194.172.141              1/1     Running   0                5m38s   10.0.85.177   54.194.172.141   <none>           <none>
weka-operator-system   cluster-dev-clientsnew-54.229.178.93               1/1     Running   0                4m43s   10.0.92.127   54.229.178.93    <none>           <none>
weka-operator-system   cluster-dev-clientsnew-54.229.216.116              1/1     Running   0                4m40s   10.0.71.46    54.229.216.116   <none>           <none>
weka-operator-system   weka-driver-builder                                1/1     Running   0                5m40s   10.42.2.19    3.252.130.226    <none>           <none>
weka-operator-system   weka-operator-controller-manager-bcf48df44-lk6cx   2/2     Running   0                14m     10.42.2.16    3.252.130.226    <none>           <none>
```

</details>

**Phase 7: Force a reboot on all client k8s nodes.**\
Use the `reboot -f` command to force a reboot on each client k8s node.

Example for one client k8s node:

```
sudo reboot -f
Rebooting.
```

After running this command, the client node restarts immediately. Repeat for all client nodes in your environment.

**Phase 8: Uncordon all client k8s nodes.**

1. Once the client k8s nodes are back online, uncordon them to restore their availability for scheduling workloads. Example command for uncordoning a single client k8s node:

```
kubectl uncordon <k8s_node_ip>
```

<details>

<summary>Example</summary>

```
$ kubectl uncordon 3.252.130.226
node/3.252.130.226 uncordoned
```

</details>

2. Verify pod status across all k8s nodes to confirm that all pods are running as expected:

```bash
kubectl get pods --all-namespaces -o wide
```

3. Validate WEKA cluster status to ensure all containers are operational:

```
weka cluster container
```

See examples in #perform-standard-verification-steps.

***

### Remove a rack or Kubernetes node

Removing a rack or Kubernetes (k8s) node is necessary when you need to decommission hardware, replace failed components, or reconfigure your cluster. This procedure guides you through safely removing nodes without disrupting your system operations.

#### Procedure

1. Create failure domain labels for your nodes:
   1.  Label nodes with two machines per failure domain:

       ```bash
       kubectl label nodes 18.201.172.13 34.240.124.21 weka.io/failure-domain=x1
       kubectl label nodes 18.202.166.64 3.255.93.171 weka.io/failure-domain=x2
       kubectl label nodes 18.203.137.243 34.254.151.249 weka.io/failure-domain=x3
       kubectl label nodes 3.254.112.77 54.247.13.91 weka.io/failure-domain=x4
       kubectl label nodes 34.245.203.245 63.35.225.98 weka.io/failure-domain=x5
       kubectl label nodes 52.215.56.158 54.247.20.174 weka.io/failure-domain=x6
       ```

       b. Label nodes with one machine per failure domain:

       ```bash
       kubectl label nodes 3.255.150.131 weka.io/failure-domain=x7
       kubectl label nodes 52.210.49.97 weka.io/failure-domain=x8
       ```
2.  Apply the NoSchedule taint to nodes in failure domains:

    ```bash
    for node in 18.201.172.13 34.240.124.21 18.202.166.64 3.255.93.171 18.203.137.243 34.254.151.249 3.254.112.77 54.247.13.91 34.245.203.245 63.35.225.98 52.215.56.158 54.247.20.174 3.255.150.131 52.210.49.97; do
      kubectl taint nodes $node weka.io/dedicated=weka-backend:NoSchedule
    done
    ```
3.  Remove WEKA labels from the untainted node:

    ```bash
    kubectl label nodes 54.78.16.52 weka.io/supports-clients-
    kubectl label nodes 54.78.16.52 weka.io/supports-backends-
    ```
4. Configure the WekaCluster:
   1.  Create a configuration file named `cluster.yaml` with the following content:

       ```yaml
       failureDomainLabel: "weka.io/failure-domain"
       ```
   2.  Apply the configuration:

       ```bash
       kubectl apply -f cluster.yaml
       ```
5. Verify failure domain configuration:
   1. Check container distribution across failure domains using the WEKA cluster container.
   2.  Test failure domain behavior by draining nodes which have same FD :

       ```bash
       kubectl drain 18.201.172.13 34.240.124.21 --ignore-daemonsets --delete-local-data
       ```
   3.  Reboot the drained nodes:

       ```bash
       ssh <node-ip>
       sudo reboot
       ```
   4.  Monitor workload redistribution:\
       Check that workloads are redistributed to other failure domains while nodes in one FD are down

       ```bash
       kubectl get pods -o wide
       ```

#### Expected results

After completing this procedure:

* Your nodes are properly configured with failure domains.
* Workloads are distributed according to the failure domain configuration.
* The system is ready for node removal with minimal disruption.

#### Troubleshooting

If workloads do not redistribute as expected after node drain:

1. Check node labels and taints.
2. Verify the WekaCluster configuration.
3. Review the Kubernetes scheduler logs for any errors.

***

### Perform a graceful node reboot on client nodes

A graceful node reboot ensures minimal service disruption when you need to restart a node for maintenance, updates, or configuration changes. The procedure involves cordoning the node, draining workloads, performing the reboot, and then returning the node to service.

#### **Procedure**

1. Cordon the Kubernetes node to prevent new workloads from being scheduled:

```bash
kubectl cordon <node-ip>
```

2. Drain the node to safely evict all pods:

```
kubectl drain <node-ip> --delete-emptydir-data --ignore-daemonsets --force
```

Note: The system displays warnings about DaemonSet-managed pods being ignored. This is expected behavior.

3. Verify the node status shows as `SchedulingDisabled`:

```bash
kubectl get nodes
```

4. Reboot the target node:

```bash
sudo reboot
```

5. Wait for the node to complete its reboot cycle and return to a `Ready` state:

```bash
kubectl get nodes
```

6. Uncordon the node to allow new workloads to be scheduled:

```bash
kubectl uncordon <node-ip>
```

7. Verify that pods are running correctly on the node:

```
kubectl get pods --all-namespaces
```

See examples in #perform-standard-verification-steps.

#### Expected results

After completing this procedure:

* The node has completed a clean reboot cycle.
* All pods is rescheduled and running.
* The node is available for new workload scheduling.

#### Troubleshooting

If pods fail to start after the reboot:

1. Check pod status and events using `kubectl describe pod <pod-name>.`
2. Review node conditions using `kubectl describe node <node-ip>.`
3. Examine system logs for any errors or warnings.

***

### Replace a drive in a converged setup

Drive replacement is necessary when hardware failures occur or system upgrades are required. Following this procedure ensures minimal system disruption while maintaining data integrity.

#### Before you begin

* Ensure a replacement drive ready for installation.
* Identify the node and drive that needs replacement.
* Ensure you have the necessary permissions to execute Kubernetes commands
* Back up any critical data if necessary.

#### Procedure

1. **List and record drive information**:
   1.  List the available drives on the target node:

       ```
       lsblk
       ```
   2.  Identify the serial ID of the drives:

       ```
| ls -l /dev/disk/by-id | grep nvme |
       ```
   3.  Record the current drive configuration:

       ```
       weka cluster drive --verbose
       ```
   4. Save the serial ID of the drive being replaced for later use.

<details>

<summary>Example</summary>

```
$ lsblk

| $ ls -l /dev/disk/by-id | grep nvme |
lrwxrwxrwx 1 root root 13 Jan 21 06:22 nvme-Amazon_EC2_NVMe_Instance_Storage_AWS22956E1E147546CE0 -> ../../nvme1n1
lrwxrwxrwx 1 root root 13 Jan 21 06:22 nvme-Amazon_EC2_NVMe_Instance_Storage_AWS22956E1E147546CE0_1 -> ../../nvme1n1
lrwxrwxrwx 1 root root 13 Jan 21 06:22 nvme-Amazon_EC2_NVMe_Instance_Storage_AWS22B489297A8BDAE28 -> ../../nvme2n1
lrwxrwxrwx 1 root root 13 Jan 21 06:22 nvme-Amazon_EC2_NVMe_Instance_Storage_AWS22B489297A8BDAE28_1 -> ../../nvme2n1

root@ip-10-0-86-72:/# weka cluster drive --verbose
UID                                   DISK ID  UUID                                  HOST ID  HOSTNAME         NODE ID  DEVICE PATH   SIZE      STATUS  STATUS TIME  FAILURE DOMAIN  FAILURE DOMAIN ID  WRITABLE  LIFETIME % USED  NVKV % USED  ATTACHMENT  VENDOR  FIRMWARE  SERIAL NUMBER         MODEL                             ADDED     REMOVED  BLOCK SIZE  SPARES REMAINING  SPARES THRESHOLD  DRIVE STATUS
cee03321-4874-c49d-d33b-ab3286e9e5e9  0        cc29f627-d90b-4ffc-9cb6-bd00c0859ba6  1        ip-10-0-86-72    21       0000:00:1e.0  6.82 TiB  ACTIVE  0:16:12h     AUTO            1                  Writable  0                1            OK          AMAZON  0         AWS1B0D53508C5F4ADC7  Amazon EC2 NVMe Instance Storage  0:16:46h           512         100               0                 OK
c7590021-e14a-1469-2dc6-fa23e3b458b0  1        98e66c84-1b2f-4c9c-98e6-175bc693daf8  2        ip-10-0-108-188  41       0000:00:1e.0  6.82 TiB  ACTIVE  0:16:12h     AUTO            4                  Writable  0                1            OK          AMAZON  0         AWS19912EA78EC9334FC  Amazon EC2 NVMe Instance Storage  0:16:46h           512         100               0                 OK
af29dd72-47f2-1ba5-49a3-cf5bb6de8bdd  2        a00537e5-128d-4a8f-9ce4-4a410c5dedb7  3        ip-10-0-107-76   61       0000:00:1e.0  6.82 TiB  ACTIVE  0:16:12h     AUTO            2                  Writable  0                1            OK          AMAZON  0         AWS2283053DCF0B24EB8  Amazon EC2 NVMe Instance Storage  0:16:46h           512         100               0                 OK
6b97cc0e-4a9a-6dae-cc16-b55e63105c9a  3        58fd7006-6727-46cd-b408-95693c70b525  0        ip-10-0-115-154  1        0000:00:1e.0  6.82 TiB  ACTIVE  0:16:12h     AUTO            0                  Writable  0                1            OK          AMAZON  0         AWS11FCE0C874C4432A2  Amazon EC2 NVMe Instance Storage  0:16:46h           512         100               0                 OK
17c58212-351b-88e2-bc2e-ead01ab4b525  4        92197281-8d39-43aa-86c2-bb8901ac2f2f  7        ip-10-0-106-169  141      0000:00:1e.0  6.82 TiB  ACTIVE  0:16:12h     AUTO            3                  Writable  0                1            OK          AMAZON  0         AWS228BDF37DD8C9F561  Amazon EC2 NVMe Instance Storage  0:16:45h           512         100               0                 OK
34fc77be-1041-02a8-f491-43ef142ced47  5        5b87de1e-1cf6-4c95-9ba8-03381e206384  6        ip-10-0-97-8     121      0000:00:1e.0  6.82 TiB  ACTIVE  0:16:12h     AUTO            5                  Writable  0                1            OK          AMAZON  0         AWS22956E1E147546CE0  Amazon EC2 NVMe Instance Storage  0:16:44h           512         100               0                 OK

```

</details>

2. **Remove node label**: Remove the WEKA backend support label from the target node:

```
kubectl label nodes <node-ip> weka.io/supports-backends-
```

<details>

<summary>Example</summary>

```
kubectl label nodes 3.250.187.202 weka.io/supports-backends-
node/3.250.187.202 unlabeled
```

</details>

3. **Delete drive container**: Delete the WEKA container object associated with the drive. Then, verify that the container pod enters a pending state and the drive is removed from the cluster.

```
kubectl delete wekacontainer <drive-container-name> -n weka-operator-system
```

<details>

<summary>Example</summary>

```
pod status
weka-operator-system   cluster-dev-drive-653b08c0-2a12-41ce-8b6d-f2bc3af9eb16                                           0/1     Pending     0          61s     <none>         <none>           <none>           <none>

wekacontainer
weka-operator-system   cluster-dev-drive-653b08c0-2a12-41ce-8b6d-f2bc3af9eb16                                           PodNotRunning   drive                                                      23s
```

</details>

3. **Sign the new drive**:
   1.  Create a YAML configuration file for drive signing:

       ```yaml
       apiVersion: weka.weka.io/v1alpha1
       kind: WekaManualOperation
       metadata:
         name: sign-specific-drives
         namespace: weka-operator-system
       spec:
         action: "sign-drives"
         image: quay.io/weka.io/weka-in-container:4.4.2.144-k8s
         imagePullSecret: "quay-io-robot-secret"
         payload:
           signDrivesPayload:
             type: device-paths
             nodeSelector:
               weka.io/supports-backends: "true"
             devicePaths:
               - /dev/nvme2n1
       ```
   2.  Apply the configuration:

       ```
       kubectl apply -f sign_devicepath_drive.yaml
       ```

<details>

<summary>Example</summary>

```
$ kubectl apply -f sign_devicepath_drive.yaml
wekamanualoperation.weka.weka.io/sign-specific-drives created

$ kubectl get wekamanualoperation --all-namespaces
NAMESPACE              NAME                   ACTION        STATUS   AGE
weka-operator-system   sign-specific-drives   sign-drives            30s                                                   23s
```

</details>

4. **Block the old drive**:
   1.  Create a YAML configuration file for blocking the old drive:

       ```yaml
       apiVersion: weka.weka.io/v1alpha1
       kind: WekaManualOperation
       metadata:
         name: block-drive
         namespace: weka-operator-system
       spec:
         action: "block-drives"
         image: quay.io/weka.io/weka-in-container:4.4.2.144-k8s
         imagePullSecret: "quay-io-robot-secret"
         payload:
           blockDrivesPayload:
             serialIDs:
               - "<old-drive-serial-id>"
             node: "<node-ip>"
       ```
   2.  Apply the configuration:

       ```bash
       kubectl apply -f blockdrive.yaml
       ```

<details>

<summary>Example</summary>

```
$ kubectl apply -f blockdrive.yaml
wekamanualoperation.weka.weka.io/block-drive created
$ kubectl get wekamanualoperation --all-namespaces
NAMESPACE              NAME                   ACTION         STATUS   AGE
weka-operator-system   block-drive            block-drives   Done     26s
weka-operator-system   sign-specific-drives   sign-drives             13m
```

</details>

5. **Restore node label**: Re-add the WEKA backend support label to the node:

```
kubectl label nodes <node-ip> weka.io/supports-backends=true
```

<details>

<summary>Example</summary>

```
$ kubectl label nodes 3.250.187.202 weka.io/supports-backends=true
node/3.250.187.202 labeled
```

</details>

5. **Verify the replacement**:
   1.  Check the cluster drive status:

       ```bash
       weka cluster drive --verbose
       ```
   2. Verify that:
      * The new drive appears in the cluster.
      * The drive status is ACTIVE.
      * The serial ID matches the replacement drive.

#### Troubleshooting

* If the container pod remains in a pending state, check the pod events and logs.
* If drive signing fails, verify the device path and node selector.
* If the old drive remains visible, ensure the block operation completed successfully.

Note: - Maintain system stability by replacing one drive at a time.
- Keep track of all serial IDs involved in the replacement process.
- Monitor system health throughout the procedure.

***

### Replace a Kubernetes node

This procedure enables systematic node replacement while maintaining cluster functionality and minimizing service interruption, addressing performance issues, hardware failures, or routine maintenance needs.

#### Prerequisites

* Identification of the node to be replaced.
* A new node prepared for integration into the cluster.

#### Procedure

1. **Remove node deployment label**: Remove the existing label used to deploy the cluster from the node:

```
kubectl label nodes <old-node-ip> weka.io/supports-backends-
```

<details>

<summary>Example</summary>

```
$ kubectl label nodes 54.247.143.85 weka.io/supports-backends-
node/54.247.143.85 unlabeled
```

</details>

2. **List existing WEKA containers to identify containers on the node**:

```
kubectl get wekacontainers --all-namespaces -o wide
```

<details>

<summary>Example</summary>

```
$ kubectl get wekacontainers --all-namespaces -o wide
NAMESPACE              NAME                                                       STATUS      MODE              MANAGEMENT IP   NODE             PROCESSES   DRIVES   MOUNTS   CPU   AGE   WEKA CID   MESSAGE
weka-operator-system   cluster-dev-compute-16ab60f0-5386-4366-97c3-b1e1e674969a   Running     compute           10.0.85.156     52.211.177.232                                       54m   3
weka-operator-system   cluster-dev-compute-6c61590e-84a6-40c4-8f4b-f225232336ac   Running     compute           10.0.64.153     3.255.86.119                                         54m   5
weka-operator-system   cluster-dev-compute-85da88e2-a554-4bea-b2c6-55cab244f0b8   Running     compute           10.0.102.49     34.242.209.110                                       54m   1
weka-operator-system   cluster-dev-compute-c92918d4-55fb-4cfe-b79f-db7341df4654   Running     compute           10.0.65.129     18.201.119.122                                       54m   8
weka-operator-system   cluster-dev-compute-ca3b9771-0ddc-4237-9e75-3af1fe5dc1ee   Running     compute           10.0.102.67     54.247.143.85                                        28m   2
weka-operator-system   cluster-dev-compute-d4fc062a-aa22-4c47-a81e-67e8ec7e5f44   Running     compute           10.0.79.86      34.243.254.47                                        54m   0
weka-operator-system   cluster-dev-drive-1d000410-af7f-4984-8e89-7718bc8f4963     Running     drive             10.0.65.129     18.201.119.122                                       54m   6
weka-operator-system   cluster-dev-drive-333de17c-be86-4601-bbca-6140cab8e98b     Running     drive             10.0.102.67     54.247.143.85                                        27m   4
weka-operator-system   cluster-dev-drive-3cebf172-89a3-44f5-b0ef-7734027dab62     Running     drive             10.0.85.156     52.211.177.232                                       54m   10
weka-operator-system   cluster-dev-drive-5274dd78-449c-4e2a-8063-5bde4bed4823     Running     drive             10.0.79.86      34.243.254.47                                        54m   7
weka-operator-system   cluster-dev-drive-6ce82377-e934-48ca-8dea-4678708b04cd     Running     drive             10.0.64.153     3.255.86.119                                         54m   11
weka-operator-system   cluster-dev-drive-d3eac8b6-505d-4e9e-90c8-c7e360e0bf3e     Running     drive             10.0.102.49     34.242.209.110                                       54m   9
weka-operator-system   weka-driver-dist                                           Running     drivers-dist                                                                           54m
weka-operator-system   weka-drivers-builder                                       Completed   drivers-builder                                                                        54m
```

</details>

3. **Delete the compute and drive containers specific to the node**:

```
kubectl delete wekacontainer <compute-container-name> -n weka-operator-system
kubectl delete wekacontainer <drive-container-name> -n weka-operator-system
```

<details>

<summary>Example</summary>

```
$ kubectl delete wekacontainer cluster-dev-compute-ca3b9771-0ddc-4237-9e75-3af1fe5dc1ee -n weka-operator-system
wekacontainer.weka.weka.io "cluster-dev-compute-ca3b9771-0ddc-4237-9e75-3af1fe5dc1ee" deleted

$ kubectl delete wekacontainer cluster-dev-drive-333de17c-be86-4601-bbca-6140cab8e98b -n weka-operator-system
wekacontainer.weka.weka.io "cluster-dev-drive-333de17c-be86-4601-bbca-6140cab8e98b" deleted

```

</details>

4. **Verify container deletion:**
   1. Verify containers are in `PodNotRunning` status.
   2.  Confirm no containers are running on the old node.

       Look for:

       * `STATUS` column showing `PodNotRunning`.
       * No containers associated with the old node.

```bash
kubectl get wekacontainers --all-namespaces -o wide
```

<details>

<summary>Example</summary>

```
$ kubectl get wekacontainers --all-namespaces -o wide
NAMESPACE              NAME                                                       STATUS          MODE              MANAGEMENT IP   NODE             PROCESSES   DRIVES   MOUNTS   CPU   AGE     WEKA CID   MESSAGE
weka-operator-system   cluster-dev-compute-16ab60f0-5386-4366-97c3-b1e1e674969a   Running         compute           10.0.85.156     52.211.177.232                                       59m     3
weka-operator-system   cluster-dev-compute-484d6228-e833-4337-a2dd-be8755063ef1   PodNotRunning   compute                                                                                2m47s
weka-operator-system   cluster-dev-compute-6c61590e-84a6-40c4-8f4b-f225232336ac   Running         compute           10.0.64.153     3.255.86.119                                         59m     5
weka-operator-system   cluster-dev-compute-85da88e2-a554-4bea-b2c6-55cab244f0b8   Running         compute           10.0.102.49     34.242.209.110                                       59m     1
weka-operator-system   cluster-dev-compute-c92918d4-55fb-4cfe-b79f-db7341df4654   Running         compute           10.0.65.129     18.201.119.122                                       59m     8
weka-operator-system   cluster-dev-compute-d4fc062a-aa22-4c47-a81e-67e8ec7e5f44   Running         compute           10.0.79.86      34.243.254.47                                        59m     0
weka-operator-system   cluster-dev-drive-1d000410-af7f-4984-8e89-7718bc8f4963     Running         drive             10.0.65.129     18.201.119.122                                       59m     6
weka-operator-system   cluster-dev-drive-3cebf172-89a3-44f5-b0ef-7734027dab62     Running         drive             10.0.85.156     52.211.177.232                                       59m     10
weka-operator-system   cluster-dev-drive-5274dd78-449c-4e2a-8063-5bde4bed4823     Running         drive             10.0.79.86      34.243.254.47                                        59m     7
weka-operator-system   cluster-dev-drive-6ce82377-e934-48ca-8dea-4678708b04cd     Running         drive             10.0.64.153     3.255.86.119                                         59m     11
weka-operator-system   cluster-dev-drive-b9204e9c-31f0-4eae-9a93-aed4aecc9555     PodNotRunning   drive                                                                                  16s
weka-operator-system   cluster-dev-drive-d3eac8b6-505d-4e9e-90c8-c7e360e0bf3e     Running         drive             10.0.102.49     34.242.209.110                                       59m     9
weka-operator-system   weka-driver-dist                                           Running         drivers-dist                                                                           59m
weka-operator-system   weka-drivers-builder                                       Completed       drivers-builder

```

</details>

5. **Add backend label to new node**: Label the new node to support backends:

```
kubectl label nodes <new-node-ip> weka.io/supports-backends=true
```

<details>

<summary>Example</summary>

```
$ kubectl label nodes 54.73.54.127 weka.io/supports-backends=true
node/54.73.54.127 labeled
```

</details>

6. **Sign drives on new node**:
   1.  Create a WekaManualOperation configuration to sign drives:

       ```yaml
       apiVersion: weka.weka.io/v1alpha1
       kind: WekaManualOperation
       metadata:
         name: sign-specific-drives
         namespace: weka-operator-system
       spec:
         action: "sign-drives"
         image: quay.io/weka.io/weka-in-container:4.4.2.144-k8s
         imagePullSecret: "quay-io-robot-secret"
         payload:
           signDrivesPayload:
             type: device-paths
             nodeSelector:
               weka.io/supports-backends: "true"
             devicePaths:
               - /dev/nvme0n1
               - /dev/nvme1n1
       ```
   2.  Apply the configuration:

       ```bash
       kubectl apply -f sign_devicepath_drive.yaml
       ```

<details>

<summary>Example</summary>

```
apiVersion: weka.weka.io/v1alpha1
kind: WekaManualOperation
metadata:
  name: sign-specific-drives
  namespace: weka-operator-system
spec:
  action: "sign-drives"
  image: quay.io/weka.io/weka-in-container:4.4.2.144-k8s
  imagePullSecret: "quay-io-robot-secret"
  payload:
    signDrivesPayload:
      type: device-paths
      nodeSelector:
        weka.io/supports-backends: "true"
      devicePaths:
        - /dev/nvme0n1
        - /dev/nvme1n1

$ kubectl apply -f sign_devicepath_drive.yaml
wekamanualoperation.weka.weka.io/sign-specific-drives created
```

</details>

7.  **Verification steps**:

    1. Verify WEKA containers are rescheduled.
    2. Check that new containers are running on the new node's IP.
    3. Validate cluster status using WEKA CLI.

    For details, see #verification-steps.

Note: **Non-functional node replacement**:\
When a node becomes unresponsive or faulty, delete the non-functional node:\
`kubectl delete node <node-name>`
Kubernetes automatically handles the following:
* Detects node failure.
* Removes affected containers.
* Reschedules containers to available nodes.

#### Troubleshooting

If containers fail to reschedule, check:

* Node labels
* Drive signing process
* Cluster resource availability
* Network connectivity

***

### Remove WEKA container from a failed node

Removing a WEKA container from a failed node is necessary to maintain cluster health and prevent any negative impact on system performance. This procedure ensures that the container is removed safely and the cluster remains operational.

#### Procedure: Remove WEKA container from an active node

Follow these steps to remove a WEKA container when the node is responsive:

1.  Request WEKA container deletion by setting the deletion timestamp:

    ```bash
    kubectl delete wekacontainer <container-name> -n weka-operator-system
    ```
2.  If this is a drive container, deactivate the drives:

    ```bash
    weka cluster drive deactivate
    ```
3.  Deactivate the container:

    ```bash
    weka cluster container deactivate
    ```
4.  For drive containers, remove the drives:

    ```bash
    weka cluster drive remove
    ```
5.  Remove the WEKA container:

    ```bash
    weka cluster container remove
    ```
6.  For drive containers, force resign the drives:

    ```bash
    kubectl apply -f resign-drives.yaml
    ```

#### Procedure: Remove WEKA container from a failed node

When the node is unresponsive, follow these steps:

1. Follow steps 1-5 from the active node procedure above.
2.  If the resign drives operation fails with the error "container node is not ready, cannot perform resign drives operation", set the skip flag:

    ```bash
    kubectl patch WekaContainer <container-name> -n weka-operator-system \
      --type='merge' \
      -p='{"status":{"skipDrivesForceResign": true}}' \
      --subresource=status
    ```
3. Wait for the pod to enter the `Terminating` state.

Note: If the dead node is removed from the Kubernetes cluster, the WEKA Container and corresponding stuck pod are automatically removed.

#### Resign drives manually

If you need to manually resign specific drives, create and apply the following YAML configuration:

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaManualOperation
metadata:
  name: sign-specific-drives-paths
  namespace: weka-operator-system
spec:
  action: "force-resign-drives"
  image: quay.io/weka.io/weka-in-container:4.3.5.105-dist-drivers.5
  imagePullSecret: "quay-io-robot-secret"
  payload:
    forceResignDrivesPayload:
      node_name: "<node-name>"
      device_serials:
        - <device-serial>
      # Alternative: use device_paths instead of device_serials
      # device_paths:
      #   - /dev/nmve1
```

<details>

<summary>Example: wekacontainer conditions added on deletion</summary>

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  creationTimestamp: "2024-11-27T08:43:45Z"
  deletionGracePeriodSeconds: 0
  deletionTimestamp: "2024-11-27T08:56:47Z"
  finalizers:
  - weka.weka.io/finalizer
  generation: 3
  labels:
    weka.io/cluster-id: 2bf91f1c-8a71-4b62-b177-78d3ba7eb4b0
    weka.io/mode: drive
  name: cluster-dev-drive-0d107d15-52b1-488e-ac66-0805ff178f19
  namespace: weka-operator-system
  ownerReferences:
  - apiVersion: weka.weka.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: WekaCluster
    name: cluster-dev
    uid: 2bf91f1c-8a71-4b62-b177-78d3ba7eb4b0
  resourceVersion: "12129"
  uid: 120deca7-a15f-4700-ad2a-26c79612f85b
spec:
  cpuPolicy: auto
  driversDistService: https://weka-driver-dist.weka-operator-system.svc.cluster.local:60002
  hugepages: 1800
  hugepagesOffset: 400
  hugepagesSize: 2Mi
  image: quay.io/weka.io/weka-in-container:4.3.5.105-dist-drivers.5
  imagePullSecret: quay-io-robot-secret
  joinIpPorts:
  - 10.0.23.55:15000
  - 10.0.18.232:15000
  - 10.0.31.156:15000
  - 10.0.16.81:15000
  - 10.0.23.55:15100
  mode: drive
  name: drivex0d107d15x52b1x488exac66x0805ff178f19
  network:
    aws: {}
  nodeSelector:
    weka.io/supports-backends: "true"
  numCores: 1
  numDrives: 2
  state: active
  upgradePolicyType: manual
  wekaSecretRef:
    secretKeyRef:
      key: weka-operator-2bf91f1c-8a71-4b62-b177-78d3ba7eb4b0
      name: ""
status:
  allocations:
    agentPort: 15303
    drives:
    - AWS1BB731F74767CF580
    - AWS1833047864CE22C1B
    wekaPort: 15000
  clusterID: 73aa92d1-23a5-48eb-ad90-6a998df4652e
  conditions:
  - lastTransitionTime: "2024-11-27T08:43:46Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ContainerAffinitySet
  - lastTransitionTime: "2024-11-27T08:43:53Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ContainerResourcesWritten
  - lastTransitionTime: "2024-11-27T08:46:59Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: EnsuredDrivers
  - lastTransitionTime: "2024-11-27T08:47:21Z"
    message: Container joined cluster
    reason: Init
    status: "True"
    type: JoinedCluster
  - lastTransitionTime: "2024-11-27T08:47:29Z"
    message: ""
    reason: PeriodicUpdate
    status: "True"
    type: JoinIpsSet
  - lastTransitionTime: "2024-11-27T08:47:32Z"
    message: Added 2 drives
    reason: Init
    status: "True"
    type: DrivesAdded
  - lastTransitionTime: "2024-11-27T08:56:50Z"
    message: Completed successfully
    reason: Deletion
    status: "True"
    type: ContainerDrivesDeactivated
  - lastTransitionTime: "2024-11-27T08:56:51Z"
    message: Completed successfully
    reason: Deletion
    status: "True"
    type: ContainerDeactivated
  - lastTransitionTime: "2024-11-27T08:57:05Z"
    message: Completed successfully
    reason: Deletion
    status: "True"
    type: ContainerDrivesRemoved
  - lastTransitionTime: "2024-11-27T08:57:07Z"
    message: Completed successfully
    reason: Deletion
    status: "True"
    type: ContainerRemoved
  - lastTransitionTime: "2024-11-27T08:57:23Z"
    message: Completed successfully
    reason: Deletion
    status: "True"
    type: ContainerDrivesResigned
  containerID: 6
  lastAppliedImage: quay.io/weka.io/weka-in-container:4.3.5.105-dist-drivers.5
  managementIP: 10.0.27.215
  nodeAffinity: 3.254.77.121
  status: Running
```

</details>

#### Verification

You can verify the removal process by checking the WEKA container conditions. A successful removal shows the following conditions in order:

1. ContainerDrivesDeactivated
2. ContainerDeactivated
3. ContainerDrivesRemoved
4. ContainerRemoved
5. ContainerDrivesResigned

***

### Replace a container on an active node

Replacing a container on an active node allows for system upgrades or failure recovery without shutting down services. This procedure ensures that the replacement is performed smoothly, keeping the cluster operational while the container is swapped out.

#### Procedure

**Phase 1: Delete the existing container**

1.  Identify the container to be replaced:

    ```bash
    kubectl get pods -n weka-operator-system
    ```
2.  Delete the selected container:

    ```bash
    kubectl delete pod <container-name> -n weka-operator-system
    ```

<details>

<summary>Example</summary>

```
$ kubectl delete pod cluster-dev-drive-05ddc629-b7f5-4090-8736-b9fc3b48ad82 -n weka-operator-system
pod "cluster-dev-drive-05ddc629-b7f5-4090-8736-b9fc3b48ad82" deleted
```

</details>

**Phase 2: Monitor deactivation process**

1.  Verify that the container and its drives are being deactivated:

    ```
    weka cluster container
    ```

    Expected status: The container shows DRAINED (DOWN) under the STATUS column.
2.  Check the process status:

    ```
    weka cluster process
    ```

    Expected status: The processes associated with the container show DOWN status.
3.  For drive containers, verify drive status:

    ```
    weka cluster drive
    ```

    Look for:

    * Drive status changes from ACTIVE to FAILED for the affected container.
    * All other drives remain ACTIVE.

<details>

<summary>Example</summary>

```
root@ip-10-0-79-159:/# weka cluster container
HOST ID  HOSTNAME         CONTAINER                                     IPS           STATUS          REQUESTED ACTION  RELEASE            FAILURE DOMAIN  CORES  MEMORY   UPTIME    LAST FAILURE  REQUESTED ACTION FAILURE
0        ip-10-0-116-144  drivexcbc24786xdce1x4f0dx93ecx174a06206c6e    10.0.116.144  UP              NONE              4.4.1.89-k8s-beta  x5              1      1.54 GB  0:10:03h
1        ip-10-0-118-174  drivex49d0f816x2a9bx45f2x844ax05ddd6182f3e    10.0.118.174  UP              NONE              4.4.1.89-k8s-beta  x2              1      1.54 GB  0:10:04h
2        ip-10-0-81-44    computex45f8ef2bx2b74x4d7fxa01bx096c857b6740  10.0.81.44    UP              NONE              4.4.1.89-k8s-beta  x8              1      2.94 GB  0:10:04h
3        ip-10-0-116-144  computexe207f94ax6b5ex4217xb963x5573c0f5201d  10.0.116.144  UP              NONE              4.4.1.89-k8s-beta  x5              1      2.94 GB  0:10:02h
4        ip-10-0-100-147  computex3f265472x00bfx4172xbaaex5e5f20364aa8  10.0.100.147  UP              NONE              4.4.1.89-k8s-beta  x6              1      2.94 GB  0:07:15h
5        ip-10-0-117-96   drivex69ffc965x6cd5x489cxb44bx9abacbce7a98    10.0.117.96   UP              NONE              4.4.1.89-k8s-beta  x2              1      1.54 GB  0:07:22h
6        ip-10-0-79-159   drivexd012285ex65a9x4dc9xa2d6x32a4c834f02f    10.0.79.159   UP              NONE              4.4.1.89-k8s-beta  x1              1      1.54 GB  0:10:00h
7        ip-10-0-81-44    drivex0657c388xd04cx4c0bx8633x445671d86657    10.0.81.44    UP              NONE              4.4.1.89-k8s-beta  x8              1      1.54 GB  0:09:56h
8        ip-10-0-82-71    drivexaac103bexbbfbx48e0x8c49x7d4c57bc1730    10.0.82.71    UP              NONE              4.4.1.89-k8s-beta  x1              1      1.54 GB  0:09:56h
9        ip-10-0-99-7     computex0a01896fx1436x4314x9bcdx59d59da50257  10.0.99.7     UP              NONE              4.4.1.89-k8s-beta  x6              1      2.94 GB  0:09:59h
10       ip-10-0-121-214  computex09541214x6d47x49e3xabfbx2410ee6c1e2b  10.0.121.214  UP              NONE              4.4.1.89-k8s-beta  x5              1      2.94 GB  0:09:50h
11       ip-10-0-117-96   computex13e5e78cx9600x4c33x98b3x7d5fc16420b7  10.0.117.96   UP              NONE              4.4.1.89-k8s-beta  x2              1      2.94 GB  0:09:58h
12       ip-10-0-82-208   drivex9d266897x9dfbx4714xa5d9xc2db327dbee0    10.0.82.208   UP              NONE              4.4.1.89-k8s-beta  x3              1      1.54 GB  0:09:57h
13       ip-10-0-79-159   s3x1770abeaxba16x46aex9f4ax91aefb70cf1e       10.0.79.159   UP              NONE              4.4.1.89-k8s-beta  x1              1      1.26 GB  0:07:23h
14       ip-10-0-100-147  drivexd8f97316x1c7cx4610xa892xd66d900e6cbe    10.0.100.147  UP              NONE              4.4.1.89-k8s-beta  x6              1      1.54 GB  0:10:09h
15       ip-10-0-99-7     s3x9cd9970fxfaf9x46fcxb9c7xb5a53a5a0ccc       10.0.99.7     UP              NONE              4.4.1.89-k8s-beta  x6              1      1.26 GB  0:09:53h
16       ip-10-0-93-213   drivex6ca2fb06xf38ax4335x9861x22b43fe4f8a6    10.0.93.213   UP              NONE              4.4.1.89-k8s-beta  x3              1      1.54 GB  0:10:09h
17       ip-10-0-93-213   computexc25efa58x2120x490cxb5c1x09fe57e45cdc  10.0.93.213   UP              NONE              4.4.1.89-k8s-beta  x3              1      2.94 GB  0:10:08h
18       ip-10-0-121-214  drivex05ddc629xb7f5x4090x8736xb9fc3b48ad82    10.0.121.214  DRAINED (DOWN)  STOP              4.4.1.89-k8s-beta  x5              1      1.54 GB
19       ip-10-0-66-157   drivexb5168f94x8cd4x4b6dx843cx65cd6919d55f    10.0.66.157   UP              NONE              4.4.1.89-k8s-beta  x7              1      1.54 GB  0:10:06h
20       ip-10-0-88-165   drivex816a0286xe173x44b4xb528x9cbde1f698c7    10.0.88.165   UP              NONE              4.4.1.89-k8s-beta  x4              1      1.54 GB  0:10:05h
21       ip-10-0-118-174  computex8ddb4d56x338fx483fxaac2x9064bccbce17  10.0.118.174  UP              NONE              4.4.1.89-k8s-beta  x2              1      2.94 GB  0:10:06h
22       ip-10-0-88-165   computex310353c8xa349x479axb34cxa172531d3927  10.0.88.165   UP              NONE              4.4.1.89-k8s-beta  x4              1      2.94 GB  0:09:55h
23       ip-10-0-66-157   computex334e89efx1230x43e1xa4c2x7c4774c5383e  10.0.66.157   UP              NONE              4.4.1.89-k8s-beta  x7              1      2.94 GB  0:09:50h
24       ip-10-0-70-16    computexfc0fd513xf1ccx49eexa228xd209579a22cb  10.0.70.16    UP              NONE              4.4.1.89-k8s-beta  x4              1      2.94 GB  0:10:04h
25       ip-10-0-82-71    computexcc79625ax5291x44bdx850ax92320ea1a791  10.0.82.71    UP              NONE              4.4.1.89-k8s-beta  x1              1      2.94 GB  0:10:03h

root@ip-10-0-79-159:/# weka cluster process
PROCESS ID  CONTAINER ID  SLOT IN HOST  HOSTNAME         CONTAINER                                     IPS           STATUS  RELEASE            ROLES       NETWORK  CPU  MEMORY   UPTIME    LAST FAILURE
0           0             0             ip-10-0-116-144  drivexcbc24786xdce1x4f0dx93ecx174a06206c6e    10.0.116.144  UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
1           0             1             ip-10-0-116-144  drivexcbc24786xdce1x4f0dx93ecx174a06206c6e    10.0.116.144  UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:52h
20          1             0             ip-10-0-118-174  drivex49d0f816x2a9bx45f2x844ax05ddd6182f3e    10.0.118.174  UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
21          1             1             ip-10-0-118-174  drivex49d0f816x2a9bx45f2x844ax05ddd6182f3e    10.0.118.174  UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:53h
40          2             0             ip-10-0-81-44    computex45f8ef2bx2b74x4d7fxa01bx096c857b6740  10.0.81.44    UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
41          2             1             ip-10-0-81-44    computex45f8ef2bx2b74x4d7fxa01bx096c857b6740  10.0.81.44    UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
60          3             0             ip-10-0-116-144  computexe207f94ax6b5ex4217xb963x5573c0f5201d  10.0.116.144  UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
61          3             1             ip-10-0-116-144  computexe207f94ax6b5ex4217xb963x5573c0f5201d  10.0.116.144  UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
80          4             0             ip-10-0-100-147  computex3f265472x00bfx4172xbaaex5e5f20364aa8  10.0.100.147  UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:59h
81          4             1             ip-10-0-100-147  computex3f265472x00bfx4172xbaaex5e5f20364aa8  10.0.100.147  UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
100         5             0             ip-10-0-117-96   drivex69ffc965x6cd5x489cxb44bx9abacbce7a98    10.0.117.96   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:58h  Host joined a new cluster (7 minutes ago)
101         5             1             ip-10-0-117-96   drivex69ffc965x6cd5x489cxb44bx9abacbce7a98    10.0.117.96   UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:53h
120         6             0             ip-10-0-79-159   drivexd012285ex65a9x4dc9xa2d6x32a4c834f02f    10.0.79.159   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
121         6             1             ip-10-0-79-159   drivexd012285ex65a9x4dc9xa2d6x32a4c834f02f    10.0.79.159   UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:52h
140         7             0             ip-10-0-81-44    drivex0657c388xd04cx4c0bx8633x445671d86657    10.0.81.44    UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
141         7             1             ip-10-0-81-44    drivex0657c388xd04cx4c0bx8633x445671d86657    10.0.81.44    UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:52h
160         8             0             ip-10-0-82-71    drivexaac103bexbbfbx48e0x8c49x7d4c57bc1730    10.0.82.71    UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
161         8             1             ip-10-0-82-71    drivexaac103bexbbfbx48e0x8c49x7d4c57bc1730    10.0.82.71    UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:53h
180         9             0             ip-10-0-99-7     computex0a01896fx1436x4314x9bcdx59d59da50257  10.0.99.7     UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
181         9             1             ip-10-0-99-7     computex0a01896fx1436x4314x9bcdx59d59da50257  10.0.99.7     UP      4.4.1.89-k8s-beta  COMPUTE     UDP      1    2.94 GB  0:06:52h
200         10            0             ip-10-0-121-214  computex09541214x6d47x49e3xabfbx2410ee6c1e2b  10.0.121.214  UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
201         10            1             ip-10-0-121-214  computex09541214x6d47x49e3xabfbx2410ee6c1e2b  10.0.121.214  UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
220         11            0             ip-10-0-117-96   computex13e5e78cx9600x4c33x98b3x7d5fc16420b7  10.0.117.96   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
221         11            1             ip-10-0-117-96   computex13e5e78cx9600x4c33x98b3x7d5fc16420b7  10.0.117.96   UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
240         12            0             ip-10-0-82-208   drivex9d266897x9dfbx4714xa5d9xc2db327dbee0    10.0.82.208   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:48h  Host joined a new cluster (7 minutes ago)
241         12            1             ip-10-0-82-208   drivex9d266897x9dfbx4714xa5d9xc2db327dbee0    10.0.82.208   UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:52h
260         13            0             ip-10-0-79-159   s3x1770abeaxba16x46aex9f4ax91aefb70cf1e       10.0.79.159   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
261         13            1             ip-10-0-79-159   s3x1770abeaxba16x46aex9f4ax91aefb70cf1e       10.0.79.159   UP      4.4.1.89-k8s-beta  FRONTEND    UDP      2    1.26 GB  0:06:52h
280         14            0             ip-10-0-100-147  drivexd8f97316x1c7cx4610xa892xd66d900e6cbe    10.0.100.147  UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:58h  Host joined a new cluster (7 minutes ago)
281         14            1             ip-10-0-100-147  drivexd8f97316x1c7cx4610xa892xd66d900e6cbe    10.0.100.147  UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:53h
300         15            0             ip-10-0-99-7     s3x9cd9970fxfaf9x46fcxb9c7xb5a53a5a0ccc       10.0.99.7     UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
301         15            1             ip-10-0-99-7     s3x9cd9970fxfaf9x46fcxb9c7xb5a53a5a0ccc       10.0.99.7     UP      4.4.1.89-k8s-beta  FRONTEND    UDP      2    1.26 GB  0:06:52h
320         16            0             ip-10-0-93-213   drivex6ca2fb06xf38ax4335x9861x22b43fe4f8a6    10.0.93.213   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:58h  Host joined a new cluster (7 minutes ago)
321         16            1             ip-10-0-93-213   drivex6ca2fb06xf38ax4335x9861x22b43fe4f8a6    10.0.93.213   UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:47h
340         17            0             ip-10-0-93-213   computexc25efa58x2120x490cxb5c1x09fe57e45cdc  10.0.93.213   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
341         17            1             ip-10-0-93-213   computexc25efa58x2120x490cxb5c1x09fe57e45cdc  10.0.93.213   UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:50h
360         18            0             ip-10-0-121-214  drivex05ddc629xb7f5x4090x8736xb9fc3b48ad82    10.0.121.214  DOWN    4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A                Host joined a new cluster (7 minutes ago)
361         18            1             ip-10-0-121-214  drivex05ddc629xb7f5x4090x8736xb9fc3b48ad82    10.0.121.214  DOWN    4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB
380         19            0             ip-10-0-66-157   drivexb5168f94x8cd4x4b6dx843cx65cd6919d55f    10.0.66.157   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
381         19            1             ip-10-0-66-157   drivexb5168f94x8cd4x4b6dx843cx65cd6919d55f    10.0.66.157   UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:53h
400         20            0             ip-10-0-88-165   drivex816a0286xe173x44b4xb528x9cbde1f698c7    10.0.88.165   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
401         20            1             ip-10-0-88-165   drivex816a0286xe173x44b4xb528x9cbde1f698c7    10.0.88.165   UP      4.4.1.89-k8s-beta  DRIVES      UDP      1    1.54 GB  0:06:53h
420         21            0             ip-10-0-118-174  computex8ddb4d56x338fx483fxaac2x9064bccbce17  10.0.118.174  UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
421         21            1             ip-10-0-118-174  computex8ddb4d56x338fx483fxaac2x9064bccbce17  10.0.118.174  UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
440         22            0             ip-10-0-88-165   computex310353c8xa349x479axb34cxa172531d3927  10.0.88.165   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
441         22            1             ip-10-0-88-165   computex310353c8xa349x479axb34cxa172531d3927  10.0.88.165   UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
460         23            0             ip-10-0-66-157   computex334e89efx1230x43e1xa4c2x7c4774c5383e  10.0.66.157   UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
461         23            1             ip-10-0-66-157   computex334e89efx1230x43e1xa4c2x7c4774c5383e  10.0.66.157   UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
480         24            0             ip-10-0-70-16    computexfc0fd513xf1ccx49eexa228xd209579a22cb  10.0.70.16    UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
481         24            1             ip-10-0-70-16    computexfc0fd513xf1ccx49eexa228xd209579a22cb  10.0.70.16    UP      4.4.1.89-k8s-beta  COMPUTE     UDP      1    2.94 GB  0:06:52h
500         25            0             ip-10-0-82-71    computexcc79625ax5291x44bdx850ax92320ea1a791  10.0.82.71    UP      4.4.1.89-k8s-beta  MANAGEMENT  UDP           N/A      0:06:57h  Host joined a new cluster (7 minutes ago)
501         25            1             ip-10-0-82-71    computexcc79625ax5291x44bdx850ax92320ea1a791  10.0.82.71    UP      4.4.1.89-k8s-beta  COMPUTE     UDP      2    2.94 GB  0:06:52h
root@ip-10-0-79-159:/# weka cluster drive
DISK ID  UUID                                  HOSTNAME         NODE ID  SIZE      STATUS  LIFETIME % USED  ATTACHMENT  DRIVE STATUS
0        2c5a2e45-72dd-481a-ae58-9672ab52fe86  ip-10-0-118-174  21       6.82 TiB  ACTIVE  0                OK          OK
1        12d86b77-24fa-4c08-9b25-b91758e17e99  ip-10-0-121-214  361      6.82 TiB  FAILED  0                OK          OK
2        0123005a-aa60-4d74-95da-dda248d41f6a  ip-10-0-93-213   321      6.82 TiB  ACTIVE  0                OK          OK
3        a6b749a7-6f7d-4099-a750-d8368bbc6174  ip-10-0-82-208   241      6.82 TiB  ACTIVE  0                OK          OK
4        8f9be465-c5e0-4a92-8c1f-c9b963a8a596  ip-10-0-79-159   121      6.82 TiB  ACTIVE  0                OK          OK
5        1850a4bb-9cbd-4e83-8428-ac2fee28ec7f  ip-10-0-116-144  1        6.82 TiB  ACTIVE  0                OK          OK
6        f45d8354-a294-4176-9d00-7ab61bc19225  ip-10-0-66-157   381      6.82 TiB  ACTIVE  0                OK          OK
7        7950caad-628e-48bf-b224-3571af78fc38  ip-10-0-81-44    141      6.82 TiB  ACTIVE  0                OK          OK
8        3270402b-794a-403d-a8ab-595afa60bb35  ip-10-0-117-96   101      6.82 TiB  ACTIVE  0                OK          OK
9        7dc6657d-3038-4b87-a4b6-c36d2fa2a08a  ip-10-0-100-147  281      6.82 TiB  ACTIVE  0                OK          OK
10       f310387c-16b7-4252-92ff-a1475b5bbbf6  ip-10-0-82-71    161      6.82 TiB  ACTIVE  0                OK          OK
11       6e3e1c8b-da59-4106-9e65-e27b586a0c71  ip-10-0-88-165   401      6.82 TiB  ACTIVE  0                OK          OK

```

</details>

**Phase 3: Monitor container recreation**

1.  Watch for the new container creation:

    ```bash
    kubectl get pods -o wide -n weka-operator-system -w
    ```
2.  Verify the new container's integration with the cluster:

    ```bash
    weka cluster container
    ```

    Expected result: A new container appears with UP status.
3.  Verify the new container's running status:

    ```bash
    kubectl get pods -n weka-operator-system
    ```

    Expected status: Running.
4.  Confirm the container's integration with the WEKA cluster:

    ```bash
    weka cluster host
    ```

    Expected status: UP.
5.  For drive containers, verify drive activity:

    ```bash
    weka cluster drive
    ```

    Expected status: All drives display ACTIVE status.

See examples in #perform-standard-verification-steps.

#### Troubleshooting

If the container remains in erminating state:

1.  Check the container events:

    ```bash
    kubectl describe pod <container-name> -n weka-operator-system
    ```
2. Review the operator logs for error messages.
3. Verify resource availability for the new container.

For failed container starts, check:

* Node resource availability
* Network connectivity
* Service status

***

### Replace a container on a denylisted node

Replacing a container on a denylisted node is necessary when the node is flagged as problematic and impacts cluster performance. This procedure ensures safe container replacement, restoring system stability.

#### Procedure

1. Remove the backend label from the node that is hosting the WEKA container (for example, weka.io/supports-backends) to prevent it from being chosen for the new container

```
<strong>kubectl label nodes <k8s-node-IP> weka.io/supports-backends-
</strong>
```

<details>

<summary>Example</summary>

<pre><code>$ kubectl get nodes 18.201.172.13 --show-labels
NAME            STATUS   ROLES                       AGE    VERSION        LABELS
18.201.172.13   Ready    control-plane,etcd,master   161m   v1.30.6+k3s1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=k3s,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=18.201.172.13,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=true,node-role.kubernetes.io/etcd=true,node-role.kubernetes.io/master=true,node.kubernetes.io/instance-type=k3s,p2p.k3s.cattle.io/enabled=true,weka.io/failure-domain=x1,weka.io/supports-backends=true,weka.io/supports-builds=true,weka.io/supports-clients=true

$ kubectl label nodes 18.201.172.13 weka.io/supports-backends-
node/18.201.172.13 unlabeled

$ kubectl get nodes 18.201.172.13 --show-labels
NAME            STATUS   ROLES                       AGE    VERSION        LABELS
18.201.172.13   Ready    control-plane,etcd,master   162m   v1.30.6+k3s1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=k3s,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=18.201.172.13,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=true,node-role.kubernetes.io/etcd=true,node-role.kubernetes.io/master=true,node.kubernetes.io/instance-type=k3s,p2p.k3s.cattle.io/enabled=true,weka.io/failure-domain=x1,weka.io/supports-builds=true,weka.io/supports-clients=true

```

</details>

2. Delete the pod containing the WEKA container.\
   This action prompts the WEKA cluster to recreate the container, ensuring it is not placed on the labeled node.

```bash
kubectl delete pod <pod-name> -n weka-operator-system
```

3. Monitor the container recreation and pod scheduling status.\
   The container remains in a pending state due to the label being removed.

```bash
kubectl get pods --all-namespaces -o wide
kubectl describe pod <pod-name> -n weka-operator-system
```

<details>

<summary>Example</summary>

```
$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                                                     READY   STATUS      RESTARTS   AGE    IP             NODE             NOMINATED NODE   READINESS GATES
kube-system            coredns-7b98449c4-kntm6                                                  1/1     Running     0          177m   10.42.0.3      3.254.112.77     <none>           <none>
kube-system            helm-install-traefik-7jvnv                                               0/1     Completed   1          177m   10.42.0.5      3.254.112.77     <none>           <none>
kube-system            helm-install-traefik-crd-crxnx                                           0/1     Completed   0          177m   10.42.0.2      3.254.112.77     <none>           <none>
kube-system            local-path-provisioner-595dcfc56f-7lgzq                                  1/1     Running     0          177m   10.42.0.6      3.254.112.77     <none>           <none>
kube-system            metrics-server-cdcc87586-5jl4x                                           1/1     Running     0          177m   10.42.0.4      3.254.112.77     <none>           <none>
kube-system            traefik-d7c9c5778-vrwtc                                                  1/1     Running     0          177m   10.42.0.7      3.254.112.77     <none>           <none>
weka-operator-system   cluster-dev-compute-09541214-6d47-49e3-abfb-2410ee6c1e2b                 1/1     Running     0          64m    10.0.121.214   63.35.225.98     <none>           <none>
weka-operator-system   cluster-dev-compute-0a01896f-1436-4314-9bcd-59d59da50257                 1/1     Running     0          64m    10.0.99.7      54.247.20.174    <none>           <none>
weka-operator-system   cluster-dev-compute-13e5e78c-9600-4c33-98b3-7d5fc16420b7                 1/1     Running     0          64m    10.0.117.96    3.255.93.171     <none>           <none>
weka-operator-system   cluster-dev-compute-310353c8-a349-479a-b34c-a172531d3927                 1/1     Running     0          64m    10.0.88.165    54.247.13.91     <none>           <none>
weka-operator-system   cluster-dev-compute-334e89ef-1230-43e1-a4c2-7c4774c5383e                 1/1     Running     0          64m    10.0.66.157    3.255.150.131    <none>           <none>
weka-operator-system   cluster-dev-compute-3f265472-00bf-4172-baae-5e5f20364aa8                 1/1     Running     0          64m    10.0.100.147   52.215.56.158    <none>           <none>
weka-operator-system   cluster-dev-compute-45f8ef2b-2b74-4d7f-a01b-096c857b6740                 1/1     Running     0          64m    10.0.81.44     52.210.49.97     <none>           <none>
weka-operator-system   cluster-dev-compute-8ddb4d56-338f-483f-aac2-9064bccbce17                 1/1     Running     0          64m    10.0.118.174   18.202.166.64    <none>           <none>
weka-operator-system   cluster-dev-compute-c25efa58-2120-490c-b5c1-09fe57e45cdc                 1/1     Running     0          64m    10.0.93.213    34.254.151.249   <none>           <none>
weka-operator-system   cluster-dev-compute-cc79625a-5291-44bd-850a-92320ea1a791                 1/1     Running     0          64m    10.0.82.71     18.201.172.13    <none>           <none>
weka-operator-system   cluster-dev-compute-e207f94a-6b5e-4217-b963-5573c0f5201d                 1/1     Running     0          64m    10.0.116.144   34.245.203.245   <none>           <none>
weka-operator-system   cluster-dev-compute-fc0fd513-f1cc-49ee-a228-d209579a22cb                 1/1     Running     0          64m    10.0.70.16     3.254.112.77     <none>           <none>
weka-operator-system   cluster-dev-drive-05ddc629-b7f5-4090-8736-b9fc3b48ad82                   1/1     Running     0          53m    10.0.121.214   63.35.225.98     <none>           <none>
weka-operator-system   cluster-dev-drive-0657c388-d04c-4c0b-8633-445671d86657                   1/1     Running     0          64m    10.0.81.44     52.210.49.97     <none>           <none>
weka-operator-system   cluster-dev-drive-49d0f816-2a9b-45f2-844a-05ddd6182f3e                   1/1     Running     0          50m    10.0.118.174   18.202.166.64    <none>           <none>
weka-operator-system   cluster-dev-drive-69ffc965-6cd5-489c-b44b-9abacbce7a98                   1/1     Running     0          64m    10.0.117.96    3.255.93.171     <none>           <none>
weka-operator-system   cluster-dev-drive-6ca2fb06-f38a-4335-9861-22b43fe4f8a6                   1/1     Running     0          64m    10.0.93.213    34.254.151.249   <none>           <none>
weka-operator-system   cluster-dev-drive-816a0286-e173-44b4-b528-9cbde1f698c7                   1/1     Running     0          64m    10.0.88.165    54.247.13.91     <none>           <none>
weka-operator-system   cluster-dev-drive-9d266897-9dfb-4714-a5d9-c2db327dbee0                   1/1     Running     0          64m    10.0.82.208    18.203.137.243   <none>           <none>
weka-operator-system   cluster-dev-drive-aac103be-bbfb-48e0-8c49-7d4c57bc1730                   0/1     Pending     0          38s    <none>         <none>           <none>           <none>
weka-operator-system   cluster-dev-drive-b5168f94-8cd4-4b6d-843c-65cd6919d55f                   1/1     Running     0          64m    10.0.66.157    3.255.150.131    <none>           <none>
weka-operator-system   cluster-dev-drive-cbc24786-dce1-4f0d-93ec-174a06206c6e                   1/1     Running     0          64m    10.0.116.144   34.245.203.245   <none>           <none>
weka-operator-system   cluster-dev-drive-d012285e-65a9-4dc9-a2d6-32a4c834f02f                   1/1     Running     0          64m    10.0.79.159    34.240.124.21    <none>           <none>
weka-operator-system   cluster-dev-drive-d8f97316-1c7c-4610-a892-d66d900e6cbe                   1/1     Running     0          64m    10.0.100.147   52.215.56.158    <none>           <none>
weka-operator-system   cluster-dev-envoy-5169557b-bc85-487e-84a1-dbbbecf15cc6                   1/1     Running     0          64m    10.0.79.159    34.240.124.21    <none>           <none>
weka-operator-system   cluster-dev-envoy-576a8e47-d21e-4d57-9790-40f6e0b0cf63                   1/1     Running     0          64m    10.0.99.7      54.247.20.174    <none>           <none>
weka-operator-system   cluster-dev-s3-1770abea-ba16-46ae-9f4a-91aefb70cf1e                      1/1     Running     0          64m    10.0.79.159    34.240.124.21    <none>           <none>
weka-operator-system   cluster-dev-s3-9cd9970f-faf9-46fc-b9c7-b5a53a5a0ccc                      1/1     Running     0          64m    10.0.99.7      54.247.20.174    <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-05d6d1a6-5665-4126-99d4-86b57fb178f3   1/1     Running     0          29m    10.42.4.97     34.254.151.249   <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-139ff869-42e9-4093-93c0-39fe4539f602   1/1     Running     0          29m    10.42.2.98     54.247.13.91     <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-14d50df6-6263-4efc-805b-6606a1ea8f73   1/1     Running     0          29m    10.42.12.98    52.215.56.158    <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-2aa3c37a-f4ca-4d2b-8567-96f25c8a939e   1/1     Running     0          29m    10.42.7.99     63.35.225.98     <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-4180ad3d-0eae-460d-8e39-a9ed62293cf8   1/1     Running     0          29m    10.42.6.98     18.202.166.64    <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-6508f280-794f-4582-b48c-28f4fd54220d   1/1     Running     0          29m    10.42.13.97    54.247.20.174    <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-8073c837-6e1c-45f1-819c-684f753796a8   1/1     Running     0          29m    10.42.14.98    3.255.93.171     <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-a410c3be-5033-4985-ad6f-485a6ab541e8   1/1     Running     0          29m    10.42.9.101    34.240.124.21    <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-a8d8d53f-201e-4112-8fe2-d5eaf9aeed1a   1/1     Running     0          29m    10.42.10.101   18.201.172.13    <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-d0fee8f9-78c1-464a-9144-280648f3ad54   1/1     Running     0          29m    10.42.1.98     52.210.49.97     <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-d5a49ce7-773f-4df0-ae97-b33d40ffb0b7   1/1     Running     0          29m    10.42.8.97     34.245.203.245   <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-d7653ca8-52b1-4564-9494-09caf326fecb   1/1     Running     0          29m    10.42.3.98     18.203.137.243   <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-f06b4061-0c81-4be4-9932-dbba22e5953f   1/1     Running     0          29m    10.42.0.104    3.254.112.77     <none>           <none>
weka-operator-system   weka-adhoc-sign-aws-drives-policy-fd6d8644-8129-4cab-8024-dbdf331cd719   1/1     Running     0          29m    10.42.11.101   3.255.150.131    <none>           <none>
weka-operator-system   weka-driver-dist                                                         1/1     Running     0          138m   10.42.5.3      54.78.16.52      <none>           <none>
weka-operator-system   weka-operator-controller-manager-fb957ff86-68q2l                         2/2     Running     0          140m   10.42.5.2      54.78.16.52      <none>           <none>
$ kubectl describe pod cluster-dev-drive-aac103be-bbfb-48e0-8c49-7d4c57bc1730 -n weka-operator-system
Name:             cluster-dev-drive-aac103be-bbfb-48e0-8c49-7d4c57bc1730
Namespace:        weka-operator-system
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app.kubernetes.io/create-by=controller-manager
                  app.kubernetes.io/name=WekaContainer
                  app.kubernetes.io/part-of=weka-operator
                  weka.io/cluster-id=34724ab7-4bf2-4765-b270-796ff63c517e
                  weka.io/mode=drive
Annotations:      <none>
Status:           Pending
IP:
IPs:              <none>
Controlled By:    WekaContainer/cluster-dev-drive-aac103be-bbfb-48e0-8c49-7d4c57bc1730
Containers:
  weka-container:
    Image:      quay.io/weka.io/weka-in-container:4.4.1.89-k8s-beta
    Port:       <none>
    Host Port:  <none>
    Command:
      python3
      /opt/weka_runtime.py
    Limits:
      cpu:             3
      hugepages-2Mi:   1600Mi
      memory:          11700Mi
      weka.io/drives:  1
    Requests:
      cpu:                3
      ephemeral-storage:  8M
      hugepages-2Mi:      1600Mi
      memory:             11700Mi
      weka.io/drives:     1
    Environment:
      AGENT_PORT:                 15406
      NAME:                       drivexaac103bexbbfbx48e0x8c49x7d4c57bc1730
      MODE:                       drive
      PORT:                       15306
      MEMORY:                     1400MiB
      NETWORK_DEVICE:             udp
      UDP_MODE:                   false
      WEKA_PORT:                  15306
      WEKA_CLI_DEBUG:             0
      DIST_SERVICE:               https://weka-driver-dist.weka-operator-system.svc.cluster.local:60002
      MAX_TRACE_CAPACITY_GB:      20
      ENSURE_FREE_SPACE_GB:       20
      IMAGE_NAME:                 quay.io/weka.io/weka-in-container:4.4.1.89-k8s-beta
      WEKA_OPERATOR_DEBUG_SLEEP:  3
      NODE_NAME:                   (v1:spec.nodeName)
      FAILURE_DOMAIN_LABEL:       weka.io/failure-domain
      WEKA_PERSISTENCE_DIR:       /opt/weka-persistence
      CORES:                      1
      CORE_IDS:                   auto
      JOIN_IPS:                   10.0.116.144:15200,10.0.118.174:15306,10.0.81.44:15200,10.0.116.144:15306,10.0.99.7:15200
    Mounts:
      /dev from dev (rw)
      /dev/hugepages from hugepages (rw)
      /etc/syslog-ng/syslog-ng.conf from weka-boot-scripts (rw,path="syslog-ng.conf")
      /host/run from run (rw)
      /hostside/etc/os-release from osrelease (rw)
      /opt/k8s-weka/boot-level from weka-container-persistence-dir (rw,path="tmpfss/boot-level")
      /opt/k8s-weka/node-cluster from weka-cluster-persistence-dir (rw,path="shared-configs")
      /opt/weka-persistence from weka-container-persistence-dir (rw)
      /opt/weka_runtime.py from weka-boot-scripts (rw,path="weka_runtime.py")
      /sys from sys (rw)
      /usr/local/bin/weka from weka-boot-scripts (rw,path="run-weka-cli.sh")
      /usr/local/bin/wekaauthcli from weka-boot-scripts (rw,path="run-weka-cli.sh")
      /var/log from weka-container-persistence-dir (rw,path="var/log")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bqvd8 (ro)
      /var/run/secrets/weka-operator/operator-user from weka-credentials (rw)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  hugepages:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     HugePages-2Mi
    SizeLimit:  <unset>
  osrelease:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/os-release
    HostPathType:  File
  dev:
    Type:          HostPath (bare host directory volume)
    Path:          /dev
    HostPathType:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run
    HostPathType:
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:
  weka-boot-scripts:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      weka-boot-scripts
    Optional:  false
  weka-container-persistence-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/k8s-weka/containers/f28bbecf-92ae-4ca7-86a9-77b3479caa68
    HostPathType:  DirectoryOrCreate
  weka-cluster-persistence-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/k8s-weka/clusters/34724ab7-4bf2-4765-b270-796ff63c517e
    HostPathType:  DirectoryOrCreate
  weka-credentials:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  weka-operator-34724ab7-4bf2-4765-b270-796ff63c517e
    Optional:    false
  kube-api-access-bqvd8:
    Type:                     Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:   3607
    ConfigMapName:            kube-root-ca.crt
    ConfigMapOptional:        <nil>
    DownwardAPI:              true
QoS Class:                    Guaranteed
Node-Selectors:               <none>
Tolerations:                  another-one:NoSchedule op=Exists
                              another-one:NoExecute op=Exists
                              node.kubernetes.io/cpu-pressure:NoSchedule op=Exists
                              node.kubernetes.io/cpu-pressure:NoExecute op=Exists
                              node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                              node.kubernetes.io/disk-pressure:NoExecute op=Exists
                              node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                              node.kubernetes.io/memory-pressure:NoExecute op=Exists
                              node.kubernetes.io/network-unavailable:NoExecute op=Exists
                              node.kubernetes.io/not-ready:NoExecute op=Exists
                              node.kubernetes.io/unreachable:NoExecute op=Exists
                              node.kubernetes.io/unschedulable:NoExecute op=Exists
                              simple-toleration:NoExecute op=Exists
                              simple-toleration:NoSchedule op=Exists
                              weka.io/dedicated=weka-backend:NoSchedule
                              weka.io/shutdown-node:NoExecute op=Exists
Topology Spread Constraints:  weka.io/failure-domain:DoNotSchedule when max skew 1 is exceeded for selector weka.io/cluster-id=34724ab7-4bf2-4765-b270-796ff63c517e,weka.io/mode=drive
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  2m    default-scheduler  0/15 nodes are available: 15 node(s) didn't match Pod's node affinity/selector. preemption: 0/15 nodes are available: 15 Preemption is not helpful for scheduling.
```

</details>

#### Expected results

* The container pod enters Pending state.
* Pod scheduling fails with message: "nodes are available: x node(s) didn't match Pod's node affinity/selector".
* The container is prevented from running on the denied node.

#### Troubleshooting

If the pod schedules successfully on the denied node:

* Verify the backend support label was removed successfully.
* Check node taints and tolerations.
* Review pod scheduling policies and constraints.

***

## Cluster scaling

Adjusting the size of a WEKA cluster ensures optimal performance and cost efficiency. Expand to meet growing workloads or shrink to reduce resources as demand decreases.

### Expand a cluster

Cluster expansion enhances system resources and storage capacity while maintaining cluster stability. This procedure describes how to expand a WEKA cluster by increasing the number of compute and drive containers.

Note: This procedure exemplifies an expansion of a cluster with 6 compute and 6 drive containers to a cluster with 7 compute and 7 drive containers. Each driveContainer has one driveCore.

#### Before you begin

Verify the following:

* Ensure sufficient resources are available.
* Ensure valid Quay.io credentials for WEKA container images.
* Ensure access to the WEKA operator namespace.
* Check the number of available Kubernetes nodes using `kubectl get nodes`.
* Ensure all existing WEKA containers are in Running state.
* Confirm your cluster is healthy with `weka status`.

#### Procedure

1. Update the cluster configuration by increasing container value from previous value in your YAML file:

```

```yaml
spec:
  template: dynamic
  dynamicTemplate:
    computeContainers: 7  # Increase from previous value
    driveContainers: 7    # Increase from previous value
    computeCores: 1
    driveCores: 1
    numDrives: 1
```

```

2. Apply the updated configuration:

```bash
kubectl apply -f cluster.yaml
```

<details>

<summary>Example</summary>

```
wekacontainer.weka.weka.io/weka-driver-dist unchanged
service/weka-driver-dist unchanged
wekacluster.weka.weka.io/cluster-dev configured
wekacontainer.weka.weka.io/weka-drivers-builder unchanged
```

</details>

3. #perform-the-standard-verification-steps.

#### Expected results

* Total of 14 backend containers (7 compute + 7 drive).
* All new containers show status as UP.
* Weka status shows increased storage capacity.
* Protection status remains Fully protected.

#### Troubleshooting

* If containers remain in Pending state, verify available node capacity.
* Check for sufficient resources across Kubernetes nodes.
* Review WEKA operator logs for expansion-related issues.

#### Considerations

* The number of containers cannot exceed available Kubernetes nodes.
* Pending containers indicate resource constraints or node availability issues.
* Each expansion requires sufficient system resources across the cluster.

Note: If your cluster has resource constraints or insufficient nodes, container creation may remain in a pending state until additional nodes become available.

***

### Expand an S3 cluster

Expanding an S3 cluster is necessary when additional storage or improved performance is required. Follow the steps below to expand the cluster while maintaining data availability and integrity.

#### Procedure

1.  **Update cluster YAML:** Increase the number of S3 containers in the cluster YAML file and re-deploy the configuration.\
    Example YAML update:

    ```yaml
    spec:
      template: dynamic
      dynamicTemplate:
        computeContainers: 6
        driveContainers: 6
        computeCores: 1
        driveCores: 1
        numDrives: 1
        s3Containers: 4  #  Icrease from previous value
    ```

    Apply the changes:

    ```bash
    kubectl apply -f cluster.yaml
    ```

<details>

<summary>Example</summary>

```
$ kubectl apply -f cluster3.yaml
wekacontainer.weka.weka.io/weka-driver-dist unchanged
service/weka-driver-dist unchanged
wekacluster.weka.weka.io/cluster-dev configured
wekacontainer.weka.weka.io/weka-drivers-builder unchanged
```

</details>

2. **Verify new pods:** Confirm that additional S3 and Envoy pods are created and running. Use the following command to list all pods:

```bash
kubectl get pods --all-namespaces
```

Ensure two new S3 and Envoy pods appear in the output and are in the `Running` state.

<details>

<summary>Example</summary>

```
$ kubectl get pods --all-namespaces
NAMESPACE              NAME                                                       READY   STATUS      RESTARTS   AGE
kube-system            coredns-7b98449c4-l2dlt                                    1/1     Running     0          26m
kube-system            helm-install-traefik-8p668                                 0/1     Completed   1          26m
kube-system            helm-install-traefik-crd-hz9dx                             0/1     Completed   0          26m
kube-system            local-path-provisioner-595dcfc56f-55vmx                    1/1     Running     0          26m
kube-system            metrics-server-cdcc87586-2wmfd                             1/1     Running     0          26m
kube-system            traefik-d7c9c5778-pf2k7                                    1/1     Running     0          25m
weka-operator-system   cluster-dev-compute-05a6a09a-432d-42fe-9df4-c129780aa410   1/1     Running     0          9m41s
weka-operator-system   cluster-dev-compute-6d9f3d37-b8e7-4db6-9df1-5aa2a82e423e   1/1     Running     0          9m23s
weka-operator-system   cluster-dev-compute-723230f4-6ed1-4ff3-94df-e8c7ebcede75   1/1     Running     0          9m33s
weka-operator-system   cluster-dev-compute-e6269c4e-b392-4951-b41b-a401a59fb11a   1/1     Running     0          9m34s
weka-operator-system   cluster-dev-compute-f25ee328-7ea6-4d83-9e53-113996c91a78   1/1     Running     0          9m31s
weka-operator-system   cluster-dev-compute-f3df3e56-aad6-4d29-b303-fdc60132f870   1/1     Running     0          9m34s
weka-operator-system   cluster-dev-drive-65376aa9-24f0-4eb2-9dfe-d72e408916e0     1/1     Running     0          9m23s
weka-operator-system   cluster-dev-drive-79df7254-cee6-4411-b78a-1e503e331e9f     1/1     Running     0          9m33s
weka-operator-system   cluster-dev-drive-ac3824ee-cb66-469e-bca9-f2c7274db4ec     1/1     Running     0          9m33s
weka-operator-system   cluster-dev-drive-af464a29-7180-445a-869d-64e274b47993     1/1     Running     0          9m23s
weka-operator-system   cluster-dev-drive-d7414597-3a96-459e-99a0-7965345c3fa0     1/1     Running     0          9m12s
weka-operator-system   cluster-dev-drive-dad14164-f118-4cfd-9401-6e061f44209d     1/1     Running     0          9m34s
weka-operator-system   cluster-dev-envoy-05de77da-8399-45bc-b904-cb62f8e9ff35     1/1     Running     0          65s
weka-operator-system   cluster-dev-envoy-a22e14cc-7fb7-488c-a7cd-3ef8ee3afc86     1/1     Running     0          65s
weka-operator-system   cluster-dev-envoy-d0249bce-f506-409a-9b54-bc5596900884     1/1     Running     0          9m30s
weka-operator-system   cluster-dev-envoy-e05fe514-e2cb-4f6b-b9c6-8e0c3784f938     1/1     Running     0          9m29s
weka-operator-system   cluster-dev-s3-1ffc8818-e647-4e5c-bbb3-95dcd8ca96f8        1/1     Running     0          65s
weka-operator-system   cluster-dev-s3-75aaeac7-da47-44bb-82d3-c3c273575bd3        1/1     Running     0          65s
weka-operator-system   cluster-dev-s3-78a7332f-1a54-429c-a34e-aa96fbbff216        1/1     Running     0          9m31s
weka-operator-system   cluster-dev-s3-ce450ceb-58c9-4049-986a-75327fa0d76a        1/1     Running     0
```

</details>

3. **Validate expansion:** Verify the S3 cluster has expanded to include the updated number of containers. Check the cluster status and ensure no errors are present.\
   Use these commands for validation:

```bash
kubectl describe wekacluster -n weka-operator-system
```

Confirm the updated configuration reflects four S3 containers and all components are operational.

<details>

<summary>Example</summary>

```
$ kubectl describe wekacluster -n weka-operator-system
Name:         cluster-dev
Namespace:    weka-operator-system
Labels:       <none>
Annotations:  <none>
API Version:  weka.weka.io/v1alpha1
Kind:         WekaCluster
Metadata:
  Creation Timestamp:  2024-11-16T11:13:19Z
  Finalizers:
    weka.weka.io/finalizer
  Generation:        3
  Resource Version:  10445
  UID:               844cec7a-f41d-45cd-9c59-9810bbd199fe
Spec:
  Additional Memory:
    Compute:             500
    Drive:               1000
    s3:                  200
  Cpu Policy:            auto
  Drivers Dist Service:  https://weka-driver-dist.weka-operator-system.svc.cluster.local:60002
  Dynamic Template:
    Compute Containers:       6
    Compute Cores:            1
    Drive Containers:         6
    Drive Cores:              1
    Num Drives:               1
    s3Containers:             4
  Graceful Destroy Duration:  24h0m0s
  Hot Spare:                  0
  Image:                      quay.io/weka.io/weka-in-container:4.4.1
  Image Pull Secret:          quay-io-robot-secret
  Network:
  Node Selector:
    weka.io/supports-backends:  true
  Ports:
  Role Node Selector:
  Template:  dynamic
Status:
  Cluster ID:  cd596d28-be9a-4864-b34b-dbe45e8914cc
  Conditions:
    Last Transition Time:  2024-11-16T11:13:19Z
    Message:               Cluster secrets are created
    Reason:                Init
    Status:                True
    Type:                  ClusterSecretsCreated
    Last Transition Time:  2024-11-16T11:13:21Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  PodsCreated
    Last Transition Time:  2024-11-16T11:22:41Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ContainerResourcesAllocated
    Last Transition Time:  2024-11-16T11:19:06Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  PodsReady
    Last Transition Time:  2024-11-16T11:19:29Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterCreated
    Last Transition Time:  2024-11-16T11:19:29Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  JoinedCluster
    Last Transition Time:  2024-11-16T11:19:30Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  DrivesAdded
    Last Transition Time:  2024-11-16T11:20:11Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  IoStarted
    Last Transition Time:  2024-11-16T11:20:12Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterSecretsApplied
    Last Transition Time:  2024-11-16T11:20:14Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  CondDefaultFsCreated
    Last Transition Time:  2024-11-16T11:20:14Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  CondS3ClusterCreated
    Last Transition Time:  2024-11-16T11:20:15Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterClientsSecretsCreated
    Last Transition Time:  2024-11-16T11:20:15Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterClientsSecretsApplied
    Last Transition Time:  2024-11-16T11:20:15Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterCSIsSecretsCreated
    Last Transition Time:  2024-11-16T11:20:16Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterCSIsSecretsApplied
    Last Transition Time:  2024-11-16T11:20:16Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  WekaHomeConfigured
    Last Transition Time:  2024-11-16T11:20:16Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterIsReady
  Last Applied Image:      quay.io/weka.io/weka-in-container:4.4.1
  Last Applied Spec:       571bbabc250fb7cfb19ed709bc40b8cb752931a7b3080e300e1b58db8c9559ee
  Ports:
    Base Port:      15000
    Lb Admin Port:  15301
    Lb Port:        15300
    Port Range:     500
    s3Port:         15302
  Status:           Ready
  Throughput:
Events:             <none>
$ kubectl exec -it cluster-dev-compute-05a6a09a-432d-42fe-9df4-c129780aa410 -n weka-operator-system -- /bin/bash
root@ip-10-0-93-212:/# weka status
WekaIO v4.4.1 (CLI build 4.4.1)

       cluster: cluster-dev (cd596d28-be9a-4864-b34b-dbe45e8914cc)
        status: OK (16 backend containers UP, 6 drives UP)
    protection: 3+2 (Fully protected)
     hot spare: 0 failure domains
 drive storage: 22.09 TiB total, 21.86 TiB unprovisioned
         cloud: connected
       license: Unlicensed

     io status: STARTED 5 minutes ago (16 io-nodes UP, 138 Buckets UP)
    link layer: Ethernet
       clients: 0 connected
         reads: 0 B/s (0 IO/s)
        writes: 0 B/s (0 IO/s)
    operations: 12 ops/s
        alerts: 31 active alerts, use `weka alerts` to list them

root@ip-10-0-93-212:/# weka cluster host
HOST ID  HOSTNAME        CONTAINER                                     IPS          STATUS  REQUESTED ACTION  RELEASE  FAILURE DOMAIN  CORES  MEMORY   UPTIME    LAST FAILURE  REQUESTED ACTION FAILURE
0        ip-10-0-124-65  drivexac3824eexcb66x469exbca9xf2c7274db4ec    10.0.124.65  UP      NONE              4.4.1    AUTO            1      1.54 GB  0:06:42h
1        ip-10-0-102-61  computex723230f4x6ed1x4ff3x94dfxe8c7ebcede75  10.0.102.61  UP      NONE              4.4.1    AUTO            1      2.94 GB  0:06:40h
2        ip-10-0-93-212  computex05a6a09ax432dx42fex9df4xc129780aa410  10.0.93.212  UP      NONE              4.4.1    AUTO            1      2.94 GB  0:06:26h
3        ip-10-0-79-87   s3xce450cebx58c9x4049x986ax75327fa0d76a       10.0.79.87   UP      NONE              4.4.1    AUTO            1      1.26 GB  0:06:36h
4        ip-10-0-113-26  drivex65376aa9x24f0x4eb2x9dfexd72e408916e0    10.0.113.26  UP      NONE              4.4.1    AUTO            1      1.54 GB  0:06:33h
5        ip-10-0-64-53   computexf3df3e56xaad6x4d29xb303xfdc60132f870  10.0.64.53   UP      NONE              4.4.1    AUTO            1      2.94 GB  0:06:35h
6        ip-10-0-107-12  s3x78a7332fx1a54x429cxa34exaa96fbbff216       10.0.107.12  UP      NONE              4.4.1    AUTO            1      1.26 GB  0:06:43h
7        ip-10-0-93-212  drivexd7414597x3a96x459ex99a0x7965345c3fa0    10.0.93.212  UP      NONE              4.4.1    AUTO            1      1.54 GB  0:06:26h
8        ip-10-0-124-65  computexe6269c4exb392x4951xb41bxa401a59fb11a  10.0.124.65  UP      NONE              4.4.1    AUTO            1      2.94 GB  0:06:42h
9        ip-10-0-113-26  computexf25ee328x7ea6x4d83x9e53x113996c91a78  10.0.113.26  UP      NONE              4.4.1    AUTO            1      2.94 GB  0:06:33h
10       ip-10-0-64-53   drivexdad14164xf118x4cfdx9401x6e061f44209d    10.0.64.53   UP      NONE              4.4.1    AUTO            1      1.54 GB  0:06:35h
11       ip-10-0-79-87   drivexaf464a29x7180x445ax869dx64e274b47993    10.0.79.87   UP      NONE              4.4.1    AUTO            1      1.54 GB  0:06:36h
12       ip-10-0-102-61  drivex79df7254xcee6x4411xb78ax1e503e331e9f    10.0.102.61  UP      NONE              4.4.1    AUTO            1      1.54 GB  0:06:39h
13       ip-10-0-107-12  computex6d9f3d37xb8e7x4db6x9df1x5aa2a82e423e  10.0.107.12  UP      NONE              4.4.1    AUTO            1      2.94 GB  0:06:43h
14       ip-10-0-124-65  s3x75aaeac7xda47x44bbx82d3xc3c273575bd3       10.0.124.65  UP      NONE              4.4.1    AUTO            1      1.26 GB  0:02:34h
15       ip-10-0-102-61  s3x1ffc8818xe647x4e5cxbbb3x95dcd8ca96f8       10.0.102.61  UP      NONE              4.4.1    AUTO            1      1.26 GB  0:02:34h

The command 'weka cluster host' is deprecated. Please use 'weka cluster container' instead.
root@ip-10-0-93-212:/# weka s3 cluster
S3 Cluster Info
        Status: Online
     All Hosts: off
          Port: 15300
    Filesystem: default
      S3 Hosts: HostId<14>, HostId<3>, HostId<6>, HostId<15>

root@ip-10-0-93-212:/# weka s3 cluster -v
S3 Cluster Info
        Status: Online
     All Hosts: off
          Port: 15300
    Filesystem: default
     Config FS: .config_fs
      S3 Hosts: HostId<14>, HostId<3>, HostId<6>, HostId<15>
 Mount Options: rw,relatime,readcache,readahead_kb=32768,dentry_max_age_positive=1000,dentry_max_age_negative=0,container_name=s3xce450cebx58c9x4049x986ax75327fa0d76a
           TLS: on
           ILM: on
 Creator Owner: off
Max Buckets Limit: 10000
MPU Background: on
     ILM Hosts: HostId<3>
Anonymous Posix UID/GID: 65534/65534
 Internal Port: 15302
SLB Admin Port: 15301
SLB Max Connections: 1024
SLB Max Pending Requests: 1024
SLB Max Requests: 1024

root@ip-10-0-93-212:/# weka s3 cluster status
ID  HOSTNAME        S3 STATUS  IP           PORT   VERSION  UPTIME    ACTIVE REQUESTS  LAST FAILURE
14  ip-10-0-124-65  Ready      10.0.124.65  15300  4.4.1    0:02:33h  0
15  ip-10-0-102-61  Ready      10.0.102.61  15300  4.4.1    0:02:31h  0
3   ip-10-0-79-87   Ready      10.0.79.87   15300  4.4.1    0:05:26h  0
6   ip-10-0-107-12  Ready      10.0.107.12  15300  4.4.1    0:05:26h  0
```

</details>

***

### Shrink a cluster

A WEKA cluster shrink operation reduces compute and drive containers to optimize resources and system footprint. Shrinking may free resources, lower costs, align capacity with demand, or decommission infrastructure. Perform carefully to ensure data integrity and service availability.

#### Before you begin

Verify the following:

* Cluster is in a healthy state before beginning.
* The WEKA cluster is operational and with sufficient redundancy.
* At least one hot spare configured for safe container removal.

#### Procedure

1. Modify the cluster configuration:

```

```yaml
spec:
  template: dynamic
  dynamicTemplate:
    computeContainers: 6    # Reduce from previous value
    driveContainers: 6      # Reduce from previous value
    computeCores: 1
    driveCores: 1
    numDrives: 1
```

```

2. Apply the updated configuration:

```bash
kubectl apply -f cluster.yaml
```

<details>

<summary>Example</summary>

```
wekacontainer.weka.weka.io/weka-driver-dist unchanged
service/weka-driver-dist unchanged
wekacluster.weka.weka.io/cluster-dev configured
wekacontainer.weka.weka.io/weka-drivers-builder unchanged
```

</details>

3. Verify the desired state change:

```bash
kubectl describe wekacluster <cluster-name> -n weka-operator-system
```

Replace `<cluster-name>` with your specific value.

<details>

<summary>Example</summary>

```
Name:         cluster-dev
Namespace:    weka-operator-system
Labels:       <none>
Annotations:  <none>
API Version:  weka.weka.io/v1alpha1
Kind:         WekaCluster
Metadata:
  Creation Timestamp:  2024-12-09T07:29:45Z
  Finalizers:
    weka.weka.io/finalizer
  Generation:        3
  Resource Version:  49974
  UID:               2406dc3d-05bb-4b96-b4b9-b72bd1f9f993
Spec:
  Additional Memory:
  Cpu Policy:            auto
  Drivers Dist Service:  https://weka-driver-dist.weka-operator-system.svc.cluster.local:60002
  Dynamic Template:
    Compute Containers:       6
    Compute Cores:            1
    Drive Containers:         6
    Drive Cores:              1
    Num Drives:               1
    s3Containers:             2
  Graceful Destroy Duration:  24h0m0s
  Hot Spare:                  1
  Image:                      quay.io/weka.io/weka-in-container:4.4.1.92-k8s
  Image Pull Secret:          quay-io-robot-secret
  Network:
  Node Selector:
    weka.io/supports-backends:  true
  Ports:
  Role Node Selector:
  Template:  dynamic
Status:
  Cluster ID:  665ebdea-96b6-4556-bba4-a38dcbff44b6
  Conditions:
    Last Transition Time:  2024-12-09T07:29:45Z
    Message:               Cluster secrets are created
    Reason:                Init
    Status:                True
    Type:                  ClusterSecretsCreated
    Last Transition Time:  2024-12-09T07:29:50Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  PodsCreated
    Last Transition Time:  2024-12-09T07:29:56Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ContainerResourcesAllocated
    Last Transition Time:  2024-12-09T07:33:29Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  PodsReady
    Last Transition Time:  2024-12-09T07:33:40Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterCreated
    Last Transition Time:  2024-12-09T07:33:42Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  JoinedCluster
    Last Transition Time:  2024-12-09T07:33:43Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  DrivesAdded
    Last Transition Time:  2024-12-09T07:34:25Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  IoStarted
    Last Transition Time:  2024-12-09T07:34:26Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterSecretsApplied
    Last Transition Time:  2024-12-09T07:34:28Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  CondDefaultFsCreated
    Last Transition Time:  2024-12-09T07:34:28Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  CondS3ClusterCreated
    Last Transition Time:  2024-12-09T07:34:29Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterClientsSecretsCreated
    Last Transition Time:  2024-12-09T07:34:29Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterClientsSecretsApplied
    Last Transition Time:  2024-12-09T07:34:29Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterCSIsSecretsCreated
    Last Transition Time:  2024-12-09T07:34:30Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterCSIsSecretsApplied
    Last Transition Time:  2024-12-09T07:34:37Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  WekaHomeConfigured
    Last Transition Time:  2024-12-09T07:34:37Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  ClusterIsReady
    Last Transition Time:  2024-12-09T07:39:27Z
    Message:               Completed successfully
    Reason:                Init
    Status:                True
    Type:                  CondAdminUserDeleted
  Last Applied Image:      quay.io/weka.io/weka-in-container:4.4.1.92-k8s
  Last Applied Spec:       002d59c371c0f3b820a4936d88449d5ec102acdbcb306b981089c5c472f513dc
  Ports:
    Base Port:      15000
    Lb Admin Port:  15301
    Lb Port:        15300
    Port Range:     500
    s3Port:         15302
  Printer:
    Compute Containers:  7/7/6
    Drive Containers:    7/7/6
    Drives:              7/7/6
    Iops:                --/--/--
    Throughput:          --/--
  Stats:
    Containers:
      Compute:
        Cpu Utilization:  14.29
        Num Containers:
          Active:   7
          Created:  7
          Desired:  6
        Processes:
          Active:   7
          Created:  7
          Desired:  6
      Drive:
        Cpu Utilization:  10.55
        Num Containers:
          Active:   7
          Created:  7
          Desired:  6
        Processes:
          Active:   7
          Created:  7
          Desired:  6
      s3:
        Cpu Utilization:  0.27
        Num Containers:
          Active:   2
          Desired:  2
        Processes:
    Drives:
      Counters:
        Active:   7
        Created:  7
        Desired:  6
    Io Stats:
      Iops:
        Metadata:  0
        Read:      0
        Total:     0
        Write:     0
      Throughput:
        Read:     0
        Write:    0
    Last Update:  2024-12-09T09:06:23Z
  Status:         Ready
Events:           <none>
```

</details>

4.  Remove specific containers:

    * Identify containers to remove
    * Delete the compute container:

    ```bash
    kubectl delete wekacontainer <compute-container-name> -n weka-operator-system
    ```

    * Delete the drive container:

    ```bash
    kubectl delete wekacontainer <drive-container-name> -n weka-operator-system
    ```
5. Verify cluster stability:
   * Check container status.
   * Monitor cluster health.
   * Verify data protection status.

#### Expected results

* Reduced number of active containers and related pod.
* Cluster status shows Running.
* All remaining containers running properly.
* Data protection maintained.
* No service disruption.

#### Troubleshooting

* If cluster shows degraded status, verify hot spare availability.
* Check operator logs for potential issues.
* Ensure proper container termination.
* Verify resource redistribution.

#### Limitations

* Manual container removal required.
* Must maintain minimum required containers for protection level.
* Hot spare needed for safe removal.
* Cannot remove containers below protection requirement.

#### Related topics

***

### Increase client cores

When system demands increase, you may need to add more processing power by increasing the number of client cores. This procedure shows how to increase client cores from 1 to 2 cores to improve system performance while maintaining stability.

#### Prerequisites

Sufficient hugepage memory (1500MiB per core).

#### Procedure

1. Update the WekaClient object configuration in your client YAML file:

```yaml
coresNum: 2 #increase num of cores
```

Note: AWS DPDK on EKS is not supported for this configuration.

2. Apply the updated client configuration:

```
kubectl apply -f client.yaml
```

<details>

<summary>Example</summary>

```
$ kubectl apply -f exclient.yaml
wekaclient.weka.weka.io/cluster-dev-clientsnew configured
wekacontainer.weka.weka.io/weka-driver-builder unchanged
service/weka-driver-builder unchanged
```

</details>

3. Verify the new client core is added:

```
kubectl get wekaclient -n weka-operator-system
kubectl describe wekaclient <cluster-name> -n weka-operator-system
```

Replace `<cluster-name>` with your specific value.

<details>

<summary>Example</summary>

```
$ kubectl get wekaclient -n weka-operator-system
NAME                     STATUS   TARGET CLUSTER   CORES
cluster-dev-clientsnew                             2

$ kubectl describe wekaclient cluster-dev-clientsnew -n weka-operator-system
Name:         cluster-dev-clientsnew
Namespace:    weka-operator-system
Labels:       <none>
Annotations:  <none>
API Version:  weka.weka.io/v1alpha1
Kind:         WekaClient
Metadata:
  Creation Timestamp:  2025-01-20T17:07:07Z
  Finalizers:
    weka.weka.io/finalizer
  Generation:        3
  Resource Version:  100867
  UID:               155dfaa0-b72c-428f-a6f7-c138a41e8d33
Spec:
  Agent Port:            45000
  Cores Num:             2
  Cpu Policy:            auto
  Drivers Dist Service:  https://weka-driver-builder.weka-operator-system.svc.cluster.local:60002
  Image:                 quay.io/weka.io/weka-in-container:4.4.2.144-k8s
  Image Pull Secret:     quay-io-robot-secret
  Join Ip Ports:
    10.0.98.109:15100
  Network:
  Node Selector:
    weka.io/supports-clients:  true
  Port:                        45001
  Target Cluster:
    Name:
    Namespace:
  Upgrade Policy:
    Type:  all-at-once
  Weka Home Config:
  Weka Secret Ref:  weka-client-cluster-dev1
Status:
  Last Applied Spec:  a76d4c891dad0036de6d099ec587808c07c3304d4fd6407a324b155c80f31c17
Events:               <none>
```

</details>

3. Delete all client container pods to trigger the reconfiguration:

```
kubectl delete wekacontainer <client-name>-<ip-address> -n weka-operator-system --force --grace-period=0
```

Replace `<client-name>` and `<ip-address>` with your specific values.

<details>

<summary>Example for one node</summary>

```

```
kubectl delete wekacontainer cluster-dev-clientsnew-18.201.248.101 -n weka-operator-system --force --grace-period=0
```

```

</details>

4. Verify the client containers have restarted and rejoined the cluster:

```
kubectl get pods --all-namespaces
```

Look for pods with your client name prefix to confirm they are in Running state.

<details>

<summary>Example</summary>

```
$ kubectl get pods --all-namespaces
NAMESPACE              NAME                                                READY   STATUS      RESTARTS   AGE
kube-system            coredns-ccb96694c-864p2                             1/1     Running     0          5h43m
kube-system            helm-install-traefik-89stn                          0/1     Completed   1          5h43m
kube-system            helm-install-traefik-crd-gth7z                      0/1     Completed   0          5h43m
kube-system            local-path-provisioner-5cf85fd84d-fsqv5             1/1     Running     0          5h43m
kube-system            metrics-server-5985cbc9d7-p9tbb                     1/1     Running     0          5h43m
kube-system            traefik-57b79cf995-2xf9g                            1/1     Running     0          5h43m
weka-operator-system   cluster-dev-clientsnew-18.201.248.101               1/1     Running     0          7m26s
weka-operator-system   cluster-dev-clientsnew-3.250.62.27                  1/1     Running     0          7m23s
weka-operator-system   cluster-dev-clientsnew-3.253.126.106                1/1     Running     0          7m25s
weka-operator-system   cluster-dev-clientsnew-3.253.243.104                1/1     Running     0          7m28s
weka-operator-system   cluster-dev-clientsnew-3.254.188.51                 1/1     Running     0          7m32s
weka-operator-system   cluster-dev-clientsnew-54.220.104.9                 1/1     Running     0          7m23s
weka-operator-system   weka-driver-builder                                 1/1     Running     0          10m
weka-operator-system   weka-operator-controller-manager-7468644bc9-4hz7w   2/2     Running     0          5h41m
weka-operator-system   weka-operator-node-agent-cmd7l                      1/1     Running     0          5h41m
weka-operator-system   weka-operator-node-agent-n48jq                      1/1     Running     0          5h41m
weka-operator-system   weka-operator-node-agent-sdbph                      1/1     Running     0          5h41m
weka-operator-system   weka-operator-node-agent-tlvbh                      1/1     Running     0          5h41m
weka-operator-system   weka-operator-node-agent-w8xlh                      1/1     Running     0          5h41m
weka-operator-system   weka-operator-node-agent-zgz5w                      1/1     Running     0          5h41m
```

</details>

5. Confirm the core increase in the WEKA cluster using the following commands :

```
weka cluster container
weka cluster process
weka status
```

<details>

<summary>Example</summary>

```
root@ip-10-0-98-109:/# weka cluster container
HOST ID  HOSTNAME         CONTAINER                                     IPS           STATUS  REQUESTED ACTION  RELEASE        FAILURE DOMAIN  CORES  MEMORY   UPTIME    LAST FAILURE  REQUESTED ACTION FAILURE
0        ip-10-0-83-118   drivex92d620e9xffc0x4d14x823ex6444a0d2a823    10.0.83.118   UP      NONE              4.4.2.144-k8s  AUTO            1      1.54 GB  2:17:27h
1        ip-10-0-83-118   s3x58e05049x0c4ex44b8x9b99xccff4dc364db       10.0.83.118   UP      NONE              4.4.2.144-k8s  AUTO            1      1.26 GB  2:17:25h
2        ip-10-0-125-187  drivex3cc5580dx303ex4c15xba6cx82ccc04a898d    10.0.125.187  UP      NONE              4.4.2.144-k8s  AUTO            1      1.54 GB  2:17:25h
3        ip-10-0-65-133   drivex4cca8165xfcaax438dx9a75xf372efc7a497    10.0.65.133   UP      NONE              4.4.2.144-k8s  AUTO            1      1.54 GB  2:17:28h
4        ip-10-0-65-133   computex133b97bcx5bf6x4612x8f2axd09fc42bb573  10.0.65.133   UP      NONE              4.4.2.144-k8s  AUTO            1      2.94 GB  2:17:25h
5        ip-10-0-110-144  computexc0ac9647xf6a5x4d77x909ax07e865469af1  10.0.110.144  UP      NONE              4.4.2.144-k8s  AUTO            1      2.94 GB  2:17:24h
6        ip-10-0-107-84   computex1b0db083x8986x45e0x96e3xe739b58808ee  10.0.107.84   UP      NONE              4.4.2.144-k8s  AUTO            1      2.94 GB  2:17:27h
7        ip-10-0-98-109   computex1d5f9c03x5b35x401exa71ex3786135e7a66  10.0.98.109   UP      NONE              4.4.2.144-k8s  AUTO            1      2.94 GB  2:17:23h
8        ip-10-0-110-144  drivex3cdd7cd0xecbbx4240x9f85xb5881b2f276c    10.0.110.144  UP      NONE              4.4.2.144-k8s  AUTO            1      1.54 GB  2:17:24h
9        ip-10-0-125-187  computexcc4c114exb720x49a7xa964xb050041220d1  10.0.125.187  UP      NONE              4.4.2.144-k8s  AUTO            1      2.94 GB  2:17:26h
10       ip-10-0-83-118   computexac9ca615x9425x48eexaafex756ee3e8e8aa  10.0.83.118   UP      NONE              4.4.2.144-k8s  AUTO            1      2.94 GB  2:17:24h
11       ip-10-0-65-133   s3xea3f0926x5063x4a8dx956cx9d8828f31232       10.0.65.133   UP      NONE              4.4.2.144-k8s  AUTO            1      1.26 GB  2:17:28h
12       ip-10-0-107-84   drivexfbc81c00xa6dex442fxb5b3x3ed4dd433741    10.0.107.84   UP      NONE              4.4.2.144-k8s  AUTO            1      1.54 GB  2:17:24h
13       ip-10-0-98-109   drivex07ddd04bx85cdx43acxbea0xb0f2c339f335    10.0.98.109   UP      NONE              4.4.2.144-k8s  AUTO            1      1.54 GB  2:17:23h
14       ip-10-0-103-75   c138a41e8d33client                            10.0.103.75   UP      NONE              4.4.2.144-k8s                  2      2.94 GB  0:01:13h
15       ip-10-0-113-108  c138a41e8d33client                            10.0.113.108  UP      NONE              4.4.2.144-k8s                  2      2.94 GB  0:01:19h
16       ip-10-0-96-250   c138a41e8d33client                            10.0.96.250   UP      NONE              4.4.2.144-k8s                  2      2.94 GB  0:01:20h
17       ip-10-0-66-16    c138a41e8d33client                            10.0.66.16    UP      NONE              4.4.2.144-k8s                  2      2.94 GB  0:01:28h
18       ip-10-0-94-223   c138a41e8d33client                            10.0.94.223   UP      NONE              4.4.2.144-k8s                  2      2.94 GB  0:01:03h
19       ip-10-0-79-235   c138a41e8d33client                            10.0.79.235   UP      NONE              4.4.2.144-k8s                  2      2.94 GB  0:01:23h

root@ip-10-0-98-109:/# weka cluster process
PROCESS ID  CONTAINER ID  SLOT IN HOST  HOSTNAME         CONTAINER                                     IPS           STATUS  RELEASE        ROLES       NETWORK  CPU  MEMORY   UPTIME    LAST FAILURE
0           0             0             ip-10-0-83-118   drivex92d620e9xffc0x4d14x823ex6444a0d2a823    10.0.83.118   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:15h
1           0             1             ip-10-0-83-118   drivex92d620e9xffc0x4d14x823ex6444a0d2a823    10.0.83.118   UP      4.4.2.144-k8s  DRIVES      UDP      6    1.54 GB  2:17:09h
20          1             0             ip-10-0-83-118   s3x58e05049x0c4ex44b8x9b99xccff4dc364db       10.0.83.118   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:14h  Host joined a new cluster (2 hours ago)
21          1             1             ip-10-0-83-118   s3x58e05049x0c4ex44b8x9b99xccff4dc364db       10.0.83.118   UP      4.4.2.144-k8s  FRONTEND    UDP      3    1.26 GB  2:17:09h
40          2             0             ip-10-0-125-187  drivex3cc5580dx303ex4c15xba6cx82ccc04a898d    10.0.125.187  UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:14h  Host joined a new cluster (2 hours ago)
41          2             1             ip-10-0-125-187  drivex3cc5580dx303ex4c15xba6cx82ccc04a898d    10.0.125.187  UP      4.4.2.144-k8s  DRIVES      UDP      3    1.54 GB  2:17:08h
60          3             0             ip-10-0-65-133   drivex4cca8165xfcaax438dx9a75xf372efc7a497    10.0.65.133   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:14h  Host joined a new cluster (2 hours ago)
61          3             1             ip-10-0-65-133   drivex4cca8165xfcaax438dx9a75xf372efc7a497    10.0.65.133   UP      4.4.2.144-k8s  DRIVES      UDP      6    1.54 GB  2:17:09h
80          4             0             ip-10-0-65-133   computex133b97bcx5bf6x4612x8f2axd09fc42bb573  10.0.65.133   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:14h  Host joined a new cluster (2 hours ago)
81          4             1             ip-10-0-65-133   computex133b97bcx5bf6x4612x8f2axd09fc42bb573  10.0.65.133   UP      4.4.2.144-k8s  COMPUTE     UDP      2    2.94 GB  2:17:07h
100         5             0             ip-10-0-110-144  computexc0ac9647xf6a5x4d77x909ax07e865469af1  10.0.110.144  UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:13h  Host joined a new cluster (2 hours ago)
101         5             1             ip-10-0-110-144  computexc0ac9647xf6a5x4d77x909ax07e865469af1  10.0.110.144  UP      4.4.2.144-k8s  COMPUTE     UDP      1    2.94 GB  2:17:07h
120         6             0             ip-10-0-107-84   computex1b0db083x8986x45e0x96e3xe739b58808ee  10.0.107.84   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:13h  Host joined a new cluster (2 hours ago)
121         6             1             ip-10-0-107-84   computex1b0db083x8986x45e0x96e3xe739b58808ee  10.0.107.84   UP      4.4.2.144-k8s  COMPUTE     UDP      1    2.94 GB  2:17:07h
140         7             0             ip-10-0-98-109   computex1d5f9c03x5b35x401exa71ex3786135e7a66  10.0.98.109   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:13h  Host joined a new cluster (2 hours ago)
141         7             1             ip-10-0-98-109   computex1d5f9c03x5b35x401exa71ex3786135e7a66  10.0.98.109   UP      4.4.2.144-k8s  COMPUTE     UDP      1    2.94 GB  2:17:07h
160         8             0             ip-10-0-110-144  drivex3cdd7cd0xecbbx4240x9f85xb5881b2f276c    10.0.110.144  UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:13h  Host joined a new cluster (2 hours ago)
161         8             1             ip-10-0-110-144  drivex3cdd7cd0xecbbx4240x9f85xb5881b2f276c    10.0.110.144  UP      4.4.2.144-k8s  DRIVES      UDP      3    1.54 GB  2:17:07h
180         9             0             ip-10-0-125-187  computexcc4c114exb720x49a7xa964xb050041220d1  10.0.125.187  UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:14h  Host joined a new cluster (2 hours ago)
181         9             1             ip-10-0-125-187  computexcc4c114exb720x49a7xa964xb050041220d1  10.0.125.187  UP      4.4.2.144-k8s  COMPUTE     UDP      1    2.94 GB  2:17:07h
200         10            0             ip-10-0-83-118   computexac9ca615x9425x48eexaafex756ee3e8e8aa  10.0.83.118   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:14h  Host joined a new cluster (2 hours ago)
201         10            1             ip-10-0-83-118   computexac9ca615x9425x48eexaafex756ee3e8e8aa  10.0.83.118   UP      4.4.2.144-k8s  COMPUTE     UDP      2    2.94 GB  2:17:08h
220         11            0             ip-10-0-65-133   s3xea3f0926x5063x4a8dx956cx9d8828f31232       10.0.65.133   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:13h  Host joined a new cluster (2 hours ago)
221         11            1             ip-10-0-65-133   s3xea3f0926x5063x4a8dx956cx9d8828f31232       10.0.65.133   UP      4.4.2.144-k8s  FRONTEND    UDP      3    1.26 GB  2:17:09h
240         12            0             ip-10-0-107-84   drivexfbc81c00xa6dex442fxb5b3x3ed4dd433741    10.0.107.84   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:13h  Host joined a new cluster (2 hours ago)
241         12            1             ip-10-0-107-84   drivexfbc81c00xa6dex442fxb5b3x3ed4dd433741    10.0.107.84   UP      4.4.2.144-k8s  DRIVES      UDP      3    1.54 GB  2:17:07h
260         13            0             ip-10-0-98-109   drivex07ddd04bx85cdx43acxbea0xb0f2c339f335    10.0.98.109   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      2:17:14h  Host joined a new cluster (2 hours ago)
261         13            1             ip-10-0-98-109   drivex07ddd04bx85cdx43acxbea0xb0f2c339f335    10.0.98.109   UP      4.4.2.144-k8s  DRIVES      UDP      3    1.54 GB  2:17:08h
280         14            0             ip-10-0-103-75   c138a41e8d33client                            10.0.103.75   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      0:01:09h  Configuration snapshot pulled (1 minute ago)
281         14            1             ip-10-0-103-75   c138a41e8d33client                            10.0.103.75   UP      4.4.2.144-k8s  FRONTEND    UDP      1    1.47 GB  0:01:03h
282         14            2             ip-10-0-103-75   c138a41e8d33client                            10.0.103.75   UP      4.4.2.144-k8s  FRONTEND    UDP      2    1.47 GB  0:01:03h
300         15            0             ip-10-0-113-108  c138a41e8d33client                            10.0.113.108  UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      0:01:14h  Configuration snapshot pulled (1 minute ago)
301         15            1             ip-10-0-113-108  c138a41e8d33client                            10.0.113.108  UP      4.4.2.144-k8s  FRONTEND    UDP      1    1.47 GB  0:01:10h
302         15            2             ip-10-0-113-108  c138a41e8d33client                            10.0.113.108  UP      4.4.2.144-k8s  FRONTEND    UDP      2    1.47 GB  0:01:10h
320         16            0             ip-10-0-96-250   c138a41e8d33client                            10.0.96.250   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      0:01:18h  Configuration snapshot pulled (1 minute ago)
321         16            1             ip-10-0-96-250   c138a41e8d33client                            10.0.96.250   UP      4.4.2.144-k8s  FRONTEND    UDP      1    1.47 GB  0:01:13h
322         16            2             ip-10-0-96-250   c138a41e8d33client                            10.0.96.250   UP      4.4.2.144-k8s  FRONTEND    UDP      2    1.47 GB  0:01:13h
340         17            0             ip-10-0-66-16    c138a41e8d33client                            10.0.66.16    UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      0:01:22h  Configuration snapshot pulled (1 minute ago)
341         17            1             ip-10-0-66-16    c138a41e8d33client                            10.0.66.16    UP      4.4.2.144-k8s  FRONTEND    UDP      1    1.47 GB  0:01:19h
342         17            2             ip-10-0-66-16    c138a41e8d33client                            10.0.66.16    UP      4.4.2.144-k8s  FRONTEND    UDP      2    1.47 GB  0:01:19h
360         18            0             ip-10-0-94-223   c138a41e8d33client                            10.0.94.223   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      57.13s    Configuration snapshot pulled (1 minute ago)
361         18            1             ip-10-0-94-223   c138a41e8d33client                            10.0.94.223   UP      4.4.2.144-k8s  FRONTEND    UDP      1    1.47 GB  54.63s
362         18            2             ip-10-0-94-223   c138a41e8d33client                            10.0.94.223   UP      4.4.2.144-k8s  FRONTEND    UDP      2    1.47 GB  54.13s
380         19            0             ip-10-0-79-235   c138a41e8d33client                            10.0.79.235   UP      4.4.2.144-k8s  MANAGEMENT  UDP           N/A      0:01:18h  Configuration snapshot pulled (1 minute ago)
381         19            1             ip-10-0-79-235   c138a41e8d33client                            10.0.79.235   UP      4.4.2.144-k8s  FRONTEND    UDP      1    1.47 GB  0:01:13h
382         19            2             ip-10-0-79-235   c138a41e8d33client                            10.0.79.235   UP      4.4.2.144-k8s  FRONTEND    UDP      2    1.47 GB  0:01:13h

root@ip-10-0-98-109:/# weka status
WekaIO v4.4.2.144-k8s (CLI build 4.4.2.144-k8s)

       cluster: cluster-dev (10d5d634-0aa2-4858-8fef-409254bdf74f)
        status: OK (14 backend containers UP, 6 drives UP)
    protection: 3+2 (Fully protected)
     hot spare: 0 failure domains
 drive storage: 22.09 TiB total, 21.86 TiB unprovisioned
         cloud: connected
       license: Unlicensed

     io status: STARTED 2 hours ago (14 io-nodes UP, 78 Buckets UP)
    link layer: Ethernet
       clients: 6 connected
         reads: 0 B/s (0 IO/s)
        writes: 0 B/s (0 IO/s)
    operations: 0 ops/s
        alerts: 35 active alerts, use `weka alerts` to list them
```

</details>

#### Verification

After completing these steps, verify that:

* All client pods are in Running state.
* The CORES value shows 2 for client containers.
* The clients have successfully rejoined the cluster.
* The system status shows no errors using `weka status`.

#### Troubleshooting

If clients fail to restart:

* Ensure sufficient hugepage memory is available.
* Check pod events for specific error messages.
* Verify the client configuration in the YAML file is correct.

***

### Increase backend cores

Increase the number of cores allocated to compute and drive containers to improve processing capacity for intensive workloads.

The following procedure exemplifies increase of the computeCores and driveCores from 1 to 2 cores.

#### **Procedure**

1. Modify the cluster YAML configuration to update core allocation:

```yaml
template: dynamic
dynamicTemplate:
  computeContainers: 6
  driveContainers: 6
  computeCores: 2    # Increased from 1
  driveCores: 2      # Increased from 1
  numDrives: 1
  s3Containers: 2
  s3Cores: 1
  envoyCores: 1
```

2. Apply the updated configuration:

```bash
kubectl apply -f <cluster-yaml-file>
```

<details>

<summary>Example</summary>

```
$ kubectl apply -f cluster3.yaml
wekacontainer.weka.weka.io/weka-driver-dist unchanged
service/weka-driver-dist unchanged
wekacluster.weka.weka.io/cluster-dev configured
wekacontainer.weka.weka.io/weka-drivers-builder unchanged
```

</details>

3. Verify the changes are applied to the cluster configuration:

```bash
kubectl get wekacluster cluster-dev -n weka-operator-system -o yaml
```

<details>

<summary>Example</summary>

```
$ kubectl get wekacluster cluster-dev -n weka-operator-system -o yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaCluster
metadata:
  annotations:
kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"weka.weka.io/v1alpha1","kind":"WekaCluster","metadata":{"annotations":{},"name":"cluster-dev","namespace":"weka-operator-system"},"spec":{"driversDistService":"https://weka-driver-dist.weka-operator-system.svc.cluster.local:60002","dynamicTemplate":{"computeContainers":6,"computeCores":2,"driveContainers":6,"driveCores":2,"envoyCores":1,"numDrives":1,"s3Containers":2,"s3Cores":1},"image":"quay.io/weka.io/weka-in-container:4.4.1","imagePullSecret":"quay-io-robot-secret","nodeSelector":{"weka.io/supports-backends":"true"},"template":"dynamic"}}
  creationTimestamp: "2024-11-12T08:56:37Z"
  finalizers:
  - weka.weka.io/finalizer
  generation: 3
  name: cluster-dev
  namespace: weka-operator-system
  resourceVersion: "47005"
  uid: 7817f6ca-7c38-4582-b5d7-fcf837d246e9
spec:
  additionalMemory: {}
  cpuPolicy: auto
  driversDistService:https://weka-driver-dist.weka-operator-system.svc.cluster.local:60002
  dynamicTemplate:
    computeContainers: 6
    computeCores: 2
    driveContainers: 6
    driveCores: 2
    envoyCores: 1
    numDrives: 1
    s3Containers: 2
    s3Cores: 1
  gracefulDestroyDuration: 24h0m0s
  hotSpare: 0
  image: quay.io/weka.io/weka-in-container:4.4.1
  imagePullSecret: quay-io-robot-secret
  network: {}
  nodeSelector:
    weka.io/supports-backends: "true"
  ports: {}
  roleNodeSelector: {}
  template: dynamic
status:
  clusterID: b74532a1-bf80-4857-b187-187dd322b25a
  conditions:
  - lastTransitionTime: "2024-11-12T08:56:38Z"
    message: Cluster secrets are created
    reason: Init
    status: "True"
    type: ClusterSecretsCreated
  - lastTransitionTime: "2024-11-12T08:56:41Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: PodsCreated
  - lastTransitionTime: "2024-11-12T08:56:53Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ContainerResourcesAllocated
  - lastTransitionTime: "2024-11-12T08:57:14Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: PodsReady
  - lastTransitionTime: "2024-11-12T08:57:28Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ClusterCreated
  - lastTransitionTime: "2024-11-12T08:57:28Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: JoinedCluster
  - lastTransitionTime: "2024-11-12T08:57:36Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: DrivesAdded
  - lastTransitionTime: "2024-11-12T08:58:41Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: IoStarted
  - lastTransitionTime: "2024-11-12T08:58:42Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ClusterSecretsApplied
  - lastTransitionTime: "2024-11-12T08:58:43Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: CondDefaultFsCreated
  - lastTransitionTime: "2024-11-12T08:58:44Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: CondS3ClusterCreated
  - lastTransitionTime: "2024-11-12T08:58:44Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ClusterClientsSecretsCreated
  - lastTransitionTime: "2024-11-12T08:58:44Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ClusterClientsSecretsApplied
  - lastTransitionTime: "2024-11-12T08:58:44Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ClusterCSIsSecretsCreated
  - lastTransitionTime: "2024-11-12T08:58:45Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ClusterCSIsSecretsApplied
  - lastTransitionTime: "2024-11-12T08:58:45Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: WekaHomeConfigured
  - lastTransitionTime: "2024-11-12T08:58:45Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: ClusterIsReady
  - lastTransitionTime: "2024-11-12T09:03:46Z"
    message: Completed successfully
    reason: Init
    status: "True"
    type: CondAdminUserDeleted
  lastAppliedImage: quay.io/weka.io/weka-in-container:4.4.1
  lastAppliedSpec: ceab3ec15455912eab20aa62b4457f7ad4c4a626f0313633feff02dfddcda03a
  ports:
    basePort: 15000
    lbAdminPort: 15301
    lbPort: 15300
    portRange: 500
    s3Port: 15302
  status: Ready
  throughput: ""
```

</details>

#### Troubleshooting

If core values are not updated after applying changes:

1. Verify the YAML syntax is correct.
2. Ensure the cluster configuration was successfully applied.
3. Check for any error messages in the cluster events:

```bash
kubectl describe wekacluster cluster-dev -n weka-operator-system
```

Note: * Core allocation changes may require additional steps for full implementation.
* Monitor cluster performance after making changes.
* Consider testing in a non-production environment first.
* Contact support if core values persist at previous settings after applying changes.

***

## **Cluster maintenance**

Cluster maintenance ensures optimal performance, security, and reliability through regular updates. Key tasks include updating WekaCluster and WekaClient configurations, rotating pods to apply changes, and creating token secret for WekaClient.

### Update WekaCluster configuration

This topic explains how to update WekaCluster configuration parameters to enhance cluster performance or resolve issues.

You can update the following WekaCluster parameters:

* AdditionalMemory (spec.AdditionalMemory)
* Tolerations (spec.Tolerations)
* RawTolerations (spec.RawTolerations)
* DriversDistService (spec.DriversDistService)
* ImagePullSecret (spec.ImagePullSecret)

After completing each of the following procedures, all pods restart within a few minutes to apply the new configuration.

#### Procedure: Update AdditionalMemory

1.  Open your cluster.yaml file and update the additional memory values:

    ```yaml
    additionalMemory:
      compute: 100
      s3: 200
      drive: 300
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f cluster.yaml
    ```
3.  Delete the WekaContainer pods:

    ```bash
    kubectl delete pod <wekacontainer-pod-name>
    ```
4. Verify that the memory values have been updated to the new settings.

#### Procedure: Update Tolerations

1.  Open your cluster.yaml file and update the toleration values:

    ```yaml
    tolerations:
      - simple-toleration
      - another-one
    rawTolerations:
      - key: "weka.io/dedicated"
        operator: "Equal"
        value: "weka-backend"
        effect: "NoSchedule"
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f cluster.yaml
    ```
3.  Delete all WekaContainer pods:

    ```bash
    kubectl delete pod <wekacontainer-pod-name>
    ```

#### Procedure: Update DriversDistService

1.  Open your cluster.yaml file and update the DriversDistService value:

    ```

    ```yaml
    driversDistService: "https://weka-driver-dist.namespace.svc.cluster.local:60002"
    ```

```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f cluster.yaml
    ```
3.  Delete the WEKA driver distribution pods:

    ```bash
    kubectl delete pod <driver-dist-pod-name>
    ```

#### Procedure: Update ImagePullSecret

1.  Open your cluster.yaml file and update the ImagePullSecret value:

    ```yaml
    imagePullSecret: "your-new-secret-name"
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f cluster.yaml
    ```
3.  Delete all WekaContainer pods:

    ```bash
    kubectl delete pod <wekacontainer-pod-name>
    ```

#### Troubleshooting

If pods do not restart automatically or the new configuration is not applied, verify:

* The syntax in your cluster.yaml file is correct.
* You have the necessary permissions to modify the cluster configuration.
* The cluster is in a healthy state.

***

### Update WekaClient configuration

This topic explains how to update WekaClient configuration parameters to ensure optimal client interactions with the cluster.

You can update the following WekaClient parameters:

* DriversDistService (spec.DriversDistService)
* ImagePullSecret (spec.ImagePullSecret)
* WekaSecretRef (spec.WekaSecretRef)
* AdditionalMemory (spec.AdditionalMemory)
* UpgradePolicy (spec.UpgradePolicy)
* DriversLoaderImage (spec.DriversLoaderImage)
* Port (spec.Port)
* AgentPort (spec.AgentPort)
* PortRange (spec.PortRange)
* CoresNumber (spec.CoresNumber)
* Tolerations (spec.Tolerations)
* RawTolerations (spec.RawTolerations)

After completing each of the following procedures, all pods restart within a few minutes to apply the new configuration.

#### Before you begin

Before updating any WekaClient configuration:

* Ensure you have access to the **client.yaml** configuration file or **client CRD**.
* Verify you have the necessary permissions to modify client configurations.
* Back up your current configuration.
* Ensure the cluster is in a healthy state and accessible to clients.

#### Procedure: Update DriversDistService

1.  Open your client.yaml file and update the DriversDistService value:

    ```yaml
    driversDistService: "https://weka-driver-dist.namespace.svc.cluster.local:60002"
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
3.  Delete the client pods:

    ```bash
    kubectl delete pod <client-pod-name>
    ```

#### Procedure: Update ImagePullSecret

1.  Open your client.yaml file and update the ImagePullSecret value:

    ```yaml
    imagePullSecret: "your-new-secret-name"
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
3.  Delete the client pods:

    ```bash
    kubectl delete pod <client-pod-name>
    ```

#### Procedure: Update Additional Memory

1.  Open your client.yaml file and update the additional memory values:

    ```yaml
    additionalMemory: 1000
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
3.  Delete the client pods:

    ```bash
    kubectl delete pod <client-pod-name>
    ```

#### Procedure:Update Tolerations

1.  Open your client.yaml file and update the toleration values:

    ```yaml
    tolerations:
      - simple-toleration
      - another-one
    rawTolerations:
      - key: "weka.io/dedicated"
        operator: "Equal"
        value: "weka-client"
        effect: "NoSchedule"
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
3.  Delete the client pods:

    ```bash
    kubectl delete pod <client-pod-name>
    ```

#### Procedure: Update WekaSecretRef

1.  Open your client.yaml file and update the WekaSecretRef value:

    ```yaml
    wekaSecretRef: "your-new-secret-ref"
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
3.  Delete the client pods:

    ```bash
    kubectl delete pod <client-pod-name>
    ```

#### Procedure: Update Port Configuration

This procedure demonstrates how to migrate from specific port and agentPort configurations to a portRange configuration for Weka clients.

1.  Deploy the initial client configuration with specific ports:

    ```yaml
    spec:
      port: 45001
      agentPort: 45000
    ```
2.  Apply the initial configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
3.  Verify the clients are running with the initial port configuration:

    ```bash
    kubectl get pods --all-namespaces
    ```
4.  Update the client YAML by removing the port and agentPort specifications and adding portRange:

    ```yaml
    spec:
      portRange:
        basePort: 45000
    ```
5.  Apply the updated configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
6.  Delete the existing client container pods to trigger reconfiguration:

    ```bash
    kubectl delete pod <client-container-pod-name> -n weka-operator-system --force --grace-period=0
    ```

    Replace `<client-name>` and `<ip-address>` with your specific values.
7.  Verify that the pods have restarted and rejoined the cluster:

    ```bash
    kubectl get pods --all-namespaces
    ```

#### Procedure: Update CoresNumber

1.  Open your client.yaml file and update the CoresNumber value:

    ```yaml
    coresNumber: <new-core-number>
    ```
2.  Apply the updated configuration:

    ```bash
    kubectl apply -f client.yaml
    ```
3.  Delete the client pods:

    ```bash
    kubectl delete pod <client-pod-name>
    ```

#### Troubleshooting

If pods do not restart automatically or the new configuration is not applied, verify:

* The syntax in your client.yaml file is correct.
* You have the necessary permissions to modify the client configuration.
* The cluster is in a healthy state and accessible to clients.
* The specified ports are available and not blocked by network policies.

***

### Rotate all pods when applying changes

Rotating pods after updating cluster configuration ensures changes are properly applied across all containers.

#### Procedure

1. Apply the updated cluster configuration:

```bash
kubectl apply -f <cluster.yaml>
```

<details>

<summary>Example: update cluster.yaml and apply</summary>

```
$ cat cluster.yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  name: weka-driver-dist
  namespace: weka-operator-system
  labels:
    app: weka-driver-dist
spec:
  agentPort: 60001
  image: quay.io/weka.io/weka-in-container:4.4.1
  imagePullSecret: "quay-io-robot-secret"
  mode: "drivers-dist"
  name: dist
  numCores: 1
  port: 60002
---
apiVersion: v1
kind: Service
metadata:
  name: weka-driver-dist
  namespace: weka-operator-system
spec:
  type: ClusterIP
  ports:
    - name: weka-driver-dist
      port: 60002
      targetPort: 60002
  selector:
    app: weka-driver-dist
---
apiVersion: weka.weka.io/v1alpha1
kind: WekaCluster
metadata:
  name: cluster-dev
  namespace: weka-operator-system
  template: dynamic
  dynamicTemplate:
    computeContainers: 6
    driveContainers: 6
    computeCores: 1
    driveCores: 1
    numDrives: 1
    s3Containers: 2

  image: quay.io/weka.io/weka-in-container:4.4.1
  nodeSelector:
    weka.io/supports-backends: "true"
  driversDistService: "https://weka-driver-dist.weka-operator-system.svc.cluster.local:60002"
  imagePullSecret: "quay-io-robot-secret"
  additionalMemory:
    compute: 500
    drive: 1000
    s3: 200
---
apiVersion: weka.weka.io/v1alpha1
kind: WekaContainer
metadata:
  name: weka-drivers-builder
  namespace: weka-operator-system
spec:
  agentPort: 60001
  image: quay.io/weka.io/weka-in-container:4.4.1
  nodeSelector:
    weka.io/supports-backends: "true"
  imagePullSecret: "quay-io-robot-secret"
  mode: "drivers-builder"
  name: dist
  numCores: 1
  uploadResultsTo: "weka-driver-dist"
  port: 60002

$ kubectl apply -f cluster.yaml
wekacontainer.weka.weka.io/weka-driver-dist unchanged
service/weka-driver-dist unchanged
wekacluster.weka.weka.io/cluster-dev configured
wekacontainer.weka.weka.io/weka-drivers-builder unchanged
```

</details>

2. Delete all container pods and verify that all pods restart and reach the Running state within a few minutes. \
   In the following commands replace the * with the actual container names.

```bash
kubectl delete pod -n weka-operator-system cluster-dev-compute-*
```

<details>

<summary>Example</summary>

```
$ kubectl delete pod -n weka-operator-system \
    cluster-dev-compute-0e6fe33b-16ac-4608-8fe5-742bf5a0b5a4 \
    cluster-dev-compute-1d8ea81a-6694-47d4-9a67-8946592423fa \
    cluster-dev-compute-3ed60c56-a97d-4160-b4b2-209c4ce508ff \
    cluster-dev-compute-45eb6107-6879-4356-a675-8ca7e4a61856 \
    cluster-dev-compute-48cb2fd4-c59a-4872-89a9-afe5b518514b \
    cluster-dev-compute-9ca0dad4-994c-4b36-a291-400c3a4c46a7
pod "cluster-dev-compute-0e6fe33b-16ac-4608-8fe5-742bf5a0b5a4" deleted
pod "cluster-dev-compute-1d8ea81a-6694-47d4-9a67-8946592423fa" deleted
pod "cluster-dev-compute-3ed60c56-a97d-4160-b4b2-209c4ce508ff" deleted
pod "cluster-dev-compute-45eb6107-6879-4356-a675-8ca7e4a61856" deleted
pod "cluster-dev-compute-48cb2fd4-c59a-4872-89a9-afe5b518514b" deleted
pod "cluster-dev-compute-9ca0dad4-994c-4b36-a291-400c3a4c46a7" deleted
```

</details>

3. Delete the drive pods:

```bash
kubectl delete pod -n weka-operator-system cluster-dev-drive-*
```

<details>

<summary>Example</summary>

```
$ kubectl delete pod -n weka-operator-system \ \
    cluster-dev-drive-494be7a2-aa98-444a-90de-84a3ce45eecd \
    cluster-dev-drive-89ccb57a-dc14-4e07-ac5e-d3a94b7f8eaa \
    cluster-dev-drive-9135ee7c-5f9d-4396-8e24-a89307d93d2f \
    cluster-dev-drive-91c0771f-d6da-4399-aed0-2e4e2741ab3b \
    cluster-dev-drive-f4ca28a5-9039-4213-ace8-bda61b8b7b3d \
    cluster-dev-drive-f9d3d087-b950-46a5-8efb-aa80b523bbf0
pod "cluster-dev-drive-494be7a2-aa98-444a-90de-84a3ce45eecd" deleted
pod "cluster-dev-drive-89ccb57a-dc14-4e07-ac5e-d3a94b7f8eaa" deleted
pod "cluster-dev-drive-9135ee7c-5f9d-4396-8e24-a89307d93d2f" deleted
pod "cluster-dev-drive-91c0771f-d6da-4399-aed0-2e4e2741ab3b" deleted
pod "cluster-dev-drive-f4ca28a5-9039-4213-ace8-bda61b8b7b3d" deleted
pod "cluster-dev-drive-f9d3d087-b950-46a5-8efb-aa80b523bbf0" deleted
```

</details>

4. Delete the S3 pods:

```bash
kubectl delete pod -n weka-operator-system cluster-dev-s3-*
```

<details>

<summary>Example</summary>

```
$ kubectl delete pod -n weka-operator-system \ \
    cluster-dev-s3-159a097d-a0f9-4faf-a00e-e79be6d5f342 \
    cluster-dev-s3-3745e1f6-afc1-4668-b535-9a4315def8f2
pod "cluster-dev-s3-159a097d-a0f9-4faf-a00e-e79be6d5f342" deleted
pod "cluster-dev-s3-3745e1f6-afc1-4668-b535-9a4315def8f2" deleted
```

</details>

5. Delete the envoy pods:

```bash
kubectl delete pod -n weka-operator-system cluster-dev-envoy-*
```

<details>

<summary>Example</summary>

```
$ kubectl delete pod -n weka-operator-system \
    cluster-dev-envoy-30e3ec65-8f06-4ab6-8f97-84f06253b415 \
    cluster-dev-envoy-d20eda4f-bc91-4e77-bd39-de1740b4ab9a
pod "cluster-dev-envoy-30e3ec65-8f06-4ab6-8f97-84f06253b415" deleted
pod "cluster-dev-envoy-d20eda4f-bc91-4e77-bd39-de1740b4ab9a" deleted
```

</details>

#### Verification

1. Monitor pod status until all pods return to Running state:

```bash
kubectl get pods --all-namespaces -o wide
```

<details>

<summary>Example</summary>

```
$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                                       READY   STATUS      RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATES
kube-system            coredns-7b98449c4-hxrz8                                    1/1     Running     0          14h     10.42.0.6      34.245.52.65    <none>           <none>
kube-system            helm-install-traefik-crd-nhg9k                             0/1     Completed   0          14h     10.42.0.3      34.245.52.65    <none>           <none>
kube-system            helm-install-traefik-xnsgm                                 0/1     Completed   1          14h     10.42.0.2      34.245.52.65    <none>           <none>
kube-system            local-path-provisioner-595dcfc56f-gw56n                    1/1     Running     0          14h     10.42.0.4      34.245.52.65    <none>           <none>
kube-system            metrics-server-cdcc87586-kmgnd                             1/1     Running     0          14h     10.42.0.5      34.245.52.65    <none>           <none>
kube-system            traefik-d7c9c5778-4gqv7                                    1/1     Running     0          14h     10.42.0.7      34.245.52.65    <none>           <none>
weka-operator-system   cluster-dev-compute-0e6fe33b-16ac-4608-8fe5-742bf5a0b5a4   1/1     Running     0          4m48s   10.0.68.116    34.245.25.64    <none>           <none>
weka-operator-system   cluster-dev-compute-1d8ea81a-6694-47d4-9a67-8946592423fa   1/1     Running     0          3m3s    10.0.103.104   34.245.52.65    <none>           <none>
weka-operator-system   cluster-dev-compute-3ed60c56-a97d-4160-b4b2-209c4ce508ff   1/1     Running     0          109s    10.0.118.28    34.245.126.85   <none>           <none>
weka-operator-system   cluster-dev-compute-45eb6107-6879-4356-a675-8ca7e4a61856   1/1     Running     0          4m16s   10.0.120.103   3.249.18.205    <none>           <none>
weka-operator-system   cluster-dev-compute-48cb2fd4-c59a-4872-89a9-afe5b518514b   1/1     Running     0          2m23s   10.0.127.24    3.250.111.147   <none>           <none>
weka-operator-system   cluster-dev-compute-9ca0dad4-994c-4b36-a291-400c3a4c46a7   1/1     Running     0          3m43s   10.0.108.100   34.241.105.15   <none>           <none>
weka-operator-system   cluster-dev-drive-494be7a2-aa98-444a-90de-84a3ce45eecd     1/1     Running     0          3m17s   10.0.118.28    34.245.126.85   <none>           <none>
weka-operator-system   cluster-dev-drive-89ccb57a-dc14-4e07-ac5e-d3a94b7f8eaa     1/1     Running     0          68s     10.0.108.100   34.241.105.15   <none>           <none>
weka-operator-system   cluster-dev-drive-9135ee7c-5f9d-4396-8e24-a89307d93d2f     1/1     Running     0          3m58s   10.0.120.103   3.249.18.205    <none>           <none>
weka-operator-system   cluster-dev-drive-91c0771f-d6da-4399-aed0-2e4e2741ab3b     1/1     Running     0          103s    10.0.127.24    3.250.111.147   <none>           <none>
weka-operator-system   cluster-dev-drive-f4ca28a5-9039-4213-ace8-bda61b8b7b3d     1/1     Running     0          22s     10.0.103.104   34.245.52.65    <none>           <none>
weka-operator-system   cluster-dev-drive-f9d3d087-b950-46a5-8efb-aa80b523bbf0     1/1     Running     0          2m22s   10.0.68.116    34.245.25.64    <none>           <none>
weka-operator-system   cluster-dev-envoy-30e3ec65-8f06-4ab6-8f97-84f06253b415     1/1     Running     0          3m20s   10.0.127.24    3.250.111.147   <none>           <none>
weka-operator-system   cluster-dev-envoy-d20eda4f-bc91-4e77-bd39-de1740b4ab9a     1/1     Running     0          3m18s   10.0.68.116    34.245.25.64    <none>           <none>
weka-operator-system   cluster-dev-s3-159a097d-a0f9-4faf-a00e-e79be6d5f342        1/1     Running     0          2m43s   10.0.127.24    3.250.111.147   <none>           <none>
weka-operator-system   cluster-dev-s3-3745e1f6-afc1-4668-b535-9a4315def8f2        1/1     Running     0          3m24s   10.0.68.116    34.245.25.64    <none>           <none>
weka-operator-system   weka-driver-dist                                           1/1     Running     0          14h     10.42.3.5      3.250.111.147   <none>           <none>
weka-operator-system   weka-operator-controller-manager-569444c54c-48kkj          2/2     Running     0          14h     10.42.2.2      3.249.18.205    <none>           <none>
```

</details>

2. Verify the configuration changes are applied by checking pod resources:

```

```bash
kubectl get pods -A -o=jsonpath="{range .items[*]}{.metadata.namespace}{' '}{.metadata.name}{':\
'}{'  Requests: '}{.spec.containers[*].resources.requests.memory}{'\
'}{'  Limits: '}{.spec.containers[*].resources.limits.memory}{'\
\
'}{end}"
```

```

<details>

<summary>Example</summary>

```
$ kubectl get pods -A -o=jsonpath="{range .items[*]}{.metadata.namespace}{' '}{.metadata.name}{':\
'}{'  Requests: '}{.spec.containers[*].resources.requests.memory}{'\
'}{'  Limits: '}{.spec.containers[*].resources.limits.memory}{'\
\
'}{end}"
kube-system coredns-7b98449c4-hxrz8:
  Requests: 70Mi
  Limits: 170Mi

kube-system helm-install-traefik-crd-nhg9k:
  Requests:
  Limits:

kube-system helm-install-traefik-xnsgm:
  Requests:
  Limits:

kube-system local-path-provisioner-595dcfc56f-gw56n:
  Requests:
  Limits:

kube-system metrics-server-cdcc87586-kmgnd:
  Requests: 70Mi
  Limits:

kube-system traefik-d7c9c5778-4gqv7:
  Requests:
  Limits:

weka-operator-system cluster-dev-compute-0e6fe33b-16ac-4608-8fe5-742bf5a0b5a4:
  Requests: 12400Mi
  Limits: 12400Mi

weka-operator-system cluster-dev-compute-1d8ea81a-6694-47d4-9a67-8946592423fa:
  Requests: 12400Mi
  Limits: 12400Mi

weka-operator-system cluster-dev-compute-3ed60c56-a97d-4160-b4b2-209c4ce508ff:
  Requests: 12400Mi
  Limits: 12400Mi

weka-operator-system cluster-dev-compute-45eb6107-6879-4356-a675-8ca7e4a61856:
  Requests: 12400Mi
  Limits: 12400Mi

weka-operator-system cluster-dev-compute-48cb2fd4-c59a-4872-89a9-afe5b518514b:
  Requests: 12400Mi
  Limits: 12400Mi

weka-operator-system cluster-dev-compute-9ca0dad4-994c-4b36-a291-400c3a4c46a7:
  Requests: 12400Mi
  Limits: 12400Mi

weka-operator-system cluster-dev-drive-494be7a2-aa98-444a-90de-84a3ce45eecd:
  Requests: 12700Mi
  Limits: 12700Mi

weka-operator-system cluster-dev-drive-89ccb57a-dc14-4e07-ac5e-d3a94b7f8eaa:
  Requests: 12700Mi
  Limits: 12700Mi

weka-operator-system cluster-dev-drive-9135ee7c-5f9d-4396-8e24-a89307d93d2f:
  Requests: 12700Mi
  Limits: 12700Mi

weka-operator-system cluster-dev-drive-91c0771f-d6da-4399-aed0-2e4e2741ab3b:
  Requests: 12700Mi
  Limits: 12700Mi

weka-operator-system cluster-dev-drive-f4ca28a5-9039-4213-ace8-bda61b8b7b3d:
  Requests: 12700Mi
  Limits: 12700Mi

weka-operator-system cluster-dev-drive-f9d3d087-b950-46a5-8efb-aa80b523bbf0:
  Requests: 12700Mi
  Limits: 12700Mi

weka-operator-system cluster-dev-envoy-30e3ec65-8f06-4ab6-8f97-84f06253b415:
  Requests: 1Gi
  Limits: 1Gi

weka-operator-system cluster-dev-envoy-d20eda4f-bc91-4e77-bd39-de1740b4ab9a:
  Requests: 1Gi
  Limits: 1Gi

weka-operator-system cluster-dev-s3-159a097d-a0f9-4faf-a00e-e79be6d5f342:
  Requests: 20665Mi
  Limits: 20665Mi

weka-operator-system cluster-dev-s3-3745e1f6-afc1-4668-b535-9a4315def8f2:
  Requests: 20665Mi
  Limits: 20665Mi

weka-operator-system weka-driver-dist:
  Requests: 3G
  Limits: 3G

weka-operator-system weka-operator-controller-manager-569444c54c-48kkj:
  Requests: 64Mi 64Mi
  Limits: 128Mi 1Gi
```

</details>

#### Expected results

* All pods return to Running state within a few minutes.
* Resource configurations match the updated values in the cluster configuration.
* No service disruption during the rotation process.

Note: - Pods automatically restart after deletion.
- The system maintains availability during pod rotation.
- Wait for each set of pods to begin restarting before proceeding to the next set.

***

### Create token secret for WekaClient

WekaClient tokens used for cluster authentication have a limited lifespan and will eventually expire. This guide walks you through the process of generating a new token, encoding it properly, and creating the necessary Kubernetes secret to maintain WekaClient connectivity.

#### Prerequisites

* Access to a running WEKA cluster with backend servers
* Kubernetes cluster with WEKA Operator deployed
* kubectl access with appropriate permissions
* Access to the `weka-operator-system` namespace

#### Step 1: Generate a new join token and encode it

The join token must be generated from within one of the WEKA backend containers. Follow these steps to create a long-lived token:

1.  **List the available pods in the weka-operator-system namespace:**

    ```bash
    kubectl get pods -n weka-operator-system
    ```
2.  **Connect to a backend pod and generate the token:**

    ```

    ```bash
    kubectl exec -it -n weka-operator-system <POD_NAME>
    weka cluster join-token generate --access-token-timeout 52w
    ```

```

    This command creates a token that remains valid for 52 weeks (one year). The system generates an output a JWT token similar to:

    ```

    ```
    eyJhbGciOiJSUzI1NiIsIml0dCI6IkNMSUVOVCIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3ODA1OTQ1NTIsImlhdCI6MTc0O
    ...truncated purposely...
    cRQZBPGSRJRWGwXAO1C_NMALdKxnABt6olHzW_gBiEQ42O30L3xF-ym3pDPrHhFQ
    ```

```
3. **Encode the token:** The generated token must be base64-encoded before use in the Kubernetes secret:

```bash
| echo <TOKEN> | base64 -w 0 && echo |
```

Save the base64-encoded output for use in the secret configuration.

<details>

<summary>Example</summary>

```

```bash
[ec2-user@ip-10-0-4-244 ~]$ kubect get pods -n weka-operator-system
NAME                                                    READY   STATUS RESTARTS   AGE
cluster1-clients-ip-10-0-4-244.ec2.internal             1/1     Running   0          88m
cluster1-compute-11b1fa70-c78c-48c2-90f8-056fda395eb9   1/1     Running   0          94m
cluster1-compute-395553b9-21ea-4c04-b20a-ce2b4c7e3b04   1/1     Running   0          94m
cluster1-compute-61b33b86-c67c-482e-8d4e-12efaf07d814   1/1     Running   0          94m
cluster1-compute-8cb4ea2f-e908-4314-9e4b-dd6a1cc9319e   1/1     Running   0          94m
cluster1-compute-c5f376ff-1cdd-4ef6-b59a-a360a20d24d0   1/1     Running   0          94m
cluster1-compute-d4c70979-e967-43b4-b414-3a291bd81179   1/1     Running   0          94m
cluster1-drive-56aacf86-27da-4252-9393-b19bd0913acd     1/1     Running   0          94m
cluster1-drive-5b51ac03-8c88-4a23-8220-15c90fdba644     1/1     Running   0          94m
cluster1-drive-67943209-1365-469b-b9a2-e7a902ff94e1     1/1     Running   0          94m
cluster1-drive-affaf947-3827-4edf-9acd-fb4e007c6426     1/1     Running   0          94m
cluster1-drive-c8475e40-62dc-4636-9d57-baf5b7c9edaf     1/1     Running   0          94m
cluster1-drive-d0bc2984-d33d-4115-86a3-4acb7c983ebb     1/1     Running   0          94m
monitoring-cluster1-5ff69896f4-qmtg8                    1/1     Running   0          93m
weka-operator-controller-manager-d5f455f7b-5nwfb        2/2     Running   0          136m
weka-operator-node-agent-4pbhn                          1/1     Running   0          136m
weka-operator-node-agent-64mh7                          1/1     Running   0          136m
weka-operator-node-agent-ctt2n                          1/1     Running   0          136m
weka-operator-node-agent-f9rrk                          1/1     Running   0          136m
weka-operator-node-agent-jmhfh                          1/1     Running   0          136m
weka-operator-node-agent-p26vl                          1/1     Running   0          95m
weka-operator-node-agent-sc594                          1/1     Running   0          136m

[ec2-user@ip-10-0-4-244 ~]$ kubect exec -it -n weka-operator-system cluster1-compute-11b1fa70-c78c-48c2-90f8-056fda395eb9 -- bash
root@ip-10-0-13-252:/# weka cluster join-token generate --access-token-timeout 52w
API access token has been generated:

eyJhbGciOiJSUzI1NiIsIml0dCI6IkNMSUVOVCIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3ODA1OTQ1N...truncated purposely...ufVkgmsLyIN21_C-cRQZBPGSRJRWGwXAO1C_NMALdKxnABt6olHzW_gBiEQ42O30L3xF-ym3pDPrHhFQ

| root@ip-10-0-13-252:/# echo eyJhbGciOiJSUzI1NiIsIml0dCI6IkNMSUVOVCIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3ODA1OTQ1N...truncated purposely...cRQZBPGSRJRWGwXAO1C_NMALdKxnABt6olHzW_gBiEQ42O30L3xF-ym3pDPrHhFQ | base64 -w 0 && echo |

ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbWwwZENJNklrTk1TVVZPVkNJc0luUjVjQ0k2SWtwWFZDSjkuZX...truncated purposely...
TQzlFckNpV19CVWtsV3NTaUk4cHhLXzZEcDNIVXV6OFpyYXVmVmtnbXNMeUlOMjFfQy1jUlFaQlBH
```

```

</details>

#### Step 2: Create the Kubernetes secret

#### Option A: Using YAML template

Create a YAML file with the following template, replacing the placeholder values:

```

```yaml
apiVersion: v1
data:
  join-secret: <BASE64_ENCODED_TOKEN>
  org: <BASE64_VALUE>
  password: <BASE64_VALUE>
  username: <BASE64_VALUE>
kind: Secret
metadata:
  name: weka-client-cluster1
  namespace: <NAMESPACE>
type: Opaque
```

```

Note: **Configuration notes:**
* **join-secret**: Use the base64-encoded token from Step 2
* **org, username, password**: Copy these values from the existing secret or create new base64-encoded values
* **namespace**: Use `default` or specify your target namespace

#### Option B: Copy from existing secret

To preserve existing credentials, export the current secret and modify only the token:

```

```bash
kubectl get secret -n weka-operator-system weka-client-cluster1 -o yaml > weka-client-cluster1_new.yaml
```

```

Edit the file to update the `join-secret` field with your new base64-encoded token.

#### Step 3: Apply the secret

Deploy the new secret to your Kubernetes cluster:

```bash
kubectl apply -f <secret_yaml_file>.yaml
```

Verify the secret creation:

```bash
kubectl get secret -n <namespace>
```

<details>

<summary>Example</summary>

In the following example, the secret is created in the default name space with name `new-weka-client-secret-cluster1`

```

```bash
[ec2-user@ip-10-0-4-244 ~]$ kubectl get secret -n weka-operator-system weka-client-cluster1 -o yaml > weka-client-cluster1_default.yaml
[ec2-user@ip-10-0-4-244 ~]$ nano weka-client-cluster1_default.yaml
[ec2-user@ip-10-0-4-244 ~]$ cat weka-client-cluster1_default.yaml
apiVersion: v1
data:
  join-secret: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbWwwZENJNklrTk1TVVZPVkNJc0luUjVjQ0k2SWtwWFZDSjkuZX...Truncated purposely.. UlFaQlBHU1JKUldHd1hBTzFDX05NQUxkS3huQUJ0Nm9sSHpXX2dCaUVRNDJPMzBMM3hGLXltM3BEUHJIaEZRCg==
  org: Um9vdA==
  password: aDc5Q3g0M1lLNndOQjNxNnhwWHdOSW1xQmJHOE5nMjE=
  username: d2VrYWNsaWVudDQ2MDVhYWI0MWE3ZA==
kind: Secret
metadata:
  creationTimestamp: "2025-06-05T16:04:56Z"
  name: new-weka-client-secret-cluster1
  namespace: default
type: Opaque
[ec2-user@ip-10-0-4-244 ~]$ kubectl apply -f weka-client-cluster1_default.yaml
secret/weka-client-cluster1 created
[ec2-user@ip-10-0-4-244 ~]$ kubectl get secret -n default
NAME                   TYPE                             DATA   AGE
quay-io-robot-secret   kubernetes.io/dockerconfigjson   1      164m
weka-client-cluster1   Opaque                           4      8s
```

```

The following yaml file creates a new client with the name `new-cluster1-clients` in the default name space using the `new-weka-client-secret-cluster1` secret.

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaClient
metadata:
  name: new-cluster1-clients
  namespace: default
spec:
  image: quay.io/weka.io/weka-in-container:4.4.5.118-k8s.4
  imagePullSecret: quay-io-robot-secret
  driversDistService: "https://drivers.weka.io"
  nodeSelector:
    weka.io/supports-clients: "true"
  wekaSecretRef: new-weka-client-secret-cluster1 # Must match secret name created using secret yaml
  targetCluster:
    name: cluster1
    namespace: weka-operator-system
  portRange:
    basePort: 45000
```

</details>

#### Step 4: Update WekaClient configuration

Remove any existing client instances and ensure no pods are actively using WEKA storage on the target node.

#### Remove active workloads

1.  **Identify pods using Weka on the target node:**

    ```bash
    kubectl get pods --field-selector spec.nodeName=<node-name>
    ```
2.  **Stop workloads using Weka storage:**

    ```bash
    kubectl delete pod <pod-name>
    ```

#### Remove existing WekaClient

1.  **List current WekaClient instances:**

    ```bash
    kubectl get wekaclient -n weka-operator-system
    ```
2.  **Delete the existing client:**

    ```bash
    kubectl delete wekaclient -n weka-operator-system <client-name>
    ```

#### Deploy new WekaClient

Create a new WekaClient configuration that references your updated secret:

```

```yaml
apiVersion: weka.weka.io/v1alpha1
kind: WekaClient
metadata:
  name: new-cluster1-clients
  namespace: default
spec:  image: quay.io/weka.io/weka-in-container:4.4.5.118-k8s.4
  imagePullSecret: quay-io-robot-secret
  driversDistService: "https://drivers.weka.io"
  nodeSelector:    weka.io/supports-clients: "true"
  wekaSecretRef: weka-client-cluster1  # Must match your secret name
  targetCluster:
    name: cluster1
    namespace: weka-operator-system
  portRange:
    basePort: 45000
```

```

Apply the configuration:

```bash
kubectl apply -f new-weka-client.yaml
```

#### Step 5: Verify client status

Monitor the new WekaClient deployment:

```bash
kubectl get wekaclientskubectl get pods
```

The new client should show a **Running** status. CSI pods may temporarily enter `CrashLoopBackOff` state while the client initializes, but will recover automatically once the client is ready.

<details>

<summary>Example</summary>

```bash
[root@ip-10-0-4-244 ec2-user]# kubectl get wekaclients
NAME                   STATUS    TARGET CLUSTER   CORES   CONTAINERS(A/C/D)
new-cluster1-clients   Running   cluster1
[root@ip-10-0-4-244 ec2-user]# kubectl get pods
NAME                                              READY   STATUS    RESTARTS   AGE
new-cluster1-clients-ip-10-0-4-244.ec2.internal   1/1     Running   0          22s
[root@ip-10-0-4-244 ec2-user]# kubectl get pods -n csi-wekafs
NAME                                     READY   STATUS             RESTARTS      AGE
csi-wekafs-controller-75cc977744-7vpwx   6/6     Running            6 (84s ago)   8m28s
csi-wekafs-node-zfbw4                    2/3     CrashLoopBackOff   6 (13s ago)   8m26s
[root@ip-10-0-4-244 ec2-user]# kubectl get pods -n csi-wekafs
NAME                                     READY   STATUS             RESTARTS      AGE
csi-wekafs-controller-75cc977744-7vpwx   6/6     Running            6 (87s ago)   8m31s
csi-wekafs-node-zfbw4                    2/3     CrashLoopBackOff   6 (16s ago)   8m29s
[root@ip-10-0-4-244 ec2-user]# kubectl get pods -n csi-wekafs
NAME                                     READY   STATUS             RESTARTS      AGE
csi-wekafs-controller-75cc977744-7vpwx   6/6     Running            6 (89s ago)   8m33s
csi-wekafs-node-zfbw4                    2/3     CrashLoopBackOff   6 (18s ago)   8m31s
[root@ip-10-0-4-244 ec2-user]# kubectl get pods -n csi-wekafs
NAME                                     READY   STATUS             RESTARTS      AGE
csi-wekafs-controller-75cc977744-7vpwx   6/6     Running            6 (96s ago)   8m40s
csi-wekafs-node-zfbw4                    2/3     CrashLoopBackOff   6 (25s ago)   8m38s
[root@ip-10-0-4-244 ec2-user]# kubectl delete pod -n csi-wekafs csi-wekafs-node-zfbw4
pod "csi-wekafs-node-zfbw4" deleted
[root@ip-10-0-4-244 ec2-user]# kubectl get pods -n csi-wekafs
NAME                                     READY   STATUS    RESTARTS       AGE
csi-wekafs-controller-75cc977744-7vpwx   6/6     Running   6 (115s ago)   8m59s
csi-wekafs-node-8q472                    3/3     Running   0              3s
```

</details>

#### Troubleshooting

#### CSI Pods in CrashLoopBackOff

If CSI pods remain in a failed state after the WekaClient is running, manually restart them:

```bash
kubectl delete pod -n csi-wekafs <csi-pod-name>
```

#### Token validation

To verify your token is working correctly, check the WekaClient logs:

```bash
kubectl logs -n <namespace> <wekaclient-pod-name>
```

#### Secret verification

Confirm your secret contains the correct base64-encoded values:

```bash
kubectl get secret <secret-name> -n <namespace> -o yaml
```

#### Best practices

* **Token Lifetime**: Generate tokens with appropriate expiration times based on your maintenance schedule
* **Secret Management**: Store secrets in appropriate namespaces with proper RBAC controls
* **Documentation**: Maintain records of token generation dates and expiration times
* **Monitoring**: Implement alerts for token expiration to prevent service disruptions
* **Testing**: Validate new tokens in non-production environments before deploying to production

#### Security considerations

* Limit access to token generation commands to authorized personnel only
* Use namespaces to isolate secrets from different environments
* Regularly rotate tokens as part of your security policy
* Monitor and audit secret access and modifications

***

## WekaContainer lifecycle management

The WekaContainer serves as a critical persistence layer within a Kubernetes environment. Understanding its lifecycle is crucial because deleting a WekaContainer, whether gracefully or forcefully, results in the permanent loss of all associated data.

The following diagram provides a visual overview of the WekaContainer's lifecycle in Kubernetes, illustrating the flow from creation through running states and the various paths taken during deletion. The subsequent sections elaborate on the specific states, processes, and decision points shown.

**Key deletion states**

The deletion process involves two primary states the container can enter:

* **`Deleting`**: This state signifies a graceful shutdown process triggered by standard Kubernetes deletion or pod deletion timeouts. It involves the controlled _Deactivation_ sequence shown in the diagram before the container is removed.
* **`Destroying`**: This state represents a forced, immediate removal, bypassing the deactivation steps. As the diagram shows, this is typically triggered by a _Cluster destroy_ event.

 **Deletion triggers and paths**

The specific path taken upon deletion depends on the trigger:

* **Kubernetes resource deletion:** When a user deletes the WekaContainer custom resource directly (for example, `kubectl delete wekacontainer...`), Kubernetes initiates the process leading to the `Deleting` state, starting the graceful deactivation cycle.
* **Pod termination (user-initiated or node drain):** As shown in the _Pod termination_ path, if the specific Pod hosting the WekaContainer is terminated, while the `WekaContainer` Custom Resource (CR) still exists (for example, due to node failure, eviction, or direct `kubectl delete pod`):
  * Kubernetes first attempts to gracefully stop the Weka process within that pod using `weka local stop`, allowing a 5-minute grace period.
  * If successful, the process stops cleanly. If `weka local stop` times out or fails, the specific Weka container _instance_ tied to that terminating pod may transition to the `Deleting` state (as per the diagram) to ensure proper deactivation and removal from the Weka cluster's perspective (leading to data loss for that instance).
  * **Important:** Because the `WekaContainer` CR itself has _not_ been deleted and still defines the desired state, the WEKA Operator detects that the required pod is missing. Consequently, the Operator automatically attempts to create a **new pod** to replace the terminated one, aiming to bring the system back to the _Running_ state defined by the CR. This new pod starts fresh.
* **Cluster destruction:** A cluster destroy operation does not immediately transition containers to the Destroying state. By default, WekaCluster uses a graceful termination period (`spec.gracefulDestroyDuration`, set to 24 hours). When the WekaCluster custom resource is deleted, WekaContainers first enter a _Paused_ state (pods are terminated), but the containers and their data remain intact. After the graceful period ends, containers transition to the _Destroying_ state for forced removal, bypassing any graceful shutdown attempts.

**The deactivation process (graceful deletion)**

When a WekaContainer follows the path into the `Deleting` state, it undergoes the multi-step _Deactivation_ process shown before drives are resigned. This sequence ensures safe removal from the WEKA cluster and includes:

* Cluster deactivation.
* Removal from the S3 cluster (if applicable).
* Removal from the main WEKA cluster.
* Skipping deactivation: By setting `overrides.skipDeactivate=true`, you can bypass the deactivation steps and route the flow directly to _Resigned drives_. However, this is considered unsafe.

**Drive management**

Regardless of whether the path taken was `Deleting` (with or without deactivation) or `Destroying`, the process ends with the storage drives being **resigned**. This makes them available for reuse.

**Health state and replacement**

In this flow diagram, it's crucial to understand that WekaContainers in the `Deleting` or `Destroying` states are deemed unhealthy. This informs Kubernetes and the WEKA operator that the container is non-functional, typically prompting replacement attempts based on the deployment configuration. However, that data from the deleted container is permanently lost.

<!-- ============================================ -->
<!-- File 209/259: performance.md -->
<!-- ============================================ -->

# Performance

## Topics in this section

### WEKA performance tests

Measure the key performance metrics of a WEKA cluster‚Äîlatency, IOPS, and bandwidth‚Äîusing standardized testing procedures.

<!-- ============================================ -->
<!-- File 210/259: performance_testing-weka-system-performance.md -->
<!-- ============================================ -->

---
description:
---

# WEKA performance tests

## Overview

When measuring a storage system's performance, there are three primary metrics:

* **Latency**: The time from the initiation of an operation to its completion.
* **IOPS**: The number of I/O operations (such as read, write, or metadata) that the system can process concurrently.
* **Bandwidth**: The amount of data that the system can process concurrently.

Each metric applies to read operations, write operations, or a mixture of both. Different [mount modes](../weka-system-overview/weka-client-and-mount-modes) can produce different performance characteristics. Additionally, the client's network configuration, such as using user-space DPDK networking or kernel UDP, significantly affects performance.

It is important to distinguish between single-client and aggregated performance. Running tests from a single client will likely be limited by the client's own performance capabilities. In general, maximizing the performance of a WEKA cluster requires running tests from several clients simultaneously.

To ensure that test results reflect the filesystem's ability to deliver data independent of client-side caching, the benchmarks are designed to negate the effects of caching where possible. This is achieved by using `o_direct` calls to bypass the client's cache for file testing and by flushing Linux caches between tests.

## Testing WEKA performance with wekatester

Use the `wekatester` command-line utility to measure the performance of a WEKA cluster. This tool automates a series of standardized FIO tests to measure key performance indicators (KPIs), such as throughput, IOPS, and latency.

Using `wekatester` is the recommended approach for performance testing as it provides consistent, reproducible, and easy-to-interpret results.

**Before you begin**

* Download the `wekatester` tool from the WEKA repository on GitHub.
* Ensure FIO is installed on all client hosts participating in the test (FIO documentation).

**Procedure**

1. Log in to a client with access to the WEKA cluster.
2. Navigate to the directory containing the `wekatester` tool.
3.  Run the performance test suite using the following command:

    ```bash
    ./wekatester -c
    ```

    The tool automatically discovers the cluster and clients, prepares the hosts for testing, runs the full suite of performance tests, and reports the aggregated results.

**Result example**

The command displays a summary of the performance results, providing a clear overview of the cluster's capabilities.

```
read bandwidth: 434.52 GiB/s
total bandwidth: 434.52 GiB/s
average bandwidth: 27.16 GiB/s per host

write bandwidth: 258.49 GiB/s
total bandwidth: 258.49 GiB/s
average bandwidth: 16.16 GiB/s per host

read latency: 143 us

write latency: 134 us

read iops: 16,526,081/s
total iops: 16,526,081/s
average iops: 1,032,880/s per host

write iops: 4,089,720/s
total iops: 4,089,720/s
average iops: 255,607/s per host
```

### Wekatester FIO job definitions

The `wekatester` tool uses a standardized set of Flexible I/O (FIO) tester jobs to ensure consistent and comparable results. These job definitions are provided for users who want to review the testing methodology or run the tests manually.

All jobs use a 2G file size for testing consistency.

#### **Read throughput job definition**

This job measures the maximum read bandwidth.

```toml
[global]
filesize=2G
time_based=1
startdelay=5
exitall_on_error=1
create_serialize=0
filename_format=$filenum/$jobnum
directory=/mnt/weka
group_reporting=1
clocksource=gettimeofday
runtime=30
ioengine=libaio
disk_util=0
direct=1
numjobs=32

[fio-createfiles-00]
blocksize=1Mi
description='pre-create files'
create_only=1

[fio-bandwidthSR-00]
stonewall
description='Sequential Read bandwidth workload'
blocksize=1Mi
rw=read
iodepth=1
```

#### **Write throughput job definition**

This job measures the maximum write bandwidth.

```toml
[global]
filesize=2G
time_based=1
startdelay=5
exitall_on_error=1
create_serialize=0
filename_format=$filenum/$jobnum
directory=/mnt/weka
group_reporting=1
clocksource=gettimeofday
runtime=30
ioengine=libaio
disk_util=0
direct=1
numjobs=32

[fio-createfiles-00]
stonewall
blocksize=1Mi
description='pre-create files'
create_only=1

[fio-bandwidthSW-00]
stonewall
description='Sequential Write bandwidth workload'
blocksize=1Mi
rw=write
iodepth=1
```

#### **Read IOPS job definition**

This job measures the maximum read IOPS using a 4k block size.

```toml
[global]
filesize=2G
time_based=1
startdelay=5
exitall_on_error=1
create_serialize=0
filename_format=$filenum/$jobnum
directory=/mnt/weka
group_reporting=1
clocksource=gettimeofday
runtime=30
ioengine=libaio
disk_util=0
direct=1
numjobs=64

[fio-createfiles-00]
blocksize=1Mi
description='pre-create files'
create_only=1

[fio-iopsR-00]
stonewall
description='Read iops workload'
iodepth=8
bs=4k
rw=randread
```

#### **Write IOPS job definition**

This job measures the maximum write IOPS using a 4k block size.

```toml
[global]
filesize=2G
time_based=1
startdelay=5
exitall_on_error=1
create_serialize=0
filename_format=$filenum/$jobnum
directory=/mnt/weka
group_reporting=1
clocksource=gettimeofday
runtime=30
ioengine=libaio
disk_util=0
direct=1
numjobs=64

[fio-createfiles-00]
blocksize=1Mi
description='pre-create files'
create_only=1

[fio-iopsW-00]
stonewall
description='Write iops workload'
iodepth=8
bs=4k
rw=randwrite
```

#### **Read latency job definition**

This job measures read latency using a 4k block size.

```toml
[global]
filesize=2G
time_based=1
startdelay=5
exitall_on_error=1
create_serialize=0
filename_format=$filenum/$jobnum
directory=/mnt/weka
group_reporting=1
clocksource=gettimeofday
runtime=30
ioengine=libaio
disk_util=0
direct=1
numjobs=1

[fio-createfiles-00]
blocksize=1Mi
description='pre-create files'
create_only=1

[fio-latencyR-00]
stonewall
description='Read latency workload'
bs=4k
rw=randread
iodepth=1
```

#### **Write latency job definition**

This job measures write latency using a 4k block size.

```toml
[global]
filesize=2G
time_based=1
startdelay=5
exitall_on_error=1
create_serialize=0
filename_format=$filenum/$jobnum
directory=/mnt/weka
group_reporting=1
clocksource=gettimeofday
runtime=30
ioengine=libaio
disk_util=0
direct=1
numjobs=1

[fio-createfiles-00]
blocksize=1Mi
description='pre-create files'
create_only=1

[fio-latencyW-00]
stonewall
description='Write latency workload'
bs=4k
rw=randwrite
iodepth=1
```

## Testing metadata performance with MDTest

MDTest is an open-source tool designed to test metadata performance, measuring the rate of operations such as file creates, stats, and deletes across the cluster.

MDTest uses an MPI framework to coordinate jobs across multiple nodes. The examples shown here assume the use of MDTest version 1.9.3 with MPICH version 3.3.2 (MPITCH documentation).

**Procedure**

Run the MDTest benchmark from a client machine with access to the WEKA filesystem. The following command runs the test across multiple clients defined in a hostfile. It uses 8 clients with 136 threads each to test the performance on 20 million files.

**Job definition**

```bash
mpiexec -f <hostfile> -np 1088 mdtest-v-N 136i 3 n 18382 -F -u-d /mnt/weka/mdtest
```

**Result example**

The following table shows an example summary from three test iterations.

 | Operation | Max | Min | Mean | Std Dev |
 | ----------------- | ----------- | ----------- | ----------- | ------- |
 | **File creation** | 40784.448 | 40784.447 | 40784.448 | 0.001 |
 | **File stat** | 2352915.997 | 2352902.666 | 2352911.311 | 6.121 |
 | **File read** | 217236.252 | 217236.114 | 217236.162 | 0.064 |
 | **File removal** | 44101.905 | 44101.896 | 44101.902 | 0.004 |
 | **Tree creation** | 3.788 | 3.097 | 3.342 | 0.316 |
 | **Tree removal** | 1.192 | 1.142 | 1.172 | 0.022 |

## Performance test results summary

The following tables show example results from tests run in specific AWS and SuperMicro environments.

### **Single client results**

 | Benchmark | AWS | SuperMicro |
 | --- | --- | --- |
 | Read Throughput | 8.9 GiB/s | 21.4 GiB/s |
 | Write Throughput | 9.4 GiB/s | 17.2 GiB/s |
 | Read IOPS | 393,333 ops/s | 563,667 ops/s |
 | Write IOPS | 302,333 ops/s | 378,667 ops/s |
 | Read Latency | 272 ¬µs avg.&#x26;lt;br>99.5% completed under 459 ¬µs | 144.76 ¬µs avg.&#x26;lt;br>99.5% completed under 260 ¬µs |
 | Write Latency | 298 ¬µs avg.&#x26;lt;br>99.5% completed under 432 ¬µs | 107.12 ¬µs avg.&#x26;lt;br>99.5% completed under 142 ¬µs |

### **Aggregated cluster results (with multiple clients)**

 | Benchmark | AWS | SuperMicro |
 | --- | --- | --- |
 | Read Throughput | 36.2 GiB/s | 123 GiB/s |
 | Write Throughput | 11.6 GiB/s | 37.6 GiB/s |
 | Read IOPS | 1,978,330 ops/s | 4,346,330 ops/s |
 | Write IOPS | 404,670 ops/s | 1,317,000 ops/s |
 | Creates | 79,599 ops/s | 234,472 ops/s |
 | Stats | 1,930,721 ops/s | 3,257,394 ops/s |
 | Deletes | 117,644 ops/s | 361,755 ops/s |

<!-- ============================================ -->
<!-- File 211/259: performance_testing-weka-system-performance_test-environment-details.md -->
<!-- ============================================ -->

---
description:
---

# Performance test environment configurations

## **AWS configuration**

#### **AWS cluster**

* Stripe Size: 4+2
* 8 backend server instances of i3en.12xlarge, placed in the same placement group
* OS: Amazon Linux AMI 2017.09.0.20170930 x86_64 HVM
* 7 dedicated cores for WEKA (4 compute, 2 drives, 1 frontend)

#### **AWS clients**

* c5n.18xlarge instances; 8 clients were used for aggregated results
* OS: Amazon Linux AMI 2017.09.0.20170930 x86_64 HVM
* 4 frontend cores
* DPDK networking
* Mount options: system defaults

## **SuperMicro configuration**

#### **SuperMicro cluster**

* Stripe Size: 4+2
* 8 backend servers (SYS-2029BT-HNR / X11DPT-B)
* OS: CentOS Linux release 7.8.2003
* CPU: 24/48 Threads (Intel Xeon Gold 6126 CPU @ 2.60GHz)
* Memory: 384 GB
* Drives: 6x Micron 9300 drives
* Network: Dual 100 Gbps Ethernet
* 19 dedicated cores for WEKA (12 compute, 6 drives, 1 frontend)

#### **SuperMicro clients**

* SYS-2029BT-HNR / X11DPT-B servers; 8 clients were used for aggregated results
* OS: CentOS Linux release 7.8.2003
* CPU: 24/48 Threads (Intel Xeon Gold 6126 CPU @ 2.60GHz)
* Memory: 192 GB
* Network: Dual 100 Gbps Ethernet
* 6 frontend cores
* DPDK networking
* Mount options: system defaults

<!-- ============================================ -->
<!-- File 212/259: licensing.md -->
<!-- ============================================ -->

# Licensing

## Topics in this section

### License overview

This page describes how licensing works in a WEKA cluster.

### Classic license

Learn how to obtain and apply a classic WEKA license, a time-based license purchased for a predetermined period, to your WEKA cluster.

<!-- ============================================ -->
<!-- File 213/259: licensing_overview.md -->
<!-- ============================================ -->

---
description: This page describes how licensing works in a WEKA cluster.
---

# License overview

A license is a legal instrument that defines the usage terms of the WEKA cluster. When applied, the cluster verifies the license‚Äôs validity by comparing its terms with actual cluster usage. Only one license can be active on a cluster at a time; applying a new license replaces the existing one.

The license terms include the following properties:

* Cluster GUID that is created during the installation
* Expiry date (usage period)
* Raw or usable hot-tier (SSD) capacity
* Object store capacity
* Data Efficiency Option (DEO) license (if provided)

## Display the license status using the GUI

The WEKA cluster license page displays the license properties: license mode, expiry date, raw or usable drive capacity, and object store capacity.

Note: The Pay As You Go (PAYG) license was deprecated in version 4.1 and is no longer available to new customers. However, the term _Classic License_ remains for backward compatibility.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the Cluster Settings pane, select **License**.

## Display the license status using the CLI

You can display the license status using one of the following commands:

* `weka cluster license`: Displays the license properties.
* `weka status`: Displays the weka status, license status, and expiry date.
* `weka alerts`: If no license is assigned to the cluster, the command displays a relevant alert.

**Example: License status using the `weka cluster license` command**

```
# weka cluster license
Licensing status: Classic

Current usage:
    1932 GB raw drive capacity
    963 GB usable capacity
    49 GB object-store capacity
    Disabled data reduction

Installed license:
    Valid from 2023-07-01T08:17:24Z
    Expires at 2023-07-31T08:17:24Z
    1932 GB raw drive capacity
    0 GB usable capacity
    1000000000000000 GB object-store capacity
    Enabled data reduction

```

**Example: Display the license status using the `weka status` command**

```
WekaIO v4.2.0 (CLI build 4.2.0)

...
       license: OK, valid thru 2023-07-19T09:22:34Z
...
```

**Example: License status when the cluster does not have a valid license**

```
# weka status
Weka v4.2.0 (CLI build 4.2.0)
...
       license: Unlicensed
...
```

**Example: License status using the `weka alerts` command for a cluster without an assigned license**

```

```
# weka alerts
...
No License Assigned
This cluster does not have a license assigned, please go to https://get.weka.io to obtain your license
```

```

<!-- ============================================ -->
<!-- File 214/259: licensing_classic-licensing.md -->
<!-- ============================================ -->

---
description:
---

# Classic license

## Obtain a classic license from get.weka.io

A classic license is a text-based license file generated via get.weka.io for a specific WEKA cluster. You must apply this license to the intended cluster.

Note: Only users with the Account Owner role can manage licenses.

#### **Before you begin**

1. **WEKA account:** Ensure you have a WEKA account. If not, follow the instructions in#register-to-get.weka.io.
2. **Cluster information:** Collect your cluster‚Äôs GUID and capacity values by running the following command on your WEKA system:

```
weka cluster license
```

**Sample output**

```

```bash
Licensing status: Unlicensed

Your cluster is currently unlicensed. Please go to https://get.weka.io/ to get a license or enroll in a subscription.

When asked, you'll need the following details to create your license:

    Cluster GUID          : bbb6639d-3eaa-483b-b532-31a560d5859d
    Raw Drive Capacity    : 11399 GB
    Usable Capacity       : 0 GB
    Object-store Capacity : 0 GB

If you already have a license, please enter it by running

    weka cluster license set <license-key>
```

```

**Procedure**

1. **Sign in to** **get.weka.io****:** Sign in using your WEKA account credentials.
2. **Obtain an entitlement:** Sales Operations will assign an entitlement to your account. You can view available entitlements on the **Account Dashboard**.

3. **Create the license:**
   1. In the Outstanding Entitlements section, select **Create a license**.
   2. In the next pane, select the entitlement line item for which the license will be created.
   3. Select **Create a license** again to proceed.

5. **Enter cluster information:** In the Create License dialog:
   1. Enter the Cluster GUID and capacity values obtained earlier.
   2. Optionally, enter higher capacity values if planning a future cluster expansion. The license defines the capacity limits for the cluster and does not need to match current usage exactly.
6. **Complete the process:** Select **Create License** to generate the license. You can apply it to your WEKA cluster.

## Apply or update a license to the cluster

After creating a license, apply it to the cluster. Only one license can be active at a time; applying a new license replaces the existing one.

**Procedure**

1. In the Licenses tab in get.weka.io, select the three dots to the right of the license details and then select **Show License Text**.

2. In the License Text dialog that opens, select **copy to clipboard**.

3. If you use the CLI to apply the license, run the following command:\
   `weka cluster license set <license-key>`\
   Where the \<license-key> is the license copied to the clipboard.
4. If you use the GUI to apply the license, do the following:
   * From the menu, select **Configure > Cluster Settings.**
   * From the Cluster Settings pane, select **License**.
   * Paste the license copied to the clipboard, and select **Save**.

## Reuse an existing license on a new cluster

When installing a new cluster, it is assigned with a new GUID. You can reuse an existing license for the newly-installed cluster.

**Procedure**

1. In the Licenses tab in get.weka.io, select the three dots to the right of the license details and then select **Change Cluster GUID**.

2\. Set the new cluster GUID and select **Save Changes**.

3\. Apply the license with the updated GUID. See #apply-or-update-a-license-to-the-cluster.

## Changing the size of your license

When you purchase additional entitlement to increase the license size of an existing cluster, you must create and apply a new license. This requires deactivating the current license for that cluster in the get.weka.io portal. Deactivating a license has no impact on cluster operations.

License deactivation reclaims capacity back to your linked entitlement, enabling you to:

* Expand existing clusters
* Rebuild clusters
* Correct licenses issued with incorrect capacity

This process allows you to redeploy the appropriate capacity without contacting support, streamlining license management.

**Procedure:**

1. **Locate the license:** Log in to get.weka.io and navigate to the Licenses page. Identify the license associated with the cluster you want to deactivate.
2. **Deactivate the license:** Select the **More options** icon (three dots) next to the relevant license, then select Deactivate License. This action does not impact the operation of the associated cluster.

3. **Confirm deactivation:** In the Deactivate License dialog, verify the license details. When confirmed, select **Deactivate License** to complete the process.

After deactivation, the license capacity is returned to the linked entitlement. You can then use the available entitlement to #apply-or-update-a-license-to-the-cluster.

<!-- ============================================ -->
<!-- File 215/259: support.md -->
<!-- ============================================ -->

# Support

## Topics in this section

### Release support and commitments

Learn about the WEKA software release cycle, support timelines, and upgrade policies to maintain system reliability and security.

### Get support for your WEKA system

Discover WEKA's support policies, proactive approach, and helpful tips for a seamless start.

### Diagnostics management

<!-- ============================================ -->
<!-- File 216/259: support_getting-support-for-your-weka-system.md -->
<!-- ============================================ -->

---
description:
---

# Get support for your WEKA system

## Contact Customer Success Team

WEKA provides a 24/7 technical support service according to WEKA's technical support policy (provided on-demand) based on the inquiry classification (severity level).

Choose the classification option to access detailed information and necessary steps for further action.

**Severity 1**: Indicates system-wide outages that critically impair business operations, potentially causing significant productivity loss, financial impact, or data integrity risks.

Do one of the following:

* **Call WEKA support number: +1 (844) 392-0665:** Leave a voice message, which is directed to the active support personnel.
* **Open a ticket in the Support Portal**: To get started, sign up as a user in the Support Portal: support.weka.io (if not done yet). Then, open a ticket and select the _Severity 1_ classification.\
  \
  You can monitor tickets and receive timely notifications and updates whenever any changes occur to the tickets.

Severity levels:

* **Severity 2**: Significant service degradation or performance issues.
* **Severity 3**: Limited feature functionality or minor system impairments.
* **Severity 4**: Inquiries and potential product improvement suggestions.

Do one of the following:

* **Open a ticket in the Support Porta**l: To get started, sign up as a user in the Support Portal: support.weka.io (if not done yet). Then, open a ticket and select the appropriate classification according to the issue severity.\
  \
  You can track tickets and receive notifications and updates when there are changes to the tickets.
* **Send an email to** [**support@weka.io**](mailto:support@weka.io): To get started, sign up as a user in the Support Portal: support.weka.io (if not done yet). When you send an email, it automatically creates a ticket in the Support Portal. Problem notifications sent via email are not considered critical.

Note: If you have a feature request, contact your sales team to submit the request.

Note: WEKA provides only remote support as part of its SLA.
The SLA detailed in the technical support policy is for WEKA software issues. Issues caused by faulty hardware depend on the hardware provider, and WEKA is not responsible for the hardware provider's timelines and response time.

## Recommended preparations for getting support

* Upload information from the WEKA cluster to Weka Home for each provisioned cluster.
* Create an account on the Weka Support Portal support.weka.io. This account allows you to submit and check the status of tickets and browse our online knowledge base.

**Related topic**

## Escalation

If you find our response to be unsatisfactory or believe that there is potential for improvement, you have the option to escalate the incident to our management team.

To initiate the escalation process, please contact WEKA Support at +1 (844) 392-0665 and select the escalation option. You can leave a voicemail stating your request for escalation of the issue.

Depending on the time of day and their respective time zones, your escalation request will be directed to one of our executive managers. We operate on a "follow the sun" approach, ensuring that one of our executive managers is available to address your escalation request.

<!-- ============================================ -->
<!-- File 217/259: support_release-support-and-commitments.md -->
<!-- ============================================ -->

---
description:
---

# Release support and commitments

WEKA provides a structured release cycle to deliver regular updates while ensuring long-term product support.

### Release cycles

WEKA offers two types of release cycles:

* **Innovation cycle:** A new innovation cycle starts twice a year and includes monthly updates for six months. These releases deliver new features, enhancements, and security updates.
* **Long-Term Support (LTS) cycle:** WEKA designates an LTS release after every two innovation cycles. An LTS release is supported for two years, providing a stable and reliable platform. For example, version 4.4 is the LTS release that follows the 4.2 LTS release, as illustrated in the support phases diagram below.

### Support phases

Each release cycle consists of distinct support phases. The length of each phase depends on whether the release is an Innovation or LTS version.

* **Innovation:** This phase lasts for six months for non-LTS releases. It includes new features and proactive updates.
* **Proactive updates:** This phase is the first year of an LTS release. It provides new features, OS and hardware compatibility enhancements, and security updates.
* **Critical updates:** This phase is the second year of an LTS release. It delivers essential security updates and minimal OS and hardware support.

### Update and upgrade policy

To ensure you benefit from the latest improvements and security enhancements, adhere to the following policies:

* **Innovation releases:** You must apply updates within 90 days of their release to remain supported. For example, it is not permissible to use version 4.3.0 for a year without updating to the newer 4.3.x releases. WEKA addresses defects in subsequent releases rather than as hotfixes for older releases.
* **Upgrade paths:** You can transition from an innovation release cycle to an LTS release. WEKA guarantees upgrades from one major LTS version to the next.
*   **End of support:** You must plan an upgrade to a supported version before your current version reaches the end of its critical update period. For assistance with planning and performing your upgrade, contact the [Customer Success Team](getting-support-for-your-weka-system).

    WEKA does not support software versions that have reached their end-of-support date and disclaims liability for any issues that arise from using unsupported versions.

### Version support dates

The following table provides the end-of-support dates for WEKA version series.

 | WEKA version series | End of proactive updates | End of critical updates | End of support |
 | ------------------- | ------------------------ | ----------------------- | --------------- |
 | Pre-4.2 | June 1, 2024 | June 1, 2025 | June 1, 2025 |
 | 4.2 | June 1, 2025 | June 1, 2026 | June 1, 2026 |
 | 4.3 | January 1, 2025 | January 1, 2025 | January 1, 2025 |
 | 4.4 | June 1, 2026 | June 1, 2027 | June 1, 2027 |
 | 5.0 | January 1, 2026 | January 1, 2026 | January 1, 2026 |

**Related topic**

<!-- ============================================ -->
<!-- File 218/259: support_diagnostics-management.md -->
<!-- ============================================ -->

# Diagnostics management

The WEKA system provides diagnostics information about internal processes at various levels. This information helps the Customer Success Team and R\&D to analyze and provide support when troubleshooting is needed.

The WEKA system has the following tools:

* **Traces:** Traces are low-level events collected by an internal tracking tool. The tracking tool collects the traces continuously on the backends and clients.
* **Protocols debug level:** The cluster enables changing the log debug level (verbosity level) for each protocol container in the cluster.
* **Diagnostics CLI command**: The diagnostics CLI command is used for collecting and uploading diagnostic data about clusters, servers, and containers for analysis by the Customer Success Team to help with troubleshooting.

**Related topics**

<!-- ============================================ -->
<!-- File 219/259: support_diagnostics-management_diagnostics-utility.md -->
<!-- ============================================ -->

---
description:
---

# Diagnostics data management

The diagnostics CLI commands enable managing diagnostics data associated with clusters, servers, and containers. The Customer Success Team then analyzes this diagnostics data to assist in troubleshooting. There are two options available for managing diagnostics:

* **Cluster-wide diagnostics commands:** Use the command `weka diags` for cluster-wide diagnostics management from any server within the cluster.
* **Local container diagnostics command:** Use the command `weka local diags` for diagnostics management of a connected local server.

## Cluster-wide diagnostics commands

Use the cluster-wide diagnostics commands to oversee diagnostics data on any cluster server. This includes functionalities to upload diagnostics data to WEKA Home, collect diagnostics data, clean up diagnostics files, and list available diagnostics files.

### Upload diagnostics data to WEKA Home

**Command:** `weka diags upload`

Use the following command to collect diagnostics information, save it, and upload it to WEKA Home (the WEKA support cloud):

`weka diags upload [--core-limit core-limit] [--dump-id dump-id] [--container-id container-id]... [--clients] [--backends]`

The command response provides an access identifier, `Diags collection ID`. Send this access identifier to the Customer Success Team to retrieve the diagnostics data from the WEKA Home.

When running the command for all servers in the cluster, a local diagnostics file (dump) is created in each server in the location `/opt/weka/diags/local`. The local diagnostics file of each server is consolidated in a single diagnostics file in the server where you run the command in the `/opt/weka/diags` directory.

Note: * HTTPS access is required to upload the diagnostics to AWS S3 endpoints.
* The upload process is asynchronous. Therefore, connectivity failure events are reflected in the events log even if the command exits successfully.

<details>

<summary>Example: collect and upload diagnostics from all backend containers</summary>

```bash
[root@wekaprod-0 ~] 2023-02-20 13:39:25 $ weka diags upload
Uploading diags from 5 hosts to the cloud
Cluster GUID: c0aca0f2-0d20-465e-9817-e747be811016
Diags collection ID: 1Ox5OYogdTP54Nah7o1cb
Cloud URL: https://api.home.weka.io

Collecting diags                             [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 5 / 5
Copying files for uploading                  [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 5 / 5
Uploading to cloud (this could take a while) [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 125 / 125

+------------------------+
 | Diags upload completed |
+------------------------+
```

</details>

**Parameters**

 | Name | Description | Default |
 | --- | --- | --- |
 | core-limit | Limit the diagnostics collection process to use the specified core number. | 1 |
 | dump-id | Uploads a pre-existing diagnostics file (dump) generated by the weka diags collect command with the specified dump ID.Using this option, the command exclusively uploads data and does not conduct data collection. Do not use this option for data previously generated by a weka diags upload command.The command evaluates the list of containers for dump upload, which may differ from those collected in the specified dump directory. Any container data not found in the collected dump is disregarded. | If no ID is provided, a new diagnostics file is generated. |
 | container-id | A list of container ID numbers separated by commas for collecting and uploading diagnostics data.If specified, the --backends and --clients options are ignored. |  |
 | clients | Collect and upload diagnostics data only from client containers. | No data is collected for clients |
 | backends | Collect and upload diagnostics data only from backend containers (same as if you are not specifying this option). To collect diagnostics for all client and backend containers, add both options --backends and --clients to the command. | Backends only |

### Collect diagnostics data

**Command:** `weka diags collect`

Use the following command to create diagnostics information and save it without uploading it to WEKA Home. This command is useful when there is no connection to WEKA Home, and you want to share the diagnostics file using other options.

`weka diags collect [--id id] [--output-dir output-dir] [--core-limit core-limit] [--container-id container-id] [--clients] [--backends] [--tar]`

If the command runs with the `local` keyword, information is collected only from the server on which the command is executed. Otherwise, information is collected from the whole cluster.

<details>

<summary>Example: collect diagnostics from all backends</summary>

```bash
[root@wekaprod-0 ~] 2023-02-20 13:38:58 $ weka diags collect
Downloading cluster diagnostics from 5 hosts to this host
Diags will be saved to: /opt/weka/diags/ody2uRl8xOfDESd6vkbYH4

Collecting diags      [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 5 / 5
Downloading artifacts [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 46.43 MiB / 46.43 MiB

CATEGORY   DIAG                       H0  H1  H2  H3  H4
host       uptime                     ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       date                       ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       uname                      ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       top                        ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       free_hugepages             ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       nr_hugepages               ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       nr_hugepages_mempolicy     ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       nr_overcommit_hugepages    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       surplus_hugepages          ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       meminfo                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       cpuinfo                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       netstat                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       etc-hosts                  ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       ps                         ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       mount                      ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       rpcinfo                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       kernel_modules             ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       pci_devices                ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       pci_tree                   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       ip_links                   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       ip_routes                  ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       arp_table                  ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       iptables                   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       resolv.conf                ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       root_disk_usage            ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       dmesg                      ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       dmidecode                  ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       network_manager            X   X   X   X   X
host       selinux                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       fstab                      ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       journalctl                 ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       systemctl                  ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       ip4addr                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       ip4link                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       syslog                     ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       boot_log                   -   -   -   -   -
host       ofed-version               -   -   -   -   -
host       ofed                       -   -   -   -   -
host       mlnx4_core                 -   -   -   -   -
host       mlnx5_core                 -   -   -   -   -
host       memtest                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       is-numa-balancing-active   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       is-swap-on                 ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       lsblk                      ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       system-release             ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       os-release                 ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       lsb-release                -   -   -   -   -
host       redhat-release             ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       lscpu                      ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       ifconfig                   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       ip_rule                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
host       weka_local_status          ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  container_list             ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  lstopo                     ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  numactl                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  ipmi-sel                   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  ipmi-sdr                   ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  logs                       ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  driver_queue               ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  local_events               ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  traces_analysis            ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  resources_files            ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
weka_host  core_dumps                 ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
cluster    api-status                 ‚óè
cluster    api-alerts                 ‚óè
cluster    api-realtime_stats         ‚óè
cluster    api-hosts                  ‚óè
cluster    api-nodes                  ‚óè
cluster    api-drives                 ‚óè
cluster    api-failure_domains        ‚óè
cluster    api-host_hardware          ‚óè
cluster    api-filesystems            ‚óè
cluster    api-filesystem_groups      ‚óè
cluster    api-snapshots              ‚óè
cluster    api-tiering                ‚óè
cluster    api-users                  ‚óè
cluster    api-default_data_net       ‚óè
cluster    api-nfs_client_groups      ‚óè
cluster    api-nfs_interface_groups   ‚óè
cluster    api-nfs_permissions        ‚óè
cluster    api-nfs_status             ‚óè
cluster    cli-status                 ‚óè
cluster    cli-filesystems            ‚óè
cluster    cli-host                   ‚óè
cluster    cli-net                    ‚óè
cluster    cli-nodes                  ‚óè
cluster    cli-drives                 ‚óè
cluster    cli-buckets                ‚óè
cluster    cli-cluster-tasks          ‚óè
cluster    cli-alerts                 ‚óè
cluster    cli-rebuild_status         ‚óè
cluster    cli-filesystem_groups      ‚óè
cluster    cli-kms                    ‚óè
cluster    cli-fs_tier_s3             ‚óè
cluster    cli-org                    ‚óè
cluster    cli-realtime_stats         ‚óè
cluster    cli-smb                    ‚óè
cluster    cli-smb-cluster-status     ‚óè
cluster    cli-smb-domain             ‚óè
cluster    cli-smb-share              ‚óè
cluster    cli-smb-share-lists-show   ‚óè
cluster    cli-cloud                  ‚óè
cluster    cli-buckets-dist           ‚óè
cluster    cli-net-links              ‚óè
cluster    cli-blacklist-list         ‚óè
cluster    cli-manual-overrides-list  ‚óè
cluster    cli-traces-status          ‚óè
cluster    cli-traces-freeze          ‚óè
cluster    cli-s3-cluster             ‚óè
cluster    cli-s3-cluster-status      ‚óè
cluster    cli-s3-bucket-list         -
cluster    cli-config-list-overrides  ‚óè
cluster    api-cfgdump                ‚óè
report     summary                    ‚óè   ‚óè   ‚óè   ‚óè   ‚óè
report     errors                     ‚óè   ‚óè   ‚óè   ‚óè   ‚óè

The following errors were found:

   h0:
      network_manager:
         - The NetworkManager service is running on this host, it must be turned off for weka to run properly

   h1:
      network_manager:
         - The NetworkManager service is running on this host, it must be turned off for weka to run properly

   h2:
      network_manager:
         - The NetworkManager service is running on this host, it must be turned off for weka to run properly

   h3:
      network_manager:
         - The NetworkManager service is running on this host, it must be turned off for weka to run properly

   h4:
      network_manager:
         - The NetworkManager service is running on this host, it must be turned off for weka to run properly
```

</details>

**Parameters**

 | Name | Description | Default |
 | --- | --- | --- |
 | id | An optional identifier for this diagnostics file. If not specified, a random ID is generated. | Auto-generated |
 | output-dir | The directory for saving the diagnostics file. | /opt/weka/diags |
 | core-limit | Limit the diagnostics collection process to use the specified core number. | 1 |
 | container-id | A list of container ID numbers separated by commas for collecting diagnostics data.If specified, the --backends and --clients options are ignored. |  |
 | clients | Collect diagnostics data only from client containers. | No data is collected for clients |
 | backends | Collect diagnostics data only from backend containers (same as if you are not specifying this option). To collect diagnostics for all client and backend containers, add both options --backends and --clients to the command. | Backends only |
 | tar | Package the collected diagnostics in a TAR file. | No TAR file is created |

### Clean up the diagnostics files

The `weka diags collect` command consolidates diagnostics from various containers into a single dump directory. Conversely, `weka diags upload` saves diagnostics data on each container, distributing files across the cluster before uploading to WEKA Home.

Collecting diagnostics data generates individual files, consuming disk space. As a result, the system may accumulate numerous diagnostics files, especially after they have been uploaded to WEKA Home. To optimize disk space usage, perform a cleanup either on a specific diagnostics file or an entire directory containing multiple diagnostics files.

**Cleanup procedure:**

1. List diagnostics files: List the diagnostics files in the system, including their corresponding IDs. This step provides an overview of the available diagnostic files.
2. Delete specific diagnostic files: Delete specific diagnostics files based on their IDs. This targeted cleanup helps efficiently manage disk space and ensures the removal of unnecessary diagnostic data.

Note: The diagnostics files are essential for troubleshooting purposes. Delete these files only if you are certain they have been successfully uploaded to WEKA Home and are no longer needed. For further clarification, contact the [Customer Success Team](../../getting-support-for-your-weka-system#contact-customer-success-team).

### List diagnostics files

**Command:** `weka diags list`

Use the following command to list the collected diagnostics files:

`weka diags list [--verbose] [<id>]...`

**Parameters**

 | Name | Description |
 | --- | --- |
 | id | The diagnostics file's ID or the path to the diagnostics file. If not specified, a list of all collected diagnostics files is displayed. |
 | verbose | Displays the results of all the diagnostics files, including the successful ones. |

### Delete specific diagnostic files

**Command:** `weka diags rm`

Use the following command to stop a running diagnostics instance, cancel its upload, and delete it from the disk:

`weka diags rm [--all] [<id>]...`

**Parameters**

 | Name | Description |
 | --- | --- |
 | all | A flag to delete all the diagnostics files. |
 | id* | The diagnostics file's ID or the path to the diagnostics files. If not specified, a list of all collected diagnostics files is displayed. This string is required unless the all option is specified. |

## Local server diagnostics command

Collecting diagnostics data from a connected local server is valuable in various scenarios, such as:

* Lack of a functional management process in the originating backend container or the specified backend containers.
* Absence of connectivity between the management process and the cluster leader.
* The cluster lacking a leader.
* The local container is offline.
* The server cannot establish communication with the leader or encountering a failure when attempting the `weka diags` command.

**Command:** `weka local diags`

Use the following command to to collect diagnostics from a connected local server:

`weka local diags [--id id] [--output-dir output-dir] [--core-dump-limit core-dump-limit] [--collect-cluster-info] [--tar]`

**Parameters**

 | Name | Description | Default |
 | --- | --- | --- |
 | id | A unique identifier for this diagnostics file. | Auto-generated |
 | output-dir | The directory for saving the diagnostics file. | /opt/weka/diags |
 | core-dump-limit | Limit the diagnostics collection process to use the specified core number. | 1 |
 | collect-cluster-info | Collect diagnostics data related to the cluster. To prevent excessive load on the cluster, use this flag for one server at a time. |  |
 | tar | Package the collected diagnostics data in a TAR file. | No TAR file is created |

<!-- ============================================ -->
<!-- File 220/259: support_diagnostics-management_traces-management.md -->
<!-- ============================================ -->

---
description: This page describes how to manage traces generated by the WEKA processes.
---

# Traces management

Traces are low-level events generated by WEKA processes and collected by an internal tracking tool. The traces are used as troubleshooting information for support purposes.

The tracking tool collects the traces continuously on the backends and clients. The number of traces retained depends on the specified free capacity on the backends and clients and the maximum capacity that traces can use. The tracking tool manages the retention of the traces as a FIFO queue.

If it is required to keep traces of a specific period for later investigation, you can freeze that period, and the traces within it are retained for a specified duration, enabling the support enough time to resolve the issue.

You can switch the verbosity level of the traces between low and high. The verbosity level affects the amount of information in the tracing data.

Note: Do not disable the traces without specific instructions from the [Customer Success Team](../../getting-support-for-your-weka-system#contact-customer-success-team). Disabling the traces reduces the amount of troubleshooting information about the system and may affect the SLA if an issue occurs.

**Related topics**

<!-- ============================================ -->
<!-- File 221/259: support_diagnostics-management_traces-management_manage-traces-using-the-cli.md -->
<!-- ============================================ -->

# Manage traces using the CLI

Manage trace settings using the CLI commands:

* Initiate trace collection
* Stop trace collection
* View traces configuration status
* Modify traces retention settings
* Adjust traces verbosity level to high or low
* Manage the traces freeze period

## Initiate trace collection

**Command:** `weka debug traces start`

## Stop trace collection

**Command:** `weka debug traces stop`

## View traces configuration status

**Command:** `weka debug traces status`

## Modify traces retention settings

**Command:** `weka debug traces retention set  [--server-max server-max] [--client-max client-max] [--server-ensure-free server-ensure-free] [--client-ensure-free client-ensure-free]`

**Parameters**

 | Parameter | Description | Default |
 | --- | --- | --- |
 | server-max | Maximum capacity to retain per server. | 50 GB per IO-node, with a minimum of 100 GB for all IO-nodes. |
 | client-max | Maximum capacity to retain per client. | 50 GB per IO-node, with a minimum of 100 GB for all IO-nodes. |
 | server-ensure-free | Always maintain at least this much capacity to remain free on servers. | 3 GB |
 | client-ensure-free | Always maintain at least this much capacity to remain free on clients. | 3 GB |

Note: To modify the trace retention setting of a single client, you can use the `traces_capacity_mb` mount command. See #additional-mount-options-using-the-stateless-clients-feature.

### Restore default traces retention values

**Command:** `weka debug traces retention restore-default`

## Adjust traces verbosity level

**Command:** `weka debug traces level set <level>`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | level* | Verbosity level.Format: low or high |

## Manage the traces freeze period

### View the frozen trace period

**Command:** `weka debug traces freeze show`

### Set the freeze period

**Command:** `weka debug traces freeze set <comment> [--start-time start-time] [--end-time end-time] [--retention retention]`

**Parameters**

 | Parameter | Description |
 | --- | --- |
 | comment * | A descriptive note providing context for easier tracking and review of debug traces. |
 | start-time | The start time of the frozen period.Format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00 |
 | end-time | The end time of the frozen period.Format: 5m, -5m, -1d, -1w, 1:00, 01:00, 18:30, 18:30:07, 2018-12-31 10:00, 2018/12/31 10:00, 2018-12-31T10:00, 2019-Nov-17 11:11:00.309, 9:15Z, 10:00+2:00 |
 | retention | The time to retain the traces.Format: 3s, 2h, 4m, 1d, 1d5h, 1w, infinite/unlimited |

### Reset the traces freeze period and delete the existing frozen traces

**Command:** `weka debug traces freeze reset`

<!-- ============================================ -->
<!-- File 222/259: support_diagnostics-management_traces-management_manage-traces-using-the-gui.md -->
<!-- ============================================ -->

# Manage traces using the GUI

Using the GUI, you can:

* Configure traces
* Freeze traces
* Change traces verbosity level
* Restore traces default settings

## Configure traces <a href="#configure-traces" id="configure-traces"></a>

The tracking tool collects the traces on the backends and clients and retains them on their disks. You can limit the capacity used by the traces by ensuring minimum free capacity and by setting the maximum capacity that traces can use.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Support**.
3. On the Traces section, select **Configure traces**.
4. On the Configure Traces dialog set the following properties:
   * The minimum free capacity to preserve on the backends.
   * The minimum free capacity to preserve on the clients.
   * The maximum capacity traces can use on backends.
   * The maximum capacity traces can use on clients.
5. Select **Save**.

## Freeze traces <a href="#freeze-traces" id="freeze-traces"></a>

Sometimes you may need to investigate an issue that occurred during a certain period. You can retain the tracing data of that period using the freeze traces action.

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Support**.
3. On the Traces section, select **Freeze traces**.
4. On the Freeze Traces dialog set the following properties:
   * **Start:** The start date and time of the period to freeze (mandatory).
   * **End:** The end date and time of the period to freeze.
   * **Retention:** The time to retain the tracing data. After this time, the tracking tool may purge the tracing data according to its purging cycle.
   * **Override:** If a freezing period is already set, you can override it by setting the **Override** button to **On**.
5. Select **Save**.

6. To clear the freeze period, select **Reset traces freeze**. Then, in the confirmation message,\
   select **Yes**.

## Change traces verbosity level <a href="#change-traces-verbosity-level" id="change-traces-verbosity-level"></a>

The verbosity level determines the amount of information in the tracing data. Switching the verbosity level to high provides more troubleshooting details but may use more space on the disk.

**Procedure**

1. In the Traces section, depending on the current verbosity level (low or high), select **Change traces level to high** or **Change traces level to low**.

## Restore traces default settings <a href="#restore-traces-default-settings" id="restore-traces-default-settings"></a>

You can restore the traces configuration to its default settings as shown in the following image.

Note: The default maximum capacity per IO-node is 50 GB and the minimum for all IO-nodes is 100 GB. The minimum free capacity is 3.22 GB

**Procedure**

1. In the Traces section, select **Restore traces default settings**. Then, in the confirmation message, select **Yes**.

<!-- ============================================ -->
<!-- File 223/259: support_diagnostics-management_protocols-debug-level-management.md -->
<!-- ============================================ -->

---
description:
---

# Protocols debug level management

The cluster enables changing the log debug level (verbosity level) for each protocol container in the cluster. While investigating issues, you can increase the verbosity level.

Note: Do not change protocols debug level without specific instructions from the [Customer Success Team](../../getting-support-for-your-weka-system#contact-customer-success-team). Changing the protocols debug level reduces the amount of troubleshooting information about the system and may affect the SLA if an issue occurs.

Note: Once the container is restarted, the log verbosity level reverts to its default.

**Related topics**

<!-- ============================================ -->
<!-- File 224/259: support_diagnostics-management_protocols-debug-level-management_manage-protocols-debug-level-using-the-cli.md -->
<!-- ============================================ -->

# Manage protocols debug level using the CLI

Using the CLI, you can:

* Show S3 debug level
* Manage NFS debug level
* Set SMB debug level

## Show S3 debug level <a href="#show-s3-debug-level" id="show-s3-debug-level"></a>

**Command:** `weka s3 log-level get`

## Manage NFS debug level <a href="#manage-nfs-debug-level" id="manage-nfs-debug-level"></a>

| **Command:** `weka nfs debug-level show | set` |

**Command options:**

`show:` Shows debug level for the NFS servers.

`set:` Sets the debug level for the NFS servers. When you complete debugging, return the debug level to default (creates an event).

## Set SMB debug level <a href="#set-smb-debug-level" id="set-smb-debug-level"></a>

**Command:**  `weka smb cluster debug`

**Parameter:**

`level:` The debug level (format: 0..10).

<!-- ============================================ -->
<!-- File 225/259: support_diagnostics-management_protocols-debug-level-management_manage-protocols-debug-level-using-the-gui.md -->
<!-- ============================================ -->

# Manage protocols debug level using the GUI

The Protocols Debug Level section displays the debug level for the S3 and NFS protocols only (the SMB debug level is not shown). You can change the debug level only for the configured protocols.

Using the GUI, you can:

* Update S3 debug level
* Update NFS debug level
* Update SMB debug level

Note: Once the server is restarted, the log verbosity level reverts to its default.

## Update S3 debug level <a href="#update-s3-debug-level" id="update-s3-debug-level"></a>

If the S3 protocol is configured, you can change the debug level for all servers or specified servers.

The available debug levels are:

* 0 - CRITICAL
* 1 - ERROR
* 2 - WARNING
* 3 - INFO
* 4 - DEBUG
* 5 - TRACE

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Support**.
3. On the Protocols Debug Level section, select **Change S3 debug level**.
4. On the Update S3 Debug Level dialog, set the following properties:
   * **Level:** Select the debug level.
   * **All servers:** If you want to apply the update on all the servers, switch to **On**. If you want to apply the update on specific servers, switch to **Off** and select the required servers.

## Update NFS debug level <a href="#update-nfs-debug-level" id="update-nfs-debug-level"></a>

If the NFS protocol is configured, you can change the debug level for all servers or specified servers.

The available debug levels are:

* 1 - EVENT
* 2 - INFO
* 3 - DEBUG
* 4 - MID DEBUG
* 5 - FULL DEBUG

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Support**.
3. On the Protocols Debug Level section, select **Change NFS debug level**.
4. On the Update NFS Debug Level dialog, set the following properties:
   * **Level:** Select the debug level.
   * **All servers:** If you want to apply the update on all the servers, switch to **On**. If you want to apply the update on specific servers, switch to **Off** and select the required servers.

## Update SMB debug level <a href="#update-smb-debug-level" id="update-smb-debug-level"></a>

If the SMB protocol is configured, you can change the debug level for all servers or specified servers.

The available debug levels are:

* 0 - NO DEBUG
* 5 - MID DEBUG
* 10 - FULL DEBUG

**Procedure**

1. From the menu, select **Configure > Cluster Settings**.
2. From the left pane, select **Support**.
3. On the Protocols Debug Level section, select **Change SMB debug level**.
4. On the Update SMB Debug Level dialog, set the following properties:
   * **Level:** Select the debug level.
   * **All servers:** If you want to apply the update on all the servers, switch to **On**. If you want to apply the update on specific servers, switch to **Off** and select the required servers.

<!-- ============================================ -->
<!-- File 226/259: aws-solutions.md -->
<!-- ============================================ -->

# AWS Solutions

## Topics in this section

### Amazon SageMaker HyperPod and WEKA Integrations

### AWS ParallelCluster and WEKA Integration

<!-- ============================================ -->
<!-- File 227/259: aws-solutions_aws-parallelcluster-and-weka-integration.md -->
<!-- ============================================ -->

# AWS ParallelCluster and WEKA Integration

## Overview

AWS ParallelCluster is an open-source tool for managing clusters, simplifying the deployment and administration of HPC clusters on AWS. By integrating with WEKA, organizations can create a high-performance data platform that significantly reduces epoch time from months to days without requiring additional infrastructure investment.

The infrastructure performance and efficiency gains made possible with WEKA for AWS ParallelCluster, organizations can accelerate their own pace of innovation, maximize their utilization of GPU-accelerated infrastructure, and control costs.

## Slurm based architecture with AWS ParallelCluster

The integration of WEKA with AWS ParallelCluster using Slurm comprises two primary components: a compute cluster and a standalone WEKA cluster. Refer to the numbered elements in the accompanying illustration for details:

The integration of WEKA with AWS ParallelCluster using Slurm consists of two main components: a compute cluster and a standalone WEKA cluster. Refer to the numbered elements in the accompanying illustration for details:

1. **WEKA cluster deployment**\
   The WEKA cluster backends are deployed within the same Virtual Private Cloud (VPC) and subnet as the AWS ParallelCluster cluster. These backends use the i3en instance family, which is optimized for high-performance storage and compute workloads.
2. **WEKA client integration**\
   The WEKA client software is installed across the AWS ParallelCluster components, including the controller node, login nodes, and worker nodes. This software facilitates seamless access to the WEKA cluster by presenting a mount point within the file system, enabling efficient data sharing and processing.
3. **Data management and tiering**\
   To optimize data handling, WEKA employs an Amazon S3 bucket for data tiering. This system ensures that data is automatically allocated to the appropriate storage tier based on access patterns and cost-efficiency considerations. Furthermore, WEKA leverages S3 for storing snapshots, providing an additional layer of data resilience and enabling robust disaster recovery.

## Deployment workflow for AWS ParallelCluster cluster

1. Deploy WEKA Cluster using Terraform.
2. Prepare for AWS ParallelCluster deployment.
3. Verify security group configuration.
4. Deploy AWS ParallelCluster cluster.

### Deploy WEKA Cluster using Terraform

### Prepare for AWS ParallelCluster deployment

**Step 1: AWS ParallelCluster CLI installation**

If you already have the AWS ParallelCluster CLI installed you can skip to the next step. Otherwise, follow the procedure Installing the AWS ParallelCluster command line interface (CLI) in AWS documentation.

**Step 2: Create S3 bucket to store AWS ParallelCluster integration scripts**

1. Create an S3 bucket in the same region AWS ParallelCluster will be deployed.

```
aws s3 mb s3://<bucket name> --region <region>
```

2. Verify bucket creation

```
| aws s3 ls | grep <bucket name> |
```

**Step 3:  Clone WEKA Cloud-Solutions repository and copy integrations scripts to S3**

1. Clone this repository and `cd` to the `aws/parallelcluster/` directory:

```
git clone https://github.com/weka/cloud-solutions.git
cd cloud-solutions/aws/parallelcluster
```

2. Upload integration scripts to S3

```
aws s3 cp ./scripts/weka-install.py s3://<bucket name>/scripts/weka-install.py
aws s3 cp ./scripts/virtualenv-setup.sh s3://<bucket name>/scripts/virtualenv-setup.sh
```

3. Create IAM policy for WEKA clients using provided template

```
aws iam create-policy --policy-name weka-client-pcluster --policy-document file://./iam/example-pcluster-policy.json
```

**Step 4: Modify AWS ParallelCluster template**

This repository includes an example cluster template file. Follow the steps below to modify the template for your AWS environment. If you already have a cluster template, combine it with the example one provided.

1. Create a copy of the example template file.

```
cp example-pcluster-template.yaml pcluster.yaml
```

2. Update Region.

```
sed -i '' -e 's/Region: us-east-2/Region: <region>/' pcluster.yaml
```

3. Update Networking settings.

```
sed -i '' -e 's/subnet-123456789abcdefg/<your subnet>/g' pcluster.yaml
sed -i '' -e' s/sg-123456789abcdefg/<your security group>/g' pcluster.yaml
```

4. Update SSH KeyName.

```
sed -i '' -e s/support_key/<your SSH KeyPair Name>/g' pcluster.yaml
```

5. Update S3 bucket name.

```
sed -i '' -e 's/MY-S3-BUCKET/<s3 bucket name>/g' pcluster.yaml
```

6. Update ALB DNS Name.

```
sed -i '' -e 's/internal-weka-lb-12345689.us-east-2.elb.amazonaws.com/<WEKA ALB DNS NAME>/g' pcluster.yaml
```

7. Update WEKA filesystem name (optional).

```
sed -i '' -e 's/--filesystem-name=default/--filesystem-name=<filesystem name>/g' pcluster.yaml
```

8. Update WEKA mount point (optional). (Escape the forward slash in the AWS ARN with a backslash.)

```
sed -i '' -e 's/--mount-point=\/mnt\/weka/--mount-point=<mount point>/g' pcluster.yaml
```

9. Update IAM Policy ARN. (Escape the forward slash in the AWS ARN with a backslash.)

```
sed -i '' -e 's/arn:aws:iam::123456789:policy\/weka-pcluster-client-policy/<IAM policy ARN>/g' pcluster.yaml
```

10. Update SlurmQueues.

While the example template shows two queues to demonstrate a common customer setup, you can configure as few as one queue. If you do implement multiple queues, update each queue's configuration.

Review and update the following parameters as necessary:

* **Name**
* **ComputeResources > Name**
* **InstanceType**
* **MinCount**
* **MaxCount**
* **CustomSlurmSettings > RealMemory**
  * This value is specific to the instance type. Refer to the table below for the correct value.
* **CustomSlurmSettings > CpuSpecList**
  * This value is specific to the instance type. Refer to the table below for the correct value.
* **OnNodeConfigured > Sequence > Script > Args > --cores**
  * This value is specific to the instance type. Refer to the table below for the correct value.

If the `--cores` argument is defined, the WEKA mount is created using a **DPDK mount** for optimal performance. If the argument is not defined, the WEKA mount defaults to **UDP mode**. **DPDK is preferred** for all instances to achieve higher storage performance. However, certain instances, such as HeadNodes, can use a UDP mount if necessary.

Additional instance types:\
If your instance type is not listed in the table below, contact the Customer Success Team for assistance.

 | Instance Type | CpuSpecList | RealMemory |
 | -------------- | ----------- | ---------- |
 | hpc7a.96xlarge | 95,191 | 742110 |

### Verify security group configurations

Ensure that the AWS ParallelCluster nodes can connect with the WEKA backends on port 14000 for both TCP and UDP. Review your security group settings to confirm that the WEKA clients can communicate with the WEKA backends effectively.

### Deploy AWS ParallelCluster cluster

1. Run create-cluster

```

```
pcluster create-cluster -c pcluster.yaml --cluster-name your-cluster-name --rollback-on-failure FALSE
```

```

2. To assist with debugging, disable `rollback-on-failure` if errors occur.

<!-- ============================================ -->
<!-- File 228/259: aws-solutions_amazon-sagemaker-hyperpod-and-weka-integrations.md -->
<!-- ============================================ -->

# Amazon SageMaker HyperPod and WEKA Integrations

## Overview

Amazon SageMaker HyperPod provides purpose-built infrastructure for large-scale model training. It supports distributed machine learning (ML), large language models (LLMs), and foundation models (FMs).

HyperPod simplifies GPU cluster creation and management. It integrates natively with Slurm and Amazon EKS for advanced orchestration. This enhances resilience and reduces training times for foundation models.

With WEKA support for Amazon SageMaker HyperPod, customers can use WEKA‚Äôs zero-copy, zero-tuning architecture to optimize performance across key workflows. These include data loading, model training, checkpointing, verification, tuning, and dataset archiving.

Using WEKA as the storage layer for SageMaker HyperPod improves GPU utilization and accelerates training workflows. This reduces the wall-clock time required to complete tasks, enabling faster and more efficient large-scale model training.

SageMaker HyperPod supports Slurm and Amazon EKS as orchestration engines. The following guide focuses on integrating SageMaker HyperPod with WEKA using Slurm as the orchestration engine.

## Slurm based architecture with SageMaker HyperPod

The integration of WEKA with SageMaker HyperPod using Slurm comprises two primary components: a compute cluster and a standalone WEKA cluster. Refer to the numbered elements in the accompanying illustration for details:

1. **WEKA cluster deployment**\
   The WEKA cluster backends are deployed within the same Virtual Private Cloud (VPC) and subnet as the SageMaker HyperPod cluster. These backends use the i3en instance family, which is optimized for high-performance storage and compute workloads.
2. **WEKA client integration**\
   The WEKA client software is installed across the SageMaker HyperPod components, including the controller node, login nodes, and worker nodes. This software facilitates seamless access to the WEKA cluster by presenting a mount point within the file system, enabling efficient data sharing and processing.
3. **Data management and tiering**\
   To optimize data handling, WEKA employs an Amazon S3 bucket for data tiering. This system ensures that data is automatically allocated to the appropriate storage tier based on access patterns and cost-efficiency considerations. Furthermore, WEKA leverages S3 for storing snapshots, providing an additional layer of data resilience and enabling robust disaster recovery.

<!-- ============================================ -->
<!-- File 229/259: aws-solutions_amazon-sagemaker-hyperpod-and-weka-integrations_deploy-a-new-amazon-sagemaker-hyperpod-cluster-with-weka.md -->
<!-- ============================================ -->

# Deploy a new Amazon SageMaker HyperPod cluster with WEKA

## Deployment workflow for new Amazon SageMaker Hyperpod cluster

1. Prepare the environment for deployment.
2. Deploy WEKA cluster using Terraform.
3. Create Amazon SageMaker HyperPod cluster.

### Prepare the environment for deployment

1. Deploy AWS CloudFormation template (or an equivalent) to create the prerequisites for the Amazon SageMaker HyperPod cluster.
   1. TheAWS CloudFormation template can be found at: Amazon SageMaker HyperPod > 0. Prerequisites > 2. Own Account.
   2. Ensure the optional parameter **"Availability zone ID to deploy the backup private subnet"** is configured with a valid entry. If the AWS CloudFormation template has already been deployed, update the existing stack using the existing template.
2. Retrieve the token required for the WEKA package installation by accessing the WEKA download command at: https://get.weka.io/.
3.  Edit the **sagemaker-hyperpod-SecurityGroup** rule created by the AWS CloudFormation template. Add the following inbound rules to allow access from your management workstation's CIDR range:

    * **TCP port 22** (SSH)
    * **TCP port 14000** (WEKA UI)

    This ensures that your management workstation can connect securely to the cluster.

### Deploy WEKA cluster using Terraform

### Create Amazon SageMaker HyperPod cluster

1.  **Clone the WEKA cloud solutions repository**\
    Download the repository from GitHub:

    ```bash
    git clone https://github.com/weka/cloud-solutions/
    ```
2.  **Navigate to the SageMaker HyperPod directory**\
    Change to the relevant directory:

    ```bash
    cd cloud-solutions/aws/sagemaker-hyperpod
    ```
3. **Verify AWS cli region configuration**

```
aws configure list
```

Verify the region listed is the desired region for the SageMaker Hyperpod cluster. If it is not correct, set the AWS_REGION environment variable to the correct region.

```
export AWS_REGION=<desired region>
```

4. **Set Cluster Configuration**\
   Run the script to set environment variables that defines the SageMaker Hyperpod cluster.

```
./set_env_vars.sh <Cloud_Formation_Stack>
```

* `Cloud_Formation_Stack`: Name of the existing CloudFormation stack.

4. **Source environment variables**

```
source env_vars
```

5. **Create the cluster**\
   Run the deploy script:

```bash
./deploy.sh <ALB_NAME> <WEKA_FS_NAME>
```

* `ALB_NAME`: Obtain this from the AWS Console or Terraform output. It is a DNS name.
* `WEKA_FS_NAME`: Obtain this from the WEKA UI. The default filesystem name is default.

6. **Monitor cluster creation**\
   Track the cluster creation process:

```bash
aws sagemaker list-clusters --output table
```

7. **Continue setup**\
   Proceed with the setup by following Section 1, Step E of the Amazon SageMaker HyperPod workshop.\
   The WEKA filesystem is mounted at `/mnt/weka` on all SageMaker HyperPod nodes.

    `<access key>@get.weka.io`

<!-- ============================================ -->
<!-- File 230/259: aws-solutions_amazon-sagemaker-hyperpod-and-weka-integrations_add-weka-to-an-existing-amazon-sagemaker-hyperpod-cluster.md -->
<!-- ============================================ -->

# Add WEKA to an existing Amazon SageMaker HyperPod cluster

## Deployment workflow for an existing Amazon SageMaker Hyperpod cluster

1. Deploy WEKA Cluster using Terraform.
2. Deploy WEKA clients in SageMaker Hyperpod.

### Deploy WEKA Cluster using Terraform

### Deploy WEKA clients in Amazon SageMaker Hyperpod

#### Step 1: Download integration scripts from GitHub

1. Clone the GitHub repository:

```
git clone https://github.com/weka/cloud-solutions.git
```

2. Enter the sagemaker-hyperpod directory:

```
cd cloud-solutions/aws/sagemaker-hyperpod/
```

#### Step 2: Verify region configuration

1. Verify AWS CLI region configuration:

```
aws configure list
```

Verify the region listed is the desired region for the SageMaker Hyperpod cluster. If it is not correct set the AWS_REGION environment variable to the correct region.

```
export AWS_REGION=<desired region>
```

#### Step 3: Verifying VPC configuration

1. Ensure the optional parameter **"Availability zone ID to deploy the backup private subnet"** is configured with a valid entry.  If the CloudFormation template has already been deployed, update the existing stack using the existing template.
2. Edit the **sagemaker-hyperpod-SecurityGroup** rule created by the CloudFormation template. Add the following inbound rules to allow access from your management workstation's CIDR range:

* **TCP port 22** (SSH)
* **TCP port 14000** (WEKA UI)

This ensures that your management workstation can connect securely to the cluster.

#### Step 3: Configure environment variables

1. Run `set_env_vars.sh`:

```
./set_env_vars.sh <stack_name> && source env_vars
```

* `Cloud_Formation_Stack`: Name of the existing CloudFormation stack.

#### Step 4: Deploy WEKA clients to existing cluster

1. Run `deploy_weka_into_existing_cluster.sh` replacing `<weka_backend_ip>` with either a WEKA backend IP or Application Load Balancer DNS name and `<FS Name>` with the name of the WEKA filesystem you wish to mount.

```
./deploy_weka_into_existing_cluster.sh <ALB_NAME> <FS name>
```

* `ALB_NAME`: Obtain this from the AWS Console or Terraform output. It is a DNS name.
* `WEKA_FS_NAME`: Obtain this from the WEKA UI. The default filesystem name is default.

#### Step 5: Verify WEKA clients are mounted

1. Login to one of the cluster nodes using SSH or SSM.
2. Verify mount using `df:`

```
df -h
```

The WEKA filesystem is mounted at `/mnt/weka` on all SageMaker HyperPod nodes.

<!-- ============================================ -->
<!-- File 231/259: azure-solutions.md -->
<!-- ============================================ -->

# Azure Solutions

## Topics in this section

### Azure CycleCloud for SLURM and WEKA Integration

Learn to integrate Azure CycleCloud with the WEKA Data Platform and SLURM scheduler to streamline HPC cluster management and enable high-performance, scalable data solutions for AI, ML, and analytics.

<!-- ============================================ -->
<!-- File 232/259: azure-solutions_azure-cyclecloud-for-slurm-and-weka-integration.md -->
<!-- ============================================ -->

---
description:
---

# Azure CycleCloud for SLURM and WEKA Integration

## Introduction

The integration of **Azure CycleCloud** with the **WEKA Data Platform** delivers a robust, high-performance solution tailored for data-intensive workloads in High-Performance Computing (HPC) environments.

Azure CycleCloud simplifies the orchestration and management of HPC clusters on Azure, providing features such as dynamic autoscaling and streamlined configuration management for complex deployments. Paired with WEKA, users benefit from a high-performance, scalable file system designed to handle low-latency, high-throughput workloads, making it ideal for applications in AI, analytics, machine learning, and other HPC domains.

This document provides a step-by-step guide to integrating WEKA with your CycleCloud environment using the **SLURM scheduler**, enabling seamless data access and management for HPC workloads.

### What is Azure CycleCloud?

Azure CycleCloud is a comprehensive solution for orchestrating and managing **High-Performance Computing (HPC)** environments in Azure. It enables users to:

* **Provision infrastructure**: Quickly set up the compute and storage resources required for HPC workloads.
* **Deploy familiar HPC schedulers**: Integrate with widely used schedulers like SLURM, Grid Engine, or HPC Pack.
* **Scale efficiently**: Automatically scale infrastructure to handle jobs of varying sizes, optimizing resource utilization and cost.
* **Simplify file system integration**: Create and mount different types of file systems onto compute cluster nodes to support demanding HPC applications.

CycleCloud also enhances HPC environments by deploying **autoscaling plugins** on supported schedulers. This eliminates the need for users to develop and manage complex autoscaling logic, allowing them to focus on scheduler-level configurations they already know.

For more details, refer to the official Azure CycleCloud documentation: Azure CycleCloud Overview.

### What is SLURM?

SLURM (Simple Linux Utility for Resource Management) is a widely adopted open-source workload manager designed for High-Performance Computing (HPC), Artificial Intelligence (AI), and cloud computing environments. It enables users to efficiently run large-scale parallel and distributed applications across clusters of compute nodes.

Key features of SLURM include:

* **Job scheduling**: Manages and prioritizes job execution based on resource availability and user-defined policies.
* **Resource management**: Allocates and tracks compute resources such as CPUs, GPUs, and memory.
* **Fault tolerance**: Supports mechanisms for recovering jobs and managing failures.
* **Power management**: Optimizes energy use by powering nodes up or down based on workload demands.

SLURM is trusted by many of the world‚Äôs top supercomputers, research institutes, universities, and enterprises due to its scalability and flexibility.

In the context of **Azure CycleCloud** and **WEKA**, SLURM is currently the only scheduler supported for integration. However, support for additional schedulers is planned for future releases.

### Solution overview

This architecture demonstrates how **Azure CycleCloud** integrates with the **WEKA Data Platform** and **SLURM** to deliver a scalable, high-performance solution for High-Performance Computing (HPC) and High-Throughput Computing (HTC) workloads. The process includes four key steps:

1. **Job submission**: Users submit HPC jobs through the SLURM scheduler, specifying the number of Azure Virtual Machines (VMs) to deploy. Azure CycleCloud provisions the required compute nodes using Virtual Machine Scale Sets (VMSS), ensuring resources match workload demands.
2. **Automatic WEKA mounting**: During the initialization of the VMs, a **cluster-init module** automatically mounts each compute node to the WEKA storage cluster, enabling seamless access to high-performance storage.
3. **Job execution**: Jobs are executed using the WEKA Data Platform, which provides a unified, high-speed storage layer. The platform combines NVMe performance with Azure Blob Storage scalability and cost efficiency, ensuring optimal performance for HPC/HTC workloads.
4. **Data persistence**: After job completion, data remains stored on the WEKA platform. This ensures continuity, allowing users to retain data for future analysis, migrate it to Azure Blob Storage for long-term archiving, or redeploy nodes for further computation.

By combining CycleCloud‚Äôs dynamic compute provisioning and scaling with WEKA‚Äôs advanced storage capabilities, this solution offers a robust and efficient framework for HPC and HTC applications.

## Prerequisites

Before proceeding, ensure that both **Azure CycleCloud** and **WEKA** are installed and configured in your Azure environment. This document focuses on integrating these two solutions.

If either solution is not yet installed, refer to the following resources to complete the installation:

* **Azure CycleCloud**: Azure CycleCloud Installation Guide
* **WEKA on Azure**: [WEKA Installation on Azure](../planning-and-installation/weka-installation-on-azure)

Once both solutions are installed, you can proceed with the integration workflow.

## Workflow: Integrate Azure CycleCloud with WEKA

The integration of **Azure CycleCloud** with the **WEKA Data Platform** involves four key steps:

1. **Download the Azure CycleCloud/WEKA integration template** >>>\
   Obtain the pre-built template that simplifies the integration process.
2. **Configure network parameters for DPDK** >>>\
   Adjust the network settings on the Azure CycleCloud nodes to enable Data Plane Development Kit (DPDK), ensuring optimal data transfer performance.
3. **Deploy the cluster initialization module** >>>\
   Create and deploy the cluster-init module on the Azure CycleCloud nodes to automatically configure WEKA integration.
4. **Set up the WEKA blade** >>>\
   Configure the WEKA blade using the CycleCloud/WEKA template installed in step 1 to finalize the integration.
5. **Test the integration** >>>\
   Verify the integration of Azure CycleCloud, SLURM, and the WEKA Data Platform.

### Step 1: Download the Azure CycleCloud/WEKA integration template

Create and deploy the cluster-init module on the Azure CycleCloud nodes to automatically configure WEKA integration.

**Procedure**

1.  **Log in to the Azure CycleCloud Virtual Machine (VM)**\
    Access the VM where Azure CycleCloud is installed.\

2.  **Retrieve the official template**\
    Browse to https://github.com/themorey/cyclecloud-weka and copy the URL to the clipboard.\

3. Clone the CycleCloud/WEKA integration template repository from GitHub to your CycleCloud instance using the previously copied URL:

```bash
git clone https://github.com/themorey/cyclecloud-weka.git
```

3. **Import the template**\
   Navigate to the cloned repository directory and import the `slurm-weka` template into Azure CycleCloud:

```bash
cyclecloud import_template -f /home/weka/cyclecloud-weka/templates/slurm-weka.txt
```

4.  **Verify the template in the CycleCloud GUI**\
    Once the template is successfully imported, it appears in the CycleCloud GUI under the templates section.\

5.  **Review the template configuration**\
    Click on the newly imported template. It includes a section labeled **Weka Cluster Info.** You will  configure this section in a later step of this guide.\

    \

### Step 2: **Configure network parameters for DPDK**

The WEKA data platform leverages the Data Plane Development Kit (DPDK) to achieve high performance and low latency across all hosts. By using DPDK, WEKA's filesystem (WekaFS) bypasses the host kernel's traditional networking stack. This enables direct communication with the Network Interface Card (NIC) in user space, reducing latency by eliminating context switches and data copying. The result is significantly improved throughput and efficiency.

To fully use DPDK, each host requires two NICs. These dual NICs allow load balancing and facilitate the segregation of network traffic types, such as data traffic and management traffic, ensuring optimal performance.

For step-by-step guidance on enabling DPDK and configuring dual NICs for high-performance scenarios, refer to  topic.

**Procedure**

1.  **Log in to the Azure CycleCloud VM**\
    Access the VM where Azure CycleCloud is installed. \

2. **Open the CycleCloud/WEKA template**\
   Navigate to the template downloaded in **Step 1**, and open it using a text editor.
3.  **Modify the template to support dual NICs**\
    Locate the section labeled `[[nodearraybase]]` and add the following configuration for dual network interfaces:

    ```javascript
    [[[network-interface eth0]]]
    AssociatePublicIpAddress = $ExecuteNodesPublic
    SubnetId = $SubnetId
    AcceleratedNetworking = true

    [[[network-interface eth1]]]
    SubnetId = $SubnetId
    ```

Note: * You can apply these network parameters to individual nodes (for example, HPC, HTC, dynamic) or add them to the `[[nodearraybase]]` configuration.
* If other nodes reference `[[nodearraybase]]` using `Extends = nodearraybase`, they inherit this configuration automatically.

4. **Save and Apply the Changes**\
   Save the modified template and ensure it is uploaded to your CycleCloud instance.

After completing these steps, your CycleCloud nodes is provisioned with two NICs, enabling DPDK to optimize performance for the WEKA Data Platform.

### Step 3: **Deploy the cluster initialization module**

The cluster initialization module ensures that each node in the Azure CycleCloud environment is configured to integrate seamlessly with the WEKA Data Platform.

**Procedure**

**1. Create the cluster Initialization script**

Copy the following shell script and save it to the `scripts` directory on your Azure CycleCloud VM:

```

```bash
#!/bin/bash
set -ex

# Add specific commands here to configure each node for WEKA
# Example: mounting storage, installing dependencies, or setting environment variables

weka@cyclecloud-vm://home/weka/cyclecloud-weka/specs/htc/cluster-init/scripts$ ls
001-htc-cluster-init.sh  README.txt
```

```

Note: Depending on your deployment, you may create separate CycleCloud specifications for each Node Array (for example, HPC and HTC nodes) and provide a distinct cloud-init script for each array.

**2. Add the script to the cluster configuration**

1. **Access the cluster configuration in the CycleCloud GUI**
   1. Log in to the CycleCloud GUI.
   2. Navigate to the cluster configuration you wish to edit.
   3.  Click **Edit** to modify the settings.\

2.  **Attach the cluster initialization script**

    1.  In the **Advanced Settings** section, scroll to the **Cluster Init** section near the bottom of the page.\

    2. Click **Browse** and navigate to the saved script location on the CycleCloud VM.
    3.  Select the script to apply it to the desired node array.\

    **Example configuration:** You can deploy the same script for multiple node arrays (for example, both HTC and HPC nodes) or assign unique scripts to different arrays.\

3. **Save changes**
   * Click **Save** and exit the **Edit Configuration** panel.

After completing these steps, the cluster initialization module is deployed to your CycleCloud nodes, ensuring they are properly configured during startup.

### Step 4: Configure the WEKA blade on the CycleCloud/WEKA template

The **cluster-init script** used in the previous step requires specific configuration parameters, including the IP addresses of the WEKA storage platform, the mount point for the nodes, and the WEKA filesystem name.

**Procedure**

1. **Retrieve WEKA configuration details**
   1.  **Log into the WEKA GUI**: Navigate to the **Cluster Servers** section and note the IP addresses of the WEKA backend servers.\

   2.  **Select or create a filesystem**:

       1. In the WEKA GUI, go to the **Filesystems** section.
       2. Identify the filesystem you want to mount to the CycleCloud VMs. You can select an existing filesystem or create a new one for this purpose.

2. **Populate the WEKA Blade in CycleCloud**
   1.  **Open the WEKA Blade Configuration**: In the **CycleCloud GUI**, click **Edit** on the cluster configuration. Navigate to the **WEKA Cluster Info** section.\

   2.  **Fill in the required parameters**:

       * **WEKA addresses**: Enter the IP addresses of the WEKA backend servers from **Step 1**. Separate multiple IP addresses with commas.
       * **Mount point**: Specify the desired mount point for the nodes.
       * **WEKA filesystem**: Enter the name of the selected WEKA filesystem from **Step 1**.

3. **Save the configuration**
   1. Click **Save** to apply the changes.
   2. Exit the **Edit Configuration** panel and return to the CycleCloud GUI.

Your CycleCloud nodes are now configured to automatically connect to the specified WEKA filesystem during initialization. This completes the integration process.

### Step 5: Test the integration

To validate the integration, run a SLURM job across multiple nodes and confirm that each node connects directly to the WEKA Data Platform through the specified mount point.

**Procedure**

1. **Run a SLURM job**:
   1.  Log into the **Scheduler VM**.\

   2.  Submit a SLURM job. For example, run a batch HTC job using 3 nodes:

       ```bash
       batch <job-script>.sh
       ```

   3.  Verify that **3 HTC nodes** are activated in CycleCloud.\

2.  **Monitor cluster initialization (optional)**:

    1. Log into one of the HTC nodes.
    2.  Navigate to the cluster-init logs:

        ```bash
        cd /opt/cycle/jetpack/logs/cluster-init/<weka-template>/<spec>/scripts
        ```
    3.  Use `tail` to monitor the script's progress and confirm mounting to WEKA:

        ```bash
        tail -f <script-name>
        ```

3.  **Verify on the WEKA GUI**:

    1. Access the **WEKA GUI**.
    2. Navigate to the **Clients** section to verify that all nodes are connected and mounted to the WEKA Data Platform.
    3. Ensure all nodes display a green status, indicating successful connectivity.

4. Once the nodes are mounted and operational, the integration is confirmed, and you can proceed with HPC analysis.

<!-- ============================================ -->
<!-- File 233/259: best-practice-guides.md -->
<!-- ============================================ -->

# Best Practice Guides

## Topics in this section

### WEKA and Slurm integration

Explore the architecture and configuration of an HPC cluster using the Slurm workload manager for job scheduling and WEKA as the high-performance data platform.

### Storage expansion best practice

Explore the best practices for expanding WEKA clusters, focusing on maintaining optimal performance in heterogeneous environments.

<!-- ============================================ -->
<!-- File 234/259: best-practice-guides_storage-expansion-best-practice.md -->
<!-- ============================================ -->

---
description:
---

# Storage expansion best practice

## **Heterogeneous systems**

For new installations, it is recommended to create a fully homogeneous cluster with identical failure domains and NVMe drives. Considerations for heterogeneity arise during expansion, serving purposes such as adding flash capacity, improving performance, or facilitating a tech refresh.

### **Expansion guidelines**

WEKA's primary guidance for expansions is to use similar server types and identical drive capacities, ensuring equal or greater capability than the existing cluster regarding CPU speed, core count, RAM, and NIC throughput. NVMe drives in the expansion should ideally match the capacity and performance of the existing cluster.

### Drive capacity considerations

Under specific considerations, exploring various drive capacities is a viable option; however, it's essential to be mindful of potential underutilization risks.

The configuration must align with optimizing performance and maximizing capacity usage. This approach allows flexibility in adapting to the availability of current NVMe capacities while ensuring optimal performance and capacity usage.

## **Example scenario**

In a scenario where a 10-server WEKA cluster has 10x 7.68TB NVMe drives per failure domain, a 50% capacity expansion is considered. The preferred option is to use the same capacity per failure domain and drive capacity as the main cluster. If 7.68TB capacities are unavailable, the next best option is to use a higher-capacity drive, usually twice the original capacity.

### **New failure domains**

When adding new failure domains during expansion, matching the total capacity of old failure domains is essential. The preference is for the expansion cluster to use the same NVMe drive capacity and drive count as the existing cluster to avoid stranded capacity.

### **Performance considerations**

In a heterogeneous cluster, ensuring the new failure domain has the same total performance as the old failure domain. New drives must be capable of twice the throughput of older drives to prevent performance bottlenecks.

## **Summary**

* Primary cluster installs require identical server and NVMe drive types.
* Expansion clusters must have servers equal to or better than those in the existing cluster.
* New failure domains in expansions must match old failure domains' total capacity and performance.
* In specific circumstances, different drive capacities require approval and careful consideration.
* For performance in heterogeneous clusters, ensure new drives can handle twice the throughput of older drives.

Note: Contact the Product Management team (pm@weka.io) for approval and guidance in such heterogeneous scenarios.

<!-- ============================================ -->
<!-- File 235/259: best-practice-guides_weka-and-slurm-integration.md -->
<!-- ============================================ -->

---
description:
---

# WEKA and Slurm integration

## Overview <a href="#overview" id="overview"></a>

Traditional high-performance computing (HPC) clusters consist of login nodes, controllers, compute nodes, and file servers.

* The login nodes are the primary access point for users to access the cluster.
* Controllers host the job scheduler or workload manager for the cluster.
* Compute nodes are used for the primary execution of user jobs.
* File servers typically host home, group, and scratch directories to ensure user files are accessible across the cluster‚Äôs login and compute nodes.

For customers using the WEKA Data Platform, a high-performance HPC solution that optimizes IO regardless of the data profile, managing network-attached storage with WEKA becomes practical. This simplifies filesystem management and ensures consistent performance regardless of the location where HPC applications run.

In this integration guide, explore the architecture and configuration of an HPC cluster using the Slurm workload manager for job scheduling and WEKA as the high-performance data platform. WEKA supports multi-protocol IO, enabling simultaneous data access through POSIX, NFS, SMB, S3, GPUDirect Storage, and Kubernetes CSI.

The guide begins by exploring two architecture designs for deploying WEKA with Slurm and introduces two mount-type options adaptable to either architecture. Subsequently, it delves into the resource requirements for WEKA, guiding the configuration of Slurm to isolate specialized cores and memory from user applications and reserve them for WEKA usage.

Note: This integration guide is intended for system administrators and engineers familiar with setting up, configuring, and managing an HPC cluster equipped with the Slurm workload manager and job scheduler on bare-metal or cloud-native systems.

## Architecture <a href="#architecture" id="architecture"></a>

### WEKA <a href="#weka" id="weka"></a>

The servers in a WEKA system are members of a cluster. A server includes multiple containers running software instances called processes that communicate with each other to provide storage services in the cluster.

The processes are dedicated to managing different functions as follows:

* Drive processes for SSD drives and IO to drives.
* Compute processes for filesystems, cluster-level functions, and IO from clients.
* Frontend processes for POSIX client access and sending IO to the compute and drive processes.
* A management process for managing the overall cluster.

Note: For more details, see .

WEKA can be configured to run with dedicated backend servers and independent clients (Figure 1) or in a converged cluster, where each participating server acts as both client and backend server (Figure 2).

#### Dedicated backend configuration

In a dedicated backend configuration, user applications run on the WEKA clients, which interact with the operating system‚Äôs Virtual File System (VFS) layer.

The VFS uses the WEKA POSIX driver to issue requests to the WEKA client on the host. The client communicates with the WEKA backend servers. The WEKA server Frontend, Compute, and Drive processes move data between the servers and clients in parallel.

#### Converged configuration

In a converged configuration, each server participating in the cluster hosts user applications alongside the frontend (POSIX + NAS; client and server), drive, compute, and management processes.

WEKA processes are allocated to designated cores on each server in the WEKA cluster through control groups. This demands careful consideration to guarantee sufficient CPUs and memory for both WEKA and user applications. For an in-depth understanding of WEKA architecture, see the WEKA Architecture Technical Brief.

#### WEKA client mount modes

WEKA clients can be configured to mount in DPDK or UDP mode.

**DPDK mode** is optimized for single-process performance and must be used when possible. When using DPDK mode, specific requirements must be met by the client host system.

The Frontend process on clients uses CPU cores and memory while the mount is active. This implies that sufficient compute cores and memory resources must be available to run the WEKA Frontend process and other user applications. Additionally, NIC hardware must have a Poll Mode Driver (PMD) and be supported by WEKA. See [Prerequisites and compatibility](../../planning-and-installation/prerequisites-and-compatibility#networking-ethernet) for more information on supported NIC hardware for bare-metal and cloud-native systems.

**UDP mode** is an option for limited-throughput WekaFS filesystem access when DPDK mode is not feasible due to network, hardware, or operating system limitations. It can serve as an alternative when necessary.

In UDP mode, the Frontend process on clients uses CPUs and memory only during IO activity. This proves advantageous for compute or memory-bound applications with sporadic file read or write operations, as seen in computational fluid dynamics.

For IO-bound applications like those in bioinformatics and AI/ML, WEKA recommends employing the DPDK mount mode.

### Slurm <a href="#slurm" id="slurm"></a>

Slurm is a robust open-source cluster management and job scheduling system tailored for Linux clusters of all sizes. Slurm delegates access to resources, provides a framework for executing and monitoring computational workloads, and manages a queue of pending work submitted by system users. Slurm manages these responsibilities through three daemons:

* Slurm controller daemon (slurmctld)
* Slurm database daemon (slurmdbd)
* Slurmd daemon (slurmd)

Typically, a Slurm cluster consists of one or more controller hosts that run the Slurm controller and Slurm database daemons. These are a set of compute nodes where users run their workloads and one or more login nodes used to access the cluster.

#### **Slurm cluster operation**

The Slurm controller daemon (`slurmctld`) is a centralized workload manager that monitors available resources and active workloads. Based on resource availability and user requests, the slurmctld node determines where to run user workloads.

The Slurm database daemon (`slurmdbd`) is optional but a recommended service to deploy in Slurm clusters. The Slurm database is used to store job history, which can provide visibility to cluster usage and can be helpful in debugging issues with user workloads or cluster resources. The `slurmctld` and `slurmdbd` services are often deployed on the controller host.

The `slurmd` service runs on compute nodes, where user workloads are executed. It functions similarly to a remote shell, receiving work requests from the `slurmctld`, performing the tasks, and reporting back the task status.

Users typically access HPC clusters through one or more login nodes. The login nodes allow users to access shared files across the compute nodes and schedule workloads using Slurm command-line tools such as sbatch, salloc, and srun. These tools are often sufficient for lightweight text editing and code compilation and are sometimes used to transfer data between local workstations and the HPC cluster.

Note: In some systems, like large academic research-oriented HPC data centers, dedicated ‚Äúdata mover‚Äù nodes are often available specifically for file transfers.

#### Slurm leverages control groups

Integral to Slurm's resource management and job handling is its ability to leverage control groups (cgroups) through the proctrack/cgroup plugin. Provided by the Linux kernel, cgroups allow nodes to be organized hierarchically and enable the distribution of system resources in a controlled fashion along this hierarchy.

Slurm leverages cgroups to manage and constrain resources for jobs, job steps, and tasks. For instance, Slurm ensures that a job only uses the CPU or memory resources allocated using the **cpuset** and **memory** controllers.

The **proctrack/cgroup** node tracker uses the **freezer** controller to keep track of all the node IDs associated with a job in a specific hierarchy in the cgroup tree, which can then be used to signal these node IDs when instructed (for example, when a user cancels a job).

## WEKA and Slurm integration <a href="#weka-and-slurm-integration" id="weka-and-slurm-integration"></a>

Having covered the fundamentals of WEKA and Slurm architectures, we can now explore integrating these two systems to establish an HPC cluster. This cluster uses the Slurm workload manager for job scheduling and leverages WEKA as the high-performance data platform. Both the dedicated backend and converged configurations are considered.

In either scenario, the Slurm login and compute nodes function as WEKA clients. The controller does not participate in the WEKA filesystem, serving neither as a backend server nor a client.

### WEKA and Slurm integration in dedicated backend architecture <a href="#weka-and-slurm-integration-in-dedicated-backend-architecture" id="weka-and-slurm-integration-in-dedicated-backend-architecture"></a>

In the dedicated backend architecture, the WEKA filesystem is mounted on the login and compute servers, requiring a WEKA frontend process on login and compute processes for file access (Figure 4).

Servers are provisioned to provide servers with compute, network, and storage resources to run the WEKA data platform.

The login and compute nodes from the Slurm cluster mount WEKA filesystems and participate in the WEKA cluster as clients. When mounting in UDP or DPDK mode, some memory must be reserved for WEKA. To determine the amount of memory appropriate for your setup, see the [Plan the WEKA system hardware requirements](../planning-and-installation/bare-metal/planning-a-weka-system-installation) topic.

In UDP mount mode, the WEKA Frontend nodes can run on any available core on the login and compute nodes (WEKA clients).

In DPDK mode, at least one CPU (physical) core must be reserved for the WEKA frontend node. This can be done by one of the following options:

* Specify a dedicated core in the mount options: `mount -o core=X ‚Ä¶`\
  Where `X` is the specific core ID to reserve
* Use non-dedicated cores: `mount -o num_cores=1 ‚Ä¶`

Mounting a dedicated core is recommended for better file IO performance than non-dedicated cores.

### WEKA and Slurm integration in converged architecture <a href="#weka-and-slurm-integration-in-converged-architecture" id="weka-and-slurm-integration-in-converged-architecture"></a>

In the converged architecture, the WEKA filesystem is mounted on login and compute nodes. The login and compute servers also run the drive and compute nodes to participate in hosting the WEKA backend (Figure 5).

The controller hosts the Slurm job scheduler, while the login and compute nodes all host data as part of the WEKA data platform.

Relative to the dedicated backend architecture, the converged architecture requires additional compute and memory resources on the login and compute processes to support the drive, compute, and management processes. In converged deployments, the WEKA processes are typically allocated to specific cores using cgroups.

### What's next?

To understand the potential configurations for an integrated Slurm and WEKA architecture, let's delve into configuring Slurm and WEKA. Allocating sufficient compute and memory resources specifically for WEKA is crucial.

WEKA processes are allocated to specific cores when using the DPDK mount mode or when deploying converged clusters (with UDP or DPDK mount modes). Slurm configuration is crucial to ensure proper resource allocation. Designating specialized cores in Slurm prevents conflicts between user workloads and Slurm services for resource usage.

Moreover, due to Slurm's typical configuration of control groups for allocating user workloads to specialized cores, WEKA must retain the CPUSets when initiating the WEKA agent process.

The following sections detail the required Slurm configurations for dedicated and converged backend setups, considering UDP and DPDK mount modes.

## Implementation <a href="#implementation" id="implementation"></a>

When using a job scheduler such as Slurm with WEKA, it is essential to ensure WEKA is allocated (bound/pinned) to specific cores and ensure the job scheduler does not allocate work to the cores used by WEKA on the WEKA clients.

To prevent user jobs from running on the same cores as the WEKA agent, Slurm must be configured so that the user jobs, Slurm daemon (slurmd), and Slurm step daemon (slurmstepd) only run on specific cores.

Additionally, the available memory for jobs on each compute process must be reduced from the total available to provide sufficient memory for the server operating system and WEKA client processes.

The following sections describe the installation and relevant configurations for Slurm. The focus shifts to the necessary configurations for pinning WEKA processes to specific cores. Finally, examples of WEKA and Slurm configurations cover both dedicated backend and converged cluster architectures.

### Prerequisite: Install and configure Slurm <a href="#prerequisite-install-and-configure-slurm" id="prerequisite-install-and-configure-slurm"></a>

While presuming your familiarity with Slurm installation and configuration, this section provides an overview of Slurm‚Äôs installation. It also highlights critical elements pertinent to the discussion of integrating Slurm with WEKA.

Note: The guidance on installing and configuring Slurm, a prerequisite for integrating it with WEKA, is provided for convenience. For the most up-to-date instructions, it is recommended that you refer to the Slurm documentation.

<details>

<summary>1. Install the munge package on all instances participating in the Slurm cluster</summary>

Slurm depends on munge for authentication between Slurm services. When installing munge from a package manager, the necessary systemd service files are installed, and the munge service is enabled.

By default, this service looks for a munge key under `/etc/munge/munge.key`.

This key must be consistent between the controller, login, and compute nodes. You can create a munge key using the following command:

```
sudo /usr/sbin/create-munge-key -r -f
```

Typically, you create this key on one of the hosts (for example, the controller) and copy it to all the other instances in the cluster. Alternatively, you can mount (using NFS) `/etc/munge` from the controller to the login and compute nodes.

**Note:** To function correctly, the munge authentication requires consistency between the clocks on all instances in the Slurm cluster.

</details>

<details>

<summary>2. Install Slurm</summary>

You can install Slurm using your package manager on RHEL and Debian-based Linux operating systems.

Ensuring that the necessary packages are installed on both the controller and login instances is essential. The following are specific package requirements:

* The controller instance requires packages that provide `slurmctld` and `slurmdbd`. \
  (On Debian-based operating systems, these packages are `slurmctld` and `slurmdbd`.)
* The login instance requires the Slurm client commands, such as `sinfo` and `squeue`. \
  (On Debian-based operating systems, these are provided by the `slurm-client` package.)

Consider the following guidelines:

* If you plan to use a configless deployment, ensure the `slurmd` package runs the `slurmd` service on the login node. Installing Slurm packages from a package manager creates the Slurm user and installs the necessary `systemd` service files.
* If you plan to use slurmdbd (recommended), a SQL database hosting the Slurm database is required. Typically, MariaDB is used. `Slurmdbd` is generally deployed on the Slurm controller host. You can host the MariaDB server on the Slurm controller host or a separate host accessible to the `slurmdbd` service.
* If you require a specific version of Slurm or integrations with particular flavors of the Message Passing Interface (MPI), it is best to build Slurm from the source. In this approach, install munge and a service like MariaDB. Additionally, create the **Slurm** user on each instance in your cluster and set up systemd service files.

The `uid` for the Slurm user must be consistent with all instances in your Slurm cluster.

When deploying Slurm clusters, a standard set of configuration files is required to determine the scheduler behavior, accounting behavior, and available resources. There are four commonly used configuration files in Slurm clusters:

* **slurm.conf** describes general Slurm configuration information, the nodes to be managed, information about how those nodes are grouped into partitions, and various scheduling parameters associated with those partitions. This file needs to be consistent across all hosts in the Slurm cluster.
* **slurmdbd.conf** describes the configuration for the Slurm database daemon, including the database host address, username, and password. This file should be protected as it contains sensitive information (database password) and must only exist on the host running the slurmdbd service.
* **cgroup.conf** describes the configurations for Slurm‚Äôs Linux cgroup plugin. This file needs to be consistent across all hosts in the Slurm cluster.
* **gres.conf** describes the Generic RESource(s) (GRES) configuration on each compute node, such as GPUs. This file needs to be consistent across all hosts in the Slurm cluster.

Most configuration files must be consistent across all hosts (Controller, Login, and Compute) participating in the Slurm cluster. The following three commonly employed strategies ensure consistency of the Slurm configuration across all hosts:

* Synchronize the Slurm configuration files across your cluster using tools such as `parallel-scp`.
* Host your Slurm configuration files on the Controller and NFS mount the directory containing these files on the Login and Compute nodes.
* Deploy a "Configless" Slurm where the `slurmd` service obtains the Slurm configuration from the `slurmctld` server on startup. The configless option is only available for Slurm 20.02 and later.

The first strategy above requires copying the Slurm configuration files across all cluster nodes when configuration changes are needed.

The second and third strategies above require maintaining the configuration files only on the Slurm controller.

The main difference between the configless deployment and the NFS mount configuration is that the login node in the configless setup also needs to run the `slurmd` service.

We recommend using a configless Slurm deployment, when possible, for ease of use. This setup installs a single set of Slurm configuration files on the Slurm controller instance.

The compute and login nodes obtain their configuration through communications between the slurmd service, hosted on the compute and login nodes, and the slurmctld service, hosted on the Slurm controller.

To enable a configless Slurm deployment, set `SlurmctldParameters=enable_configless` in the **slurm.conf** file.

Additionally, start the slurmd services with the `--conf-server host[:port]` flag to obtain Slurm configurations from slurmctld at `host[:port]`.

In all strategies, the configuration files are stored in the same directory. A common choice is to use `/usr/local/etc/slurm` (this location can vary between systems).

For non-standard locations, the slurmctld slurmd services can be launched with the `-f` flag to indicate the path to the slurm.conf file. Alternatively, if you build Slurm from the source, you can use the  `--sysconfdir=DIR`  option during the configuration stage of the build to set the default directory for the Slurm configuration files.

</details>

### Configure Slurm and WEKA <a href="#configure-slurm-and-weka" id="configure-slurm-and-weka"></a>

The configuration of Slurm and WEKA includes settings for isolating CPU and memory resources dedicated to WEKA processes. This involves preventing conflicts between Slurm services (primarily slurmd) and user workloads attempting to use the same cores as the WEKA processes.

Additionally, it includes allocating exclusive cores and memory to avoid oversubscription on Slurm compute nodes.

#### 1. Set the `task/affinity` and `task/cgroup` plugins to prevent user jobs from using the same cores as WEKA processes

Slurm offers `task/affinity` and `task/cgroup` plugins, controlling compute resource exposure for user workloads. The task/affinity plugin binds nodes to designated resources, while the task/cgroup plugin confines nodes to specified resources using the cgroups `cpuset` interface.

To ensure that Slurm daemons (slurmd and slurmstepd) do not run on cores designated for the WEKA agent, it is advisable to set the `TaskPluginParam` to `SlurmdOffSpec`.

Set the following in the **slurm.conf** file:

* Set the `SelectType` option to `select/cons_tres` to indicate that cores, memory, and GPUs are consumable by user jobs.
* Set the `SelectTypeParameters` option to `CR_Core_Memory` to indicate that cores and memory are used explicitly for scheduling workloads.
* Set the `PrologFlags` option to `Contain` to use cgroups to contain all user nodes on their allocated resources.

The following code snippet summarizes the required settings for a **slurm.conf** file.

```

```
ProctrackType=proctrack/cgroup
TaskPlugin=task/affinity,task/cgroup
TaskPluginParam=SlurmdOffSpec
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
JobAcctGatherType=jobacct_gather/cgroup # (optional) for gathering metrics
PrologFlags=Contain
```

```

#### 2. Allocate cores and memory for the WEKA agent

Set each compute node definition in the slurm.conf file to allocate exclusive cores and memory for the WEKA agent. The cores and memory designated for WEKA (as well as other operating system nodes) are termed Specialized Resources.

Set the following parameters in the slurm.conf file:

* `RealMemory`_:_ Specify the available memory on each compute node.
* `CPUSpecList`_:_ Define a list of virtual CPU IDs reserved for system use, including WEKA nodes.
* `MemSpecLimit`**:** When using `SelectTypeParameters=CR_Core_Memory`, specify the amount of memory (in MB) reserved for system use.

Note: To use the `CPUSpecList` and `MemSpecLimit` parameters, ensure the following are set in the `cgroup.conf` file:
```
ConstrainCores=yes
ConstrainRamSpace=yes
```

### Example: Slurm and WEKA dedicated backend architecture with DPDK mount mode <a href="#example-slurm-and-weka-dedicated-backend-architecture-with-dpdk-mount-mode" id="example-slurm-and-weka-dedicated-backend-architecture-with-dpdk-mount-mode"></a>

This example uses the a2-ultragpu-8g instances on Google Cloud Platform, which have 1360 GB (1360000 MB) of available memory, 48 physical cores on two sockets with two hyperthreads per core, and 8 A100 GPUs.

Following the [Plan the WEKA system hardware requirements](../planning-and-installation/bare-metal/planning-a-weka-system-installation) topic, suppose we want to set aside 5 GB of memory and the last core (Core ID 47) for the WEKA Frontend node in a dedicated backend architecture using a DPDK mount mode.

In this example, we set the `RealMemory` to the total memory available and then set the `MemSpecLimit` to 5000 (MB) to set aside that amount of memory for the WEKA agent and the operating system.

When using the `select/cons_tres` with `CR_Core_Memory` parameters in Slurm on systems with hyperthreading, the CPU IDs refer to the ‚Äúprocessor ID‚Äù for the hyperthread, and not the physical core ID.

To determine the relationship between the processor ID and the core ID, we can use the `/proc/cpuinfo` file on Linux systems. This file lists properties for each processor, including its processor ID, associated core ID, and physical ID. The physical ID refers to the physical multi-core CPU chip that is plugged into a socket on the motherboard.

For example, one entry might look like the code snippet below (output is intentionally truncated). On this system, we see that core id 0 hosts processor 0, which is hosted on socket 0.

```
$ cat /proc/cpuinfo
processor : 0
vendor_id : GenuineIntel
cpu family : 6
model : 106
model name : Intel(R) Xeon(R) CPU @ 2.60GHz
stepping : 6
microcode : 0xffffffff
cpu MHz : 2600.034
cache size : 55296 KB
physical id : 0
siblings : 48
core id : 0
cpu cores : 24
```

Continuing through the `/proc/cpuinfo` file we find a relationship between the sockets, cores, and processors that is summarized in Figure 6. Namely, the processor IDs are ordered from 0-23 on Socket 0, then 24-47 on Socket 1. Then, the second hyperthreads are numbered from 48-71 on Socket 0 and 72-95 on Socket 1.

In this example, to reserve core 47, we see that this corresponds to vCPUs 47 and 95. In the slurm.conf file, we would then set `CpuSpecList=47,95`.

An example of the node configuration in slurm.conf is shown below. This code snippet shows a compute node named ‚Äúcompute-node-0‚Äù with 96 CPUs (processors) with two sockets per board, 24 cores per socket, two threads per core, and 8 A100 GPUs.

```

```
NodeName=compute-node-0 CPUs=96 Boards=1 SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=2 RealMemory=1360000 MemSpecLimit=5000 State=CLOUD CpuSpecList=47,95 Gres=gpu:a100:8
```

```

If you are always reserving the last cores for WEKA, an alternative approach is to use the `CoreSpecCount` parameter in the compute node configuration in slurm.conf to specify the number of physical cores for resource specialization.

When using the `CoreSpecCount` parameter, the first core selected is the highest numbered core on the highest numbered socket by default (see core selection in Slurm documentation). Subsequent cores selected are the highest numbered core on lower numbered sockets. In this case, use the snippet below to reserve core 47 (processors 47 and 95) for WEKA.

```

```
NodeName=compute-node-0 CPUs=96 Boards=1 SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=2 RealMemory=1360000 MemSpecLimit=5000 State=CLOUD CoreSpecCount=1 Gres=gpu:a100:8
```

```

To apply these changes on an existing Slurm cluster, restart the Slurm controller daemon after updating your slurm.conf and cgroup.conf files. The Slurm nodes are managed by systemd, allowing you to restart them with systemctl.

```
# On the controller
sudo systemctl restart slurmctld
```

Additionally, restart the Slurm daemon on any compute nodes. On each compute node, run the command shown below. As a user with administrative privileges, you can use `pdsh`, `xargs`, or through a Slurm job. The latter option may be necessary if your cluster is configured using the pam_slurm_adopt plugin, where `ssh` access to compute nodes is limited to users with a job allocation.

```
# On the compute nodes
sudo systemctl restart slurmd
```

Continuing with this example, Slurm and WEKA in a dedicated backend architecture with DPDK mount modes, we turn to necessary WEKA configurations. To reserve core 47 for the WEKA Frontend node on a compute node using a DPDK mount mode, you can use the `core` mount option as follows:

```
mount -t wekafs -o core=47 -o net=ib0 backend-host-0/fs1 /mnt/weka
```

In this example, the NIC used for DPDK is `ib0`, the WEKA backend host can be acceesed at `backend-host-0`, the filesystem name is `fs1`, and the mount location on the compute node is `/mnt/weka`.

By default, WEKA resets cpusets, which can interfere with configurations enforced by Slurm. To prevent this, set the `isolate_cpusets=false` option in `/etc/wekaio/service.conf` and restart the weka-agent node as follows:

```
sudo sed -i 's/isolate_cpusets=true/isolate_cpusets=false/g' /etc/wekaio/service.conf
sudo systemctl restart weka-agent
```

For more information, see #modify-the-cgroups-usage

    For details, see PROCTRACK/CGROUP PLUGIN.

    For details, see "Configless" Slurm.

    For details, see the Slurm Quick Start Administrator Guide.

    For details, see Core Specialization.

    For details, see pam_slurm_adopt.

<!-- ============================================ -->
<!-- File 236/259: best-practice-guides_weka-and-slurm-integration_avoid-conflicting-cpu-allocations.md -->
<!-- ============================================ -->

# Avoid conflicting CPU allocations

In a WEKA and Slurm integration, efficient CPU allocation is crucial to prevent conflicts between the WEKA filesystem and Slurm job scheduling. Improper CPU allocation can lead to performance degradation, CPU starvation, or resource contention. This section outlines best practices to ensure WEKA and Slurm coexist harmoniously by carefully managing CPUsets and NUMA node allocations.

### 1. Disable WEKA CPUset isolation

Ensure that WEKA's default CPUset isolation is disabled to avoid conflicts with Slurm.

```bash
[root@example01 ~]# grep 'isolate_cpusets=' /etc/wekaio/service.conf
isolate_cpusets = false
```

### 2. Verify hyperthreading and NUMA configuration

Verify your system's hyperthreading and NUMA configuration. Typically, hyperthreading is disabled in most Slurm-managed environments. In this example, hyperthreading is disabled, and there are four NUMA nodes.

```bash
| [root@example01 ~]# lscpu | egrep 'Thread | NUMA' |
Thread(s) per core:  1
NUMA node(s):        4
NUMA node0 CPU(s):   0-13
NUMA node1 CPU(s):   14-27
NUMA node2 CPU(s):   28-41
NUMA node3 CPU(s):   42-55
```

### 3. Identify the NUMA node of the dataplane network interface

Determine the NUMA node associated with the dataplane network interface. For instance, `ib0` is located in NUMA node 1.

```bash
[root@example01 ~]# cat /sys/class/net/ib0/device/numa_node
1
[root@example01 ~]# cat /sys/class/net/ib0/device/local_cpulist
14-27
```

### 4. Assign CPU cores to WEKA

When mounting the WEKA filesystem, specify the CPU cores for the WEKA client to use. These cores should be in the same NUMA node as the network interface.

Avoid using core 0. Typically, the last cores in the NUMA node are chosen. For example:

```

```bash
[root@example01 ~]# mount -t wekafs -o core=24,core=25,core=26,core=27,net=ib0 /mnt/wekafs
```

```

After mounting, confirm the cores and network interfaces used by WEKA:

```bash
| [root@example01 ~]# weka local resources | head |
ROLES       NODE ID  CORE ID
MANAGEMENT  0        <auto>
FRONTEND    1        24
FRONTEND    2        25
FRONTEND    3        26
FRONTEND    4        27

NET DEVICE  IDENTIFIER    DEFAULT GATEWAY  IPS  NETMASK  NETWORK LABEL
ib0         0000:4b:00.0                        19
```

### 5. Configure Slurm to exclude WEKA's cores

Configure Slurm to exclude WEKA's cores from those available for user jobs by setting the `CPUSpecList` parameter.

Verify the configuration with:

```bash
| [root@example01 ~]# scontrol show node $(hostname -s) | grep CPUSpecList |
CoreSpecCount=4 CPUSpecList=24-27 MemSpecLimit=20480
```

### 6. Verify CPUset configuration

Ensure that the Slurm CPUset excludes the cores assigned to the WEKA client.

```bash
[root@example01 ~]# grep "" /sys/fs/cgroup/cpuset/weka-client/*cpus
/sys/fs/cgroup/cpuset/weka-client/cpuset.cpus:24-27
/sys/fs/cgroup/cpuset/weka-client/cpuset.effective_cpus:24-27

[root@example01 ~]# grep "" /sys/fs/cgroup/cpuset/slurm/system/*cpus
/sys/fs/cgroup/cpuset/slurm/system/cpuset.cpus:0-23,28-55
/sys/fs/cgroup/cpuset/slurm/system/cpuset.effective_cpus:0-23,28-55
```

### 7. Manage hyperthreading

If hyperthreading is enabled, identify the sibling CPUs and include them in both the WEKA mount options and Slurm‚Äôs `CPUSpecList`. For clarity, even though WEKA automatically reserves these CPUs, explicitly specifying them can help avoid potential issues.

In this example, hyperthreading is disabled, so no additional CPUs are required:

```

```bash
| [root@example01 ~]# grep "" /sys/devices/system/cpu/*/topology/thread_siblings_list | egrep 'cpu24 | cpu25 | cpu26 | cpu27' |
/sys/devices/system/cpu/cpu24/topology/thread_siblings_list:24
/sys/devices/system/cpu/cpu25/topology/thread_siblings_list:25
/sys/devices/system/cpu/cpu26/topology/thread_siblings_list:26
/sys/devices/system/cpu/cpu27/topology/thread_siblings_list:27
```

```

### 8. Address logical and physical CPU index mismatch

In certain situations, environmental factors like BIOS or hypervisor settings may cause discrepancies between logical CPU numbers and the physical or OS-assigned numbers. This can result in the Slurm CPUset mistakenly including CPUs that should be reserved for the WEKA client, potentially leading to resource conflicts such as CPU starvation.

For example, if the CPUset configuration shows that Slurm is not correctly excluding the WEKA-assigned CPUs, you might see something like this, where CPUs 56, 58, 60, and 62 are listed in _both_ CPUsets, which will cause conflicts:

```bash
[root@example01 ~]# grep "" /sys/fs/cgroup/cpuset/weka*/cpuset.effective_cpus
56-63
[root@example01 ~]# grep "" /sys/fs/cgroup/cpuset/slurm/system/cpuset.effective_cpus
0-48,50,52,54,56,58,60,62
```

The issue may arise from non-sequential CPU numbering, where CPUs are interleaved between NUMA nodes:

```bash
| [root@example01 ~]# lscpu | egrep 'Thread | NUMA' |
Thread(s) per core:  1
NUMA node(s):        2
NUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
NUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63
```

To address this, do the following:

1. Ensure that the WEKA agent's `isolate_cpuset=false` setting is applied (see Step 1), and that the agent has been restarted.
2. Use `hwloc-ls` or `lstopo-no-graphics` to map the logical index (L#) to the physical/OS index (P#) for the CPUs assigned to WEKA. If the logical and physical indexes don‚Äôt match, use the logical index numbers in Slurm‚Äôs `CPUSpecList` parameter.

Note: Before starting any Weka container or mounting WekaFS filesystems, ensure that you examine the output of `hwloc-ls` or `lstopo-no-graphics`. This step is critical as it verifies the logical index numbers provided by these user-space tools. Failure to perform this check may result in incorrect logical index mappings, which can lead to configuration or performance issues.

In this example, the output indicates a mismatch between the L# and P#:

```bash
| [root@example01 ~]# weka local resources | egrep 'FRONTEND' | awk '{print "hwloc-ls | grep P\\#"$3}' | bash |
      L2 L#28 (2048KB) + L1d L#28 (48KB) + L1i L#28 (32KB) + Core L#28 + PU L#28 (P#56)
      L2 L#29 (2048KB) + L1d L#29 (48KB) + L1i L#29 (32KB) + Core L#29 + PU L#29 (P#58)
      L2 L#30 (2048KB) + L1d L#30 (48KB) + L1i L#30 (32KB) + Core L#30 + PU L#30 (P#60)
      L2 L#31 (2048KB) + L1d L#31 (48KB) + L1i L#31 (32KB) + Core L#31 + PU L#31 (P#62)
      L2 L#60 (2048KB) + L1d L#60 (48KB) + L1i L#60 (32KB) + Core L#60 + PU L#60 (P#57)
      L2 L#61 (2048KB) + L1d L#61 (48KB) + L1i L#61 (32KB) + Core L#61 + PU L#61 (P#59)
      L2 L#62 (2048KB) + L1d L#62 (48KB) + L1i L#62 (32KB) + Core L#62 + PU L#62 (P#61)
      L2 L#63 (2048KB) + L1d L#63 (48KB) + L1i L#63 (32KB) + Core L#63 + PU L#63 (P#63)
```

Although WEKA uses _physical_ cores `56-63`, set Slurm‚Äôs `CPUSpecList` to `28-31,60-63` to correctly allocate the CPUs based on their _logical_ index.

**Related information**

Slurm GRES documentation (for more details on logical and physical core index mapping)

<!-- ============================================ -->
<!-- File 237/259: wekapod.md -->
<!-- ============================================ -->

# WEKApod

## Topics in this section

### WEKApod Data Platform Appliance overview

### WEKApod servers overview

### Rack installation

### WEKApod initial system setup and configuration

### WEKApod support process

<!-- ============================================ -->
<!-- File 238/259: wekapod_overview.md -->
<!-- ============================================ -->

# WEKApod Data Platform Appliance overview

The WEKApod‚Ñ¢ is a turnkey data platform appliance that delivers high-performance storage solutions supporting NVIDIA DGX SuperPOD and other environments. Each appliance includes pre-configured storage servers and integrated software, enabling simplified deployment and faster time to value.

It is powered by the WEKA Data Platform and supports NVMe technology (POSIX, RDMA, and TCP), Magnum IO‚Ñ¢ GPUDirect Storage (GDS), NFS, S3, and SMB protocol. The WEKApod starts with a minimum of 8 servers and scales to hundreds of servers.

#### Purpose and benefits of the WEKA data platform appliance

The WEKApod appliance is designed to overcome traditional storage scaling and file sharing limitations, and it allows parallel file access to the same namespace through POSIX, NFS, SMB, S3, and Magnum IO‚Ñ¢ GPUDirect Storage.

It provides a rich enterprise feature set, including local snapshots, automated tiering, self healing, private cloud multi-tenancy, backup, encryption, authentication, key management, user groups, quotas, and more.

The WEKApod‚Äôs exceptional performance density improves GPU and workload processing efficiencies, optimizes rack space utilization, and reduces idle energy consumption and carbon emissions, contributing to overall power savings and helping organizations meet their sustainability goals.

It also enables various hybrid cloud workflows, including bursting into the cloud, running workflows that span locations, and using the cloud to protect and archive data.

### Guide overview

This guide provides detailed instructions for setting up and managing the WEKApod Data Platform Appliance. It covers the following sections:

1. **WEKApod Data Platform Appliance overview** (this page)\
   An introduction to the WEKApod solution, highlighting its key features and benefits.
2. \
   A breakdown of the core WEKApod server components, including descriptions of the front and back panels.
3. \
   Step-by-step instructions on unpacking and installing the WEKApod system into a rack, including safety and regulatory information.
4. \
   An outline of the initial setup tasks and configuration, which will be carried out by the WEKA support team.
5. \
   A description of the WEKApod multi-tiered support system, providing customer assistance.

Each section guides you through key processes to ensure a smooth setup and operation of the WEKApod system. For detailed information, click on the relevant section link above.

**Related topics**

WEKA Data Platform introduction

**Related information**

The WEKApod Data Platform Appliance Datasheet

NVIDIA DGX SuperPOD Documentation

<!-- ============================================ -->
<!-- File 239/259: wekapod_setup.md -->
<!-- ============================================ -->

# WEKApod initial system setup and configuration

## Workflow

Follow these procedures to complete the installation, initial setup, and configuration for the WEKApod system.

1. #prepare-for-installation
2. #install-hardware
3. #connect-cables
4. #configure-the-idrac (if the WMS server and Ethernet switch are unavailable)
5. #configure-the-weka-software-using-wms

### Prepare for installation

Ensure the customer site is ready for deployment according to the site requirements outlined during the service personnel's site survey.

A detailed site requirements document will be provided before installation, which includes, but is not limited to, the following:

* **Power**: Estimated consumption (for an 8-server setup, approximately 7,712 watts).
* **Cooling**: Required capacity (for an 8-server setup, approximately 30,027.2 BTU/hour).
* **Space**: Rack dimensions and required space (one server requires 1 rack unit with a maximum depth of 787 mm).

### Install hardware

Note: **Heavy load:** Use proper lifting techniques, and seek help if the load is too heavy or awkward. Consider using lifting equipment if necessary.

1. **Rack preparation:** Confirm that the rack is securely mounted according to the rack installation guidelines.
2. **Device mounting:** Mount the following devices according to the rack installation guidelines:
   * **Ethernet switch:** Mount on the top rail of the rack.
   * **WMS server:** Mount on the rail directly below the Ethernet switch.
   * **WEKApod servers:** Mount on the rails below the WMS server in the order specified in the provided spreadsheet.\
     Match each server's service tag, which is available on the front panel and on the box, to the corresponding entry in the spreadsheet.

**Related topic**

### Connect cables

1. Connect the peripherals to the system as follows:
   * **OS management:**
     * Connect the first WMS Ethernet port to the 1 Gbps Ethernet switch (if the switch is not available, connect to the customer's Ethernet network).
     * Connect the second WMS Ethernet port to the customer's Ethernet network.
     * Connect the NIC port of each WEKApod server to the 1 Gbps Ethernet switch (if the switch is not available, connect to the customer's Ethernet network).
   * **BMC/iDRAC/iLO/IPMI:** Connect the BMC Ethernet port of each WEKApod server to the 1 Gbps Ethernet switch.
   * **InfiniBand (IB):** Connect the two IB ports of each WEKApod server to the IB network.
   * **25 Gbps Ethernet:** Connect the two OPC NIC ports of each WEKApod server to the customer's network, enabling tiering to object store.
2. Connect the system to the electrical outlet.
3. Power on the system.

### Configure the iDRAC

iDRAC (Integrated Dell Remote Access Controller) is a proprietary technology developed by Dell. It provides remote management capabilities for Dell servers, allowing administrators to manage and monitor the server hardware independently of the operating system.

When the WEKApod system is shipped with a WMS server and Ethernet switch, the WSA servers come pre-configured with IP address information. Therefore, perform this procedure only if the **WMS server and Ethernet switch are unavailable**; otherwise, skip this step.

Note: The pre-configured IP address of the iDRAC/BMC interfaces of the backend servers is **192.168.2.x**, as indicated in the provided spreadsheet and Packing List included with the shipment.

**Repeat this procedure for all WSA servers:**

1. Connect a crash cart (KVM) to the server.
2. Power on or reboot the server.

3. Press **F2** when prompted to enter the **System Setup**.

Note: Alternatively, you can configure these settings using the **Lifecycle Controller** (press **F10** during boot).

4. Navigate to **Network** settings.

5. In **NETWORK SETTINGS**, ensure the NIC is enabled.

6. Scroll to the **IPV4 SETTINGS** section and configure the settings as shown in the following example according to your environment. If necessary, configure the settings in the additional sections.

7. Select **Finish** and exit the System Setup.

### Configure the WEKA software using WMS

For WEKApod systems that include a WMS server and Ethernet switch, follow the steps below to configure the software using the WMS.

If your WEKApod system does not include a WMS server and Ethernet switch, refer to the instructions for reinstalling the operating system, WEKA software, and configuring the system. See .

#### Configuration tips and troubleshooting

* **Installation troubleshooting**: If the installation halts without error messages, access the system console and review the logs located in the `/tmp/` directory. The primary log file is `/tmp/ks-pre.log`.
* **Accessing logs**: To open a command prompt from the installation GUI for log review:
  * On macOS, press `Ctrl+Option+F2`
  * On Windows, press `Ctrl+Alt+F2`
* **BMC access**: In some cases, you may need to use the Baseboard Management Controller (BMC) virtual console to complete the configuration.
* **Best practice**: Run the `dnf update` command on all WEKApod servers and the WMS. Applying necessary security patches before configuration is essential for system security.

#### **Procedure**

1. Log in on the console or through SSH as the weka user (root password: `WekaService`; weka user password: `weka.io123`).
2. Browse the WMS Admin UI using the URL: `http://<WMS-hostname-or-ip>:8501`.

3. Enter username and password (default: _admin_/_admin_), and select **Login**. The Landing Page appears.
4. Select **Deploy a WEKA Cluster** and ensure any popup blockers are disabled.

5. Select **WEKApod Install**.
6. In Step 1 - Number of servers, the default **Server Count** is set to 8. If your deployment requires a different number, adjust this value as needed, then click **Next**.

7. In Step 2 - Verify IPMI Connectivity, the WMS automatically fills in the IPMI IPs. Ensure the IPMI/iDRAC first IP, username, and password are correct (default: root/WekaService). Click **Verify IPMI IPs** and confirm that the **Brand** column shows Dell. Then, click **Next**.

8. In Step 3 - Dataplane Settings, do the following:
   1. Enter the required IP information for the Dataplane network, then click **Update Dataplanes**.
   2. Ensure the Dataplanes are configured correctly, then click **Next**.

9. In Step 4 - Save Files and Run Validation Checks, click **Save Files and Run Validation Checks**.

11. Confirm that all settings are correct. Click **Download Configuration CSV File** to save the configuration, then click **Next**.

12. In Step 5 - Optional (Re)install WEKA Software, you can update the WEKA version if needed (the current version may be outdated based on the WSA version used to image the servers).\
    If you do not want to update the WEKA version, click **Next/Skip**.\
    \
    To update the WEKA version:
    1. Place the required WEKA version in the weka user's home directory (`~/weka`) on the WSA server.
    2. Click **Refresh Weka Software File List**.
    3. Select the new version from the list.
    4. Click **Start WEKA Software Install**. This process may take a few minutes.
    5. Once the installation is complete, click **Next/Skip**.

13. In Step 6 - Apply OS and Dataplane settings, click **Run system configuration scripts**. This process typically takes 3-5 minutes for an 8-server cluster, but larger clusters take longer.

14. Optional. You can review the messages providing a summary of the installation. To display more details, click **Show Data**.

15. Upon completion of the preceding steps, proceed with the standard configuration of the cluster as outlined in .

## Next steps

After configuring the WEKApod servers, start managing the system using the GUI, CLI, or REST API, and add clients to your WEKA cluster.

**Related topics**

<!-- ============================================ -->
<!-- File 240/259: wekapod_server-overview.md -->
<!-- ============================================ -->

# WEKApod servers overview

## **WEKApod server core** component

The WEKApod's core component is a 1U server that includes:

* **Processor:** One AMD EPYC 9454P 48-Core Processor.
* **Memory:** 12 DDR5 DIMM slots with 384 GB.
* **Power supply:** Two redundant AC power units.
* **NVMe drive options:** (order-dependent)
  * 10 x 2.5-inch NVMe drives (WEKApod Prime)
  * 14 x E3.S NVMe drives (WEKApod Nitro)
  * 8 x 2.5-inch NVMe drives (WMS)
* **High-speed storage connectivity:** InfiniBand cards (order-dependent):
  * NVIDIA CX-6 MCX653105A-HDAT (WEKApod Prime)
  * NVIDIA CX-7 MCX75310AAS-NEAT (WEKApod Nitro)
  * NVIDIA CX-7 MCX75210AAS-NEAT (WEKApod Nitro)
* **Networking:** Network Interface Card (NIC) for general-purpose networking.

Note: A WEKApod server installed with WSA is referred to as a **WSA server**.
A WEKApod server installed with WMS is referred to as a **WMS server**.

## Front view of the WEKApod servers

The front view of the WEKApod server configuration is order-dependent:

* WEKApod Prime: A WSA server equipped with 10 X 2.5-inch NVMe drives.
* WEKApod Nitro: A WSA server equipped with 14 X E3.S NVMe drives.
* WMS: A WMS server equipped with 8 X 2.5-inch NVMe drives.

**Front view of the WSA and WMS servers: Ports, panels, and slots descriptions** (all configurations)

 | Item | Ports, panels, and slots | Description |
 | --- | --- | --- |
 | 1 | Left control panel | Contains the system health, system ID, and the status LED indicators. |
 | 2 | Drives | Enables you to install NVMe drives supported on your system (order-dependent: E3.S or 2.5-inch). |
 | 3 | Right control panel | Contains the power button with integrated power LED, 1 x VGA port, 1 x 2.0 USB port, iDRAC Direct (Micro-AB USB) port, and the iDRAC Direct status LED. |
 | 4 | VGA | Enables you to connect a display device to the system. |
 | 5 | Information tag | The Express Service Tag is a slide-out label panel that contains system information such as Service Tag, NIC, MAC address, and so on. If you have opted for the secure default access to iDRAC, the Information tag also contains the iDRAC secure default password. |
 | 6 | E3.S blank | Enables you to install blanks for 14 x E3.S configuration. |

## Rear view of the WSA and WMS servers

**Rear view of the WEKApod server: Ports, panels, and slots**

 | Item | Ports, panels, and slots | Description |
 | --- | --- | --- |
 | 1 | Power supply unit (PSU1) | Primary power supply unit. |
 | 2 | InfiniBand port 1 | 400 Gbps HDR InfiniBand port (ib1) for storage network connectivity. |
 | 3 | InfiniBand port 2 | 400 Gbps HDR InfiniBand port (ib0) for storage network connectivity. |
 | 4 | Power supply unit (PSU2) | Secondary power supply unit. |
 | 5 | OS management Ethernet ports | 1 Gbps Ethernet ports (left to right): WEKA Linux eno8303 and eno8403. |
 | 6 | 25 Gbps Ethernet ports | Extra 25 Gbps Ethernet ports for auxiliary uses like WEKA Object Storage tiering, Snapshot-to-Object backup, and other secondary applications. |
 | 7 | BMC Ethernet port | Ethernet port for in-band management (iDRAC). |

**Rear view of the WMS server: Ports, panels, and slots**

 | Item | Ports, panels, and slots | Description |
 | --- | --- | --- |
 | 1 | Power supply unit (PSU1) | Primary power supply unit. |
 | 2 | Power supply unit (PSU2) | Secondary power supply unit. |
 | 3 | OS management Ethernet ports | 1 Gbps Ethernet ports (left to right): WEKA Linux eno12399, eno12409, eno12419, and eno12429. |
 | 4 | BMC Ethernet port | Ethernet port for in-band management (iDRAC). |

## Server Information tag location

Each server can be identified by its unique Express Service Code and Service Tag. To access this information, pull out the Information tag located at the front of the server. Alternatively, this information may be found on a sticker on the chassis. The mini Enterprise Service Tag (EST) is located on the back of the server and is used to route support calls to the appropriate personnel.

A spreadsheet containing the corresponding Service Tag number provides details on the WEKApod server's IP address and mount order.

The Information tag is a slide-out label that displays key server details, including the Service Tag, NIC, MAC address, and more. It also includes the iDRAC secure default password; however, note that the password has been reset to **WekaService** through the ID Module.

<!-- ============================================ -->
<!-- File 241/259: wekapod_rack-installation.md -->
<!-- ============================================ -->

# Rack installation

Note: Before you begin, read the **Enterprise Products Safety, Environmental, and Regulatory Information** attached below.

{% file src="https://content.gitbook.com/content/0yXyIrnroN3zIG3qa4W3/blobs/mprDHnF3lRVjti2aGCdH/SERI-WW.pdf" %}
Enterprise Products Safety, Environmental, and Regulatory Information (world-wide)
{% endfile %}

Note: Only trained service technicians are authorized to remove the system cover and access any components inside the system.

## Inspect packaging

1. **Examine packaging for damage**: Upon receiving your package, inspect it for any signs of damage. If you notice damage, take clear photographs for documentation.
2. **Inspect contents**: Open the package and carefully inspect the contents for any damage or discrepancies.
3. **Contact WEKA Customer Success**: If you find any issues‚Äîsuch as damage, missing items, or incorrect items‚Äîcontact WEKA Customer Success at support.weka.io. Ensure you have your order number, full shipping address, and serial number (if applicable) ready to expedite the resolution process.

Note: * Begin installing the rails in the allotted space that is closest to the bottom of the rack enclosure.
* The illustrations in this document do not represent a specific system.
* The tooled rail mounting configuration requires eight user-supplied screws: #10-32, #12-24, #M5, or #M6. The head diameter of the screws must be less than 10 mm (0.4‚Äù).

Note: **Heavy load:** Use proper lifting techniques, and seek help if the load is too heavy or awkward. Consider using lifting equipment if necessary.

## Ready rails (sliding rails) installation instructions

### Identifying the rail kit contents

Locate the components for installing the rail kit assembly:

* Two B6 ReadyRails sliding rail assemblies (1).
* Two hook and loop straps (2).

### Installing and removing tool-less rails (square hole or round hole racks)

1. **Position the rail end pieces:** Orient the left and right rail end pieces labeled FRONT inward, and align each end piece with the holes on the front side of the vertical rack flanges (1).
2. **Align in U spaces:** Position each end piece in the bottom and top holes of the desired U spaces (2).
3. **Engage the rail:** Push the back end of the rail until it fully seats on the vertical rack flange and the latch clicks into place. Repeat this step to position and seat the front end piece on the vertical rack flange (3).
4. **Remove the rails:** To remove, press the latch release button at the midpoint of the end piece and unseat each rail (4).

### Installing and removing tooled rails (threaded hole racks)

1. **Remove the pins:** Use a flat-tipped screwdriver to remove the pins from the front and rear mounting brackets (1).
2. **Detach the rail latch subassemblies:** Pull and rotate the rail latch subassemblies to remove them from the mounting brackets (2).
3. **Attach the front mounting rails:** Secure the left and right mounting rails to the front vertical rack flanges using two pairs of screws (3).
4. **Attach the rear brackets:** Slide the left and right back brackets forward against the rear vertical rack flanges and secure them using two pairs of screws (4).

### Installing the system in a rack

1. **Extend the slide rails:** Pull the inner slide rails out of the rack until they lock into place (1).
2. **Position the system:** Locate the rear rail standoffs on each side of the system and lower them into the rear J-slots on the slide assemblies (2).
3. **Seat the rail standoffs:** Rotate the system downward until all the rail standoffs are securely seated in the J-slots (3).
4. **Lock and insert the system:** Push the system inward until the lock levers click into place. Then, press the slide-release lock buttons on both rails and slide the system fully into the rack (4).

### Removing the system from the rack

1. **Locate the lock levers:** Identify the lock levers on the sides of the inner rails (1).
2. **Unlock the levers:** Rotate each lever upward to its release position (2).
3. **Remove the system:** Firmly grasp the sides of the system and pull it forward until the rail standoffs reach the front of the J-slots. Then, lift the system up and away from the rack and place it on a level surface (3).

## Cable management arm (CMA) installation instructions

Note: The illustrations in this document are not intended to represent a specific server. These installation instructions show a 3U Cable Management Arm installation. Other CMAs may vary slightly in appearance.

### Identifying the cable management arm kit contents

1. Locate the components for installing the Cable Management Arm (CMA) assembly:
   * Cable Management Arm tray (1)
   * Cable Management Arm (2)
   * Nylon cable tie wraps (3)

Note: To secure the CMA for shipment in the rack, loop the tie wraps around both baskets and tray and cinch them firmly.
For larger CMAs, the tie wraps can be threaded through the inner and outer baskets and around the tray to secure them. Securing the CMA in this manner also secures your system in unstable environments.

### Installing and removing the cable management arm tray

* **Install the tray:** Align and engage each side of the CMA tray with the receiver brackets on the inner edges of the rails. Push the tray forward until it clicks into place (1).
* **Remove the tray:** To remove, squeeze the latch-release buttons on both sides toward the center, then pull the tray out of the receiver brackets (2).

### Installing and removing the cable management arm

1. **Determine mounting position:** Attach the CMA to either the right or left mounting rail based on your cable routing needs. It is recommended to mount the CMA on the side opposite the power supplies. If mounted on the same side as the power supplies, you must disconnect the CMA to remove the outer power supply.
2. **Remove the tray:** Before removing the power supplies, ensure to remove the tray.
3. **Attach the CMA:**
   * At the back of the system, align the latch on the front end of the CMA with the innermost bracket of the slide assembly until the latch engages (1).
   * Align the other latch on the CMA with the outermost bracket until the latch engages (2).
4. **Remove the CMA:** To detach the CMA, press the release buttons at the top of the inner and outer latch housings to disengage both latches (3).

### Moving the CMA Away from the CMA Tray

1. **Extend the CMA:** Pull the CMA away from the system and extend it away from the tray for access and service (1).
2. **Unseat the CMA:** At the hinged end, lift the CMA up and off the tray to unseat it from the tray catch.
3. **Swing the CMA away:** Once unseated, swing the CMA away from the system (2).

Note: If the CMA is already cabled, you can extend it into the service position to access the back of the system.

### Cabling the System Using the CMA

Note: **CAUTION:** To prevent potential damage from protruding cables, secure any slack in the status indicator cable before routing it through the CMA.

1. **Bundle the cables:** Using the provided tie wraps, bundle the cables together as they enter and exit the baskets to prevent interference with adjacent systems (1).
2. **Route the cable bundle:** With the CMA in the service position, route the cable bundle through the inner and outer baskets (2).
3. **Secure the cables:** Use the preinstalled Velcro straps on both ends of the baskets to secure the cables (3).
4. **Adjust cable slack:** Adjust the cable slack as needed at the hinge position (4).
5. **Reposition the CMA:** Swing the CMA back into place on the tray (5).
6. **Install the status indicator cable:** At the back of the system, install the status indicator cable and secure it by routing it through the CMA. Attach the other end of the cable to the corner of the outer CMA basket (6).

<!-- ============================================ -->
<!-- File 242/259: wekapod_support.md -->
<!-- ============================================ -->

# WEKApod support process

The WEKApod support process is a multi-tiered system designed to provide comprehensive assistance to customers:

1. **Initial contact**: For SuperPOD-related issues, the first point of contact is NVIDIA. They handle the initial call and provide immediate support.
2. **Direct WEKA support**: For storage-related issues, customers can contact WEKA directly. This ensures specialized assistance from the team with in-depth knowledge of the storage system.
3. **Hardware issue identification**: If a storage issue is identified as a hardware problem, WEKA ensures it is properly logged and addressed.
4. **Hardware shipment**: Once a hardware issue is confirmed, the necessary parts are shipped to the customer, ensuring quick replacement of faulty or damaged components to minimize downtime.

For NVIDIA related issues, contact the NVIDIA enterprise support services.

For storage issues, contact the [WEKA Customer Success Team](../../support/getting-support-for-your-weka-system#contact-customer-success-team).

<!-- ============================================ -->
<!-- File 243/259: appendices.md -->
<!-- ============================================ -->

# Appendices

## Topics in this section

### WEKA CSI Plugin

Connect Kubernetes worker nodes to the WEKA data platform to leverage its capabilities.

### Convert cluster to multi-container backend

Professional services workflow for converting the cluster architecture from a single-container backend to a multi-container backend.

### Create a client image

### Update WMS and WSA

Learn to manually update the WEKA Management Station (WMS) and WEKA Software Appliance (WSA) on sites with Internet access and dark sites to maintain system security and functionality.

### BIOS tool

This tool simplifies managing BIOS settings across multiple servers, ensuring consistency and reducing manual configuration efforts.

<!-- ============================================ -->
<!-- File 244/259: appendices_create-a-client-image.md -->
<!-- ============================================ -->

# Create a client image

When a stateless client first mounts or connects to a WEKA cluster, it downloads containers and builds drivers, taking 30 seconds to several minutes, based on the environment. For conventional machines, this is acceptable, occurring only once.

However, in scenarios with frequent client re-imaging or launching new cloud instances, the operation duration may become impractical, raising concerns about delays in downloading containers and building drivers.

This client image creation procedure is effective in on-premises environments, using the Bright Cluster Manager tool, and is also used to prepare templates for cloud-based instances (AWS AMIs).

Note: * After a major WEKA cluster release upgrade, this procedure must be repeated using the new release.
* Considerations for diskless clients are not addressed in this procedure.

**Procedure**

1. Provision a standard and unmodified (vanilla) instance or container.
2. Install a WEKA client. \
   Perform one of the following options:

<details>

<summary>Option 1: Install WEKA client using the tarball</summary>

1. Download and untar the WEKA tarball (same as used for backend installation).
2. Open the `install.sh` and comment out the `weka local start` command.
3. Run `./install.sh`
4. Remove the default client container:\
   `weka local rm default --force`

</details>

<details>

<summary>Option 2: Install WEKA client using curl and container preparation commands</summary>

1. Install a WEKA client: \
| `curl http://<backendserverip>:14000/dist/v1/install | sh` |
2. Download and set the current version of WEKA from the backend:\
   `weka version get <current version>`\
   `weka version set <current version>`
3. Setup the client and prepare the drivers: \
   `weka local setup container --name <client name> --no-start`\
   `weka version prepare <current version>`

</details>

3. Check for the machine identifier file, and if it exists, remove it using the command:\
   `rm /opt/weka/data/agent/machine-identifier`
4. Stop the weka-agent service:\
   `systemctl stop weka-agent`
5. Optional: If you used the tarball, remove it and the unzipped directory.
6. Create the image of the WEKA client using the method that is preferred for your environment.
   * If you use the Bright Cluster Manager's `grabimage` command, exclude `/opt/weka/*` from the `imageupdate` command.
   * In AWS, create an AMI from Amazon EC2 instance.

**Related information**

NVIDIA Base Command Manager

Create an AMI from an Amazon EC2 Instance

**Related topics**

<!-- ============================================ -->
<!-- File 245/259: appendices_convert-the-cluster-architecture-from-a-single-container-backend-to-a-multi-container-backend.md -->
<!-- ============================================ -->

---
description:
---

# Convert cluster to multi-container backend

Since WEKA introduced the multi-container backend (MCB or MBC) architecture, it is required to convert existing single-container backend (SCB) architecture to MCB.

In SCB, the drive, compute, and frontend processes are in the same container. In MCB, a server includes multiple containers, each running a specific process type. The MCB offers benefits such as:

* Non-disruptive upgrades
* Effective hardware cores usage
* Less disruptive maintenance

Conversion to MCB is supported from version 4.0.2 and above.

For more details about MCB, see the .

Note: This workflow is intended for experienced professionals in the field of professional services who are familiar with WEKA concepts and maintenance procedures.

## SCB to MCB conversion workflow

The conversion runs on one server at a time (rolling). It takes about 4.5 minutes per server, so the cluster performance hit is minimal. Therefore, it is recommended (not mandatory) to perform this workflow during the corporate maintenance window.

1. Prepare the source cluster for conversion
2. Remove the protocol cluster configurations (if exist)
3. Ensure failure domains are named
4. Convert the cluster from SCB to MCB
5. Restore the protocol cluster configurations (if required)

### 1. Prepare the source cluster for conversion

1. Ensure the source cluster meets the following requirements:
   * The source cluster is version 4.0.2 or higher.
   * The cluster must not download a snapshot during the conversion (snapshot upload is allowed).
   * You must have passwordless SSH access to all backend servers. This access can be granted to either the root user or a regular user. If you opt for a non-root user, it must also have passwordless sudo privileges.
2. Download the latest Tools Repository. It is recommended to pull the latest version before starting the migration.
3. Copy the following scripts from the downloaded tools repository to the **/tmp** directory on the server from which you plan to run it:
   * **All** the following conversion scripts:
     * MBC cluster conversion
     * Change failure domains to manual
     * Protocols
   * Resources generator
4. Verify that all backends are up and with no rebuilds in progress.
5. Ensure no WEKA filesystem mounts exist on the backends.\
   If required, run `umount -a -t wekafs`.
6. The backend container must not have converged processes (nodes) in the same core. Each process must be either frontend, compute, or drive. You cannot share these on the same core. Some clusters may share the frontend and compute processes, especially in AWS. If you have one of these clusters, you must first use the `core_allocation` script to change to core allocations.
7. Ensure the root user is logged into the WEKA cluster on all backends. Otherwise, copy the `/root/.weka/auth-token.json` to all backend servers. The script runs the `weka` commands. Without this file, the commands do not complete, and the message is ‚Äúerror: Authentication Failed‚Äù.
8. The conversion script starts running on three containers on ports 14000, 14200, and 14300. Ensure no other processes use ports in the range 14000 to 14299.
9. Changing the `/opt/weka/logs` loop device to 2 GB is recommended. After the MCB conversion, each container has its own set of logs, so the space required is tripled. Visit the Support Portal and search for the KB: _How-to-increase-the-size-of-opt-weka-logs_.
10. If the cluster has network device names in the old schema, convert these names to real NIC names. To identify the network devices, run `weka cluster host net -b`. If the result shows network device names such as `host0net0`, it is the old schema.

### 2.  Remove the protocol cluster configurations (if exist)

If protocol cluster configurations are set, remove them if possible. Otherwise, once you convert some containers (later in this workflow), you can move the protocol containers to the converted containers.

Using the `protocols` script (from the tools repository), perform the following:

1. Back up the configuration of the protocol clusters.
2. Destroy the configuration of the protocol clusters.

Note: During the conversion process, the HostIDs are changed. After the conversion, manually change the HostIDs in the configuration backup file.

### 3. Ensure failure domains are named

Only clusters with named failure domains can be converted. The conversion script does not support automatic or invalid failure domains.

1. Check the failure domain names. Run the following command line:

```sh
weka cluster host -b -v --output hostname,fd,fdName,fdType,fdId
```

<details>

<summary>Example: Named failure domains</summary>

```
$ weka cluster host -b -v --output hostname,fd,fdName,fdType,fdId
HOSTNAME                       FAILURE DOMAIN  FAILURE DOMAIN NAME  FAILURE DOMAIN TYPE  FAILURE DOMAIN ID
ip-172-31-39-174.ec2.internal  FD_16           FD_16                USER                 16
ip-172-31-36-151.ec2.internal  FD_25           FD_25                USER                 25
ip-172-31-33-66.ec2.internal   FD_63           FD_63                USER                 63
ip-172-31-39-101.ec2.internal  FD_0            FD_0                 USER                 0
ip-172-31-34-33.ec2.internal   FD_23           FD_23                USER                 23
ip-172-31-46-217.ec2.internal  FD_20           FD_20                USER                 20
ip-172-31-45-42.ec2.internal   FD_31           FD_31                USER                 31
ip-172-31-36-77.ec2.internal   FD_30           FD_30                USER                 30
```

</details>

<details>

<summary>Example: Automatic failure domains</summary>

```
# weka cluster host -b -v --output hostname,fd,fdName,fdType,fdId
HOSTNAME  FAILURE DOMAIN  FAILURE DOMAIN NAME  FAILURE DOMAIN TYPE  FAILURE DOMAIN ID
cst1      AUTO                                 AUTO                 0
cst2      AUTO                                 AUTO                 4
cst5      AUTO                                 AUTO                 2
cst6      AUTO                                 AUTO                 3
cst7      AUTO                                 AUTO                 6sh
```

</details>

<details>

<summary>Example: Invalid failure domains</summary>

Invalid failure domains may appear in clusters started on older WEKA versions.

```sh
HOST ID  HOSTNAME         CONTAINER NAME  STATUS  VERSION  MODE     FAILURE DOMAIN  FAILURE DOMAIN NAME  FAILURE DOMAIN TYPE  FAILURE DOMAIN ID
0        drp-srcf-ffb001  default         UP      3.13.6   backend                                       INVALID              10
1        drp-srcf-ffb002  default         UP      3.13.6   backend                                       INVALID              2
2        drp-srcf-ffb003  default         UP      3.13.6   backend                                       INVALID              0
3        drp-srcf-ffb004  default         UP      3.13.6   backend                                       INVALID              3
4        drp-srcf-ffb005  default         UP      3.13.6   backend                                       INVALID              13
5        drp-srcf-ffb006  default         UP      3.13.6   backend                                       INVALID              11
6        drp-srcf-ffb007  default         UP      3.13.6   backend                                       INVALID              8she
```

</details>

If the cluster has automatic or invalid failure domains, do the following:

* Ensure there are no filesystem mounts on the backends.
* Run the `change_failure_domains_to_manual.py` script from the **/tmp** directory.

This script converts each backend to a named failure domain and restarts it (rolling conversion). This operation causes a short rebuild.

<details>

<summary>Example: Change failure domains to manual (named failure domains)</summary>

```
[ec2-user@ip-172-31-34-33 postinstall]$ ./change_failure_domains_to_manual.py |
2023-01-19 15:09:31 LOG: Queried ip-172-31-39-174.ec2.internal: currently running with failure domain type AUTO (id: 16, name=)
No rebuild is currently in progress

Data in each protection level:

2 Protections [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 32.33 TiB / 32.33 TiB
1 Protections [                                        ] 0 B / 32.33 TiB
0 Protections [                                        ] 0 B / 32.33 TiB
2023-01-19 15:09:31 LOG: Cluster is fully protected (status OK)
2023-01-19 15:09:31 LOG: Change ip-172-31-39-174.ec2.internal:default failure domain to manual? [y]es / [s]kip / all>
y
2023-01-19 15:09:42 LOG: Failure domain ID of ip-172-31-39-174.ec2.internal:default is currently 16 (type=AUTO)
2023-01-19 15:09:42 LOG: New failure domain selected for ip-172-31-39-174.ec2.internal:default is FD_16 (based on ID 16)
2023-01-19 15:09:42 LOG: Changing of failure-domain on ip-172-31-39-174.ec2.internal to FD_16
2023-01-19 15:09:42 LOG: Running 'weka local resources failure-domain --name FD_16' on ip-172-31-39-174.ec2.internal (172.31.39.174) via ssh
Set failure_domain of default to FD_16
2023-01-19 15:09:43 LOG: Applying resources ip-172-31-39-174.ec2.internal
2023-01-19 15:09:43 LOG: Running 'weka local resources apply -f' on ip-172-31-39-174.ec2.internal (172.31.39.174) via ssh
default: Allocated network device "eth1" (with identifier "0000:00:06.0") to slots [1] on "ip-172-31-39-174.ec2.internal":"default" (1/7)
default: Allocated network device "eth2" (with identifier "0000:00:07.0") to slots [2] on "ip-172-31-39-174.ec2.internal":"default" (2/7)
default: Allocated network device "eth3" (with identifier "0000:00:08.0") to slots [3] on "ip-172-31-39-174.ec2.internal":"default" (3/7)
default: Allocated network device "eth4" (with identifier "0000:00:09.0") to slots [4] on "ip-172-31-39-174.ec2.internal":"default" (4/7)
default: Allocated network device "eth5" (with identifier "0000:00:0a.0") to slots [5] on "ip-172-31-39-174.ec2.internal":"default" (5/7)
default: Allocated network device "eth6" (with identifier "0000:00:0b.0") to slots [6] on "ip-172-31-39-174.ec2.internal":"default" (6/7)
default: Allocated network device "eth7" (with identifier "0000:00:0c.0") to slots [7] on "ip-172-31-39-174.ec2.internal":"default" (7/7)
default: Allocated core 6 to slot 6 on "ip-172-31-39-174.ec2.internal":"default" (1/7)
default: Allocated core 7 to slot 7 on "ip-172-31-39-174.ec2.internal":"default" (2/7)
default: Allocated core 1 to slot 1 on "ip-172-31-39-174.ec2.internal":"default" (3/7)
default: Allocated core 3 to slot 3 on "ip-172-31-39-174.ec2.internal":"default" (4/7)
default: Allocated core 2 to slot 2 on "ip-172-31-39-174.ec2.internal":"default" (5/7)
default: Allocated core 5 to slot 5 on "ip-172-31-39-174.ec2.internal":"default" (6/7)
default: Allocated core 4 to slot 4 on "ip-172-31-39-174.ec2.internal":"default" (7/7)
default: Starting hugepages allocation for "ip-172-31-39-174.ec2.internal":"default"
default: Allocated 139456MB hugepages memory from 1 NUMA nodes for "ip-172-31-39-174.ec2.internal":"default"
default: Bandwidth of "ip-172-31-39-174.ec2.internal":"default" set to unlimited
Container "default" is ready (pid = 8497)
Container "default" is RUNNING (pid = 8497)
2023-01-19 15:09:57 LOG: Waiting for container to become ready on ip-172-31-39-174.ec2.internal
2023-01-19 15:09:57 LOG: Running 'weka local status -J' on ip-172-31-39-174.ec2.internal (172.31.39.174) via ssh, capturing output
2023-01-19 15:10:01 LOG: Container on ip-172-31-39-174.ec2.internal is now ready
2023-01-19 15:10:01 LOG: Getting host-id from the current container on ip-172-31-39-174.ec2.internal
2023-01-19 15:10:01 LOG: Running 'weka debug manhole -s 0 getServerInfo' on ip-172-31-39-174.ec2.internal (172.31.39.174) via ssh, capturing output
2023-01-19 15:10:02 LOG: Waiting for host with ip-172-31-39-174.ec2.internal to become UP in 'weka cluster host'
2023-01-19 15:10:02 LOG: Running 'weka cluster host -J -F id=0' on ip-172-31-39-174.ec2.internal (172.31.39.174) via ssh, capturing output
2023-01-19 15:10:02 LOG: Validate the failure domain in the stable resources failure domain is FD_16, which means the container loaded properly with the right resources
2023-01-19 15:10:02 LOG: Running 'weka local resources --stable -J' on ip-172-31-39-174.ec2.internal (172.31.39.174) via ssh, capturing output
2023-01-19 15:10:03 LOG: Timed out waiting for the cluster to become unhealthy - Assuming it's healthy
Rebuild about to start...

Data in each protection level:

2 Protections [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] 32.85 TiB / 32.87 TiB
1 Protections [‚ñ†                                       ] 18 GiB / 32.87 TiB
0 Protections [                                        ] 0 B / 32.87 TiB
2023-01-19 15:10:08 LOG: Rebuilding at rate of 128MiB/sec (scrubber rate)
2023-01-19 15:10:08 LOG: Still has failures (status REBUILDING)
```

</details>

### 4. Convert the cluster to MCB

Before running the conversion script: `./convert_cluster_to_mbc.sh -<flag>`, adhere to the following:

* The script includes flags that change the allocated cores for each process type. It is helpful if you want to increase the number of cores allocated for the compute, frontend (for protocols), and drive processes. Leave at least two cores for the OS and protocols.\
  If you run the script without any options, it preserves the existing core settings.
* Do not use the `-s` flag without development approval.
* If the cluster configuration uses IB PKEY, running the conversion script with the `-p` flag is mandatory.
* It is recommended to convert a single backend first. If the conversion is successful, convert the rest of the cluster. Do not continue converting the cluster until you know it works fine for a single backend. Use the `-b` flag to do this single host. Ensure all the WEKA buckets are available after the conversion and the default container is removed.
* If a previous conversion fails, remove the file `/root/resources.json.backup` from **all** backends. Otherwise, use the `-f` flag in the following conversion attempt.

#### SCB to MCB conversion flags

```sh
# ./convert_cluster_to_mbc.sh -v -h
  -v force resources generator to use nics as VFs.
  -f force override of backup resources file if exist.
  -a run with active alerts.
  -s skip failed hosts (this is a dangerous flag, use with caution!).
  -d override drain grace period for s3 in seconds.
  -b to perform conversion on a single host.
  -p use only the nic identifier and don't resolve the PCI address.
  -l log file will be saved to this location instead of the current dir
  -D assign drive dedicated cores (use only for on-prem deployment, this flag overrides pinned cores).
  -F assign frontend dedicated cores (use only for on-prem deployment, this flag overrides pinned cores).
  -C assign compute dedicated cores (use only for on-prem deployment, this flag overrides pinned cores).
  -m override max memory assignment after conversion. Specify the value in GiB.
  -i path to the ssh identity file.
  -h show this help.

```

{% file src="https://content.gitbook.com/content/0yXyIrnroN3zIG3qa4W3/blobs/yKiIYKJLotXdE0VRpc1G/successfull_conversion_example.txt" %}
Successful SCB to MCB conversion example
{% endfile %}

### 5. Restore the protocol cluster configurations (if required)

If you have destroyed the protocol clusters configuration before the conversion, restore them as follows:

1. Open the backup file of the protocol cluster configurations created before the conversion. Search for the `HostId` lines and replace these to match the `frontend0` container HostIDs.\
   To retrieve the new HostIDs, run the following command line: \
| `weka cluster container -b | grep frontend0`. |
2. Run the `protocols` script from the `/tmp` directory.

## Troubleshooting

### MCB script fails

*   No pause between host deactivation and removal:

    During the conversion, it failed to remove the old host/container because it deactivated the host and then didn‚Äôt wait for the host/container to be INACTIVE.
* During the conversion of SMB hosts, it failed.

**Corrective action**

1. Clean up the host state using the following commands:

```sh
# Clean up the cluster (change XXX to host-id of OLD host, which must be DOWN)
weka cluster host deactivate XXX
weka cluster host remove XXX --no-unimprint

# Clean up the host (remove the old container)
weka local ps
sudo weka local rm default
sudo weka local enable
```

2. Continue with the conversion process.

### Clients in a bad state cause new backends to get stuck in SYNCING state

On a large cluster with about 2000 clients and above, the rebuild between each MCB conversion hangs. The reason is that the baseline configuration failed to sync to all the clients at the end of the rebuild.

**Corrective action**

1. Deactivate and remove the clients that were down or degraded.
2. Wait for the sync to finish.
3. If the issue persists, look at the WEKA cluster events and search for `NoConnectivityToLivingNode` events where the event is a `peer`.
4. Translate the node ID to a HostID and add the host to the denylist.

### Drives are not activated correctly during phase-out

When two drives or four phase out, the conversion script goes on a loop with the error message:

`‚Äú 2023-09-04 14:55:38 mbc divider - ERROR: Error querying container status and invoking scan: The following drives did not move to the new container, ['4a7d4250-179e-42b4-ab3f-90cfcb968f64'] retrying ‚Äú`

This symptom occurs because the script assumes the drives belonged to the default cluster, already deleted from the host.

**Corrective action**

1. Deactivate the phased-out drives and immediately activate them.\
   The drives automatically move to the correct container. The default container is automatically removed from the cluster.
2. If the default container is not moved as expected, manually remove the default container from the cluster configuration. Run the command: `weka cluster container remove`, with the `--no-unimprint` flag.
3. Check the host with the error.
4. If the containers do not start at boot, run the command: `weka local enable`.

### Too much memory on the Compute container

The compute container takes too long to start due to RAM allocation.

**Corrective action**

Do one of the following:

* Before starting the conversion again, reduce the memory size of each container. When the conversion completes, change the memory size back.
* Change the memory at the MCB conversion using the flag `-m 150`. This flag sets the RAM to 150 GiB. When the conversion completes, change the memory size back.

To change the RAM  to the previous size, run the command `weka local resources` for the compute container. The drive and frontend containers do not need more memory.

Note: When this error occurs, the drive container is still active, with all the drives, and the default container is stopped.
To reverse, create the other containers manually or remove the drives container and restart the default container.\
If the drives are phased out or unavailable, deactivate the phased-out drives and immediately activate them. Alternatively, remove the drives and re-add them.\
See Drives are not activated correctly during phase-out.

### Compute0 conversion failed on VMware hosts

On a Weka cluster deployed on _vsphere_, where each container is a VM, running the conversion script succeeds with `drives0` but fails on `compute0` with the following error:

```
error: Container "compute0" has run into an error: Cannot resolve PCI 0000:0c:00.0 to a valid network device

```

**Corrective action**

Run the conversion script with the `-v` flag: `./convert_cluster_to_mbc.sh -a -v`

### Missing NICs were passed issue

The conversion failed due to network interface names that contained uppercase letters. For example, `bond4-100G`.

The `resources_generator.py` script contains the following line that converts the incoming values to lowercase:

`402 parser.add_argument("--net", nargs="+", type=str.lower, metavar="net-devices"`

**Corrective action**

Do one of the following:

* Change the network interface names to lowercase. For example, from `bond4-100G` to `bond4-100g.` Add the adapter resource to the host and re-run the conversion script.
* Edit the `resources_generator.py` script line 402, replace `type=str.lower` with `type=str`, and re-run the conversion script.

```sh
2023-05-02 10:57:13 mbc divider - INFO: Running resources-generator with cmd: /bin/sh -c "/tmp/resources_generator.py --net  bond4-100G/10.18.4.1/24/255.255.255.255  --compute-dedicated-cores 11 --drive-dedicated-cores 6 --frontend-dedicated-cores 2 -f"
ERROR:resources generator:Detected net devices: {'enp1s0f1': {'mac': '1a:75:4d:14:bf:2c', 'master': 'bond4-100G'}, 'usb0': {'mac': '42:06:bf:33:9e:b5'}, 'bond4-100G': {'mac': '1a:75:4d:14:bf:2c'}, 'enp129s0f1': {'mac': '5a:e4:39:5f:be:b3', 'master': 'bond4-1G'}, 'enp1s0f0': {'mac': '1a:75:4d:14:bf:2c', 'master': 'bond4-100G'}, 'bond4-1G': {'mac': '5a:e4:39:5f:be:b3'}, 'enp129s0f0': {'mac': '5a:e4:39:5f:be:b3', 'master': 'bond4-1G'}}
ERROR:resources generator:Missing NICs were passed: {'bond4-100g'}
2023-05-02 10:57:14 mbc divider - WARNING: Something went wrong running: /bin/sh -c "/tmp/resources_generator.py --net  bond4-100G/10.18.4.1/24/255.255.255.255  --compute-dedicated-cores 11 --drive-dedicated-cores 6 --frontend-dedicated-cores 2 -f"
2023-05-02 10:57:14 mbc divider - WARNING: Return Code: 1
2023-05-02 10:57:14 mbc divider - WARNING: Output: b''
2023-05-02 10:57:14 mbc divider - WARNING: Stderr: None
Traceback (most recent call last):
  File "/tmp/mbc_divider_script.py", line 787, in <module>
    main()
  File "/tmp/mbc_divider_script.py", line 625, in main
    run_shell_command(resource_generator_command)
  File "/tmp/mbc_divider_script.py", line 44, in run_shell_command
    raise Exception("Error running command (exit code {}): {}".format(process.returncode, command))
Exception: Error running command (exit code 1): /bin/sh -c "/tmp/resources_generator.py --net  bond4-100G/10.18.4.1/24/255.255.255.255  --compute-dedicated-cores 11 --drive-dedicated-cores 6 --frontend-dedicated-cores 2 -f"
```

<!-- ============================================ -->
<!-- File 246/259: appendices_update-wms-and-wsa.md -->
<!-- ============================================ -->

---
description:
---

# Update WMS and WSA

Maintaining system security and functionality requires regular updates to the WEKA Management Station (WMS) and WEKA Software Appliance (WSA). The primary updates needed are security patches, and the update process differs for sites with Internet access and dark sites.

## Update WMS and WSA for sites with Internet access

1. **Access the system:** Log in to each system that needs to be updated.
2.  **Run the update command:** Run the following command to install the latest security patches:

    ```bash
    dnf update
    ```
3. **Reboot the servers for major OS upgrades** (for example, Rocky Linux 8.6 to 8.10):
   * To prevent downtime, use rolling reboots across the servers. Alternatively, you can take the system offline to complete the update with a single reboot.
4. **Verify the update:** Once the update is complete, confirm that all relevant patches have been applied by reviewing the update log.
5. **Repeat periodically:** To maintain system security, repeat this process regularly on all systems.

## Update WMS and WSA in dark sites

In dark sites‚Äîenvironments without Internet access‚Äîupdating requires a manual process. Administrators must download the necessary update packages from a system with Internet access, transfer them to the isolated environment, and configure the WSA/WMS to use a local repository. This method allows dark sites to manage updates securely without relying on external connectivity.

**Prerequisites**

* A basic understanding of Linux and `dnf` package management.
* Access to a Linux system with Internet connectivity.
* Access to WEKA Customer Success Team if assistance is needed.
* The WMS and WSA ISO versions is below `2.0.0` (the last three digits of the ISO file name).

* **Check the existing version:**
  * **WMS:** Log into the WMS station and run `cat /.wms-version`
  * **WSA:**  Log into the WSA station and run `cat /.version`

Note: You can also check the version from the login message. For example:
```
$ ssh weka64
Welcome to the Weka Software Appliance!
Version 1.2.6
Web console: https://weka64:9090/ or https://172.29.0.64:9090/
Last login: Tue Sep 10 23:43:40 2024 from 10.41.226.0
```

**Procedure**

1. **Copy the repository file:**
   * On a WMS/WSA system, copy the `/etc/yum.repos.d/ciq.repo` file.
   * Transfer this file to a Linux system that has Internet access.
   * Place the file in a dedicated directory (referred to as `reposdir`).
2.  **Download repository contents:**

    * Use the `reposync` command to download the contents of the patch repository.
    * Run the following command, specifying the `reposdir` directory and download path:

    ```

    ```bash
    reposync --destdir=./reposdir --download-metadata --repoid=lts-8.6-hashed-ciq_lts_86 --download-path ./weka-patches --norepopath
    ```

```

    Example output:

    ```
    LTS for Rocky Linux 8.6
| LTS for Rocky Linux 8.6 11 kB/s | 3.4 kB 00:00 |
| [SKIPPED] rocky-gpg-keys-8.6-6.el8.noarch.rpm: Already downloaded 6.8 MB/s | 12 MB 00:01 |
    [SKIPPED] ciq-rocky86-repos-8.6-6.el8.noarch.rpm: Already downloaded
    [SKIPPED] ciq-lts86-rocky-release-8.6-6.el8.noarch.rpm: Already downloaded
    [SKIPPED] stalld-1.17.1-2.el8.ciqlts.x86_64.rpm: Already downloaded
    [SKIPPED] libkadm5-1.18.2-16.el8_6.86ciq_lts.x86_64.rpm: Already downloaded
    [SKIPPED] krb5-workstation-1.18.2-16.el8_6.86ciq_lts.x86_64.rpm: Already downloaded
    [SKIPPED] krb5-server-ldap-1.18.2-16.el8_6.86ciq_lts.x86_64.rpm: Already downloaded
    [SKIPPED] krb5-server-1.18.2-16.el8_6.86ciq_lts.x86_64.rpm: Already downloaded
    [SKIPPED] krb5-pkinit-1.18.2-16.el8_6.86ciq_lts.x86_64.rpm: Already downloaded
    [SKIPPED] krb5-libs-1.18.2-16.el8_6.86ciq_lts.x86_64.rpm: Already downloaded
    [SKIPPED] krb5-devel-1.18.2-16.el8_6.86ciq_lts.x86_64.rpm: Already downloaded
    [SKIPPED] dpdk-tools-21.11-3.el8.ciqlts.x86_64.rpm: Already downloaded
    [SKIPPED] dpdk-devel-21.11-3.el8.ciqlts.x86_64.rpm: Already downloaded
| (14/2191): libXpm-devel-3.5.12-9.el8_6.0.ciqlts.x86_64.rpm 84 kB/s | 38 kB 00:00 |
| (15/2191): libXpm-3.5.12-9.el8_6.0.ciqlts.x86_64.rpm 264 kB/s | 57 kB 00:00 |
    ```
3. **Transfer repository to dark site:**
   * Copy the `weka-patches` directory to a location accessible to the WSA systems in the dark site. This can be a local repository server or a directory on the WSA image.
   * Use `tar` to archive the directory for transfer, then unpack it on the target system.
4.  **Modify WMS/WSA repository configuration:**

    * Update the `/etc/yum.repos.d/ciq.repo` file on the WMS/WSA to point to the local repository you just created.
    * Modify the `baseurl` setting to reference the local path or repository server. For example:

    ```
    baseurl=file:///root/weka-patches
    ```
5.  **Run the update:**

    * On the WMS/WSA system, run the following `dnf` command to update from the local repository:

    ```
    dnf --disablerepo=* --enablerepo=lts-8.6-hashed-ciq_lts_86 update
    ```
6. **Reboot the servers for major OS upgrades** (for example, Rocky Linux 8.6 to 8.10):
   * To prevent downtime, use rolling reboots across the servers. Alternatively, you can take the system offline to complete the update with a single reboot.

<!-- ============================================ -->
<!-- File 247/259: appendices_bios-tool.md -->
<!-- ============================================ -->

---
description:
---

# BIOS tool

## BIOS tool overview

The `bios_tool` is a command-line utility designed to manage BIOS settings on WEKA servers. It lets administrators view, set, and correct BIOS configurations across multiple servers. This tool is crucial for ensuring that servers in a cluster are properly configured and compliant with desired BIOS settings, using pre-defined configuration files or command-line inputs.

You can find the BIOS tool on GitHub at: https://github.com/weka/tools/tree/master/bios_tool.

### Features

* **View and set BIOS settings**: Inspect and apply BIOS settings across multiple servers.
* **BMC configuration**: Configure BMC (Baseboard Management Controller) to allow RedFish and IPMI Over LAN access.
* **Fix BIOS settings**: Correct any BIOS settings that deviate from specified configurations.
* **Reboot control**: Optionally reboot servers after applying BIOS changes.
* **Compare settings**: Diff the BIOS configurations between two servers to identify mismatches.
* **Support for multiple manufacturers**: Compatible with servers from manufacturers like Dell, HPE, Lenovo, and Supermicro.

## Usage

The `bios_tool` offers several command-line options to customize its behavior:

```

```bash
bios_tool [-h] [-c [HOSTCONFIGFILE]] [-b [BIOS]] [--bmc_config] [--fix] [--reboot] [--dump] [--reset_bios] [--diff DIFF DIFF] [--bmc_ips [BMC_IPS ...]] [--bmc_username BMC_USERNAME] [--bmc_password BMC_PASSWORD] [-v] [--version]
```

```

#### Parameters

 | Parameter | Description |
 | --- | --- |
 | -h, --help | Displays the help message. |
 | -c, --hostconfigfile [HOSTCONFIGFILE] | Specify the host configuration file (YAML/CSV) containing server details. |
 | -b, --bios [BIOS] | Specify the BIOS settings configuration file (YAML). |
 | --bmc_config | Enables RedFish and IPMI Over LAN on all servers. |
 | --fix | Corrects BIOS settings based on the configuration file. |
 | --reboot | Reboots the server after applying any changes to the BIOS settings. Only a server with modifications will be rebooted. |
 | --dump | Displays BIOS settings without making changes. |
 | --reset_bios | Resets BIOS to default settings. Use --reboot for automatic reboot after resetting. |
 | --diff DIFF DIFF | Compares BIOS settings between two servers. |
 | --bmc_ips [BMC_IPS ...] | List of BMC IP addresses (space-separated) to configure servers without a config file. For example, --bmc_ips 192.168.1.1 192.168.1.2. Combined with --bmc_username and --bmc_password, this option enables quick configuration of multiple servers with identical credentials, eliminating the need for a configuration file. |
 | --bmc_username--bmc_password | Credentials for BMC access. |
 | -v, --verbose | Provides verbose output. |
 | --version | Displays the current version of BIOS tool. |

## Getting started

The **bios_tool** requires two configuration files:

* **Host configuration file** (`host_config.yml` or `host_config.csv`)
* **BIOS settings configuration file** (`bios_config.yml`)

You can use the default filenames or specify custom ones using the command-line options `-c/--hostconfigfile` for the host configuration file and `-b/--bios` or `--bmc_ips` for BIOS settings.

**Host configuration (`host_config.yml` or `.csv`)**

The host configuration file defines the servers (hosts) and their corresponding BMC credentials (IPMI, iLO, iDRAC). You can use either YAML or CSV format, indicated by the file extension (`.yml` or `.csv`).

The default format is CSV, which can be easily edited in spreadsheet tools like Excel. Example CSV format:

```plaintext
name,user,password
172.29.3.164,ADMIN,_PASSWORD_1!
172.29.3.1,Administrator,Administrator
172.29.1.74,root,Administrator
172.29.1.75,root,Administrator
```

Alternatively, you can use YAML for more structured data:

```yaml
hosts:
  - name: 172.29.3.1
    user: Administrator
    password: Administrator
  - name: 172.29.3.2
    user: Administrator
    password: Administrator
```

Define all the servers you plan to manage in this file.

**BIOS configuration (`bios_config.yml` and others)**

The BIOS settings configuration file (`bios_config.yml`) specifies the BIOS configurations you want applied to the servers listed in the host configuration file.

There are additional preset files available, such as `Dell-Genoa-bios.yml` for Dell servers with Genoa AMD processors, and `WEKApod.yml` for WEKApod servers (which come pre-configured but can be applied to similar servers).

The BIOS configuration file follows standard YAML format. Example:

```yaml
server-manufacturer:
  architecture:
    setting: value
    setting: value
  architecture2:
    setting: value
    setting: value
```

The `server-manufacturer` corresponds to the `Oem` field in RedFish data, allowing compatibility with most RedFish-supported manufacturers. Supported manufacturers include Dell, HPE, Lenovo, and Supermicro, and defaults for these are included in the sample file.

The `architecture` can be either `AMD` or `Intel`(no other architectures are supported).

For a detailed example, refer to the provided `bios_config.yml`. Here‚Äôs a sample snippet:

```yaml
Dell:
  AMD:
    LogicalProc: Disabled
    NumaNodesPerSocket: "1"
    PcieAspmL1: Disabled
    ProcCStates: Disabled
    ProcPwrPerf: MaxPerf
```

### Default mode

When run without command-line options, **bios_tool** operates in a read-only mode, scanning the hosts listed in the `host_config.csv` and comparing their current BIOS settings with those specified in the `bios_settings.yml` file. No changes are applied to the servers during this process.

Example output:

```plaintext
Fetching BIOS settings of host 172.29.3.1
Fetching BIOS settings of host 172.29.3.2
Fetching BIOS settings of host 172.29.3.3
[...snip...]
No changes are needed on 172.29.3.1
No changes are needed on 172.29.3.2
No changes are needed on 172.29.3.3
[...snip...]
172.29.3.4: BIOS setting ApplicationPowerBoost is Enabled, but should be Disabled
172.29.3.4: BIOS setting CStateEfficiencyMode is Enabled, but should be Disabled
172.29.3.4: BIOS setting DataFabricCStateEnable is Auto, but should be Disabled
172.29.3.4: BIOS setting DeterminismControl is DeterminismCtrlAuto, but should be DeterminismCtrlManual
[...snip...]
```

In this mode, the tool provides a detailed report on any discrepancies between the current BIOS configurations and the desired settings, allowing you to review changes without affecting the servers.

### Optional modes

**BMC configuration mode**

Using the `--bmc_config` option, **bios_tool** will SSH into each server to enable both RedFish and IPMI Over LAN. RedFish is **required** for the tool‚Äôs operation, while IPMI Over LAN is necessary for WMS deployment, so it is automatically enabled.

**Fix mode**

The `--fix` option allows **bios_tool** to apply the BIOS settings specified in the `bios_settings.yml` file. The tool does not reboot the servers unless the `--reboot` option is also specified.

**Reboot mode**

When used with the `--fix` option, `--reboot` will instruct **bios_tool** to reboot the servers after applying any BIOS changes. Only servers that have been modified will be rebooted, ensuring the changes take effect.

**Dump mode**

The `--dump` option provides a read-only output of all current BIOS settings for each server. No changes are made to the servers in this mode.

**Diff mode**

Using `--diff` compares BIOS settings between two servers and highlights any discrepancies.

Example diff output:

```plaintext
Fetching BIOS settings of host 172.29.3.1
Fetching BIOS settings of host 172.29.3.6

Settings that are different between the servers:
Setting                     172.29.3.1                 172.29.3.6
--------------------------  -------------------------  ----------------------------
ApplicationPowerBoost       Disabled                   Enabled
CStateEfficiencyMode        Disabled                   Enabled
DataFabricCStateEnable      Disabled                   Auto
DeterminismControl          DeterminismCtrlManual      DeterminismCtrlAuto
InfinityFabricPstate        P0                         Auto
MinProcIdlePower            NoCStates                  C6
NumaGroupSizeOpt            Clustered                  Flat
NumaMemoryDomainsPerSocket  TwoMemoryDomainsPerSocket  Auto
PerformanceDeterminism      PerformanceDeterministic   PowerDeterministic
PowerRegulator              StaticHighPerf             OsControl
ProcAmdIoVt                 Disabled                   Enabled
ProcSMT                     Disabled                   Enabled
SerialNumber                MXQ2201FNK                 MXQ2201FND
Sriov                       Disabled                   Enabled
ThermalConfig               IncreasedCooling           OptimalCooling
WorkloadProfile             I/OThroughput              GeneralPowerEfficientCompute
```

<!-- ============================================ -->
<!-- File 248/259: appendices_weka-csi-plugin.md -->
<!-- ============================================ -->

---
description:
---

# WEKA CSI Plugin

The WEKA CSI Plugin interfaces Kubernetes worker nodes and control plane with the WEKA clients. It enables the creation and configuration of persistent storage external to Kubernetes. It is based on the CSI Plugin standard.

<details>

<summary>What is the CSI Plugin standard?</summary>

The CSI (Container Storage Interface) Plugin is a standardized interface that enables container orchestration platforms, such as Kubernetes, to interact with different storage systems in a vendor-agnostic manner. CSI was introduced to address the challenges of integrating and managing storage in containerized environments.

For more details, see the CSI standard specifications.

</details>

The WEKA CSI Plugin provides the following features:

* Static and dynamic volumes provisioning.
* Mount a volume as a WEKA filesystem directory, a writable snapshot of an existing filesystem, or a whole filesystem.
* Support all volume access modes: ReadWriteMany, ReadWriteOnce, and ReadOnlyMany.
* Volume expansion.
* Snapshots and volume cloning.
* Quota enforcement on:
  * Snapshot-backed persistent volumes in WEKA version 4.2 and up.
  * Filesystem-backed persistent volumes in WEKA version 4.2 and up.
  * Directory-backed persistent volumes in WEKA version 3.14 and up. See [Upgrade legacy persistent volumes for capacity enforcement](weka-csi-plugin/upgrade-legacy-persistent-volumes-for-capacity-enforcement).

## Interoperability

* CSI protocol: 1.0 - 1.2.
* Kubernetes: 1.18 - 1.3x.
* WEKA: V3.14 and up. WEKA V4.2 and up and WEKA CSI Plugin V2.0 are required to get all features.
* SELinux is supported (AppArmor for Ubuntu is not supported).
* Starting with WEKA CSI Plugin version 2.5.0, NFS transport is available. This option is intended for non-performance-critical scenarios or environments where installing the WEKA client is not feasible.

Note: The WEKA CSI Plugin replaces plugins developed earlier in the Kubernetes evolution. It replaces the `hostPath` method to expose WEKA mounts as Kubernetes volumes.

<!-- ============================================ -->
<!-- File 249/259: appendices_weka-csi-plugin_deployment.md -->
<!-- ============================================ -->

# Deployment

You can deploy the WEKA CSI Plugin using the helm chart from the official WEKA ArtifactHub repository.

## Prerequisites

Ensure the following prerequisites are met:

* The privileged mode must be allowed on the Kubernetes cluster.
* The following Kubernetes feature gates must be enabled: DevicePlugins, CSINodeInfo, CSIDriverRegistry, and ExpandCSIVolumes (all these gates are enabled by default).
* A WEKA cluster is installed.
* For snapshot and directory backing, a filesystem must be pre-defined on the WEKA cluster.
* For filesystem backing, a filesystem group must be pre-defined on the WEKA cluster.
* Your workstation has a valid connection to the Kubernetes worker nodes.
* The WEKA CSI container must run as the root user to enable the management of WEKA filesystem mounts at the Worker Node level.

### Prerequisites for using WekaFS transport

The WEKA client must be installed on the Kubernetes worker nodes. Follow these guidelines:

* It is recommended to use a WEKA persistent client (a client that is part of the cluster) rather than a stateless client. See .
* If the Kubernetes worker nodes are running on WEKA cluster backends (converged mode), ensure the WEKA processes are running before the `kubelet` process starts.

### **Prerequisites for Using NFS transport**

* The WEKA cluster must be installed and properly configured.
* The NFS protocol must be enabled on the WEKA cluster.
* An NFS interface group must be created on the WEKA cluster with at least one floating IP address. For optimal performance and load sharing, it's recommended to assign at least one IP address per protocol node in the cluster.
* NFS interface group IP addresses must be accessible from the Kubernetes cluster nodes.

### Guidelines for NFS Interface Groups and configuration

* **Setting the NFS Interface Group**\
  When defining multiple NFS interface groups on WEKA clusters, set the `pluginConfig.mountProtocol.interfaceGroupName` parameter in the `values.yaml` file to specify the desired NFS interface group name. Failure to do so will result in the use of an arbitrary NFS interface group, which may lead to performance or networking issues.
* **Configuring the NFS Client Group**\
  The WEKA CSI Plugin automatically creates an NFS Client group named `WekaCSIPluginClients`. During volume creation or publishing, the Kubernetes node IP address is added to this group. For larger deployments, instead of adding node IP addresses individually (which is more secure), consider defining a CIDR network range for the NFS Client group in the WEKA cluster. Use the `pluginConfig.mountProtocol.nfsClientGroupName` parameter in the `values.yaml` file to specify this group.
* **Manual IP Address Specification**\
  In cloud or on-premises deployments where virtual IP addresses cannot be assigned to the interface group, the WEKA CSI Plugin can still be used with NFS transport. In this case, manually specify the IP addresses of the NFS protocol nodes as NFS target IP addresses (using API secret). Note that automatic failover for NFS connections will not be available, and the failure of a protocol node may disrupt NFS connections.

## Installation

### Install CSI Snapshot Controller and Snapshot CRDs (optional)

To enable Kubernetes-controlled snapshots, install the CSI Snapshot Controller and the CSI external-snapshotter CRD manifests.

Note: On RedHat OpenShift Container Platform (OCP) those definitions might be preinstalled.

1. On the workstation you manage Kubernetes from, clone the CSI external-snapshotter from its GitHub repository.

```
git clone https://github.com/kubernetes-csi/external-snapshotter
```

2. Create and deploy the proper Custom Resource Definitions for the CSI external-snapshotter.

```

```
| kubectl -n kube-system kustomize external-snapshotter/deploy/kubernetes/snapshot-controller | kubectl create -f - |
| kubectl kustomize external-snapshotter/client/config/crd | kubectl create -f - |
```

```

Note: A directory snapshot results in a snapshot of the entire filesystem.

### WEKAFS CSI

1. On the workstation you manage Kubernetes from, add the `csi-wekafs` repository:

```
helm repo add csi-wekafs https://weka.github.io/csi-wekafs

```

2. Install the WEKA CSI Plugin. Run the following command:

```

```
helm install csi-wekafs csi-wekafs/csi-wekafsplugin --namespace csi-wekafs --create-namespace

```

```

Note: If you need SELinux support, see the [SELinux support](add-selinux-support) section.

<details>

<summary>Installation output example</summary>

Once the installation completes successfully, the following output is displayed:

```
NAME: csi-wekafs
LAST DEPLOYED: Mon May 29 08:36:19 2023
NAMESPACE: csi-wekafs
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Thank you for installing csi-wekafs.

Your release is named csi-wekafs.
The release is installed in namespace csi-wekafs

To learn more about the release, try:

  $ helm status -n csi-wekafs csi-wekafs
  $ helm get all -n csi-wekafs csi-wekafs

Official Weka CSI Plugin documentation can be found here: https://docs.weka.io/appendix/weka-csi-plugin

Examples of how to configure a storage class and start using the driver are here:
https://github.com/weka/csi-wekafs/tree/master/examples

-------------------------------------------------- NOTICE --------------------------------------------------
 | THIS VERSION INTRODUCES SUPPORT FOR ADDITIONAL VOLUME TYPES, AS WELL AS SNAPSHOT AND VOLUME CLONING CAPS |
 | TO BETTER UNDERSTAND DIFFERENT TYPES OF VOLUMES AND THEIR IMPLICATIONS, REFER TO THE DOCUMENTATION ABOVE |
 | ALSO, IT IS RECOMMENDED TO CAREFULLY GO OVER NEW CONFIGURATION PARAMETERS AND ITS MEANINGS, AS BEHAVIOR |
 | OF THE PLUGIN AND ITS REPORTED CAPABILITIES LARGELY DEPEND ON THE CONFIGURATION AND WEKA CLUSTER VERSION |
------------------------------------------------------------------------------------------------------------

-------------------------------------------------- WARNING -------------------------------------------------
 | SUPPORT OF LEGACY VOLUMES WITHOUT API BINDING WILL BE REMOVED IN NEXT MAJOR RELEASE OF WEKA CSI PLUGIN. |
 | NEW FEATURES RELY ON API CONNECTIVITY TO WEKA CLUSTER AND WILL NOT BE SUPPORTED ON API-UNBOUND VOLUMES. |
 | PLEASE MAKE SURE TO MIGRATE ALL EXISTING VOLUMES TO API-BASED SCHEME PRIOR TO NEXT VERSION UPGRADE. |
------------------------------------------------------------------------------------------------------------

```

</details>

## Upgrade from any previous version to WEKA CSI Plugin v2.0

In WEKA CSI Plugin v2.0, the CSIDriver object has undergone changes. Specifically, CSIDriver objects are now immutable. Consequently, the upgrade process involves uninstalling the previous CSI version using Helm and subsequently installing the new version. It is important to note that the uninstall operation does not delete any existing secrets, StorageClasses, or PVC configurations.

Note: If you plan to upgrade the existing WEKA CSI Plugin and enable directory quota enforcement for already existing volumes, bind the legacy volumes to a single secret. See the #bind-legacy-volumes-to-api section.

#### 1. Prepare for the upgrade

1. Uninstall the existing CSI Plugin. Run the following command line:

```
helm uninstall csi-wekafs --namespace csi-wekafs
```

2. Update the `csi-wekafs` helm repository. Run the following command line:

```
helm repo update csi-wekafs

```

3. Download the `csi-wekafs` git repository.

```
git clone https://github.com/weka/csi-wekafs.git --branch main --single-branch
```

#### 2. Install the upgraded helm release

Run the following command line:

```
helm install csi-wekafs --namespace csi-wekafs csi-wekafs/csi-wekafsplugin

```

## Upgrade from WEKA CSI Plugin WEKA 2.0 forward

If the WEKA CSI plugin source is v2.0 or higher, follow this workflow.

#### 1. Update helm repo

Run the following command line:

```
helm repo update csi-wekafs
```

#### 2. Update the helm deployment

Run the following command line:

```
helm upgrade --install csi-wekafs --namespace csi-wekafs csi-wekafs/csi-wekafsplugin
```

<details>

<summary>Output example</summary>

Once the upgrade completes successfully, the following output is displayed:

```
Release "csi-wekafs" has been upgraded. Happy Helming!
NAME: csi-wekafs
LAST DEPLOYED: Tue June  2 15:39:01 2023
NAMESPACE: csi-wekafs
STATUS: deployed
REVISION: 10
TEST SUITE: None
NOTES:
Thank you for installing csi-wekafsplugin.

Your release is named csi-wekafs.

To learn more about the release, try:

  $ helm status csi-wekafs
  $ helm get all csi-wekafs

Official Weka CSI Plugin documentation can be found here: https://docs.weka.io/appendix/weka-csi-plugin

```

</details>

#### 3. Elevate the OpenShift privileges

If the Kubernetes worker nodes run on RHEL and use OpenShift, elevate the OpenShift privileges for the WEKA CSI Plugin. (RHCoreOS on Kubernetes worker nodes supports NFS connections.)

To elevate the OpenShift privileges, run the following command lines:

```

```
oc create namespace csi-wekafs
oc adm policy add-scc-to-user privileged system:serviceaccount:csi-wekafs:csi-wekafs-node
oc adm policy add-scc-to-user privileged system:serviceaccount:csi-wekafs:csi-wekafs-controller

```

```

#### 4. Delete the CSI Plugin pods

The CSI Plugin fetches the WEKA filesystem cluster capabilities during the first login to the API endpoint and caches it throughout the login refresh token validity period to improve the efficiency and performance of the plugin.

However, the WEKA filesystem cluster upgrade might be unnoticed if performed during this time window, continuing to provision new volumes in the legacy mode.

To expedite the update of the Weka cluster capabilities, it is recommended to delete all the CSI Plugin pods to invalidate the cache. The pods are then restarted.

Run the following command lines:

```
kubectl delete pod -n csi-wekafs -lapp=csi-wekafs-controller
kubectl delete pod -n csi-wekafs -lapp=csi-wekafs-node
```

<!-- ============================================ -->
<!-- File 250/259: appendices_weka-csi-plugin_storage-class-configurations.md -->
<!-- ============================================ -->

# Storage class configurations

The WEKA CSI Plugin supports the following persistent volume types:

* **Dynamic:** Persistent Volume Claim (PVC).
* **Static:** Persistent Volume (PV).

The WEKA CSI Plugin communicates with the WEKA cluster using REST API, leveraging this integration to provide extended capabilities, such as strictly enforcing volume capacity usage through integration with filesystem directory quota functionality. For details, see [Quota management](../../weka-filesystems-and-object-stores/quota-management).

Starting from CSI Plugin **v2.0,** three StorageClass configurations are available:

* Directory-backed StorageClass
* Snapshot-backed StorageClass
* Filesystem-backed StorageClass

## API-based communication model

In the API-based model, the API endpoint addresses and authentication credentials must be provided to the WEKA CSI Plugin to establish a REST API connection with the WEKA cluster and perform configuration tasks.

The information is stored securely in Kubernetes secret, referred to by the Storage Class.

Adhere to the following:

* The configuration described in this section applies to WEKA CSI Plugin version **0.8.4** and higher. To get all features, WEKA CSI Plugin version **2.0** is required.
* Directory quota integration requires WEKA cluster version **3.13.0** and higher.
* Snapshot quota integration requires WEKA cluster version **4.2** and higher.
* Authenticated mounts for filesystems set with `auth-required=true`, and filesystems in the non-root organization, require WEKA cluster version **3.14.0** and higher.

Note: The legacy communication model is deprecated and will be removed in the next release. If you are using the legacy communication model, replacing it with the API-based one is recommended.

## Prerequisites

* To provision any persistent volume type, a Storage Class must exist in Kubernetes deployment that matches the secret name and namespace in the WEKA cluster configuration.
* For directory-backed and snapshot-backed storage class configurations, a filesystem must be pre-created on the WEKA cluster to create PVCs.
* For the filesystem-backed StorageClass configuration, the filesystem name is generated automatically based on the PVC name, but the filesystem group name must be declared in the Storage Class configuration.

## Configure secret data

1. Create a secret data file (see the following example).

<details>

<summary>Example: csi-wekafs-api-secret.yaml file</summary>

```

```
apiVersion: v1
kind: Secret
metadata:
  name: csi-wekafs-api-secret
  namespace: csi-wekafs
type: Opaque
data:
  # A username to connect to the cluster API (base64-encoded)
  username: YWRtaW4=
  # A password to connect to the cluster API (base64-encoded)
  password: YWRtaW4=
  # An organization to connect to (default Root, base64-encoded)
  organization: Um9vdA==
  # A comma-separated list of cluster management endpoints. Format: <IP:port> (base64-encoded)
  # It is recommended to configure at least 2 management endpoints (cluster backend nodes), or a load-balancer if used
  # e.g. 172.31.15.113:14000,172.31.12.91:14000
  endpoints: MTcyLjMxLjQxLjU0OjE0MDAwLDE3Mi4zMS40Ny4xNTI6MTQwMDAsMTcyLjMxLjM4LjI1MDoxNDAwMCwxNzIuMzEuNDcuMTU1OjE0MDAwLDE3Mi4zMS4zMy45MToxNDAwMCwxNzIuMzEuMzguMTU1OjE0MDAwCg==
  # protocol to use for API connection (may be either http or https, base64-encoded)
  scheme: aHR0cA==
  # for multiple clusters setup, set a specific container name (base64-encoded)
  localContainerName: ""
  # for cloud deployments with automatic healing and auto-scaling, set to "true" to enable automatic updates of the endpoints.
  # The API endpoints will be updated automatically on first connection to the cluster API, as well as on each re-login
  # maybe either (true/false), base64-encoded
  # NOTE: if a load balancer is used to access the cluster API, leave this setting as "false"
  autoUpdateEndpoints: ZmFsc2U=
  # It is recommended to configure all NFS server IP addresses to better share the load/balance the traffic.
  # NOTE: this setting is optional and should be used only when the NFS Group IP addresses are not set in the cluster
  # WARNING: providing a load balancer IP address that uses NFS connection redirects (also known as `referrals`) to other servers is not supported.
  # e.g. 10.100.100.1,10.100.100.2
  nfsTargetIps: ""
  # When using HTTPS connection and self-signed or untrusted certificates, provide a CA certificate in PEM format, base64-encoded
  # for cloud deployments or other scenarios where setting an NFS Group IP addresses is not possible,
  # provide a comma-separated list of NFS target IP addresses in form of <IP> (base64-encoded)
  # caCertificate: <base64-encoded-PEM>
  caCertificate: ""

```

```

</details>

2. Apply the secret data and validate it is created successfully.

<details>

<summary>Apply the yaml file</summary>

```
# apply the secret .yaml file
$ kubectl apply -f csi-wekafs-api-secret.yaml

# Check the secret was successfully created
$ kubectl get secret csi-wekafs-api-secret -n csi-wekafs
NAME                    TYPE     DATA   AGE
csi-wekafs-api-secret   Opaque   5      7m
```

</details>

Note: To provision CSI volumes on filesystems residing in non-root organizations or filesystems, set with `auth-required=true`. A CSI Plugin of version **0.8.4** and higher and WEKA cluster version **3.14** and higher are required.

#### Secret data parameters

All values in the secret data file must be in base64-encoded format.

 | Key | Description | Comments |
 | --- | --- | --- |
 | username | The user name for API access to the WEKA cluster. | To run the CSI Plugin in a non-default organization, OrgAdmin permission is required. In other cases, ClusterAdmin permission is required.It is recommended that you create a separate user for the CSI Plugin. See user-management. |
 | password | The user password for API access to the WEKA cluster. |  |
 | organization | The WEKA organization name for the user.For a single organization, use Root. | You can use multiple secrets to access multiple organizations, which are specified in different storage classes. |
 | scheme | The URL scheme that is used for communicating with the WEKA cluster API. | http or https can be used. The user must ensure that the WEKA cluster was configured to use the same connection scheme. As of WEKA version 4.3.0, HTTPS communication is mandatory. |
 | endpoints | Comma-separated list of endpoints consisting of IP address and port. For example, 172.31.15.113:14000,172.31.12.91:14000 | For redundancy, specify the management IP addresses of at least 2 backend servers. |
 | localContainerName | WEKA client container name for the client connected to the relevant WEKA cluster.Only required when connecting a K8s worker node to multiple WEKA clusters.For a single cluster, do not set this parameter. | All local container names relevant to a specific WEKA cluster must be the same across all k8s worker nodes.For details, see Connect k8s worker nodes to multiple WEKA clusters. |
 | autoUpdateEndpoints | Specify whether the WEKA CSI Plugin performs automatic periodic updates of API endpoints (true or false). | In cloud-based clusters with automatic scaling and healing, the IP addresses of management containers can change over time. To prevent losing API connectivity, the plugin can automatically retrieve all management container IP addresses from the cluster at login.If using an external load balancer, set this option to false. |
 | nfsTargetIps | A comma-separated list of IP addresses to use when publishing volumes over NFS. | Normally, the system automatically retrieves IP addresses from the interface group defined on the WEKA cluster, leaving this parameter empty. However, if no Virtual IP addresses are set on the interface group (for example, in cloud environments), manually provide the IP addresses in this parameter. |
 | caCertificate | custom CA certificate used to generate the HTTPS certificate for the WEKA cluster. The certificate must be in PEM format and Base64-encoded. | As of WEKA version 4.3.0, HTTPS communication is mandatory. To ensure a secure connection without bypassing certificate checks, it's recommended to provide the certificate file within a secret. |

## Connect K8s worker nodes to multiple WEKA clusters

A single K8s worker node can be connected to multiple WEKA clusters (maximum 7 clusters) simultaneously.

#### Procedure

1. For each k8s worker node, create a number of WEKA client containers according to the number of clusters you want to connect to.\
   The WEKA client container name must be according to the WEKA cluster that is connected to. For example, to connect to 7 WEKA clusters, it is required to create 7 WEKA client containers named client 1, client 2, and so on.
2. Create secret data files for each WEKA cluster. In the `localContainerName` set the relevant client container name. For example, for client 1 set the name of the client container connected to cluster 1.
3. Configure storage classes using the relevant secret data file.

Note: Filesystem names used for k8s (defined in the storage classes) must be unique across all clusters.

**Related topic**

## Configure directory-backed StorageClass

1. Create a directory-backed storage class yaml file (see the following example).

<details>

<summary>Example: storageclass-wekafs-dir-api.yaml</summary>

```

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storageclass-wekafs-dir-api
provisioner: csi.weka.io
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
parameters:
  volumeType: dir/v1
  filesystemName: default
  capacityEnforcement: HARD
  # optional parameters setting UID, GID and permissions on volume
  # UID of the volume owner, default 0 (root)
  #ownerUid: "1000"
  # GID of the volume owner, default 0 (root)
  #ownerGid: "1000"
  # permissions in Unix octal format, default "0750"
  #permissions: "0775"
  # name of the secret that stores API credentials for a cluster
  # change the name of secret to match secret of a particular cluster (if you have several Weka clusters)
  csi.storage.k8s.io/provisioner-secret-name: &secretName csi-wekafs-api-secret
  # change the name of the namespace in which the cluster API credentials
  csi.storage.k8s.io/provisioner-secret-namespace: &secretNamespace csi-wekafs
  # do not change anything below this line, or set to same parameters as above
  csi.storage.k8s.io/controller-publish-secret-name: *secretName
  csi.storage.k8s.io/controller-publish-secret-namespace: *secretNamespace
  csi.storage.k8s.io/controller-expand-secret-name: *secretName
  csi.storage.k8s.io/controller-expand-secret-namespace: *secretNamespace
  csi.storage.k8s.io/node-stage-secret-name: *secretName
  csi.storage.k8s.io/node-stage-secret-namespace: *secretNamespace
  csi.storage.k8s.io/node-publish-secret-name: *secretName
  csi.storage.k8s.io/node-publish-secret-namespace: *secretNamespace

```

```

</details>

2. Apply the directory-backed storage class and validate it is created successfully.

<details>

<summary>Apply the yaml file</summary>

```
# apply the storageclass .yaml file
$ kubectl apply -f storageclass-wekafs-dir-api.yaml
storageclass.storage.k8s.io/storageclass-wekafs-dir-api created

# check the storageclass resource has been created successfully
$ kubectl get sc
NAME                           PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass-wekafs-dir-api    csi.weka.io         Delete          Immediate           true                   75s
```

</details>

Adhere to the following:

* You can define multiple storage classes using different filesystem groups for filesystem backups.
* You can use the same secret for multiple storage classes, as long as the credentials are valid to access the filesystem.
* You can use several secret data files for different organizations on the same WEKA cluster, or for different WEKA clusters spanning across the same Kubernetes cluster.

#### Directory-backed StorageClass **parameters**

 | Parameter | Description |
 | --- | --- |
 | filesystemName | The name of the WEKA filesystem to create directories as Kubernetes volumes.The filesystem must exist on the WEKA cluster.The filesystem may not be defined as authenticated. |
 | capacityEnforcement | Possible values: HARD or SOFT.HARD: Strictly enforce quota and deny any write operation to the persistent volume consumer until space is freed.SOFT: Do not strictly enforce the quota. If the quota is reached, create an alert on the WEKA cluster. |
 | ownerUid | Effective User ID of the owner user for the provisioned CSI volume. Might be required for application deployments running under non-root accounts.Defaults to 0 CSI plugin v2.0 adds fsgroup features so this is optional. |
 | ownerGid | Effective Group ID of the owner user for the provisioned CSI volume. Might be required for application deployments running under non-root accounts.Defaults to 0 CSI plugin v2.0 adds fsgroup features so this is optional. |
 | permissions | Unix permissions for the provisioned volume root directory in octal format. It must be set in quotes. Defaults to 0775 |
 | csi.storage.k8s.io/provisioner-secret-name | Name of the K8s secret. For example, csi-wekafs-api-secret.It is recommended to use a trust anchor definition to avoid mistakes because the same value (&#x26;secretName) must be specified in the additional parameters according to the CSI specifications.Format: see Example: storageclass-wekafs-dir-api.yaml above (the additional parameters appear at the end of the example). |
 | csi.storage.k8s.io/provisioner-secret-namespace | The namespace the secret is located in.The secret may reside in the CSI plugin namespace or a different one.It is recommended to use a trust anchor definition to avoid mistakes because the same value (&#x26;secretNamespace) must be specified in the additional parameters according to the CSI specifications.Format: see Example: storageclass-wekafs-dir-api.yaml above (the additional parameters appear at the end of the example). |

## Configure snapshot-backed StorageClass

1. Create a snapshot-backed StorageClass yaml file (see the following example).

<details>

<summary>Example: storageclass-wekafs-snap-api.yaml</summary>

```

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storageclass-wekafs-snap-api
provisioner: csi.weka.io
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
parameters:
  volumeType: weka/v2  # this line can be ommitted completely

  # name of an EMPTY filesystem to provision volumes on
  filesystemName: default

  # name of the secret that stores API credentials for a cluster
  # change the name of secret to match secret of a particular cluster (if you have several Weka clusters)
  csi.storage.k8s.io/provisioner-secret-name: &secretName csi-wekafs-api-secret
  # change the name of the namespace in which the cluster API credentials
  csi.storage.k8s.io/provisioner-secret-namespace: &secretNamespace csi-wekafs
  # do not change anything below this line, or set to same parameters as above
  csi.storage.k8s.io/controller-publish-secret-name: *secretName
  csi.storage.k8s.io/controller-publish-secret-namespace: *secretNamespace
  csi.storage.k8s.io/controller-expand-secret-name: *secretName
  csi.storage.k8s.io/controller-expand-secret-namespace: *secretNamespace
  csi.storage.k8s.io/node-stage-secret-name: *secretName
  csi.storage.k8s.io/node-stage-secret-namespace: *secretNamespace
  csi.storage.k8s.io/node-publish-secret-name: *secretName
  csi.storage.k8s.io/node-publish-secret-namespace: *secretNamespace
```

```

</details>

2. Apply the snapshot-backed StorageClass and validate it is created successfully.

<details>

<summary>Apply the yaml file</summary>

```
# apply the storageclass.yaml file
$ kubectl apply -f storageclass-wekafs-snap-api.yaml
storageclass.storage.k8s.io/storageclass-wekafs-snap-api created

# check the storageclass resource has been created successfully
$ kubectl get sc
NAME                           PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass-wekafs-snap-api   csi.weka.io         Delete          Immediate           true                   75s
```

</details>

Adhere to the following:

* You can define multiple storage classes with different filesystems.
* You can use the same secret for multiple storage classes, as long as the credentials are valid to access the filesystem.
* You can use several secret data files for different organizations on the same WEKA cluster, or for different WEKA clusters spanning across the same Kubernetes cluster.

#### snapshot-backed StorageClass parameters

 | Parameter | Description |
 | --- | --- |
 | volumeType | The CSI Plugin volume type.For snapshot-backed StorageClass configurations, use weka/v2. |
 | filesystemName | The name of the WEKA filesystem to create snapshots as Kubernetes volumes.The filesystem must exist on the WEKA cluster and be empty. |
 | csi.storage.k8s.io/provisioner-secret-name | Name of the K8s secret. For example, csi-wekafs-api-secret.It is recommended to use a trust anchor definition to avoid mistakes because the same value must be specified in the additional parameters according to the CSI specifications.Format: see Example: storageclass-wekafs-snap-api.yaml above (the additional parameters appear at the end of the example). |
 | csi.storage.k8s.io/provisioner-secret-namespace | The namespace the secret is located in.The secret must be located in a different namespace than the installed CSI Plugin.It is recommended to use a trust anchor definition to avoid mistakes because the same value must be specified in the additional parameters according to the CSI specifications.Format: see Example: storageclass-wekafs-snap-api.yaml above (the additional parameters appear at the end of the example). |

## Configure filesystem-backed StorageClass

1. Create a filesystem-backed StorageClass yaml file (see the following example).

<details>

<summary>Example: storageclass-wekafs-fs-api.yaml</summary>

```

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storageclass-wekafs-fs-api
provisioner: csi.weka.io
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
parameters:
  volumeType: weka/v2  # this line can be ommitted completely

  # name of the filesystem group to create FS in.
  filesystemGroupName: default
  # minimum size of filesystem to create (preallocate space for snapshots and derived volumes)
  initialFilesystemSizeGB: "100"

  # name of the secret that stores API credentials for a cluster
  # change the name of secret to match secret of a particular cluster (if you have several Weka clusters)
  csi.storage.k8s.io/provisioner-secret-name: &secretName csi-wekafs-api-secret
  # change the name of the namespace in which the cluster API credentials
  csi.storage.k8s.io/provisioner-secret-namespace: &secretNamespace csi-wekafs
  # do not change anything below this line, or set to same parameters as above
  csi.storage.k8s.io/controller-publish-secret-name: *secretName
  csi.storage.k8s.io/controller-publish-secret-namespace: *secretNamespace
  csi.storage.k8s.io/controller-expand-secret-name: *secretName
  csi.storage.k8s.io/controller-expand-secret-namespace: *secretNamespace
  csi.storage.k8s.io/node-stage-secret-name: *secretName
  csi.storage.k8s.io/node-stage-secret-namespace: *secretNamespace
  csi.storage.k8s.io/node-publish-secret-name: *secretName
  csi.storage.k8s.io/node-publish-secret-namespace: *secretNamespace

```

```

</details>

2. Apply the filesystem-backed StorageClass and validate it is created successfully.

<details>

<summary>Apply the yaml file</summary>

```
# apply the storageclass.yaml file
$ kubectl apply -f storageclass-wekafs-fs-api.yaml
storageclass.storage.k8s.io/storageclass-wekafs-fs-api created

# check the storageclass resource has been created successfully
$ kubectl get sc
NAME                           PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass-wekafs-fs-api     csi.weka.io         Delete          Immediate           true                   75s
```

</details>

Adhere to the following:

* You can define multiple storage classes with different filesystems.
* You can use the same secret for multiple storage classes, as long as the credentials are valid to access the filesystem.
* You can use several secret data files for different organizations on the same WEKA cluster, or for different WEKA clusters spanning across the same Kubernetes cluster.

#### filesystem-backed StorageClass **parameters**

 | Parameter | Description |
 | --- | --- |
 | volumeType | The CSI Plugin volume type.For filesystem-backed StorageClass configurations, use weka/v2. |
 | filesystemGroupName | The name of the WEKA filesystem to create filesystems as Kubernetes volumes.The filesystem group must exist on the WEKA cluster. |
 | initialFilesystemSizeGB | The default size to create new filesystems.Set this parameter in the following cases:When the PVC requested size is smaller than the specified value.For additional space required by snapshots of a volume or snapshot-backed volumes derived from this filesystem. |
 | csi.storage.k8s.io/provisioner-secret-name | Name of the K8s secret. For example, csi-wekafs-api-secret.It is recommended to use a trust anchor definition to avoid mistakes because the same value must be specified in the additional parameters below, according to the CSI specifications.Format: see Example: storageclass-wekafs-snap-api.yaml above (the additional parameters appear at the end of the example). |
 | csi.storage.k8s.io/provisioner-secret-namespace | The namespace the secret is located in.The secret must be located in a different namespace than the installed CSI Plugin.It is recommended to use a trust anchor definition to avoid mistakes because the same value must be specified in the additional parameters according to the CSI specifications.Format: see Example: storageclass-wekafs-fs-api.yaml above (the additional parameters appear at the end of the example). |

<!-- ============================================ -->
<!-- File 251/259: appendices_weka-csi-plugin_dynamic-and-static-provisioning.md -->
<!-- ============================================ -->

# Dynamic and static provisioning

The section provides some examples of dynamic and static provisioning. For more examples, see https://github.com/weka/csi-wekafs/tree/main/examples.

## Dynamic provisioning

Dynamic provisioning means defining a persistent volume claim (PVC) for the pods using a storage class similar to the storage class described in the [Storage class configuration](storage-class-configurations) section.

**Procedure**

1. Create a PVC yaml file (see the following example).

<details>

<summary>Example: pvc-wekafs-dir.yaml</summary>

```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-wekafs-dir
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: storageclass-wekafs-dir-api
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
```

```

</details>

2. Apply the PVC yaml file and validate it is created successfully.

<details>

<summary>Apply the pvc .yaml file</summary>

```
# apply the pvc .yaml file
$ kubectl apply -f pvc-wekafs-dir.yaml
persistentvolumeclaim/pvc-wekafs-dir created

# check the pvc resource has been created
$ kubectl get pvc
NAME                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                       AGE
pvc-wekafs-dir        Bound    pvc-d00ba0fe-04a0-4916-8fea-ddbbc8f43380   1Gi        RWX            storageclass-wekafs-dir-api        2m10s
```

</details>

#### Persistent volume claim **parameters**

 | Parameter | Description |
 | --- | --- |
 | spec.accessModes | The volume access mode.Possible values: ReadWriteMany, ReadWriteOnce, ReadOnlyMany |
 | spec.storageClassName | The storage class to use to create the PVC.The storage class must exist. |
 | spec.resources.requests.storage | The required capacity for the volume.The capacity quota is not enforced but is stored on the filesystem directory extended and attributed for future use. |

The directory is created in the filesystem under the `csi-volumes` directory starting with the volume name.

## Static provisioning

The Kubernetes admin can prepare persistent volumes in advance to be used by pods. The persistent volume must be an existing directory, and can contain pre-populated data used by the PODs.

The persistent volume can be a directory previously provisioned by the CSI or a an existing directory in the WEKA filesystem.

To expose an existing directory in the WEKA filesystem through the CSI, define a persistent volume, and bind the persistent volume claim to this persistent volume.

Note: You can use an existing storage class from dynamic provisioning for static provisioning. However, the Persistent Volume parameters (`filesystemName`, `filesystemGroupName`, and `volumeType`) will override those in the storage class.

**Procedure**

1. Create a PV yaml file (see the following example).

<details>

<summary>Example: PV yaml file</summary>

```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-wekafs-dir-static
spec:
  storageClassName: storageclass-wekafs-dir-api
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
  capacity:
    storage: 1Gi
  csi:
    driver: csi.weka.io
    # volumeHandle must be formatted as following:
    # dir/v1/<FILE_SYSTEM_NAME>/<INNER_PATH_IN_FILESYSTEM>
    # The path must exist, otherwise publish request will fail
    volumeHandle: dir/v1/podsFilesystem/my-dir
```

```

</details>

2. Apply the PV yaml file and validate it is created successfully.

<details>

<summary>Apply the PV yaml file</summary>

```
# apply the pv .yaml file
$ kubectl apply -f pv-wekafs-dir-static.yaml
persistentvolume/pv-wekafs-dir-static created

# check the pv resource has been created
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                         STORAGECLASS                       REASON   AGE
pv-wekafs-dir-static                       1Gi        RWX            Retain           Available                                 storageclass-wekafs-dir=api                 3m33s
```

</details>

#### Persistent volume **parameters**

 | Parameter | Description |
 | --- | --- |
 | spec.accessModes | The volume access mode.Possible values: ReadWriteMany, ReadWriteOnce, ReadOnlyMany |
 | spec.storageClassName | The storage class to use to create the PV.The storage class must exist. |
 | spec.capacity.storage | A required capacity for the volume.The capacity quota is not enforced but is stored on the filesystem directory extended and attributed for future use. |
 | spec.csi.volumeHandle | The path previously created.A string containing the volumeType (dir/v1) filesystem name, and the directory path.Example: dir/v1/podsFilesystem/my-dirThe filesystem and path must exist in the WEKA cluster. |

3. Bind a PVC to this specific PV using the `volumeName` parameter under the PVC `spec` and provide it with the specific PV name.

<details>

<summary>Example: persistent volume claim for static provisioning</summary>

```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-wekafs-dir-static
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: storageclass-wekafs-dir-api
  volumeName: pv-wekafs-dir-static
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
```

```

</details>

#### Persistent volume claim for static provisioning

 | Parameter | Description |
 | --- | --- |
 | spec.accessModes | The volume access mode.Possible values: ReadWriteMany, ReadWriteOnce, ReadOnlyMany |
 | spec.storageClassName | The storage class to use to create the PVC.It must be the same storage class as the PV requested to bind in spec.volumeName. |
 | spec.resources.requests.storage | The required capacity for the volume.The capacity quota is not enforced but is stored on the filesystem directory extended and attributed for future use. |
 | spec.volumeName | The name of a pre-configured persistent volume.The persistent volume name must exist. |

4. Validate that the PVC resource is created and successfully bounded (the status is `Bound`).

<details>

<summary>Validate the PVC resource is created</summary>

```
# check the pv resource has been created
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                           STORAGECLASS                       REASON   AGE
pv-wekafs-dir-static                       1Gi        RWX            Retain           Bound       default/pvc-wekafs-dir-static   storageclass-wekafs-dir-api                 6m30s
```

</details>

<!-- ============================================ -->
<!-- File 252/259: appendices_weka-csi-plugin_tailor-your-storage-class-configuration-with-mount-options.md -->
<!-- ============================================ -->

---
description: Leverage mount options for tailored storage control with the CSI Plugin.
---

# Tailor your storage class configuration with mount options

## **Overview**

The CSI Plugin empowers you with **mount options**, allowing you to customize how WEKA volumes are presented to pods. This enables granular control over storage behavior, optimizing performance and data management for containerized workloads.

Mount options are key-value pairs specified during volume mounting that modify the default filesystem or storage provider behavior. These settings influence caching, data integrity, filesystem limits, and more.

**When to use mount options:**

* **Tailor performance:** Optimize caching strategies for read-heavy or write-intensive workloads (`noatime`, `readcache`).
* **Enhance data integrity:** Enforce data consistency and reliability (example: `sync`).
* **Troubleshoot issues:** Fine-tune settings to resolve performance bottlenecks or compatibility problems.

### **Standard mount options and use cases**

The CSI Plugin supports all standard mount options except the read-only (`ro`) option. The following table briefly lists the supported mount options for convenience.

 | Option | Description | Use cases |
 | -------------- | ---------------------------------------------------------- | ------------------------------------------------ |
 | `sync` | Ensure data is written to disk before mount | Database workloads requiring high data integrity |
 | `noatime` | Disable write timestamp updates | Reduce write amplification, improve performance |
 | `nodev` | Disallow device nodes | Security-sensitive environments |
 | `noexec` | Disallow program execution | Security-focused deployments |
 | `atime` | Enable access time recording | Monitor file access patterns |
 | `diratime` | Enable directory access time recording | Track directory access time |
 | `relatime` | Update access and modification times relative to stat time | Reduce write amplification, improve performance |
 | `data=ordered` | Ensure sequential writes are flushed to disk immediately | Databases requiring strict write ordering |

## **Set custom mount options with CSI Plugin**

This example procedure demonstrates how to set custom mount options using the WEKA CSI Plugin.

#### **Prerequisites:**

* The Kubernetes environment is set up and accessible.
* The kubectl command-line tool is installed and configured.

#### **Procedure:**

1.  **Create StorageClass:**

    a. Open or create a YAML file for your StorageClass definition (for example, `storageclass-wekafs-mountoptions.yaml`).

    b. Add the following content to define the StorageClass with custom mount options:

    ```

    ```yaml
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: storageclass-wekafs-mountoptions
    provisioner: csi.weka.io
    parameters:
      mountOptions: "rw,relatime,readcache,noatime,readahead_kb=32768,dentry_max_age_positive=1000,dentry_max_age_negative=0"
    ```

```

````
c. Apply the StorageClass using the following command:

```bash
kubectl apply -f storageclass-wekafs-mountoptions.yaml
```
````

2\. **Create CSI secret:**\
a. Execute the following command to create a CSI secret named `csi-wekafs-api-secret` (located in ../common/csi-wekafs-api-secret.yaml):

````
```bash
kubectl apply -f ../common/csi-wekafs-api-secret.yaml
```

This step ensures that the necessary credentials are available for the CSI Plugin.
````

3\. **Provision a new volume:**

````
Apply the StorageClass to provision a new volume. Use the following command:

```bash
kubectl apply -f <FILE>.yaml
```

* Replace `<FILE>` with the path to your YAML file containing the Persistent Volume Claim (PVC) definition.
````

4\. **Create application:**

````
a. Create an application manifest file (for example, `csi-app-fs-mountoptions.yaml`) or use an existing one.

b. In the manifest, specify the PVC with the custom mount options:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: csi-app-fs-mountoptions
spec:
  replicas: 1
  selector:
    matchLabels:
      app: csi-app-fs-mountoptions
  template:
    metadata:
      labels:
        app: csi-app-fs-mountoptions
    spec:
      containers:
      - name: csi-app-fs-mountoptions
        image: <YOUR_IMAGE>
        volumeMounts:
        - mountPath: "/data"
          name: wekafs-volume
      volumes:
      - name: wekafs-volume
        persistentVolumeClaim:
          claimName: pvc-wekafs-fs-mountoptions
```

* Replace `<YOUR_IMAGE>` with the desired container image.

c. Deploy the application:

```bash
kubectl apply -f csi-app-fs-mountoptions.yaml
```
````

5\. **Attach and validate:**

````
Attach to the application pod:

```bash
kubectl exec csi-app-fs-mountoptions -- mount -t wekafs
```

b. Verify that the output resembles to the following example:

```bash
csivol-pvc-15a45f20-Z72GJXDCEWQ5 on /data type wekafs (rw,relatime,readcache,noatime,readahead_kb=32768,dentry_max_age_positive=1000,dentry_max_age_negative=0)

```
````

<!-- ============================================ -->
<!-- File 253/259: appendices_weka-csi-plugin_launch-an-application-using-weka-as-the-pods-storage.md -->
<!-- ============================================ -->

# Launch an application using WEKA as the POD's storage

Once a storage class and a PVC are in place, you can configure the Kubernetes pods to provision volumes via the WEKA system.

We'll take an example application that echos the current timestamp every 10 seconds and provide it with the previously created `pvc-wekafs-dir` PVC.

Note that multiple pods can share a volume produced by the same PVC as long as the `accessModes` parameter is set to `ReadWriteMany`.

```

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-csi-app
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: "/data"
        name: my-csi-volume
      command: ["/bin/sh"]
      args: ["-c", "while true; do echo `date` >> /data/temp.txt; sleep 10;done"]
  volumes:
    - name: my-csi-volume
      persistentVolumeClaim:
        claimName: pvc-wekafs-dir # defined in pvc-wekafs-dir.yaml
```

```

Create the pod by applying manifest:

```yaml
$ kubectl apply -f csi-app-on-dir.yaml
pod/my-csi-app created
```

Kubernetes allocates a persistent volume and attach it to the pod, it uses a directory within the WEKA filesystem as defined in the storage class mentioned in the persistent volume claim. The pod is in `Running` status, and the `temp.txt` file is updated with occasional `date` information.

```
$ kubectl get pod my-csi-app
NAME                      READY   STATUS              RESTARTS   AGE
my-csi-app                1/1     Running             0          85s

# if we go to a wekafs mount of this filesystem we can see a directory has been created
$ ls -l /mnt/weka/podsFilesystem/csi-volumes
drwxr-x--- 1 root root 0 Jul 19 12:18 pvc-d00ba0fe-04a0-4916-8fea-ddbbc8f43380-a1659c8a7ded3c3c05d6facffd69cbf79b95604c

# inside that directory, the temp.txt file from the running pod can be found
 $ cat /mnt/weka/podsFilesystem/csi-volumes/pvc-d00ba0fe-04a0-4916-8fea-ddbbc8f43380-a1659c8a7ded3c3c05d6facffd69cbf79b95604c/temp.txt
Sun Jul 19 12:50:25 IDT 2020
Sun Jul 19 12:50:35 IDT 2020
Sun Jul 19 12:50:45 IDT 2020
```

<!-- ============================================ -->
<!-- File 254/259: appendices_weka-csi-plugin_nfs-transport-failback.md -->
<!-- ============================================ -->

# NFS transport failback

## Overview

While using the native WekaFS driver as the storage connectivity layer is the recommended approach for integrating WekaFS with Kubernetes, version 2.5.0 introduces the option to utilize the WEKA CSI Plugin over NFS transport. This allows you to employ WekaFS as a storage backend for your Kubernetes cluster without requiring the installation of the WEKA client on every Kubernetes node.

### Benefits of using WEKA CSI Plugin with NFS transport

* **Simplified deployment**: Eliminate the need to install the WEKA client on each Kubernetes node.
* **Interoperability**: Use the WEKA CSI Plugin on nodes where the WEKA client is not yet installed or currently unsupported.
* **Flexibility**: Leverage the WEKA CSI Plugin with NFS transport for specific use cases while continuing to use the native WekaFS driver for others.
* **Performance**: Mount pods across multiple IPs within the same NFS interface group, maximizing performance and simplifying management.
* **Ease of migration**: Use the WEKA CSI Plugin with NFS transport as an interim solution while transitioning to the native WekaFS driver. Once the WEKA client is deployed on all nodes, you can switch to the native WekaFS driver without altering the storage configuration‚Äîsimply reboot the node.

### Limitations and constraints

Note: As of version 2.5.0 and until further notice, publishing snapshot-backed volumes via NFS transport is not recommended. This is an open issue currently under investigation.

* **Feature parity**: Certain features and capabilities available with the native WekaFS driver may be absent when using the WEKA CSI Plugin with NFS transport.
* **Complexity**: NFS transport necessitates additional configuration on the WEKA cluster and may require further networking setup on the Kubernetes cluster.
* **Interoperability**: A single Kubernetes node cannot use both NFS and WekaFS transport simultaneously.
* **Migration**: Transitioning from NFS transport to WekaFS transport requires rebooting the Kubernetes nodes after the WEKA client has been deployed.
* **Network configuration**: NFS interface group IP addresses must be accessible from the Kubernetes cluster nodes.
* **Security**: NFS transport is generally less secure than the native WekaFS driver and may necessitate additional security measures.
* **Quality of Service (QoS)**: QoS is not supported with NFS transport.

### Host Network mode

The WEKA CSI Plugin installs automatically in `hostNetwork` mode when using NFS transport. `hostNetwork` mode is required for NFS transport, so the `hostNetwork` parameter in the `values.yaml` file is ignored.

### Security considerations

The WEKA CSI Plugin with NFS transport uses NFSv4.1 (default) or NFSv3 protocols to connect to the WEKA cluster. However, support for Kerberos authentication is not available in this version of the WEKA CSI Plugin. Therefore, it is recommended to use NFS transport only in secure and trusted networks.

### Interoperability with WekaFS driver

The WEKA CSI Plugin with NFS transport is fully interoperable with the native WekaFS driver. This allows you to use both WekaFS transport and NFS within the same Kubernetes cluster, including the ability to publish the same volume to different pods using different transport layers from different nodes. However, only one transport layer can be used on a single node at any given time.

#### Mount options

Mount options for NFS transport are automatically set by the WEKA CSI Plugin. When custom mount options are specified in the storage class, the WEKA CSI Plugin translates them into NFS alternatives. Any unknown or unsupported mount options are ignored.

#### QoS and performance

Quality of Service (QoS) is not supported for NFS transport. Performance is constrained by the NFS protocol and the network configuration.

#### Scheduling workloads by transport protocol

The WEKA CSI Plugin node server populates the transport protocol in the node topology. This feature enables the use of node affinity rules to direct workloads to use WekaFS transport on nodes where the WEKA client is installed. For example, in hybrid deployments where some nodes have the WEKA client installed while others do not, administrators may prefer to run storage-intensive workloads on nodes with the WEKA client, allowing more generic workloads to operate on other nodes.

To enforce a specific transport type for a workload, a `nodeSelector` can be applied in the workload manifest.

**Example: a nodeSelector for WekaFS transport**

```yaml
nodeSelector:
  topology.weka.com/transport: wekafs
```

**Example: a nodeSelector for NFS transport**

```yaml
nodeSelector:
  topology.weka.com/transport: nfs
```

#### Switching from NFS to WekaFS transport

To switch from NFS to WekaFS transport, follow these steps:

1. Install the WEKA client on the Kubernetes node.
2. Reboot the Kubernetes node.

After rebooting, the WEKA CSI Plugin automatically switches to WekaFS transport. Existing volumes can be reattached to the pods without any changes.

## Minimum prerequisites for using WEKA CSI Plugin with NFS transport

* **WEKA cluster installation**: The WEKA cluster must be installed and configured.
* **NFS protocol configuration**: The NFS protocol must be configured on the WEKA cluster.
* **NFS interface group creation**: An NFS interface group must be created on the WEKA cluster.
* **Accessibility of IP addresses**: NFS interface group IP addresses must be accessible from the Kubernetes cluster nodes.

Adhere to the following considerations:

* **Interface group name configuration**: If multiple NFS interface groups are defined, set the `pluginConfig.mountProtocol.interfaceGroupName` parameter to the desired NFS interface group name in the `values.yaml` file. If this parameter is not set, an arbitrary NFS interface group is used, which may lead to performance or networking issues.
* **NFS client group**: The plugin automatically creates an NFS client group called `WekaCSIPluginClients`. During each volume creation or publishing, the Kubernetes node IP address is added to this group.
* **IP address configuration**: While adding node IP addresses one by one is the most secure method for configuring the NFS client group, it can be cumbersome for large deployments. In such cases, it is recommended to use a network range (CIDR). Predefine the NFS client group with a network range in the WEKA cluster, then specify the NFS client group name using the `pluginConfig.mountProtocol.nfsClientGroupName` parameter in the `values.yaml` file.

## WEKA CSI Plugin operation over NFS transport

Upon starting, the WEKA CSI Plugin performs the following steps:

1. **Check WEKA client installation**: Verifies if the WEKA client is installed on the Kubernetes node.
2. **NFS failback check**:
   * If the WEKA client is not set up, the plugin checks if NFS failback is enabled or if NFS use is forced.
   * If NFS failback is enabled, the plugin uses NFS transport for volume provisioning and publishing.
   * If NFS failback is disabled, the plugin does not start and logs an error message. See the  section to enable NFS failback.

When NFS mode is enabled, the plugin uses NFS transport for all volume operations. For any volume creation or publishing request, the WEKA CSI plugin performs the following:

1. **Connect to WEKA Cluster API**: Fetch interface groups and their IP addresses. If an interface group name is specified in the `values.yaml` file, the plugin uses that; otherwise, it selects an arbitrary interface group.
2. **Client group verification**: Ensure the Client Group exists on the WEKA cluster. If it does not, the plugin creates it.
3. **Determine node IP address**: Identify the node IP address facing the interface group IP addresses by checking the node's network configuration. The plugin issues a UDP connection to one of the interface group IP addresses, using the determined source IP address as the node IP address.
4. **Add node IP to client group**: Confirm the node IP address is included in the Client Group. If not, the plugin adds it. If the Client Group already contains the node IP address or a matching CIDR definition, this step is skipped.
   * **Example**: For two nodes with IP addresses 192.168.100.1 and 192.168.200.1, if the Client Group has a rule for 192.168.100.0/255.255.255.0, no new rule is added for the first node. However, for the second node, a new rule 192.168.200.1/255.255.255.255 is created.
5. **Identify filesystem**: Determine the filesystem name to be mounted, either from StorageClass parameters (during provisioning) or from the Volume Handle (when publishing an existing volume).
6. **NFS permissions**: Ensure that NFS permissions are granted for the Client Group to access the filesystem. If permissions are not set, the plugin establishes them. If permissions are already in place, this step is skipped.
7. **Select random IP address**: Choose a random IP address from the selected NFS interface group to be used for mounting the filesystem.
8. **NFS mount operation**: Perform the NFS mount operation on the Kubernetes node using the selected IP address and filesystem name.
9. **Subsequent operations**:
   * Execute remaining operations similarly to how they would be done with the native WekaFS driver.
   * If a client group name is specified in the `values.yaml` file, the plugin uses that name; otherwise, it defaults to the `WekaCSIPluginClients` client group.

## NFS permissions required for WEKA CSI Plugin

The WEKA CSI Plugin requires and it sets the following NFS permissions on the WEKA cluster:

1. **Client Group**: `WekaCSIPluginClients` (or custom client group name if set in the `values.yaml` file)
2. **Filesystem**: The filesystem name to be mounted
3. **Path**: `/` (root of the filesystem)
4. **Type**: `RW`
5. **Priority**: No priority set
6. **Supported Versions**: `V3, V4`
7. **User Squash**: `None`
8. **Authentication Types**: `NONE`, `SYS`

Note: WEKA NFS servers evaluate permissions based on the order of the permissions list. If multiple permissions match the IP address of a Kubernetes node and the filesystem, conflicts may arise.
To avoid this, refrain from creating additional permissions for the same filesystem. Additionally, if multiple client groups are in use, ensure that the IP addresses (or ranges of IP addresses) do not overlap between the client groups.

## WEKA cluster preparation

Before using the WEKA CSI Plugin with NFS transport, prepare the WEKA cluster for NFS access. This preparation involves:

* Configuring the NFS protocol on the WEKA cluster.
* Creating an NFS interface group.
* Configuring at least one Group IP address.

In cloud deployments where setting a Group IP address is not possible, you can use the WEKA server IP addresses instead. In this case, set the IP addresses through the API secret, which will replace the Group IP addresses. This configuration can be done by providing the `nfsTargetIps` parameter in the API secret. See  for more information.

Note: Using an NFS load balancer that redirects NFS connections to multiple WEKA servers (also known as NFSv4 directory referrals) is not supported.

## Install the WEKA CSI Plugin with NFS transport

By default, the WEKA CSI Plugin components do not start if a WEKA driver is detected on a Kubernetes node. This prevents potential misconfigurations where volumes may be provisioned or published on a node without an installed WEKA client.

**Procedure**

1. **Configure NFS failback:** Explicitly configure the WEKA CSI Plugin to use NFS failback by setting the `pluginConfig.mountProtocol.allowNfsFailback` parameter to `true` in the `values.yaml` file.
2. **Set NFS transport enforcement (optional):** If you want to enforce the use of NFS transport even when the WEKA client is installed on the node, set the `pluginConfig.mountProtocol.useNfs` parameter to `true`. This option is recommended for testing purposes only.
3. **Follow Helm installation instructions:** Follow the Helm installation instructions to install the WEKA CSI Plugin. Most installation steps are similar to those for the native WekaFS driver.
4. **Set additional parameters:** Set any additional parameters in the `values.yaml` file or pass them as command-line arguments to the Helm install command.
5. **Run the Helm install command:** Run the following example Helm install command for using NFS transport:

```

```bash
helm upgrade csi-wekafs -n csi-wekafs --create-namespace --install csi-wekafs/csi-wekafsplugin csi-wekafs \
--set logLevel=6 \
--set pluginConfig.mountProtocol.allowNfsFailback=true \
--set pluginConfig.allowInsecureHttps=true \
[ --set pluginConfig.mountProtocol.interfaceGroupName=MyInterfaceGroup \ ]
# optional, recommended if multiple interface groups are defined
[ --set pluginConfig.mountProtocol.clientGroupName=MyClientGroup \ ]
# optional, recommended if the client group is predefined
```

```

<!-- ============================================ -->
<!-- File 255/259: appendices_weka-csi-plugin_upgrade-legacy-persistent-volumes-for-capacity-enforcement.md -->
<!-- ============================================ -->

# Upgrade legacy persistent volumes for capacity enforcement

## Bind legacy volumes to API

Capacity enforcement and integration with the WEKA filesystem directory quotas require the following prerequisites:

* WEKA CSI Plugin version **0.7.0** and up.
* WEKA software version **v3.13.0** and up.
* WEKA CSI Plugin can communicate with the WEKA filesystem using REST API and correlate between a specific persistent volume and the WEKA cluster serving this volume.

In the [API-Based communication model](../storage-class-configurations#api-based-communication-model), Kubernetes StorageClass refers to a secret that specifies all the required parameters for API calls to the WEKA cluster. However, this is different from the situation in the legacy communication model, where the storage class doesn't specify the API credentials.

Kubernetes does not allow modifying the StorageClass parameters; hence every volume created with the legacy-model storage class never reports its credentials.

WEKA CSI Plugin **0.7.0** provides a unique configuration mode in which legacy volumes can be bound to a single secret, referring to a single WEKA cluster API connection parameters. In this configuration mode, every request to serve, such as create, delete, and expand, a legacy Persistent Volume (or Persistent Volume Claim) that originates from a Legacy Storage Class (without reference to an API secret) communicates to that cluster.

Note: Volumes provisioned by the CSI Plugin of version **0.7.0** in the API-Based communication model, but on older versions of the WEKA cluster (below version **3.13.0**), are still in legacy mode. However, because the storage class already contains the secret reference, specifying the `legacyVolumeSecretName` parameter is unnecessary, and you can safely skip to the next procedure #migrate-legacy-volumes.

To bind legacy volumes to a single secret, perform the following:

1. Create a Kubernetes secret that describes the API communication parameters for legacy volumes. Adhere to the following:
   * The format of the secret is identical to the secret defined in the API-Based Communication Model section.
   * This secret must be located in the same Kubernetes namespace of the WEKA CSI Plugin.
2. Set the `legacyVolumeSecretName` parameter to match the secret's name above during the plugin upgrade or installation. Do one of the following:
   * You can modify the `values.yaml` directly.
   * Create the Kubernetes secret and only then set the `legacyVolumeSecretName` parameter during the Helm upgrade as follows:

```
helm upgrade csi-wekafs --namespace csi-wekafs csi-wekafs/csi-wekafsplugin \
 --set legacyVolumeSecretName="csi-wekafs-api-secret"

```

Note: If you do not create the Kubernetes secret before executing the helm upgrade, the CSI Plugin components remain `Pending` after the upgrade.

## Migrate legacy volumes

Once you bind legacy volumes to a single secret procedure, you can migrate the volumes by binding a new WEKA filesystem directory quota object to an existing persistent volume.

WEKA provides a migration script that automates the process.

Run the migration procedure only once from any Linux server connected to the same WEKA cluster. Additional script runs migrate only those volumes created in legacy mode after migration. It is safe to run the migration script multiple times, although usually, this is not required.

The migration process might take significant time and depends on the number of persistent volumes and their capacity. The migration process is transparent and does not require downtime.

#### Before you begin

The migration script requires several dependencies, which must be installed in advance: `jq`, `xattr`, `getfattr`, and `setfattr.`

Refer to the specific OS package management documentation to install the necessary packages.

#### Procedure

1. Check the `csi-wekafs` repository from any server connected to the WEKA cluster:

```
git clone https://github.com/weka/csi-wekafs.git
```

2. Run the migration script using the following command line (for multiple filesystems, run the command line for each filesystem):

```
$ sudo migration/migrate-legacy-csi-volumes.sh <filesystem_name> [--csi-volumes-dir <csi_volumes_dir>] [--endpoint-address BACKEND_IP_ADDRESS:BACKEND_PORT]
```

Where:

* `<filesystem_name>`: Specifies the filesystem name on which the CSI volumes are located.
* `<csi_volumes_dir>`: Optional parameter. Specifies the directory in the filesystem where the CSI volumes are stored. Set this parameter only if the directory differs from default values.

Note: On a stateless client, you must specify the `--endpoint-address` to successfully mount a filesystem. However, if the container is part of the WEKA cluster (either client or backend), this is not necessary.

Example:

```
$ ./migrate-legacy-csi-volumes.sh default
Weka CSI Volume migration utility. Copyright 2021 Weka
[2021-11-04 14:33:04] NOTICE     Initializing volume migration for filesystem default
[2021-11-04 14:33:04] NOTICE     Successfully mounted filesystem default
[2021-11-04 14:33:04] NOTICE     Starting Persistent Volume migration
[2021-11-04 14:33:04] INFO       Processing directory 'pvc-e5379b17-4612-4fa3-aa57-64d5b37d7f57-1025f14ca92d2e18dd92a05efadf15a4972675f0'
[2021-11-04 14:33:04] INFO       Creating quota of 1073741824 bytes for directory pvc-e5379b17-4612-4fa3-aa57-64d5b37d7f57-1025f14ca92d2e18dd92a05efadf15a4972675f0
[2021-11-04 14:33:05] INFO       Quota was successfully set for directory pvc-e5379b17-4612-4fa3-aa57-64d5b37d7f57-1025f14ca92d2e18dd92a05efadf15a4972675f0
[2021-11-04 14:33:05] NOTICE     Migration process complete!
[2021-11-04 14:33:05] NOTICE     1 directories migrated successfully
[2021-11-04 14:33:05] NOTICE     0 directories skipped
```

<!-- ============================================ -->
<!-- File 256/259: appendices_weka-csi-plugin_add-selinux-support.md -->
<!-- ============================================ -->

# Add SELinux support

Security-Enhanced Linux (SELinux) is a Linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (MAC).

To add SELinux support, perform the following procedures:

1. Install a custom SELinux policy.
2. Install and configure the WEKA CSI Plugin.
3. Test the WEKA CSI Plugin operation.

### Install a custom SELinux policy <a href="#install-a-custom-selink-p" id="install-a-custom-selink-p"></a>

1. Distribute the SELinux policy package to all Kubernetes nodes using one of the following options:
   *   Clone WEKA CSI Plugin Github repository:

       ```
       git clone https://github.com/weka/csi-wekafs.git
       ```
   * Copy the content of the `selinux` directory directly to Kubernetes nodes
2.  Apply the policy package directly:

    ```
    $ semodule -i csi-wekafs.pp
    ```

    Verify that the policy is applied correctly:

    ```
| $ getsebool -a | grep wekafs |
    container_use_wekafs --> off
    ```

    If the output matches mentioned above, skip to step 4. Otherwise, proceed to step 3 to build the policy from the sources.
3.  In certain circumstances, the pre-compiled policy installation could fail. For example, in a different Kernel version or Linux distribution. In this case, build the policy and install it from the source using the following steps:

    ```
    $ checkmodule -M -m -o csi-wekafs.mod csi-wekafs.te
    $ semodule_package -o csi-wekafs.pp -m csi-wekafs.mod
    $ make -f /usr/share/selinux/devel/Makefile csi-wekafs.pp
    $ semodule -i csi-wekafs.pp
    ```

    For this purpose, the `policycoreutils-devel` package (or its alternative in case of Linux distribution different from the Red Hat family) is required.

    Verify that the policy is applied correctly:

    ```
| $ getsebool -a | grep wekafs |
    container_use_wekafs --> off
    ```
4.  The policy provides a boolean setting that allows on-demand enablement of relevant permissions. To enable WekaFS CSI volumes access from pods, run the command:

    ```
    $ setsebool container_use_wekafs=on
    ```

    To disable access, perform the command:

    ```
    $ setsebool container_use_wekafs=off
    ```

    The configuration changes are applied immediately.

### Install and configure the WEKA CSI Plugin <a href="#install-config-csi-plugin" id="install-config-csi-plugin"></a>

1. To label volumes correctly, install the WEKA CSI Plugin in an SELinux-compatible mode. To do that, set the `selinuxSupport` value to `"enforced"` or `"mixed‚Äù` by editing the file `values.yaml` or passing the parameter directly in the `helm` installation command.

Example:

```
$ helm install --upgrade csi-wekafsplugin csi-wekafs/csi-wekafsplugin --namespace csi-wekafsplugin --create-namespace --set selinuxSupport=enforced
```

Follow these considerations:

* WEKA CSI Plugin supports both the `enforced` and `mixed` modes of `selinuxSupport`. The installation depends on the following mode settings:
  * When `selinuxSupport` is `enforced`, only SELinux-enabled CSI plugin node components are installed.
  * When `selinuxSupport` is `mixed`, both non-SELinux and SELinux-enabled components are installed.
  * When `selinuxSupport` is `off`, only non-SELinux CSI plugin node components are installed.
*   The SELinux status cannot be known from within the CSI plugin pod. Therefore, a way of distinguishing between SELinux-enabled and non-SELinux nodes is required. WEKA CSI Plugin relies on the node affinity mechanism by matching the value of a certain node label in a mutually exclusive way. Only when the label exists and is set to true, an SELinux-enabled node component will start on that node. Otherwise, the non-SELinux node component will start.

    To ensure that the plugin starts in compatibility mode, set the following label on each SELinux-enabled Kubernetes node:
* If a node label is modified after installing the WEKA CSI Plugin node component on that node, terminate the csi-wekafs-node-XXXX component on the affected node. As a result, a replacement pod is automatically scheduled on the node but with the correct SELinux configuration.

```
csi.weka.io/selinux_enabled="true"
```

*   If another label stating SELinux support is already maintained on nodes, you can modify the expected label name in the `selinuxNodeLabel` parameter by editing the file `values.yaml` or by setting it directly during the WEKA CSI Plugin installation.

    Example:

```
$ helm install --upgrade csi-wekafsplugin csi-wekafs/csi-wekafsplugin --namespace csi-wekafsplugin --create-namespace --set selinuxSupport=mixed --set selinuxNodeLabel="selinux_enabled"
```

* If a node lab

### Test the WEKA CSI plugin operation <a href="#test-csi-plugin" id="test-csi-plugin"></a>

1. Make sure you have configured a valid CSI API `secret`. Create a valid WEKA CSI Plugin `storageClass`.
2. Provision a `PersistentVolumeClaim`.
3. Provision a `DaemonSet` to enable access to all pods on all nodes.
4.  Monitor the pod logs using the following command (expect no printing in the log files):

    ```
    $ kubectl logs -f -lapp=csi-daemonset-app-on-dir-api
    ```

    If the command returns a repeating message like the following one, it is most likely that the node on which the relevant pod is running is misconfigured:

    ```
    /bin/sh: can't create /data/csi-wekafs-test-api-gldmk.txt: Permission denied
    ```
5.  Obtain the node name from the pod:

    ```
    $ kubectl get pod csi-wekafs-test-api-gldmk -o wide
    NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE         NOMINATED NODE   READINESS GATES
    csi-wekafs-test-api-gldmk   1/1     Running   0          98m   10.244.15.2   don-kube-8   <none>           <none>
    ```
6.  Connect to the relevant node and check if the WEKA CSI SELinux policy is installed and enabled:

    ```
| $ getsebool -a | grep wekafs |
    container_use_wekafs --> on
    ```

    * If the result matches the example, proceed to the next step.
    * If there is no result, the policy is not installed. Perform the Install a custom SELinux policy procedure.
    *   If the policy is off, enable it and check the pod output again by running:

        ```
        $ setsebool container_use_wekafs=on
        ```
7.  Check if the node is labeled with the plugin is operating in SELinux-compatible mode by running the following command:

    ```
| $ kubectl describe node don-kube-8 | grep csi.weka.io/selinux_enabled |
                 csi.weka.io/selinux_enabled=true
    ```

    *   If the output is empty, Perform the Install and configure the Weka CSI Plugin procedure.

        If the label was missing and added by you during troubleshooting, the CSI node server component must be restarted on the node.\
        Perform the following command to terminate the relevant pod, and another instance will start automatically:

```

```
| $ POD=$(kubectl get pod -n csi-wekafs -lcomponent=csi-wekafs-node -o wide | grep -w don-kube-8 | cut -d" " -f1) |
$ kubectl delete pod -n csi-wekafs $POD
```

```

8. Collect CSI node server logs from the matching Kubernetes nodes and contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team).

```
| $ POD=$(kubectl get pod -n csi-wekafs -lcomponent=csi-wekafs-node -o wide | grep -w don-kube-8 | cut -d" " -f1) |
$ kubectl logs -n csi-wekafs -c wekafs $POD > log.txt
```

<!-- ============================================ -->
<!-- File 257/259: appendices_weka-csi-plugin_troubleshooting.md -->
<!-- ============================================ -->

# Troubleshooting

you can use the following basic commands to check the status and debug the service:

```
# get all resources
kubectl get all --all-namespaces

# get all pods
kubectl get pods --all-namespaces -o wide

# get all k8s nodes
kubectl get nodes

# get storage classes
$ kubectl get sc

# get persistent volume claims
$ kubectl get pvc

# get persistent volumes
$ kubectl get pv

# kubectl describe pod/<pod-name> -n <namespace>
kubectl describe pod/csi-wekafsplugin-dvdh2 -n csi-wekafsplugin

# get logs from a pod
kubectl logs <pod name> <container name>

# get logs from the weka csi plugin
# container (-c) can be one of: [node-driver-registrar wekafs liveness-probe csi-provisioner csi-attacher csi-resizer]
kubectl logs pods/csi-wekafsplugin-<ID> --namespace csi-wekafsplugin -c wekafs
```

## Known issues

### Mixed hugepages size issue in Kubernetes v1.18 and below

Due to a Kubernetes v1.18 issue with allocating mixed hugepages sizes, the WEKA cluster must not allocate mixed sizes of hugepages on the Kubernetes nodes.

#### Workaround

Only if the default memory for the client is increased, do one of the following:

* If the WEKA client is installed on the K8s nodes by a manual stateless client mount, set the `reserve_1g_hugepages` mount option to `false` in the mount command.
* If this is a WEKA server or a WEKA client part of the WEKA cluster, contact the [Customer Success Team](../../../support/getting-support-for-your-weka-system#contact-customer-success-team).

Advanced examples and detailed instructions are also available at https://github.com/weka/csi-wekafs/tree/main/examples.

<!-- ============================================ -->
<!-- File 258/259: readme.md -->
<!-- ============================================ -->

# WEKA v5.0 documentation

Welcome to the WEKA documentation portal, your guide to the latest WEKA version. Whether you're a newcomer or a seasoned user, explore topics from system fundamentals to advanced optimization strategies. Choose your WEKA version from the top menu for version-specific documentation.

Note: **Important:** This documentation applies to the WEKA system's **latest minor version** (5.0.**X**). For information on new features and supported prerequisites released with each minor version, refer to the relevant release notes available at get.weka.io.
Check the release notes for details about any updates or changes accompanying the latest releases.

## **Get answers from WEKA documentation with Sevii AI**

Sevii AI quickly delivers answers from WEKA documentation. Type your question and click .\
For the best results, ask clear, context-rich questions.

{% @sevii-ai/sevii-gitbook %}

Note: Sevii, our AI-powered assistant, is built into the documentation to support your search experience. However, it might provide inaccurate information. Always use the **Ask or search** options at the top right to find and verify answers directly from the source.

## About WEKA documentation

This portal encompasses all documentation essential for comprehending and operating the WEKA system. It covers a range of topics:

**WEKA system overview:** Delve into the fundamental components, principles, and entities constituting the WEKA system.

**Planning and installation:** Discover prerequisites, compatibility details, and installation procedures for WEKA clusters on bare metal, AWS, GCP, and Azure environments.

**Getting started with WEKA:** Initiate your WEKA journey by learning the basics of managing a WEKA filesystem through the GUI and CLI, executing initial IOs, and exploring the WEKA REST API.

**Performance:** Explore the results of FIO performance tests on the WEKA filesystem, ensuring optimal system performance.

**WEKA filesystems & object stores:** Understand the role and management of filesystems, object stores, filesystem groups, and key-management systems within WEKA configurations.

**Additional protocols:** Learn about the supported protocols‚ÄîNFS, SMB, and S3‚Äîfor accessing data stored in a WEKA filesystem.

**Operation guide:** Navigate through various WEKA system operations, including events, statistics, user management, upgrades, expansion, and more.

**Licensing:** Gain insights into WEKA system licensing options.

**Monitor the WEKA cluster:** Deploy the WEKA Management Server (WMS) alongside tools like Local WEKA Home, WEKAmon, and SnapTool to effectively monitor your WEKA cluster.

**Kubernetes**: The Kubernetes guides cover deploying and managing the WEKA Data Platform. Learn how to use the WEKA Operator for high-performance storage deployment and handle day-2 operations including scaling, hardware management, and performance optimization.

**WEKApod:** Explore the WEKApod Data Platform Appliance Guide for step-by-step instructions on setting up and configuring the WEKApod‚Ñ¢. This turnkey solution, designed for NVIDIA DGX SuperPOD, features pre-configured storage and software for quick deployment and faster value.

**AWS solutions**: Learn how to integrate the WEKA Data Platform with Amazon SageMaker HyperPod to enable high-performance distributed training of large language and foundation models. Explore best practices for configuring storage, optimizing performance, and scaling machine learning workloads in AWS environments.

**Azure solutions**: Learn how to integrate the WEKA Data Platform with Azure CycleCloud and SLURM scheduler for streamlined HPC cluster management. Learn configuration steps, performance optimization, and architectural patterns for running AI, machine learning, and analytics workloads at scale in Azure environments.

**Best practice guides:** Explore our carefully selected guides, starting with WEKA and Slurm integration, to discover expert-recommended strategies and insights for optimizing your WEKA system and achieving peak performance in various scenarios.

**Support:** Find guidance on obtaining support for the WEKA system and effectively managing diagnostics.

**Appendices:** Explore the Appendices for various topics, including the WEKA CSI Plugin, which connects Kubernetes worker nodes to the WEKA data platform, and other tools and procedures that can enhance your work with WEKA.

Note: For maintenance and troubleshooting articles, search the WEKA Knowledge Base in the WEKA support portal or contact the [Customer Success Team](../support/getting-support-for-your-weka-system#contacting-weka-technical-support-team).

### Conventions

* The documentation marks the CLI mandatory parameters with an asterisk (*).
* New additions are marked with two asterisks (**) in the  and  topics.

### Documentation feedback

We welcome your feedback to improve our documentation. Include the document version and topic title with your suggestions and email them to [documentation@weka.io](mailto:documentation@weka.io). For technical inquiries, contact our [Customer Success Team](support/getting-support-for-your-weka-system). Thank you for helping us maintain high-quality resources.

<!-- ============================================ -->
<!-- File 259/259: readme_documentation-revision-history.md -->
<!-- ============================================ -->

# Documentation revision history

 | WEKA version | Description of changes |
 | --- | --- |
 | 5.0.2 | WEKA Data Platform is now NeuralMeshTM by WEKA. This version introduces the following enhancements.Platform enhancementsSingle-hop writes: Improves write throughput with single-hop writes. This feature enables large write operations to send data blocks directly from the frontend to the drive container, bypassing the compute container.Shared-core persistent clients: Persistent clients can now operate without dedicated cores, allowing for the sharing of compute resources that might otherwise be dedicated to clients.Drive pools: When adding a drive to a new cluster, you can use pools to organize capacity by indirection unit size.Audit and forwarding new topic: This feature provides continuous event streams that record data access, modifications, and deletions, enabling organizations to monitor and respond to activity across their storage environment for enhanced security and compliance. >>>S3 enhancementsS3 open connection tracking: Introduces open connection tracking to measure active S3 clients. This enhancement improves the monitoring of active client sessions.NFS enhancementsIncreased NFS floating IPs: Increases the maximum number of floating IPs for NFS from 50 to 200, allowing greater scalability for customers with a large number of NFS clients.Client enhancementsConfigurable client verbosity: Clients now have configurable verbosity settings for the WEKA Agent, using conventional syslog log levels.Client telemetry relay: Supports relaying telemetry and statistics from clients to WEKA Home through backend servers. |
 | 5.0.1 | WEKA Data Platform 5.0.1 focuses on delivering innovation to WEKA customers and contains the following enhancements:Release highlights:RDMA enablement: RDMA is now enabled by default when using RDMA-capable network interfaces.Container deactivation check: Introduced a new command and API to simulate container deactivation to assess whether the operation can be performed safely without impacting the cluster. CLI: weka cluster container deactivation-check <Container-ids> API: POST‚Äã/containers‚Äã/deactivation-checkNew snapshot APIs: Added two APIs to support efficient paging through changes between points-in-time. See the /Snapshots/diffPrepare and /snapshot/difflist APIs documentation for details. >>>S3 enhancements:GET request optimization: Performance improvements for most GET requests, achieving up to a 70% increase in efficiency.Zero-copy architecture extension: The WEKA zero-copy architecture now applies to S3, reducing both memory usage and system call overhead.New SLB metrics: Added statistics to track traffic distribution across adjacent SLBs: SLB_1xx_RQ, SLB_2xx_RQ, SLB_3xx_RQ, SLB_4xx_RQ, and SLB_5xx_RQ.Additional enhancements:Support for Debian 12 clients running Linux kernel 6.6.Support for the Oracle Cloud Infrastructure (OCI) shape VM.Standard.E5.Flex.Deprecations:The legacy SMB protocol is no longer included with the WEKA Data Platform. |
