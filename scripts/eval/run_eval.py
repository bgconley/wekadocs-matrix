#!/usr/bin/env python3
"""
Phase 7C.9 - Quality Validation Script

Establishes quality baseline for Jina v4 @ 1024-D deployment.
Calculates NDCG, MRR, latency metrics and validates quality gates.

Usage:
    python scripts/eval/run_eval.py
    python scripts/eval/run_eval.py --queries data/test/golden_queries.json
    python scripts/eval/run_eval.py --output reports/phase7c-quality-baseline.json
"""

import argparse
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.mcp_server.query_service import QueryService
from src.shared.config import get_config, get_settings

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class QualityEvaluator:
    """
    Evaluates retrieval quality and establishes baseline metrics.
    Complete implementation with no stubs or TODOs.
    """

    def __init__(self, query_service: Optional[QueryService] = None):
        """
        Initialize evaluator.

        Args:
            query_service: QueryService instance (creates new if None)
        """
        self.query_service = query_service or QueryService()
        self.config = get_config()
        self.settings = get_settings()

        logger.info("Quality Evaluator initialized")
        logger.info(f"Embedding provider: {self.config.embedding.provider}")
        logger.info(f"Embedding model: {self.config.embedding.embedding_model}")
        logger.info(f"Embedding dimensions: {self.config.embedding.dims}")

    def load_golden_queries(self, queries_path: str) -> Dict[str, Any]:
        """
        Load golden query test set.

        Args:
            queries_path: Path to golden_queries.json

        Returns:
            Dict containing queries and metadata

        Raises:
            FileNotFoundError: If queries file doesn't exist
            ValueError: If queries file is invalid
        """
        queries_file = Path(queries_path)

        if not queries_file.exists():
            raise FileNotFoundError(f"Golden queries file not found: {queries_path}")

        with open(queries_file, "r") as f:
            data = json.load(f)

        # Validate structure
        if "queries" not in data:
            raise ValueError("Invalid golden queries file: missing 'queries' field")

        logger.info(f"Loaded {len(data['queries'])} queries from {queries_path}")

        return data

    def run_query(
        self, query_text: str, top_k: int = 10
    ) -> Tuple[List[Dict], float, bool]:
        """
        Execute a single query and measure latency.

        Args:
            query_text: Query string
            top_k: Number of results to retrieve

        Returns:
            Tuple of (results, latency_ms, success)
        """
        start_time = time.time()

        try:
            response = self.query_service.search(
                query=query_text, top_k=top_k, verbosity="graph"
            )

            latency_ms = (time.time() - start_time) * 1000

            # Extract results
            evidence = []
            if hasattr(response, "answer_json") and response.answer_json:
                if hasattr(response.answer_json, "evidence"):
                    evidence = response.answer_json.evidence or []
                elif isinstance(response.answer_json, dict):
                    evidence = response.answer_json.get("evidence", [])
            elif isinstance(response, dict):
                evidence = response.get("evidence", [])

            return evidence, latency_ms, True

        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            logger.error(f"Query failed: {query_text[:50]}... - {e}")
            return [], latency_ms, False

    def calculate_ndcg(
        self, results: List[Dict], relevance_scores: List[float], k: int
    ) -> float:
        """
        Calculate Normalized Discounted Cumulative Gain @ k.

        Args:
            results: Retrieved results
            relevance_scores: Relevance score for each result (0-1)
            k: Cutoff position

        Returns:
            NDCG@k score (0-1)
        """
        if not results or not relevance_scores:
            return 0.0

        # Limit to k results
        results = results[:k]
        relevance_scores = relevance_scores[:k]

        # Calculate DCG
        dcg = 0.0
        for i, rel in enumerate(relevance_scores):
            # Position i+1 (1-indexed for log)
            dcg += rel / np.log2(i + 2)  # i+2 because positions start at 1

        # Calculate IDCG (ideal DCG with perfect ranking)
        ideal_scores = sorted(relevance_scores, reverse=True)
        idcg = 0.0
        for i, rel in enumerate(ideal_scores):
            idcg += rel / np.log2(i + 2)

        # Normalized DCG
        if idcg == 0:
            return 0.0

        return dcg / idcg

    def calculate_mrr(
        self, results: List[Dict], relevance_scores: List[float]
    ) -> float:
        """
        Calculate Mean Reciprocal Rank.

        Args:
            results: Retrieved results
            relevance_scores: Relevance score for each result (0-1)

        Returns:
            MRR score (0-1)
        """
        if not results or not relevance_scores:
            return 0.0

        # Find first relevant result (score > 0.5)
        for i, rel in enumerate(relevance_scores):
            if rel > 0.5:
                return 1.0 / (i + 1)  # Reciprocal rank (1-indexed)

        return 0.0  # No relevant results found

    def estimate_relevance(
        self, result: Dict, expected_topics: List[str], expected_entity_types: List[str]
    ) -> float:
        """
        Estimate relevance score for a result based on expected topics and entities.

        This is a heuristic relevance estimator for baseline evaluation
        when ground truth annotations are not available.

        Args:
            result: Retrieved result dict
            expected_topics: Expected topic keywords
            expected_entity_types: Expected entity types

        Returns:
            Relevance score (0-1)
        """
        score = 0.0

        # Extract result fields
        title = result.get("title", "").lower()
        text = result.get("text", "").lower()
        node_label = result.get("node_label", "")

        # Topic matching (up to 0.6 points)
        topic_matches = 0
        for topic in expected_topics:
            topic_lower = topic.lower()
            if topic_lower in title:
                topic_matches += 2  # Title match worth more
            elif topic_lower in text:
                topic_matches += 1

        # Normalize topic score
        max_topic_score = len(expected_topics) * 2
        topic_score = (
            min(0.6, (topic_matches / max_topic_score) * 0.6)
            if max_topic_score > 0
            else 0.0
        )

        # Entity type matching (up to 0.4 points)
        entity_score = 0.0
        if node_label in expected_entity_types:
            entity_score = 0.4

        score = topic_score + entity_score

        # Cap at 1.0
        return min(1.0, score)

    def evaluate_query(self, query: Dict, top_k: int = 10) -> Dict[str, Any]:
        """
        Evaluate a single query.

        Args:
            query: Query dict from golden set
            top_k: Number of results to retrieve

        Returns:
            Evaluation results dict
        """
        query_id = query["id"]
        query_text = query["text"]
        expected_topics = query.get("expected_topics", [])
        expected_entity_types = query.get("expected_entity_types", [])
        min_relevant = query.get("min_relevant_results", 1)

        logger.info(f"Evaluating query {query_id}: {query_text}")

        # Run query
        results, latency_ms, success = self.run_query(query_text, top_k)

        # Calculate relevance scores
        relevance_scores = []
        for result in results:
            rel_score = self.estimate_relevance(
                result, expected_topics, expected_entity_types
            )
            relevance_scores.append(rel_score)

        # Calculate metrics
        ndcg_3 = self.calculate_ndcg(results, relevance_scores, k=3)
        ndcg_5 = self.calculate_ndcg(results, relevance_scores, k=5)
        ndcg_10 = self.calculate_ndcg(results, relevance_scores, k=10)
        mrr = self.calculate_mrr(results, relevance_scores)

        # Count relevant results (score > 0.5)
        relevant_count = sum(1 for score in relevance_scores if score > 0.5)

        # Check if query met expectations
        meets_expectations = relevant_count >= min_relevant and success

        return {
            "query_id": query_id,
            "query_text": query_text,
            "category": query.get("category", "unknown"),
            "difficulty": query.get("difficulty", "unknown"),
            "success": success,
            "latency_ms": latency_ms,
            "results_count": len(results),
            "relevant_count": relevant_count,
            "meets_expectations": meets_expectations,
            "ndcg@3": ndcg_3,
            "ndcg@5": ndcg_5,
            "ndcg@10": ndcg_10,
            "mrr": mrr,
            "relevance_scores": relevance_scores,
            "results": [
                {
                    "node_id": r.get("node_id"),
                    "title": r.get("title"),
                    "node_label": r.get("node_label"),
                    "score": r.get("score", 0.0),
                }
                for r in results[:5]  # Top 5 for report
            ],
        }

    def evaluate_all(
        self, golden_queries: Dict[str, Any], top_k: int = 10
    ) -> Dict[str, Any]:
        """
        Evaluate all queries and aggregate metrics.

        Args:
            golden_queries: Golden query test set
            top_k: Number of results per query

        Returns:
            Complete evaluation results
        """
        queries = golden_queries["queries"]
        eval_config = golden_queries.get("evaluation_config", {})

        logger.info(f"Starting evaluation of {len(queries)} queries...")

        # Evaluate each query
        query_results = []
        for query in queries:
            result = self.evaluate_query(query, top_k)
            query_results.append(result)

        # Aggregate metrics
        successful_queries = [r for r in query_results if r["success"]]
        failed_queries = [r for r in query_results if not r["success"]]

        if not successful_queries:
            logger.error("No successful queries - cannot calculate aggregate metrics")
            return {
                "error": "No successful queries",
                "total_queries": len(queries),
                "failed_queries": len(failed_queries),
            }

        # Latency percentiles
        latencies = [r["latency_ms"] for r in successful_queries]
        p50_latency = np.percentile(latencies, 50)
        p95_latency = np.percentile(latencies, 95)
        p99_latency = np.percentile(latencies, 99)
        mean_latency = np.mean(latencies)

        # NDCG metrics
        ndcg_3_values = [r["ndcg@3"] for r in successful_queries]
        ndcg_5_values = [r["ndcg@5"] for r in successful_queries]
        ndcg_10_values = [r["ndcg@10"] for r in successful_queries]

        mean_ndcg_3 = np.mean(ndcg_3_values)
        mean_ndcg_5 = np.mean(ndcg_5_values)
        mean_ndcg_10 = np.mean(ndcg_10_values)

        # MRR
        mrr_values = [r["mrr"] for r in successful_queries]
        mean_mrr = np.mean(mrr_values)

        # Coverage metrics
        avg_results_per_query = np.mean(
            [r["results_count"] for r in successful_queries]
        )
        avg_relevant_per_query = np.mean(
            [r["relevant_count"] for r in successful_queries]
        )

        # Success rate
        success_rate = len(successful_queries) / len(queries)
        failure_rate = len(failed_queries) / len(queries)

        # Quality gates from config
        latency_targets = eval_config.get("latency_targets", {})
        quality_gates = eval_config.get("quality_gates", {})

        # Validate quality gates
        gates_passed = {
            "p95_latency": p95_latency <= latency_targets.get("p95_ms", 900),
            "avg_results": avg_results_per_query
            >= quality_gates.get("min_avg_results_per_query", 1),
            "failure_rate": failure_rate <= quality_gates.get("max_failure_rate", 0.05),
        }

        all_gates_passed = all(gates_passed.values())

        # Category breakdown
        category_stats = {}
        for category in set(r["category"] for r in query_results):
            cat_results = [r for r in query_results if r["category"] == category]
            cat_successful = [r for r in cat_results if r["success"]]

            if cat_successful:
                category_stats[category] = {
                    "total_queries": len(cat_results),
                    "successful": len(cat_successful),
                    "mean_ndcg@10": np.mean([r["ndcg@10"] for r in cat_successful]),
                    "mean_mrr": np.mean([r["mrr"] for r in cat_successful]),
                    "mean_latency_ms": np.mean(
                        [r["latency_ms"] for r in cat_successful]
                    ),
                }

        return {
            "summary": {
                "total_queries": len(queries),
                "successful_queries": len(successful_queries),
                "failed_queries": len(failed_queries),
                "success_rate": success_rate,
                "failure_rate": failure_rate,
            },
            "latency": {
                "p50_ms": p50_latency,
                "p95_ms": p95_latency,
                "p99_ms": p99_latency,
                "mean_ms": mean_latency,
            },
            "quality": {
                "mean_ndcg@3": mean_ndcg_3,
                "mean_ndcg@5": mean_ndcg_5,
                "mean_ndcg@10": mean_ndcg_10,
                "mean_mrr": mean_mrr,
                "avg_results_per_query": avg_results_per_query,
                "avg_relevant_per_query": avg_relevant_per_query,
            },
            "quality_gates": {
                "gates": gates_passed,
                "all_passed": all_gates_passed,
                "targets": {
                    "p95_latency_ms": latency_targets.get("p95_ms", 900),
                    "min_avg_results": quality_gates.get(
                        "min_avg_results_per_query", 1
                    ),
                    "max_failure_rate": quality_gates.get("max_failure_rate", 0.05),
                },
            },
            "category_breakdown": category_stats,
            "query_results": query_results,
        }

    def generate_report(
        self, evaluation_results: Dict[str, Any], output_path: Optional[str] = None
    ) -> str:
        """
        Generate quality baseline report.

        Args:
            evaluation_results: Results from evaluate_all()
            output_path: Optional path to save report JSON

        Returns:
            Report as JSON string
        """
        # Build complete report
        report = {
            "meta": {
                "timestamp": datetime.utcnow().isoformat(),
                "phase": "7C",
                "task": "7C.9",
                "description": "Quality baseline for Jina v4 @ 1024-D deployment",
            },
            "system_config": {
                "provider": self.config.embedding.provider,
                "model": self.config.embedding.embedding_model,
                "dimensions": self.config.embedding.dims,
                "version": self.config.embedding.version,
                "similarity": self.config.embedding.similarity,
            },
            "evaluation": evaluation_results,
        }

        # Convert numpy types to Python native types for JSON serialization
        report = self._convert_numpy_types(report)

        # Save to file if requested
        if output_path:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)

            with open(output_file, "w") as f:
                json.dump(report, f, indent=2)

            logger.info(f"Report saved to: {output_path}")

        return json.dumps(report, indent=2)

    def _convert_numpy_types(self, obj: Any) -> Any:
        """
        Recursively convert numpy types to Python native types for JSON serialization.

        Args:
            obj: Object to convert (can be dict, list, numpy type, or primitive)

        Returns:
            Converted object with Python native types
        """
        if isinstance(obj, dict):
            return {key: self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj


def print_report_summary(evaluation_results: Dict[str, Any]) -> None:
    """
    Print human-readable report summary to console.

    Args:
        evaluation_results: Results from evaluate_all()
    """
    print("\n" + "=" * 70)
    print("PHASE 7C.9 - QUALITY BASELINE EVALUATION RESULTS")
    print("=" * 70)

    summary = evaluation_results.get("summary", {})
    latency = evaluation_results.get("latency", {})
    quality = evaluation_results.get("quality", {})
    gates = evaluation_results.get("quality_gates", {})

    print("\nüìä SUMMARY")
    print(f"  Total Queries: {summary.get('total_queries', 0)}")
    print(f"  Successful: {summary.get('successful_queries', 0)}")
    print(f"  Failed: {summary.get('failed_queries', 0)}")
    print(f"  Success Rate: {summary.get('success_rate', 0):.1%}")

    print("\n‚è±Ô∏è  LATENCY")
    print(f"  P50: {latency.get('p50_ms', 0):.1f} ms")
    print(f"  P95: {latency.get('p95_ms', 0):.1f} ms")
    print(f"  P99: {latency.get('p99_ms', 0):.1f} ms")
    print(f"  Mean: {latency.get('mean_ms', 0):.1f} ms")

    print("\nüìà QUALITY METRICS")
    print(f"  NDCG@3: {quality.get('mean_ndcg@3', 0):.3f}")
    print(f"  NDCG@5: {quality.get('mean_ndcg@5', 0):.3f}")
    print(f"  NDCG@10: {quality.get('mean_ndcg@10', 0):.3f}")
    print(f"  MRR: {quality.get('mean_mrr', 0):.3f}")
    print(f"  Avg Results/Query: {quality.get('avg_results_per_query', 0):.1f}")
    print(f"  Avg Relevant/Query: {quality.get('avg_relevant_per_query', 0):.1f}")

    print("\n‚úÖ QUALITY GATES")
    gate_status = gates.get("gates", {})
    all_passed = gates.get("all_passed", False)

    for gate_name, passed in gate_status.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"  {gate_name}: {status}")

    print(f"\n{'üéâ ALL GATES PASSED' if all_passed else '‚ö†Ô∏è  SOME GATES FAILED'}")

    # Category breakdown
    category_breakdown = evaluation_results.get("category_breakdown", {})
    if category_breakdown:
        print("\nüìÇ CATEGORY BREAKDOWN")
        for category, stats in category_breakdown.items():
            print(f"  {category}:")
            print(f"    Queries: {stats['successful']}/{stats['total_queries']}")
            print(f"    NDCG@10: {stats['mean_ndcg@10']:.3f}")
            print(f"    MRR: {stats['mean_mrr']:.3f}")

    print("\n" + "=" * 70)


def main():
    """Main entry point for evaluation script."""
    parser = argparse.ArgumentParser(
        description="Phase 7C.9 Quality Validation - Baseline Evaluation"
    )
    parser.add_argument(
        "--queries",
        type=str,
        default="data/test/golden_queries.json",
        help="Path to golden queries JSON file",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="reports/phase7c-quality-baseline.json",
        help="Path to save evaluation report",
    )
    parser.add_argument(
        "--top-k", type=int, default=10, help="Number of results to retrieve per query"
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # Initialize evaluator
        logger.info("Initializing Quality Evaluator...")
        evaluator = QualityEvaluator()

        # Load golden queries
        logger.info(f"Loading golden queries from: {args.queries}")
        golden_queries = evaluator.load_golden_queries(args.queries)

        # Run evaluation
        logger.info("Running evaluation...")
        results = evaluator.evaluate_all(golden_queries, top_k=args.top_k)

        # Generate report
        logger.info("Generating report...")
        evaluator.generate_report(results, output_path=args.output)

        # Print summary
        print_report_summary(results)

        # Check if all gates passed
        all_gates_passed = results.get("quality_gates", {}).get("all_passed", False)

        if all_gates_passed:
            logger.info("‚úÖ Quality validation PASSED - All gates met")
            sys.exit(0)
        else:
            logger.warning(
                "‚ö†Ô∏è  Quality validation COMPLETED with warnings - Some gates failed"
            )
            sys.exit(0)  # Still exit 0 for baseline establishment

    except FileNotFoundError as e:
        logger.error(f"File not found: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Evaluation failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
