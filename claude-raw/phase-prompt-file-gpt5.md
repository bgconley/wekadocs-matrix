Below are **five copyâ€‘pasteâ€‘ready prompts**â€”one for each phase. Give each prompt to your agentic coder at the start of that phase. They are **selfâ€‘contained**, reference only the **v2** materials, and enforce the **phase gates**, **noâ€‘mocks test policy**, and **artifact outputs** you requested. The task numbers (1.1â†’5.4) match your v2 spec/plan/pseudocode and the earlier expert guidance.

---

## ğŸ”¶ Phase 1 Prompt â€” Core Infrastructure (Tasks **1.1 â†’ 1.4**)

**You are the agentic coder for Phase 1.** Your mission is to stand up a productionâ€‘shaped local stack and a minimal MCP server skeleton with security and schema creation, then **prove it works** using **noâ€‘mocks** tests and machineâ€‘readable reports.

### Context & constraints (canonical v2)

* Stack: **FastAPI MCP server**, **Neo4j 5.x**, **Qdrant (optional primary vector store)**, **Redis**, **ingestion worker**; containerized via **Docker Compose** on a single private bridge network.
* Security **now**, not later: **JWT auth**, **Redis token bucket rateâ€‘limits**, **audit log** with correlation IDs.
* Schema: **Document/Section** nodes, domain nodes (Command/Configuration/Procedure/Error/Concept/Example/Step/Parameter/Component), **vector indexes created from config** (no hardâ€‘coded dims), and a `schema_version` singleton.
* **No mocks**. All tests must hit the real services started by Compose.

### Deliverables

1. **Compose & Dockerfiles**
   `docker-compose.yml`, `.env.example`, `docker/*` with healthchecks, CPU/mem limits, volumes, secrets (no plaintext credentials).
2. **MCP server skeleton**
   `src/mcp_server/main.py` with endpoints `/mcp`, `/health`, `/ready`, `/metrics`; tool registry scaffold; connection pools; graceful shutdown; **OpenTelemetry** tracing & structured logs.
3. **Security layer**
   JWT verification middleware, Redis token bucket rateâ€‘limiter, audit logger writing request method, client, correlation ID, and result (success/error).
4. **Schema creation**
   `scripts/neo4j/create_schema.cypher` and `src/shared/schema.py` that create **Document/Section** (with constraints), domain nodes (constraints/indexes), **configâ€‘driven vector indexes**, and write a `schema_version` node.
5. **Config**
   `config/development.yaml` with `embedding.model_name`, `embedding.dims`, `embedding.similarity`, `search.vector.primary âˆˆ {qdrant, neo4j}`.

### Tests (NO MOCKS) & artifacts

* Put tests under `tests/` with names prefixed `p1_*.py`.
* Start the stack in tests (compose or preâ€‘started), then assert:

  * **Health**: `/health`, `/ready` return OK; Redis `PING`; Qdrant `/health` (if enabled); **Neo4j Bolt** connect.
  * **Security**: invalid JWT â†’ 401; burst requests â†’ 429; audit entries logged.
  * **Schema**: run schema script twice (idempotent), verify constraints/indexes exist; vector index dims match config.
* Emit:

  * **JUnit XML** â†’ `reports/phase-1/junit.xml`
  * **Summary JSON** â†’ `reports/phase-1/summary.json` (include pass/fail per test and key metrics)
* Provide a tiny helper (`scripts/test/run_phase.sh`) to run only Phase 1 tests and write artifacts.

### Exit gate (must pass before Phase 2)

* All Phaseâ€‘1 tests **green**, artifacts present.
* Security enforced (401/429) and audit lines observed.
* Schema idempotent; vector indexes use config dims; OTel traces visible.

### Work plan (do in this order)

1. Compose & healthchecks â†’ 2. MCP skeleton + OTel â†’ 3. JWT + rateâ€‘limit + audit â†’ 4. Schema scripts (configâ€‘driven vectors) â†’ 5. Tests & reports.

---

## ğŸ”· Phase 2 Prompt â€” Query Processing Engine (Tasks **2.1 â†’ 2.4**)

**You are the agentic coder for Phase 2.** Your mission is to turn NL queries into **safe, parameterized Cypher**, implement **planâ€‘gated validation**, build **hybrid retrieval**, and produce **structured responses** with evidence and confidenceâ€”then **prove** safety and performance with **noâ€‘mocks** tests and artifacts.

### Context & constraints (canonical v2)

* **Templatesâ€‘first** NLâ†’Cypher; LLM proposal allowed **only as proposer**, never executed raw.
* **Validator** combines static guards with **Neo4j `EXPLAIN` plan checks**; blocks excessive scans/expansions/depth; enforces **timeouts**/**LIMITs** early.
* **Hybrid search**: vector topâ€‘K (Sections + optional Entities) â†’ bounded 1â€“2 hop graph expansion â†’ optional connecting paths â†’ rank and dedupe.
* Responses are dualâ€‘form: **Markdown** + **JSON** `{answer, evidence[{section_id, path}], confidence, diagnostics{ranking_features}}`.

### Deliverables

1. **NLâ†’Cypher Planner** (`src/query/planner.py`, `src/query/templates/*.cypher`)

   * Intent classifier; entity linker; parameterization; inject `LIMIT` and depth caps early.
2. **Cypher Validator** (`src/mcp_server/validation.py`)

   * Regex guards, **correct `*min..max` pattern**, parameter enforcement, **`EXPLAIN` plan gate**, timeout application.
3. **Hybrid Search** (`src/query/hybrid_search.py`, `src/query/ranking.py`)

   * Respect `search.vector.primary`; seed from vectors; expand H=1..2 typed edges; ranking blends semantic score, path proximity, recency; deterministic tieâ€‘break.
4. **Response Builder** (`src/query/response_builder.py`)

   * Render Markdown; emit JSON with evidence & confidence; â€œ**Why these results?**â€ diagnostics.

### Tests (NO MOCKS) & artifacts

* Name tests `p2_*.py`. **Seed a real miniâ€‘graph** for tests via a deterministic script (no mocks):
  `scripts/dev/seed_minimal_graph.py` â†’ Creates ~10 Sections + a handful of Commands/Procedures/Errors, sets embeddings (use local model), and (if Qdrant is primary) inserts vectors with payload.
* Required test groups:

  * **Validator negative**: injection attempts, deep traversals, Cartesian products â†’ **blocked with helpful errors**.
  * **Templates happy path**: common intents map to parameterized templates; queries pass validator and return results.
  * **Hybrid performance**: with the seeded graph, warmed caches â†’ P95 < **500ms** at K=20 (measure and record).
  * **Response schema**: JSON contains evidence Section IDs, optional paths, confidence âˆˆ [0,1].
* Emit:

  * `reports/phase-2/junit.xml`
  * `reports/phase-2/summary.json` (include latency percentiles, block/allow counts, schema validations)

### Exit gate (must pass before Phase 3)

* Validator blocks attacks; **false positives < 5%** on provided positive set.
* Hybrid search P95 < **500ms** (warmed).
* Responses carry evidence & confidence; artifacts present.

### Work plan

1. Templates + planner â†’ 2. Validator (regex + `EXPLAIN`) â†’ 3. Hybrid search + ranking â†’ 4. Response builder â†’ 5. Seed script + tests + reports.

---

## ğŸŸ© Phase 3 Prompt â€” Ingestion Pipeline (Tasks **3.1 â†’ 3.4**)

**You are the agentic coder for Phase 3.** Your mission is to implement a **deterministic, idempotent ingestion pipeline** from Markdown/HTML/Notion into **Document â†’ Section â†’ Entities** with provenance and vectors, plus **incremental updates** and **reconciliation**â€”then prove determinism and drift control with **noâ€‘mocks** tests.

### Context & constraints (canonical v2)

* **Document** and **Section** are firstâ€‘class; `Section.id := hash(source_uri + anchor + normalized_text)`.
* **MENTIONS** edges from Section to Entities capture `confidence`, `start`, `end`, and `source_section_id`.
* **Primary vector store**: configurable (Qdrant **or** Neo4j vectors). **Dualâ€‘write** possible but behind a flag.
* Inference edges (REQUIRES/AFFECTS/RESOLVES/etc.) can happen later; first pass focuses on MENTIONS + provenance.

### Deliverables

1. **Parsers** (`src/ingestion/parsers/{markdown,html,notion}.py`)

   * Preserve headings/anchors, code fences, tables; compute `section_checksum`, token counts; deterministic `section_id`.
2. **Entity extraction** (`src/ingestion/extract/*`)

   * Rules + light NLP for Commands/Configurations/Procedures/Errors/Concepts/Examples/Steps/Parameters; create **MENTIONS** with spans/confidence.
3. **Graph builder** (`src/ingestion/build_graph.py`)

   * MERGE by deterministic IDs; provenance on edges; batch via `apoc.periodic.iterate`; timeouts; embeddings; vector upsert; set `embedding_version`.
4. **Incremental & reconciliation** (`src/ingestion/{incremental.py,reconcile.py}`)

   * Diff on `section_checksum`; stage with `:Section_Staged` then atomic relabel swap; reâ€‘embed changed & adjacent; nightly reconciliation (graph vs vectors) with repair.

### Tests (NO MOCKS) & artifacts

* Name tests `p3_*.py`. Use **real sample docs** in `data/samples/` (at least 3 Markdown + 1 HTML). Notion tests may target a sandbox workspace or be skipped behind a flag if credentials unavailable.
* Required test groups:

  * **Determinism**: reâ€‘ingest same docs twice â†’ **no diffs** in node/edge counts and ID sets.
  * **Provenance**: MENTIONS edges have source Section IDs and spans.
  * **Vectors parity**: count(Sections with embedding_version) == count(vectors in primary store).
  * **Incremental**: edit one Section; only that Section (and immediately dependent items) update; reâ€‘embed minimal set.
  * **Reconciliation**: simulate vector deletion for N sections; nightly job repairs to zero drift.
* Emit:

  * `reports/phase-3/junit.xml`
  * `reports/phase-3/summary.json` (include determinism hashes, drift %, changed counts for incremental)

### Exit gate (must pass before Phase 4)

* Deterministic ingestion (repeat runs stable).
* Incremental updates limited to changed sections.
* Vector/graph **drift < 0.5%**; artifacts present.

### Work plan

1. Parsers â†’ 2. Extractors â†’ 3. Graph builder + embeddings â†’ 4. Incremental + reconciliation â†’ 5. Tests + reports.

---

## ğŸŸ¦ Phase 4 Prompt â€” Advanced Query Features (Tasks **4.1 â†’ 4.4**)

**You are the agentic coder for Phase 4.** Your mission is to deliver **complex query templates**, **optimizer**, **caching**, and a simple **learning loop**â€”and prove performance uplift and cache correctness with **noâ€‘mocks** tests.

### Context & constraints (canonical v2)

* Complex templates: **dependency chains**, **impact analysis**, **troubleshooting paths**, **temporal â€œas of version Yâ€**, **comparisons**; **preâ€‘approved** and **planâ€‘gated**.
* Optimizer: analyze slow queries; recommend/create indexes; support **compiled plan cache** for hot patterns.
* Caching: L1 inâ€‘proc + L2 Redis, **keys prefixed with `{schema_version}:{embedding_version}`**; optional materialization for expensive patterns.
* Learning: log queryâ†’resultâ†’feedback, adjust ranking weights; propose new templates/indexes.

### Deliverables

1. **Advanced templates** (`src/query/templates/advanced/*.cypher`) with input/output schemas and plan guardrails.
2. **Optimizer** (`src/ops/optimizer.py`) to analyze plans, suggest indexes/hints, and cache compiled plans.
3. **Caching** (`src/shared/cache.py`, `src/ops/warmers/`) with versionâ€‘prefixed keys; daily warmers/materializers.
4. **Learning loop** (`src/learning/*`) to collect feedback and tune ranking weights; emit suggestions.

### Tests (NO MOCKS) & artifacts

* Name tests `p4_*.py`. Populate graph with Phaseâ€‘3 data.
* Required test groups:

  * **Templates**: each advanced template runs within configured depth/time budgets; outputs match schema.
  * **Optimization uplift**: run before/after perf suite; demonstrate statistically significant P95 improvement on hot queries; export CSV of timings.
  * **Cache correctness**: rotate `embedding_version` and `schema_version`; assert caches invalidate; measure steadyâ€‘state hit rate > **80%** under load.
  * **Learning**: offline evaluation updates ranking weights and improves heldâ€‘out NDCG.
* Emit:

  * `reports/phase-4/junit.xml`
  * `reports/phase-4/summary.json` (include perf deltas, cache hit rate, NDCG lift)
  * Attach `perf_before.csv` / `perf_after.csv` if produced.

### Exit gate (must pass before Phase 5)

* Advanced templates pass guardrails and tests.
* Cache hit rate > **80%** steadyâ€‘state; **before/after** perf shows uplift.
* Learning loop demonstrates offline improvement; artifacts present.

### Work plan

1. Advanced templates â†’ 2. Optimizer & plan cache â†’ 3. Caching & warmers â†’ 4. Learning loop â†’ 5. Perf/cache tests + reports.

---

## ğŸŸ¥ Phase 5 Prompt â€” Integration & Deployment (Tasks **5.1 â†’ 5.4**)

**You are the agentic coder for Phase 5.** Your mission is to ship productionâ€‘grade **connectors**, **observability**, **test matrix**, and **deployment**â€”with disasterâ€‘ready runbooks and verifiable artifacts.

### Context & constraints (canonical v2)

* Connectors: Notion/GitHub/Confluence (at least one endâ€‘toâ€‘end), queue ingestion, **circuit breakers/backoff** under rate limits.
* Monitoring: Prometheus metrics, Grafana dashboards, **OpenTelemetry** traces, alerts (P99, error rate, drift, OOM).
* Tests: complete **noâ€‘mocks** matrixâ€”unit/integration/E2E/perf/security/chaosâ€”executed against the Compose stack.
* Deployment: blue/green + canary; backups; quarterly DR drills (**RTO 1h / RPO 15m**).

### Deliverables

1. **Connectors** (`src/connectors/*`) and runbooks; Slack notifications optional.
2. **Monitoring** (`deploy/monitoring/*`) with dashboards & alert rules; trace exemplars for slow queries.
3. **Testing framework** (`tests/*`, CI workflow, chaos scenarios) running the **entire** suite **without mocks**.
4. **Deployment** (`deploy/k8s/*` or Helm, `.github/workflows/ci.yml`, backup/restore scripts, feature flags, canary controls) + DR runbook.

### Tests (NO MOCKS) & artifacts

* Name tests `p5_*.py`. Required groups:

  * **Connector E2E**: ingest deltas from a real sandbox (or local webhook generator), queue processing, backoff on throttling.
  * **Monitoring drills**: synthetic alerts fire; dashboards render key panels; traces include query exemplars.
  * **Chaos**: kill vector service â†’ degraded (graphâ€‘only) operation; simulate Neo4j backpressure â†’ ingestion backs off.
  * **Canary**: script deploys new build to 5%, monitors SLIs for 60 minutes (shortened in CI), rolls forward/backs off.
  * **Backup/restore**: snapshot, restore into clean environment, point app, verify parity.
* Emit:

  * `reports/phase-5/junit.xml`
  * `reports/phase-5/summary.json` (include alert outcomes, chaos results, DR timings)
  * Attach screenshots/links to dashboards (paths or exported PNG/JSON).

### Exit gate (Launch)

* Full **test matrix green**; monitoring & alerts live; canary + rollback proven; **DR drill passes** within targets; artifacts present.

### Work plan

1. One connector endâ€‘toâ€‘end â†’ 2. Monitoring + alerts + traces â†’ 3. Full test matrix + chaos â†’ 4. Deployment & DR runbooks â†’ 5. Canary rehearsal + reports.

---

### Handâ€‘off tips for the coder (applies to all phases)

* **Do not** reference any earlier documentsâ€”v2 files are canonical.
* **Never** use mocks. If a dependency is needed, stand up the real service in Compose.
* Every phase must produce: `reports/phase-*/junit.xml` and `reports/phase-*/summary.json`.
* Do not start Phase N+1 until Phase N **exit gate** is met and artifacts are committed.

If you want, I can also generate **phaseâ€‘specific kickstart checklists** as issue templates (one per phase) or deliver these prompts as readyâ€‘toâ€‘paste GitHub issues to assign to the agentic coder.
