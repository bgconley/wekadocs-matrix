"""
Phase 6, Task 6.1: File System Watcher

Implements:
- Debounced file system watching
- SHA-256 checksum for duplicate prevention
- Spool pattern (copy to immutable location)
- Job enqueue to Redis stream

See: /docs/implementation-plan-phase-6.md â†’ Task 6.1
"""

import time
import os
import json
import uuid
import hashlib
import shutil
import structlog
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import redis

logger = structlog.get_logger()

# Configuration from environment
DEBOUNCE_MS = int(os.getenv("DEBOUNCE_MS", "350"))
REDIS_URI = os.getenv("REDIS_URI", "redis://localhost:6379")
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", "")
INBOX = os.getenv("INGEST_INBOX", "/app/documents/inbox")
SPOOL = os.getenv("INGEST_SPOOL", "/app/documents/spool")

# Redis connection
if REDIS_PASSWORD:
    r = redis.Redis.from_url(REDIS_URI, password=REDIS_PASSWORD, decode_responses=True)
else:
    r = redis.Redis.from_url(REDIS_URI, decode_responses=True)


def sha256(path):
    """Compute SHA-256 checksum of file."""
    h = hashlib.sha256()
    with open(path, 'rb') as f:
        for chunk in iter(lambda: f.read(1 << 20), b''):
            h.update(chunk)
    return h.hexdigest()


def enqueue_job(checksum, ext, tags=None):
    """
    Enqueue job to Redis stream ingest:jobs.

    Job schema:
    {
      "job_id": "<uuid>",
      "source_uri": "file://<spool>/<checksum>.<ext>",
      "checksum": "<sha256>",
      "tags": ["manual", "md"],
      "created_at": "2025-10-16T20:00:00Z",
      "resume_from": null
    }
    """
    job = {
        "job_id": str(uuid.uuid4()),
        "source_uri": f"file://{SPOOL}/{checksum}.{ext}",
        "checksum": checksum,
        "tags": tags or ["md"],
        "created_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "resume_from": None
    }
    r.lpush("ingest:jobs", json.dumps(job))
    logger.info("Job enqueued", job_id=job["job_id"], checksum=checksum)
    return job


class DebouncedHandler(FileSystemEventHandler):
    """
    Debounced file system event handler.

    - Tracks file modifications
    - Only processes files after DEBOUNCE_MS of no activity
    - Computes checksum and checks for duplicates
    - Copies to spool and enqueues job
    """

    def __init__(self):
        self._pending = {}
        logger.info("Debounced handler initialized", debounce_ms=DEBOUNCE_MS)

    def on_modified(self, event):
        self._touch(event)

    def on_created(self, event):
        self._touch(event)

    def _touch(self, event):
        """Record file activity timestamp."""
        if event.is_directory:
            return
        path = event.src_path
        self._pending[path] = time.time()
        logger.debug("File activity detected", path=path)

    def sweep(self):
        """
        Process debounced files.

        For each file that hasn't been modified in DEBOUNCE_MS:
        1. Compute SHA-256 checksum
        2. Check Redis for duplicate (ingest:seen:{checksum})
        3. If new, copy to spool and enqueue job
        """
        now = time.time()
        for path, t0 in list(self._pending.items()):
            elapsed_ms = (now - t0) * 1000

            if elapsed_ms >= DEBOUNCE_MS and os.path.exists(path):
                del self._pending[path]

                try:
                    # Compute checksum
                    checksum = sha256(path)
                    logger.debug("File checksum computed", path=path, checksum=checksum[:8])

                    # Check for duplicate (SETNX returns 1 if key was set, 0 if already exists)
                    is_new = r.setnx(f"ingest:seen:{checksum}", 1)

                    if is_new:
                        # Set TTL on dedup key (7 days)
                        r.expire(f"ingest:seen:{checksum}", 7 * 24 * 3600)

                        # Determine file extension
                        ext = os.path.splitext(path)[1].lstrip(".") or "bin"

                        # Copy to spool (immutable location)
                        os.makedirs(SPOOL, exist_ok=True)
                        dest = f"{SPOOL}/{checksum}.{ext}"
                        shutil.copy2(path, dest)
                        logger.info("File spooled", path=path, dest=dest, checksum=checksum[:8])

                        # Enqueue job
                        enqueue_job(checksum, ext, tags=[ext])
                    else:
                        logger.info("Duplicate file ignored", path=path, checksum=checksum[:8])

                except Exception as exc:
                    logger.error("Error processing file", path=path, error=str(exc))


def run_watcher():
    """
    Run the file system watcher.

    - Creates inbox directory if needed
    - Starts watchdog observer
    - Periodically sweeps for debounced files
    """
    os.makedirs(INBOX, exist_ok=True)
    os.makedirs(SPOOL, exist_ok=True)

    handler = DebouncedHandler()
    observer = Observer()
    observer.schedule(handler, INBOX, recursive=False)
    observer.start()

    logger.info("Watcher started", inbox=INBOX, spool=SPOOL, debounce_ms=DEBOUNCE_MS)

    try:
        while True:
            time.sleep(0.2)
            handler.sweep()
    except KeyboardInterrupt:
        logger.info("Watcher stopping...")
    finally:
        observer.stop()
        observer.join()
        logger.info("Watcher stopped")


if __name__ == "__main__":
    run_watcher()
